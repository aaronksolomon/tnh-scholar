{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport yt_dlp\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "%aimport whisper\n",
    "%aimport logging\n",
    "from typing import List, Dict\n",
    "%aimport json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"audio_extraction_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.gpt_processing import token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_storage_dir = Path(\"test_extracted_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio_yt(url: str, output_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Downloads audio from a YouTube video using yt_dlp.YoutubeDL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the YouTube video.\n",
    "        output_dir (Path): Directory to save the downloaded audio file.\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the downloaded audio file.\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': str(output_dir / '%(title)s.%(ext)s'),  # Save in the output directory\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "        downloaded_file = Path(ydl.prepare_filename(result)).with_suffix('.mp3')\n",
    "        return downloaded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_into_chunks(audio_file: Path, chunk_duration: int, output_dir: Path = None) -> Path:\n",
    "    \"\"\"\n",
    "    Splits an audio file into chunks of a specified time duration.\n",
    "\n",
    "    Args:\n",
    "        audio_file (Path): Path to the input audio file.\n",
    "        chunk_duration (int): Duration of each chunk in milliseconds (e.g., 10 * 60 * 1000 for 10 minutes).\n",
    "        output_dir (Path): Path to the directory where chunks will be saved. \n",
    "                           If None, a subdirectory is created in the same directory as the input file.\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the directory containing the chunks.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    total_duration = len(audio)  # Total duration in milliseconds\n",
    "\n",
    "    # Create output directory based on filename\n",
    "    if output_dir is None:\n",
    "        output_dir = audio_file.parent / f\"{audio_file.stem}_chunks\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Split the audio into chunks\n",
    "    for i, start in enumerate(range(0, total_duration, chunk_duration)):\n",
    "        chunk = audio[start:start + chunk_duration]\n",
    "        chunk_path = output_dir / f\"chunk_{i + 1}.mp3\"\n",
    "        chunk.export(chunk_path, format=\"mp3\")\n",
    "        print(f\"Exported: {chunk_path}\")\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_boundaries(audio_file: Path, model_size: str = 'tiny') -> List[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Use Whisper to detect sentence boundaries in the audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_file (Path): Path to the audio file.\n",
    "        model_size (str): Whisper model size (e.g., 'tiny', 'base', 'small').\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, float]]: List of timestamps with sentence boundaries. Each entry contains:\n",
    "            - \"start\": Start time of the sentence (in seconds).\n",
    "            - \"end\": End time of the sentence (in seconds).\n",
    "            - \"text\": Sentence text.\n",
    "    \"\"\"\n",
    "    # Load the Whisper model\n",
    "    logger.info(\"Loading model...\")\n",
    "    model = whisper.load_model(model_size)\n",
    "    logger.info(f\"Model '{model_size}' loaded.\")\n",
    "\n",
    "    # Transcribe the audio file\n",
    "    result = model.transcribe(str(audio_file), task=\"transcribe\", word_timestamps=True)\n",
    "    \n",
    "    # Extract sentence boundaries from segments\n",
    "    sentence_boundaries = []\n",
    "    for segment in result['segments']:\n",
    "        sentence_boundaries.append({\n",
    "            \"start\": segment['start'],\n",
    "            \"end\": segment['end'],\n",
    "            \"text\": segment['text']\n",
    "        })\n",
    "    \n",
    "    return sentence_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_at_boundaries(audio_file: Path, boundaries: List[Dict[str, float]], output_dir: Path  = None, max_duration: int = 10 * 60) -> Path:\n",
    "    \"\"\"\n",
    "    Split the audio file into chunks close to a specified duration while respecting boundaries.\n",
    "\n",
    "    Args:\n",
    "        audio_file (Path): Path to the audio file.\n",
    "        boundaries (List[Dict[str, float]]): List of boundaries with start and end times.\n",
    "        output_dir (Path): Directory to save the chunks.\n",
    "        max_duration (int): Maximum duration for each chunk in seconds (default is 10 minutes).\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the directory containing the chunks.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    \n",
    "    # Create output directory based on filename\n",
    "    if output_dir is None:\n",
    "        output_dir = audio_file.parent / f\"{audio_file.stem}_chunks\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True) \n",
    "       \n",
    "    # Initialize variables\n",
    "    current_chunk = AudioSegment.empty()\n",
    "    current_start = boundaries[0][\"start\"]\n",
    "    chunk_count = 1\n",
    "\n",
    "    for i, boundary in enumerate(boundaries):\n",
    "        # Calculate segment duration\n",
    "        segment_start_ms = boundary[\"start\"] * 1000\n",
    "        segment_end_ms = boundary[\"end\"] * 1000\n",
    "        segment = audio[segment_start_ms:segment_end_ms]\n",
    "        \n",
    "        # Add segment to the current chunk if it fits\n",
    "        if len(current_chunk) + len(segment) <= max_duration * 1000:\n",
    "            current_chunk += segment\n",
    "        else:\n",
    "            # Export the current chunk\n",
    "            chunk_path = output_dir / f\"chunk_{chunk_count}.mp3\"\n",
    "            current_chunk.export(chunk_path, format=\"mp3\")\n",
    "            logger.info(f\"Exported: {chunk_path}\")\n",
    "            \n",
    "            # Start a new chunk\n",
    "            chunk_count += 1\n",
    "            current_chunk = segment\n",
    "            current_start = boundary[\"start\"]\n",
    "    \n",
    "    # Export the final chunk\n",
    "    if len(current_chunk) > 0:\n",
    "        chunk_path = output_dir / f\"chunk_{chunk_count}.mp3\"\n",
    "        current_chunk.export(chunk_path, format=\"mp3\")\n",
    "        print(f\"Exported: {chunk_path}\")\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_chunks(\n",
    "    directory: Path, \n",
    "    output_file: Path, \n",
    "    jsonl_file: Path, \n",
    "    model: str = \"whisper-1\", \n",
    "    prompt: str = \"\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Processes all audio chunks in the specified directory using OpenAI's transcription API,\n",
    "    saves the transcription objects into a JSONL file, and stitches the transcriptions\n",
    "    into a single text file.\n",
    "\n",
    "    Args:\n",
    "        directory (Path): Path to the directory containing audio chunks.\n",
    "        output_file (Path): Path to the output file to save the stitched transcription.\n",
    "        jsonl_file (Path): Path to save the transcription objects as a JSONL file.\n",
    "        model (str): The transcription model to use (default is \"whisper-1\").\n",
    "        prompt (str): Optional prompt to provide context for better transcription.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    jsonl_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Collect all audio chunks in the directory\n",
    "    audio_files = sorted(directory.glob(\"*.mp3\"))  # Sort files by name for sequential processing\n",
    "    logger.info(f\"Audio files found:\\n\\t{audio_files}\")\n",
    "\n",
    "    # Initialize the output content\n",
    "    stitched_transcription = []\n",
    "\n",
    "    # Open the JSONL file for writing\n",
    "    with jsonl_file.open(\"w\", encoding=\"utf-8\") as jsonl_out:\n",
    "        # Process each audio chunk\n",
    "        for audio_file in audio_files:\n",
    "            logger.info(f\"Processing {audio_file.name}...\")\n",
    "            try:\n",
    "                with audio_file.open(\"rb\") as file:\n",
    "                    transcript = client.audio.transcriptions.create(\n",
    "                        model=model,\n",
    "                        prompt=prompt,\n",
    "                        file=file\n",
    "                    )\n",
    "                    # Save the full response object to the JSONL file\n",
    "                    jsonl_out.write(json.dumps(transcript.to_dict()) + \"\\n\")\n",
    "                    \n",
    "                    # Append the transcribed text to the stitched output\n",
    "                    stitched_transcription.append(transcript.text)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {audio_file.name}: {e}\", exc_info=True)\n",
    "                \n",
    "\n",
    "    # Write the stitched transcription to the output file\n",
    "    with output_file.open(\"w\", encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(\" \".join(stitched_transcription))\n",
    "\n",
    "    logger.info(f\"Stitched transcription saved to {output_file}\")\n",
    "    logger.info(f\"Full transcript objects saved to {jsonl_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_audio_yt(\"https://www.youtube.com/watch?v=SEc28BCHgu8&ab_channel=DeerParkMonastery\", audio_storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = audio_storage_dir / \"Taking Care of Our Fear ｜ Br. Phap Luu ｜ 2024-11-06.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 11:09:55,086 - audio_extraction_testing - INFO - Loading model...\n",
      "2024-12-08 11:09:55,524 - audio_extraction_testing - INFO - Model 'tiny' loaded.\n",
      "/opt/anaconda3/envs/tnh-scholar/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load audio: ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n  built with Apple clang version 16.0.0 (clang-1600.0.26.4)\n  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1_3 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n  libavutil      59. 39.100 / 59. 39.100\n  libavcodec     61. 19.100 / 61. 19.100\n  libavformat    61.  7.100 / 61.  7.100\n  libavdevice    61.  3.100 / 61.  3.100\n  libavfilter    10.  4.100 / 10.  4.100\n  libswscale      8.  3.100 /  8.  3.100\n  libswresample   5.  3.100 /  5.  3.100\n  libpostproc    58.  3.100 / 58.  3.100\n[in#0 @ 0x13e004200] Error opening input: No such file or directory\nError opening input file extracted_audio/Taking Care of Our Fear ｜ Br. Phap Luu ｜ 2024-11-06.mp3.\nError opening input files: No such file or directory\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/tnh-scholar/lib/python3.11/site-packages/whisper/audio.py:58\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tnh-scholar/lib/python3.11/subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    572\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', 'extracted_audio/Taking Care of Our Fear ｜ Br. Phap Luu ｜ 2024-11-06.mp3', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 254.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m boundaries \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_boundaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtiny\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36mdetect_boundaries\u001b[0;34m(audio_file, model_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Transcribe the audio file\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Extract sentence boundaries from segments\u001b[39;00m\n\u001b[1;32m     24\u001b[0m sentence_boundaries \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tnh-scholar/lib/python3.11/site-packages/whisper/transcribe.py:139\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    136\u001b[0m     decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43mlog_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m content_frames \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m N_FRAMES\n\u001b[1;32m    141\u001b[0m content_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(content_frames \u001b[38;5;241m*\u001b[39m HOP_LENGTH \u001b[38;5;241m/\u001b[39m SAMPLE_RATE)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tnh-scholar/lib/python3.11/site-packages/whisper/audio.py:140\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(audio):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 140\u001b[0m         audio \u001b[38;5;241m=\u001b[39m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     audio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(audio)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tnh-scholar/lib/python3.11/site-packages/whisper/audio.py:60\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     58\u001b[0m     out \u001b[38;5;241m=\u001b[39m run(cmd, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstdout\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mfrombuffer(out, np\u001b[38;5;241m.\u001b[39mint16)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32768.0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load audio: ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n  built with Apple clang version 16.0.0 (clang-1600.0.26.4)\n  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1_3 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n  libavutil      59. 39.100 / 59. 39.100\n  libavcodec     61. 19.100 / 61. 19.100\n  libavformat    61.  7.100 / 61.  7.100\n  libavdevice    61.  3.100 / 61.  3.100\n  libavfilter    10.  4.100 / 10.  4.100\n  libswscale      8.  3.100 /  8.  3.100\n  libswresample   5.  3.100 /  5.  3.100\n  libpostproc    58.  3.100 / 58.  3.100\n[in#0 @ 0x13e004200] Error opening input: No such file or directory\nError opening input file extracted_audio/Taking Care of Our Fear ｜ Br. Phap Luu ｜ 2024-11-06.mp3.\nError opening input files: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "boundaries = detect_boundaries(audio_file_path, \"tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_dir = audio_storage_dir / \"test_output_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boundaries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m split_audio_at_boundaries(audio_file_path, \u001b[43mboundaries\u001b[49m, test_output_dir, max_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boundaries' is not defined"
     ]
    }
   ],
   "source": [
    "split_audio_at_boundaries(audio_file_path, boundaries, test_output_dir, max_duration=3 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'extracted_audio/Taking Care of Our Fear ｜ Br. Phap Luu ｜ 2024-11-06.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_duration_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# 10 minutes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[43msplit_audio_into_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_duration_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunks saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36msplit_audio_into_chunks\u001b[0;34m(audio_file, chunk_duration, output_dir)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mSplits an audio file into chunks of a specified time duration.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Path: Path to the directory containing the chunks.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load the audio file\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[43mAudioSegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m total_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(audio)  \u001b[38;5;66;03m# Total duration in milliseconds\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create output directory based on filename\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tnh-scholar/lib/python3.11/site-packages/pydub/audio_segment.py:651\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m file, close_file \u001b[38;5;241m=\u001b[39m \u001b[43m_fd_or_path_or_tempfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m:\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tnh-scholar/lib/python3.11/site-packages/pydub/utils.py:65\u001b[0m, in \u001b[0;36m_fd_or_path_or_tempfile\u001b[0;34m(fd, mode, tempfile)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fd, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[0;32m---> 65\u001b[0m         fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m         close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# module os has no attribute PathLike, so we're on python < 3.6.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# The protocol we're trying to support doesn't exist, so just pass.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'extracted_audio/Taking Care of Our Fear ｜ Br. Phap Luu ｜ 2024-11-06.mp3'"
     ]
    }
   ],
   "source": [
    "\n",
    "chunk_duration_ms = 8 * 60 * 1000  # 10 minutes\n",
    "output_directory = split_audio_into_chunks(audio_file_path, chunk_duration_ms)\n",
    "print(f\"Chunks saved to: {output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_directory = audio_storage_dir / \"test_output_dir\"\n",
    "output_transcription_file = Path(\"taking_care_of_fear_transcript.txt\")\n",
    "output_jsonl_file = Path(\"taking_care_of_fear_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:23:55,293 - audio_extraction_testing - INFO - Audio files found:\n",
      "\t[PosixPath('extracted_audio/test_output_dir/chunk_1.mp3'), PosixPath('extracted_audio/test_output_dir/chunk_2.mp3'), PosixPath('extracted_audio/test_output_dir/chunk_3.mp3'), PosixPath('extracted_audio/test_output_dir/chunk_4.mp3'), PosixPath('extracted_audio/test_output_dir/chunk_5.mp3')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk_1.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:24:03,538 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk_2.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:24:11,661 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk_3.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:24:17,874 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk_4.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:24:26,367 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk_5.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:24:30,977 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "2024-12-06 14:24:30,982 - audio_extraction_testing - INFO - Stitched transcription saved to taking_care_of_fear_transcript.txt\n",
      "2024-12-06 14:24:30,983 - audio_extraction_testing - INFO - Full transcript objects saved to taking_care_of_fear_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "process_audio_chunks(\n",
    "    directory=chunks_directory,\n",
    "    output_file=output_transcription_file,\n",
    "    jsonl_file=output_jsonl_file,\n",
    "    prompt=\"Dharma, Deer Park, Thay, Thich Nhat Hanh, Bodhicitta, Bodhisattva, Mahayana\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 11:14:34,516 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Open the file and perform transcription\n",
    "with audio_file_path.open(\"rb\") as audio_file:\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        response_format=\"verbose_json\",\n",
    "        prompt=\"Dharma, Deer Park, Thay, Thich Nhat Hanh, Bodhicitta, Bodhisattva, Mahayana\",\n",
    "        file=chunks_directory / \"chunk_1.mp3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' When I do that I feel stable, I feel'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript.segments[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.audio.transcription_verbose.TranscriptionVerbose"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.audio.transcription_verbose import TranscriptionVerbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formatting prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the following text into paragraphs. Make minimal corrections to grammar if required for logical flow. Make no other changes; add no content. Output the final text only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transcript' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtranscript\u001b[49m\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transcript' is not defined"
     ]
    }
   ],
   "source": [
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count(transcript.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
