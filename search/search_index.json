{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TNH Scholar","text":"<p>Prototype implementation of Thich Nhat Hanh Scholar text processing and searching tools. Analyze, process and explore Dharma talks and teachings.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>pip install tnh-scholar\n</code></pre>"},{"location":"#core-tools","title":"Core Tools","text":"<ul> <li>audio-transcribe: Convert dharma talk recordings to text with speaker detection</li> <li>nfmt: Format and normalize transcribed text to TNH Scholar standards</li> <li>tnh-fab: Process text files using customizable pattern templates </li> <li>tnh-setup: Initialize project workspace with required configurations</li> <li>token-count: Count tokens in a text file</li> <li>ytt-fetch: Download and prepare YouTube transcripts for processing</li> </ul>"},{"location":"#pattern-system","title":"Pattern System","text":"<p>Text processing templates for common transformation needs</p> <ul> <li>Metadata extraction</li> <li>Section marking</li> <li>Quote detection</li> <li>Reference formatting</li> </ul> <ul> <li>See Getting Started for installation and startup.</li> <li>See User Guide for detailed usage examples and patterns.</li> </ul>"},{"location":"gpt_development_convos/","title":"Gpt development convos","text":""},{"location":"gpt_development_convos/#design","title":"Design","text":"<ul> <li>Augmented LLM for Thich Nhat Hanh: Initial design and feasibility convo.</li> <li>Feasibility Study Outline: Converting feasibility study to Markdown, other documentation considerations.</li> <li>Project Directory Naming Conventions. Brief convo on file and directory naming conventions for the project. Agreed on all lower case, '_' separated filenames.</li> </ul>"},{"location":"gpt_development_convos/#data-processing","title":"Data Processing","text":"<ul> <li>TNHS: Initial Data Processing. First convo on data processing.</li> <li>TNHS: Text Extraction Strategies. First explorations of data extraction code. Epub vs. text; human-machine collaboration strategy.</li> </ul>"},{"location":"metadata_report/","title":"Summary Report on Metadata Extraction, Source Parsing, and Model Training for TNH-Scholar","text":""},{"location":"metadata_report/#overview","title":"Overview","text":"<p>The process of extracting and handling metadata for training models from Thich Nhat Hanh\u2019s works has proven to be a complex but feasible task. While initial expectations were that this would be straightforward, we\u2019ve learned that every book and text source has its own formatting and structural peculiarities, requiring tailored approaches for each. As we move forward with building the three primary models (search, conversation, and translation), it\u2019s clear that managing metadata correctly will be essential for effective training and retrieval.</p>"},{"location":"metadata_report/#1-key-learnings-on-metadata-extraction-source-parsing","title":"1. Key Learnings on Metadata Extraction &amp; Source Parsing","text":""},{"location":"metadata_report/#challenges-identified","title":"Challenges Identified:","text":"<ul> <li>Inconsistent formatting: Each book (or other source) has unique formatting, especially when comparing different types of publications (e.g., EPUBs vs PDFs).</li> <li>Non-uniform structure: Some books have chapters, others have only sections or exercises, while some include quotes and author signatures that are not always easily distinguishable from other text elements.</li> <li>Manual intervention: Parsing tools help automate much of the extraction, but each book requires some degree of human intervention or book-specific rules.</li> </ul>"},{"location":"metadata_report/#tools-used-so-far","title":"Tools Used So Far:","text":"<ul> <li>ebooklib: Used to read and extract content from EPUB books, providing access to the structural elements present in the ebook\u2019s HTML.</li> <li>BeautifulSoup: For parsing the HTML content extracted from EPUBs, allowing for extraction based on tags and attributes (e.g., <code>&lt;p&gt;</code>, <code>&lt;blockquote&gt;</code>, and class names).</li> <li>Regular Expressions (Regex): Used for detecting patterns in text, such as chapter titles, quotes, and headings.</li> <li>SpaCy: Considered for creating custom NLP models for detecting metadata like \"paragraphs\" or \"quotes,\" but this would require additional training data.</li> </ul>"},{"location":"metadata_report/#potential-tools-for-further-exploration","title":"Potential Tools for Further Exploration:","text":"<ul> <li>Prodigy: A powerful annotation tool designed for training NLP models. It can be used to manually annotate text for metadata like \"heading,\" \"paragraph,\" \"quote,\" etc. This helps create a labeled dataset that can train models to automatically identify such elements.</li> <li>Label Studio: An open-source data labeling tool that supports text, image, and audio annotation. It can be used to manually annotate and label different structural elements like chapters, quotes, and sections for model training or evaluation.</li> <li>LayoutParser: A deep learning-based tool for document layout analysis. It can be useful for extracting structural information from scanned PDFs or documents with more complex layouts. This would be particularly useful if you're dealing with non-EPUB formatted books or sources.</li> <li>Tesseract OCR: An open-source optical character recognition (OCR) engine, useful for converting scanned images and PDFs into text that can then be processed for structural extraction. If you have scanned books, Tesseract can be the starting point for transforming them into searchable text.</li> <li>PDFMiner: A library for extracting text and metadata from PDFs. It can be used when working with PDF versions of books and helps retain the structure of the content for further processing.</li> <li>Hugging Face Transformers: A versatile library that offers pre-trained models for text classification, zero-shot classification, and other tasks. It can be leveraged to automate the detection of structural elements, such as headings, quotes, or sections, without the need for extensive manual training.</li> <li>Fairseq: An open-source sequence-to-sequence learning library from Facebook AI. It can be explored for training custom translation models or for tasks that require document-level context understanding across large texts.</li> <li>OpenRefine: A tool for cleaning and transforming data. It could be useful for preprocessing and standardizing the extracted text metadata, ensuring consistency across your dataset.</li> <li>Whoosh: A fast, featureful full-text indexing and searching library. Once the metadata is extracted, Whoosh can be used to build a lightweight search engine for indexing and retrieving relevant text chunks.</li> <li>NLTK</li> <li>Textacy</li> <li>Tika</li> <li>Polyglot</li> <li>Gensim</li> <li>Doccano</li> </ul>"},{"location":"metadata_report/#current-approaches-for-metadata-extraction","title":"Current Approaches for Metadata Extraction:","text":""},{"location":"metadata_report/#a-rule-based-extraction","title":"A. Rule-Based Extraction:","text":"<ul> <li>Strategy: Use HTML tags and class attributes to map elements like chapters, paragraphs, headings, quotes, and exercises to custom markers (e.g., <code>&lt;&lt;Chapter&gt;&gt;</code>, <code>&lt;&lt;Quote&gt;&gt;</code>).</li> <li>Tools: BeautifulSoup, regex.</li> <li>Pros: Can automate a large portion of the process for well-structured EPUBs.</li> <li>Cons: Requires manual tweaking for each source, not suitable for handling inconsistencies or malformed HTML.</li> </ul>"},{"location":"metadata_report/#b-custom-metadata-labeling","title":"B. Custom Metadata Labeling:","text":"<ul> <li>Strategy: Create custom training data to fine-tune NLP models for metadata labeling, detecting structural elements like \"quote,\" \"exercise,\" \"chapter,\" and \"paragraph.\"</li> <li>Tools: SpaCy for NER (Named Entity Recognition).</li> <li>Pros: Once trained, the model can generalize to new books, reducing the need for manual intervention.</li> <li>Cons: Requires significant upfront work in annotating data, time-consuming.</li> </ul>"},{"location":"metadata_report/#c-pdf-extraction-last-resort","title":"C. PDF Extraction (Last Resort):","text":"<ul> <li>Strategy: For books only available as PDFs, use OCR tools to extract text, then apply regex or rule-based extraction.</li> <li>Tools: PDFMiner, Tesseract OCR.</li> <li>Pros: Works when EPUB or other formats are not available.</li> <li>Cons: PDF extraction tends to lose structural fidelity, making it harder to recover headings and paragraphs.</li> </ul>"},{"location":"metadata_report/#lessons-learned","title":"Lessons Learned:","text":"<ul> <li>Structural extraction complexity: Parsing books is harder than expected, and managing structural elements (headings, chapters, etc.) requires a flexible, multi-step approach.</li> <li>Time and resource investment: Extracting metadata is time-consuming, and managing multiple books will require templated rules or automation combined with manual oversight.</li> </ul>"},{"location":"metadata_report/#2-approaches-to-train-3-primary-models","title":"2. Approaches to Train 3 Primary Models","text":"<p>We aim to create three primary models: search, conversational, and translation. Each model requires a different approach to training, with metadata playing various roles in enhancing the model\u2019s functionality.</p>"},{"location":"metadata_report/#a-search-model-fine-tuned-bert","title":"A. Search Model (Fine-Tuned BERT)","text":"<p>Goal: Retrieve relevant passages when users search for topics (e.g., \"mindfulness and compassion\").</p> <p>Training Strategy:    - Text Segmentation: Fine-tune BERT on text chunks (paragraphs, sections) without metadata.    - Metadata Use: Metadata like \"Introduction,\" \"Quote,\" or \"Chapter\" can be used to rank results or filter them during retrieval but should be excluded during training.    - Inclusion Criteria: Include all relevant content, including quotes and cited sutras.</p> <p>Considerations:    - Metadata helps provide context in the search results (e.g., \u201cThis passage is from the introduction to Love in Action\u201d).    - Ranking by metadata (e.g., prioritize results from exercises or introductions) will improve user experience.</p>"},{"location":"metadata_report/#b-conversational-model-fine-tuned-gpt-like","title":"B. Conversational Model (Fine-Tuned GPT-like)","text":"<p>Goal: Simulate conversations with the text in Thay\u2019s voice, responding naturally to user questions.</p> <p>Training Strategy:    - Voice Consistency: Exclude quotes, sutras, or references that are not directly in Thich Nhat Hanh\u2019s voice. Focus on Thay\u2019s personal writings and speeches.    - Metadata Use: Metadata such as \"Chapter\" or \"Section\" might not be as useful here. Instead, focus on ensuring that only relevant content is included for training.    - Fine-tuning: Use conversation datasets to improve the model\u2019s ability to respond in Thay\u2019s style.</p> <p>Considerations:    - Metadata isn\u2019t as critical for training but will be important when parsing the data (i.e., excluding non-Thay voice content).</p>"},{"location":"metadata_report/#c-translation-model-fine-tuned-marianmt-or-mbart","title":"C. Translation Model (Fine-Tuned MarianMT or mBART)","text":"<p>Goal: Provide high-quality translations between English, Vietnamese, and French for Thich Nhat Hanh\u2019s teachings.</p> <p>Training Strategy:    - Parallel Texts: Align bilingual or multilingual texts from Thich Nhat Hanh\u2019s works.    - Metadata Use: Metadata should be stripped from the training data, focusing purely on the text for translation.    - Fine-tuning: Use existing pre-trained translation models (MarianMT, mBART) and fine-tune on the specific language pairs and content relevant to Thich Nhat Hanh\u2019s works.</p> <p>Considerations:    - Metadata is largely irrelevant for the translation task but could be helpful when cross-referencing translations back to the original text.</p>"},{"location":"metadata_report/#3-summary-of-current-tools-and-their-roles","title":"3. Summary of Current Tools and Their Roles","text":"Tool Purpose Role in Project ebooklib EPUB parsing Extracts content and structure from EPUB books. BeautifulSoup HTML parsing Processes HTML from EPUBs to extract structured elements. Regex Pattern matching Identifies chapters, quotes, and headings via patterns. SpaCy Custom NLP model for metadata extraction Can be used for fine-tuning to detect specific metadata entities. PDFMiner PDF text extraction Alternative for PDF books (fallback). Tesseract OCR for scanned documents Used if only image-based PDFs are available. Hugging Face Model fine-tuning BERT for search, GPT-like models for conversation, MarianMT for translation."},{"location":"metadata_report/#4-important-considerations-going-forward","title":"4. Important Considerations Going Forward","text":"<ul> <li>Refining Extraction: Further experiments with rule-based extraction or custom models for metadata will be essential.</li> <li>Model-Specific Training: The distinction between training data for search, conversation, and translation must be carefully managed (e.g., include sutras for search, exclude for conversational training).</li> <li>Proof of Concept Focus: While full automation is a long-term goal, manual handling will be required in the short term, especially for book-specific edge cases.</li> </ul>"},{"location":"preliminary_feasibility_study/","title":"Preliminary Feasibility Study","text":""},{"location":"preliminary_feasibility_study/#an-interactive-study-and-translation-system-based-on-thich-nhat-hanhs-teachings","title":"An Interactive Study and Translation System Based on Thich Nhat Hanh\u2019s Teachings","text":""},{"location":"preliminary_feasibility_study/#1-project-overview","title":"1. Project Overview","text":"<p>This project aims to create a multipart system using large language models (LLMs) trained on Thich Nhat Hanh\u2019s Teachings. The system will serve primarily Plum Village monastics, and secondarily, mindfulness practitioners, providing:</p> <ul> <li> <p>A query-based text search system to retrieve relevant teachings.</p> </li> <li> <p>An interactive model for exploring Thich Nhat Hanh\u2019s teachings through text-based dialogue.</p> </li> <li> <p>A multilingual translation engine, initially supporting English, Vietnamese, and French, with potential to expand to other languages such as Mandarin, Spanish, Japanese, etc.</p> </li> </ul> <p>This project is designed as a foundational system, which can be expanded in the future to handle deeper conversational tasks and additional languages as technological advancements and resources become available.</p>"},{"location":"preliminary_feasibility_study/#2-scope-and-objectives","title":"2. Scope and Objectives","text":""},{"location":"preliminary_feasibility_study/#primary-objectives","title":"Primary Objectives:","text":"<ol> <li>Text Query and Search (BERT-Based):<ul> <li>A system where users can input queries on mindfulness topics (e.g., \u201cmindfulness and compassion\u201d) and receive relevant resources (text, video, or audio excerpts).</li> <li>Prioritize contextual understanding to ensure relevant and accurate search results from Thich Nhat Hanh\u2019s teachings.</li> </ul> </li> <li>Interactive Textual Exploration (GPT-3-Based):<ul> <li>An exploratory interactive tool for users to engage with Thich Nhat Hanh\u2019s teachings in a conversational format.</li> <li>GPT-3 will be fine-tuned to respond meaningfully to user input, guiding exploration of teachings.</li> </ul> </li> <li>Multilingual Translation Engine (mBART-Based, Expanding to Multiple Languages):<ul> <li>A translation tool focusing initially on English, Vietnamese, and French.</li> <li>Capacity to expand later into other languages such as Mandarin, Spanish, and others.</li> <li>Emphasis on nuanced, Plum Village style language to ensure accurate and meaningful translation, especially for complex content.</li> </ul> </li> </ol>"},{"location":"preliminary_feasibility_study/#3-research-component-model-evaluation","title":"3. Research Component: Model Evaluation","text":"<p>Given the rapid development of models in NLP, this project will include a research phase to evaluate and choose the most appropriate models for the three primary system components.</p>"},{"location":"preliminary_feasibility_study/#31-model-research-for-text-query-and-search","title":"3.1 Model Research for Text Query and Search:","text":"<ul> <li>BERT (Bidirectional Encoder Representations from Transformers): BERT excels at understanding context within text, making it ideal for retrieving relevant passages based on search queries.</li> <li>Evaluation: Review BERT variations, such as multilingual BERT, to assess their suitability for understanding context in a multilingual corpus of teachings.</li> <li>Alternatives: Other Transformer-based models such as RoBERTa or Longformer will be evaluated for handling large datasets or longer documents.</li> </ul>"},{"location":"preliminary_feasibility_study/#32-model-research-for-interactive-dialogue","title":"3.2 Model Research for Interactive Dialogue:","text":"<ul> <li>GPT-3: The project will leverage GPT-3 for generating conversational responses based on user input. GPT-3 is powerful for generating coherent, meaningful interactions.</li> <li>Evaluation: The research will assess DialoGPT (a dialogue-optimized version of GPT-2) as an alternative, especially for its focus on human-like conversations.</li> <li>Factors to Consider: Trade-offs between response quality (GPT-3\u2019s strength) and fine-tuning flexibility (DialoGPT\u2019s ease of customization for dialogue).</li> </ul>"},{"location":"preliminary_feasibility_study/#33-model-research-for-translation-engine","title":"3.3 Model Research for Translation Engine:","text":"<ul> <li>mBART (Multilingual BART): The research will focus on mBART due to its strength in handling complex contextual relationships, which is essential for translating spiritual texts.</li> <li>Evaluation: Compare mBART with MarianMT, which offers efficiency and lower computational demands. mBART\u2019s superior handling of nuanced language will be balanced against the resource-efficient MarianMT, which is specifically designed for translation tasks.</li> <li>Initial Language Focus: Fine-tune the translation engine for English, Vietnamese, and French, with a plan to expand to other languages as needed.</li> </ul>"},{"location":"preliminary_feasibility_study/#4-technical-feasibility","title":"4. Technical Feasibility","text":""},{"location":"preliminary_feasibility_study/#41-data-collection-and-preparation","title":"4.1 Data Collection and Preparation","text":"<ul> <li>Text Corpus: Collect, preprocess, and check for accuracy, Thich Nhat Hanh\u2019s writings (books, articles, Dharma talks, interviews) in English, Vietnamese, and French. The initial training set will be equivalent to approximately 100 books, sourced from talks, interviews, and books across the three languages, with the majority of the volume in Vietnamese. This dataset will serve as the foundation for query, translation, and interactive systems.</li> <li>Audio/Visual Corpus: Include video/audio resources with transcriptions to support multimedia queries. Audio-to-text conversion models may be considered in the future.</li> <li>Translation Data: Collect parallel texts for English-Vietnamese and English-French pairs to fine-tune the translation engine. This dataset will serve as the foundation for expanding into additional languages.</li> </ul>"},{"location":"preliminary_feasibility_study/#42-model-fine-tuning-and-deployment","title":"4.2 Model Fine-Tuning and Deployment","text":"<ul> <li>Cloud Infrastructure: Use cloud-based services like Google Cloud or AWS to fine-tune models and ensure scalability. The GPT-3 system can be trained through the Open AI API interface. Pre-trained models significantly reduce training time and resources, but fine-tuning will still require access to GPUs/TPUs.</li> <li>Deployment: Hugging Face provides tools and APIs for easy model deployment. This infrastructure will be used for both training and serving models.</li> </ul>"},{"location":"preliminary_feasibility_study/#43-computational-resources","title":"4.3 Computational Resources","text":"<ul> <li>Fine-tuning pre-trained models like BERT, and mBART will require significant computational resources, but this is mitigated by using pre-trained models as a foundation. GPT-3 can be trained entirely on the Open AI platform.</li> <li>Ongoing resource needs will depend on the scalability of translation tasks and the number of supported languages.</li> </ul>"},{"location":"preliminary_feasibility_study/#5-cost-feasibility","title":"5. Cost Feasibility","text":""},{"location":"preliminary_feasibility_study/#51-initial-costs","title":"5.1 Initial Costs:","text":"<ul> <li>Fine-tuning pre-trained models and deploying them on cloud infrastructure.</li> <li>Data preprocessing, audio to text conversion, and cleaning of the text corpus.</li> <li>Training GPT-3 on the initial dataset (equivalent to 100 books) will cost approximately \\$300. This does not include other Open AI platform costs, but provides a ballpark figure for the initial training phase.</li> <li>Translation engine setup for initial languages (English, Vietnamese, French).</li> </ul>"},{"location":"preliminary_feasibility_study/#52-ongoing-costs","title":"5.2 Ongoing Costs:","text":"<ul> <li>Hosting the models, ongoing training as the dataset grows, and maintaining cloud infrastructure for serving requests.</li> <li>Expanding the translation system to include additional languages as needed.</li> </ul>"},{"location":"preliminary_feasibility_study/#53-mitigation","title":"5.3 Mitigation:","text":"<ul> <li>Starting with smaller datasets and key languages (English, Vietnamese, French) reduces initial costs and allows for gradual scaling.</li> </ul>"},{"location":"preliminary_feasibility_study/#6-operational-feasibility","title":"6. Operational Feasibility","text":""},{"location":"preliminary_feasibility_study/#61-user-interface-and-experience","title":"6.1 User Interface and Experience:","text":"<ul> <li>Develop a user-friendly interface allowing monastics and practitioners to search, interact, and request translations of texts.</li> <li>Ensure accessibility across various devices (web and mobile) to facilitate widespread use among the Plum Village community and beyond.</li> </ul>"},{"location":"preliminary_feasibility_study/#62-testing-and-feedback","title":"6.2 Testing and Feedback:","text":"<ul> <li>Conduct pilot testing with a small group of Plum Village monastics to refine the system.</li> <li>Collect feedback to fine-tune the query system, conversational responses, and translation accuracy.</li> </ul>"},{"location":"preliminary_feasibility_study/#7-future-scalability-and-expansion","title":"7. Future Scalability and Expansion","text":"<ol> <li>Language Expansion: As new bilingual data becomes available, expand the translation system to support languages like Mandarin, Spanish, Japanese, and Korean.</li> <li>Advanced Conversational Capabilities: Explore future opportunities to refine the interactive system, potentially scaling toward a more advanced conversational model that reflects deeper spiritual dialogues.</li> <li>Multimedia Integration: Add real-time transcription of audio content and expand the multimedia query engine to handle video/audio teachings.</li> </ol>"},{"location":"preliminary_feasibility_study/#8-ethical-and-legal-considerations","title":"8. Ethical and Legal Considerations","text":"<ol> <li>Data Ownership: Ensure permissions are secured for digitizing and using Thich Nhat Hanh\u2019s teachings.</li> <li>Ethical AI: Implement guidelines to prevent misrepresentation or biased interpretations of the teachings. Ensure transparency in how models handle sensitive spiritual content.</li> </ol>"},{"location":"preliminary_feasibility_study/#conclusion","title":"Conclusion","text":"<p>The project is technically feasible using pre-trained models such as BERT, GPT-3, and mBART, with a strong focus on accuracy for translations and queries. It can scale over time to include additional languages and advanced functionalities. Starting with smaller, well-defined goals (English, Vietnamese, French translations; query and interactive systems) ensures cost-effective deployment and lays the groundwork for future expansion.</p>"},{"location":"translation_research/","title":"Translation Research","text":"<p>Vietnamese translation model looked at: https://huggingface.co/VietAI/envit5-translation</p> <p>May be useful for baseline testing.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#tnh_scholar","title":"<code>tnh_scholar</code>","text":"<p>TNH Scholar: Text Processing and Analysis Tools</p> <p>TNH Scholar is an AI-driven project designed to explore, query, process and translate the teachings of Thich Nhat Hanh and other Plum Village Dharma Teachers. The project aims  to create a resource for practitioners and scholars to deeply engage with mindfulness  and spiritual wisdom through natural language processing and machine learning models.</p> Core Features <ul> <li>Audio transcription and processing</li> <li>Multi-lingual text processing and translation</li> <li>Pattern-based text analysis</li> <li>OCR processing for historical documents</li> <li>CLI tools for batch processing</li> </ul> Package Structure <ul> <li>tnh_scholar/</li> <li>CLI_tools/          - Command line interface tools</li> <li>audio_processing/   - Audio file handling and transcription</li> <li>journal_processing/ - Journal and publication processing</li> <li>ocr_processing/     - Optical character recognition tools</li> <li>openai_interface/   - OpenAI API integration</li> <li>text_processing/    - Core text processing utilities</li> <li>video_processing/   - Video file handling and transcription</li> <li>utils/             - Shared utility functions</li> <li>xml_processing/    - XML parsing and generation</li> </ul> Environment Configuration <ul> <li>The package uses environment variables for configuration, including:</li> <li>TNH_PATTERN_DIR - Directory for text processing patterns</li> <li>OPENAI_API_KEY     - OpenAI API authentication</li> <li>GOOGLE_VISION_KEY  - Google Cloud Vision API key for OCR</li> </ul> CLI Tools <ul> <li>audio-transcribe  - Audio file transcription utility</li> <li>tnh-fab          - Text processing and analysis toolkit</li> </ul> <p>For more information, see:     - Documentation: https://aaronksolomon.github.io/tnh-scholar/     - Source: https://github.com/aaronksolomon/tnh-scholar     - Issues: https://github.com/aaronksolomon/tnh-scholar/issues</p> Dependencies <ul> <li>Core: click, pydantic, openai, yt-dlp</li> <li>Optional: streamlit (GUI), spacy (NLP), google-cloud-vision (OCR)</li> </ul>"},{"location":"api/#tnh_scholar.TNH_CLI_TOOLS_DIR","title":"<code>TNH_CLI_TOOLS_DIR = TNH_ROOT_SRC_DIR / 'cli_tools'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_CONFIG_DIR","title":"<code>TNH_CONFIG_DIR = Path.home() / '.config' / 'tnh-scholar'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_DEFAULT_PATTERN_DIR","title":"<code>TNH_DEFAULT_PATTERN_DIR = TNH_CONFIG_DIR / 'patterns'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_LOG_DIR","title":"<code>TNH_LOG_DIR = TNH_CONFIG_DIR / 'logs'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_PROJECT_ROOT_DIR","title":"<code>TNH_PROJECT_ROOT_DIR = TNH_ROOT_SRC_DIR.resolve().parent.parent</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_ROOT_SRC_DIR","title":"<code>TNH_ROOT_SRC_DIR = Path(__file__).resolve().parent</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.__version__","title":"<code>__version__ = '0.1.3'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing","title":"<code>ai_text_processing</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing","title":"<code>ai_text_processing</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_MIN_SECTION_COUNT","title":"<code>DEFAULT_MIN_SECTION_COUNT = 3</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_OPENAI_MODEL","title":"<code>DEFAULT_OPENAI_MODEL = 'gpt-4o'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_PARAGRAPH_FORMAT_PATTERN","title":"<code>DEFAULT_PARAGRAPH_FORMAT_PATTERN = 'default_xml_paragraph_format'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_PUNCTUATE_MODEL","title":"<code>DEFAULT_PUNCTUATE_MODEL = 'gpt-4o'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_PUNCTUATE_PATTERN","title":"<code>DEFAULT_PUNCTUATE_PATTERN = 'default_punctuate'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_PUNCTUATE_STYLE","title":"<code>DEFAULT_PUNCTUATE_STYLE = 'APA'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_REVIEW_COUNT","title":"<code>DEFAULT_REVIEW_COUNT = 5</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_SECTION_PATTERN","title":"<code>DEFAULT_SECTION_PATTERN = 'default_section'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_SECTION_RANGE_VAR","title":"<code>DEFAULT_SECTION_RANGE_VAR = 2</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_SECTION_RESULT_MAX_SIZE","title":"<code>DEFAULT_SECTION_RESULT_MAX_SIZE = 4000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_SECTION_TOKEN_SIZE","title":"<code>DEFAULT_SECTION_TOKEN_SIZE = 650</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_TARGET_LANGUAGE","title":"<code>DEFAULT_TARGET_LANGUAGE = 'English'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_TRANSLATE_CONTEXT_LINES","title":"<code>DEFAULT_TRANSLATE_CONTEXT_LINES = 3</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_TRANSLATE_SEGMENT_SIZE","title":"<code>DEFAULT_TRANSLATE_SEGMENT_SIZE = 20</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_TRANSLATE_STYLE","title":"<code>DEFAULT_TRANSLATE_STYLE = \"'American Dharma Teaching'\"</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_TRANSLATION_PATTERN","title":"<code>DEFAULT_TRANSLATION_PATTERN = 'default_line_translation'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_TRANSLATION_TARGET_TOKENS","title":"<code>DEFAULT_TRANSLATION_TARGET_TOKENS = 650</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_XML_FORMAT_PATTERN","title":"<code>DEFAULT_XML_FORMAT_PATTERN = 'default_xml_format'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.FOLLOWING_CONTEXT_MARKER","title":"<code>FOLLOWING_CONTEXT_MARKER = 'FOLLOWING_CONTEXT'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.PRECEDING_CONTEXT_MARKER","title":"<code>PRECEDING_CONTEXT_MARKER = 'PRECEDING_CONTEXT'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SECTION_SEGMENT_SIZE_WARNING_LIMIT","title":"<code>SECTION_SEGMENT_SIZE_WARNING_LIMIT = 5</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TRANSCRIPT_SEGMENT_MARKER","title":"<code>TRANSCRIPT_SEGMENT_MARKER = 'TRANSCRIPT_SEGMENT'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor","title":"<code>GeneralProcessor</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class GeneralProcessor:\n    def __init__(\n        self,\n        processor: TextProcessor,\n        pattern: Pattern,\n        source_language: Optional[str] = None,\n        review_count: int = DEFAULT_REVIEW_COUNT,\n    ):\n        \"\"\"\n        Initialize punctuation generator.\n\n        Args:\n            text_punctuator: Implementation of TextProcessor\n            punctuate_pattern: Pattern object containing punctuation instructions\n            section_count: Target number of sections\n            review_count: Number of review passes\n        \"\"\"\n\n        self.source_language = source_language\n        self.processor = processor\n        self.pattern = pattern\n        self.review_count = review_count\n\n    def process_text(\n        self,\n        text: str,\n        source_language: Optional[str] = None,\n        template_dict: Optional[Dict] = None,\n    ) -&gt; str:\n        \"\"\"\n        process a text based on a pattern and source language.\n        \"\"\"\n\n        if not source_language:\n            if self.source_language:\n                source_language = self.source_language\n            else:\n                source_language = get_language_name(text)\n\n        template_values = {\n            \"source_language\": source_language,\n            \"review_count\": self.review_count,\n        }\n\n        if template_dict:\n            template_values |= template_dict\n\n        logger.info(\"Processing text...\")\n        instructions = self.pattern.apply_template(template_values)\n\n        logger.debug(f\"Process instructions:\\n{instructions}\")\n\n        text = self.processor.process_text(text, instructions)\n        logger.info(\"Processing completed.\")\n\n        # normalize newline spacing to two newline between lines and return\n        # commented out to allow pattern to dictate newlines:\n        # return normalize_newlines(text)\n        return text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.pattern","title":"<code>pattern = pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.processor","title":"<code>processor = processor</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.review_count","title":"<code>review_count = review_count</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.source_language","title":"<code>source_language = source_language</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.__init__","title":"<code>__init__(processor, pattern, source_language=None, review_count=DEFAULT_REVIEW_COUNT)</code>","text":"<p>Initialize punctuation generator.</p> <p>Parameters:</p> Name Type Description Default <code>text_punctuator</code> <p>Implementation of TextProcessor</p> required <code>punctuate_pattern</code> <p>Pattern object containing punctuation instructions</p> required <code>section_count</code> <p>Target number of sections</p> required <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(\n    self,\n    processor: TextProcessor,\n    pattern: Pattern,\n    source_language: Optional[str] = None,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n):\n    \"\"\"\n    Initialize punctuation generator.\n\n    Args:\n        text_punctuator: Implementation of TextProcessor\n        punctuate_pattern: Pattern object containing punctuation instructions\n        section_count: Target number of sections\n        review_count: Number of review passes\n    \"\"\"\n\n    self.source_language = source_language\n    self.processor = processor\n    self.pattern = pattern\n    self.review_count = review_count\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.process_text","title":"<code>process_text(text, source_language=None, template_dict=None)</code>","text":"<p>process a text based on a pattern and source language.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text(\n    self,\n    text: str,\n    source_language: Optional[str] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; str:\n    \"\"\"\n    process a text based on a pattern and source language.\n    \"\"\"\n\n    if not source_language:\n        if self.source_language:\n            source_language = self.source_language\n        else:\n            source_language = get_language_name(text)\n\n    template_values = {\n        \"source_language\": source_language,\n        \"review_count\": self.review_count,\n    }\n\n    if template_dict:\n        template_values |= template_dict\n\n    logger.info(\"Processing text...\")\n    instructions = self.pattern.apply_template(template_values)\n\n    logger.debug(f\"Process instructions:\\n{instructions}\")\n\n    text = self.processor.process_text(text, instructions)\n    logger.info(\"Processing completed.\")\n\n    # normalize newline spacing to two newline between lines and return\n    # commented out to allow pattern to dictate newlines:\n    # return normalize_newlines(text)\n    return text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LineTranslator","title":"<code>LineTranslator</code>","text":"<p>Translates text line by line while maintaining line numbers and context.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class LineTranslator:\n    \"\"\"Translates text line by line while maintaining line numbers and context.\"\"\"\n\n    def __init__(\n        self,\n        processor: TextProcessor,\n        pattern: Pattern,\n        review_count: int = DEFAULT_REVIEW_COUNT,\n        style: str = DEFAULT_TRANSLATE_STYLE,\n        context_lines: int = DEFAULT_TRANSLATE_CONTEXT_LINES,  # Number of context lines before/after\n    ):\n        \"\"\"\n        Initialize line translator.\n\n        Args:\n            processor: Implementation of TextProcessor\n            pattern: Pattern object containing translation instructions\n            review_count: Number of review passes\n            style: Translation style to apply\n            context_lines: Number of context lines to include before/after\n        \"\"\"\n        self.processor = processor\n        self.pattern = pattern\n        self.review_count = review_count\n        self.style = style\n        self.context_lines = context_lines\n\n    def translate_segment(\n        self,\n        num_text: NumberedText,\n        start_line: int,\n        end_line: int,\n        source_language: Optional[str] = None,\n        target_language: Optional[str] = DEFAULT_TARGET_LANGUAGE,\n        template_dict: Optional[Dict] = None,\n    ) -&gt; str:\n        \"\"\"\n        Translate a segment of text with context.\n\n        Args:\n            text: Full text to extract segment from\n            start_line: Starting line number of segment\n            end_line: Ending line number of segment\n            source_language: Source language code\n            target_language: Target language code (default: English)\n            template_dict: Optional additional template values\n\n        Returns:\n            Translated text segment with line numbers preserved\n        \"\"\"\n\n        # Extract main segment and context\n        lines = num_text.numbered_lines\n\n        # Calculate context ranges\n        preceding_start = max(1, start_line - self.context_lines)  # lines start on 1.\n        following_end = min(num_text.end + 1, end_line + self.context_lines)\n\n        # Extract context and segment\n        preceding_context = num_text.get_numbered_segment(preceding_start, start_line)\n        transcript_segment = num_text.get_numbered_segment(start_line, end_line)\n        following_context = num_text.get_numbered_segment(end_line, following_end)\n\n        # build input text\n        translation_input = self._build_translation_input(\n            preceding_context, transcript_segment, following_context\n        )\n\n        # Prepare template values\n        template_values = {\n            \"source_language\": source_language,\n            \"target_language\": target_language,\n            \"review_count\": self.review_count,\n            \"style\": self.style,\n        }\n\n        if template_dict:\n            template_values |= template_dict\n\n        # Get and apply translation instructions\n        logger.info(f\"Translating segment (lines {start_line}-{end_line})\")\n        translate_instructions = self.pattern.apply_template(template_values)\n\n        if start_line &lt;= 1:\n            logger.debug(\n                f\"Translate instructions (first segment):\\n{translate_instructions}\"\n            )\n\n        logger.debug(f\"Translation input:\\n{translation_input}\")\n\n        return self.processor.process_text(translation_input, translate_instructions)\n\n    def _build_translation_input(\n        self, preceding_context: str, transcript_segment: str, following_context: str\n    ) -&gt; str:\n        \"\"\"\n        Build input text in required XML-style format.\n\n        Args:\n            preceding_context: Context lines before segment\n            transcript_segment: Main segment to translate\n            following_context: Context lines after segment\n\n        Returns:\n            Formatted input text\n        \"\"\"\n        parts = []\n\n        # Add preceding context if exists\n        if preceding_context:\n            parts.extend(\n                [\n                    PRECEDING_CONTEXT_MARKER,\n                    preceding_context,\n                    PRECEDING_CONTEXT_MARKER,\n                    \"\",\n                ]\n            )\n\n        # Add main segment (always required)\n        parts.extend(\n            [\n                TRANSCRIPT_SEGMENT_MARKER,\n                transcript_segment,\n                TRANSCRIPT_SEGMENT_MARKER,\n                \"\",\n            ]\n        )\n\n        # Add following context if exists\n        if following_context:\n            parts.extend(\n                [\n                    FOLLOWING_CONTEXT_MARKER,\n                    following_context,\n                    FOLLOWING_CONTEXT_MARKER,\n                    \"\",\n                ]\n            )\n\n        return \"\\n\".join(parts)\n\n    def translate_text(\n        self,\n        text: str,\n        segment_size: Optional[int] = None,  # Number of lines per segment\n        source_language: Optional[str] = None,\n        target_language: Optional[str] = None,\n        template_dict: Optional[Dict] = None,\n    ) -&gt; str:\n        \"\"\"\n        Translate entire text in segments while maintaining line continuity.\n\n        Args:\n            text: Text to translate\n            segment_size: Number of lines per translation segment\n            source_language: Source language code\n            target_language: Target language code (default: English)\n            template_dict: Optional additional template values\n\n        Returns:\n            Complete translated text with line numbers preserved\n        \"\"\"\n\n        # Auto-detect language if not specified\n        if not source_language:\n            source_language = get_language_name(text)\n\n        # Convert text to numbered lines\n        num_text = NumberedText(text)\n        total_lines = num_text.size\n\n        if not segment_size:\n            segment_size = _calculate_segment_size(\n                num_text, DEFAULT_TRANSLATION_TARGET_TOKENS\n            )\n\n        translated_segments = []\n\n        logger.debug(\n            f\"Total lines to translate: {total_lines} | Translation segment size: {segment_size}.\"\n        )\n        # Process text in segments using segment iteration\n        for start_idx, end_idx in num_text.iter_segments(\n            segment_size, segment_size // 5\n        ):\n            translated_segment = self.translate_segment(\n                num_text,\n                start_idx,\n                end_idx,\n                source_language,\n                target_language,\n                template_dict,\n            )\n\n            # validate the translated segment\n            translated_content = self._extract_content(translated_segment)\n            self._validate_segment(translated_content, start_idx, end_idx)\n\n            translated_segments.append(translated_content)\n\n        return \"\\n\".join(translated_segments)\n\n    def _extract_content(self, segment: str) -&gt; str:\n        segment = segment.strip()  # remove any filling whitespace\n        if segment.startswith(TRANSCRIPT_SEGMENT_MARKER) and segment.endswith(\n            TRANSCRIPT_SEGMENT_MARKER\n        ):\n            return segment[\n                len(TRANSCRIPT_SEGMENT_MARKER) : -len(TRANSCRIPT_SEGMENT_MARKER)\n            ].strip()\n        logger.warning(\"Translated segment missing transcript_segment tags\")\n        return segment\n\n    def _validate_segment(\n        self, translated_content: str, start_index: int, end_index: int\n    ) -&gt; None:\n        \"\"\"\n        Validate translated segment format, content, and line number sequence.\n        Issues warnings for validation issues rather than raising errors.\n\n        Args:\n            translated_segment: Translated text to validate\n            start_idx: the staring index of the range (inclusive)\n            end_line: then ending index of the range (exclusive)\n\n        Returns:\n            str: Content with segment tags removed\n        \"\"\"\n\n        # Validate lines\n\n        lines = translated_content.splitlines()\n        line_numbers = []\n\n        start_line = start_index  # inclusive start\n        end_line = end_index - 1  # exclusive end\n\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            if \":\" not in line:\n                logger.warning(f\"Invalid line format: {line}\")\n                continue\n\n            try:\n                line_num = int(line[: line.index(\":\")])\n                if line_num &lt; 0:\n                    logger.warning(f\"Invalid line number: {line}\")\n                    continue\n                line_numbers.append(line_num)\n            except ValueError:\n                logger.warning(f\"Line number parsing failed: {line}\")\n                continue\n\n        # Validate sequence\n        if not line_numbers:\n            logger.warning(\"No valid line numbers found\")\n        else:\n            if line_numbers[0] != start_line:\n                logger.warning(\n                    f\"First line number {line_numbers[0]} doesn't match expected {start_line}\"\n                )\n\n            if line_numbers[-1] != end_line:\n                logger.warning(\n                    f\"Last line number {line_numbers[-1]} doesn't match expected {end_line}\"\n                )\n\n            expected = set(range(start_line, end_line + 1))\n            if missing := expected - set(line_numbers):\n                logger.warning(f\"Missing line numbers in sequence: {missing}\")\n\n        logger.debug(f\"Validated {len(lines)} lines from {start_line} to {end_line}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LineTranslator.context_lines","title":"<code>context_lines = context_lines</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LineTranslator.pattern","title":"<code>pattern = pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LineTranslator.processor","title":"<code>processor = processor</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LineTranslator.review_count","title":"<code>review_count = review_count</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LineTranslator.style","title":"<code>style = style</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LineTranslator.__init__","title":"<code>__init__(processor, pattern, review_count=DEFAULT_REVIEW_COUNT, style=DEFAULT_TRANSLATE_STYLE, context_lines=DEFAULT_TRANSLATE_CONTEXT_LINES)</code>","text":"<p>Initialize line translator.</p> <p>Parameters:</p> Name Type Description Default <code>processor</code> <code>TextProcessor</code> <p>Implementation of TextProcessor</p> required <code>pattern</code> <code>Pattern</code> <p>Pattern object containing translation instructions</p> required <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> <code>style</code> <code>str</code> <p>Translation style to apply</p> <code>DEFAULT_TRANSLATE_STYLE</code> <code>context_lines</code> <code>int</code> <p>Number of context lines to include before/after</p> <code>DEFAULT_TRANSLATE_CONTEXT_LINES</code> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(\n    self,\n    processor: TextProcessor,\n    pattern: Pattern,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n    style: str = DEFAULT_TRANSLATE_STYLE,\n    context_lines: int = DEFAULT_TRANSLATE_CONTEXT_LINES,  # Number of context lines before/after\n):\n    \"\"\"\n    Initialize line translator.\n\n    Args:\n        processor: Implementation of TextProcessor\n        pattern: Pattern object containing translation instructions\n        review_count: Number of review passes\n        style: Translation style to apply\n        context_lines: Number of context lines to include before/after\n    \"\"\"\n    self.processor = processor\n    self.pattern = pattern\n    self.review_count = review_count\n    self.style = style\n    self.context_lines = context_lines\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LineTranslator.translate_segment","title":"<code>translate_segment(num_text, start_line, end_line, source_language=None, target_language=DEFAULT_TARGET_LANGUAGE, template_dict=None)</code>","text":"<p>Translate a segment of text with context.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>Full text to extract segment from</p> required <code>start_line</code> <code>int</code> <p>Starting line number of segment</p> required <code>end_line</code> <code>int</code> <p>Ending line number of segment</p> required <code>source_language</code> <code>Optional[str]</code> <p>Source language code</p> <code>None</code> <code>target_language</code> <code>Optional[str]</code> <p>Target language code (default: English)</p> <code>DEFAULT_TARGET_LANGUAGE</code> <code>template_dict</code> <code>Optional[Dict]</code> <p>Optional additional template values</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Translated text segment with line numbers preserved</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def translate_segment(\n    self,\n    num_text: NumberedText,\n    start_line: int,\n    end_line: int,\n    source_language: Optional[str] = None,\n    target_language: Optional[str] = DEFAULT_TARGET_LANGUAGE,\n    template_dict: Optional[Dict] = None,\n) -&gt; str:\n    \"\"\"\n    Translate a segment of text with context.\n\n    Args:\n        text: Full text to extract segment from\n        start_line: Starting line number of segment\n        end_line: Ending line number of segment\n        source_language: Source language code\n        target_language: Target language code (default: English)\n        template_dict: Optional additional template values\n\n    Returns:\n        Translated text segment with line numbers preserved\n    \"\"\"\n\n    # Extract main segment and context\n    lines = num_text.numbered_lines\n\n    # Calculate context ranges\n    preceding_start = max(1, start_line - self.context_lines)  # lines start on 1.\n    following_end = min(num_text.end + 1, end_line + self.context_lines)\n\n    # Extract context and segment\n    preceding_context = num_text.get_numbered_segment(preceding_start, start_line)\n    transcript_segment = num_text.get_numbered_segment(start_line, end_line)\n    following_context = num_text.get_numbered_segment(end_line, following_end)\n\n    # build input text\n    translation_input = self._build_translation_input(\n        preceding_context, transcript_segment, following_context\n    )\n\n    # Prepare template values\n    template_values = {\n        \"source_language\": source_language,\n        \"target_language\": target_language,\n        \"review_count\": self.review_count,\n        \"style\": self.style,\n    }\n\n    if template_dict:\n        template_values |= template_dict\n\n    # Get and apply translation instructions\n    logger.info(f\"Translating segment (lines {start_line}-{end_line})\")\n    translate_instructions = self.pattern.apply_template(template_values)\n\n    if start_line &lt;= 1:\n        logger.debug(\n            f\"Translate instructions (first segment):\\n{translate_instructions}\"\n        )\n\n    logger.debug(f\"Translation input:\\n{translation_input}\")\n\n    return self.processor.process_text(translation_input, translate_instructions)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LineTranslator.translate_text","title":"<code>translate_text(text, segment_size=None, source_language=None, target_language=None, template_dict=None)</code>","text":"<p>Translate entire text in segments while maintaining line continuity.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to translate</p> required <code>segment_size</code> <code>Optional[int]</code> <p>Number of lines per translation segment</p> <code>None</code> <code>source_language</code> <code>Optional[str]</code> <p>Source language code</p> <code>None</code> <code>target_language</code> <code>Optional[str]</code> <p>Target language code (default: English)</p> <code>None</code> <code>template_dict</code> <code>Optional[Dict]</code> <p>Optional additional template values</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Complete translated text with line numbers preserved</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def translate_text(\n    self,\n    text: str,\n    segment_size: Optional[int] = None,  # Number of lines per segment\n    source_language: Optional[str] = None,\n    target_language: Optional[str] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; str:\n    \"\"\"\n    Translate entire text in segments while maintaining line continuity.\n\n    Args:\n        text: Text to translate\n        segment_size: Number of lines per translation segment\n        source_language: Source language code\n        target_language: Target language code (default: English)\n        template_dict: Optional additional template values\n\n    Returns:\n        Complete translated text with line numbers preserved\n    \"\"\"\n\n    # Auto-detect language if not specified\n    if not source_language:\n        source_language = get_language_name(text)\n\n    # Convert text to numbered lines\n    num_text = NumberedText(text)\n    total_lines = num_text.size\n\n    if not segment_size:\n        segment_size = _calculate_segment_size(\n            num_text, DEFAULT_TRANSLATION_TARGET_TOKENS\n        )\n\n    translated_segments = []\n\n    logger.debug(\n        f\"Total lines to translate: {total_lines} | Translation segment size: {segment_size}.\"\n    )\n    # Process text in segments using segment iteration\n    for start_idx, end_idx in num_text.iter_segments(\n        segment_size, segment_size // 5\n    ):\n        translated_segment = self.translate_segment(\n            num_text,\n            start_idx,\n            end_idx,\n            source_language,\n            target_language,\n            template_dict,\n        )\n\n        # validate the translated segment\n        translated_content = self._extract_content(translated_segment)\n        self._validate_segment(translated_content, start_idx, end_idx)\n\n        translated_segments.append(translated_content)\n\n    return \"\\n\".join(translated_segments)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LocalPatternManager","title":"<code>LocalPatternManager</code>","text":"<p>A simple singleton implementation of PatternManager that ensures only one instance is created and reused throughout the application lifecycle.</p> <p>This class wraps the PatternManager to provide efficient pattern loading by maintaining a single reusable instance.</p> <p>Attributes:</p> Name Type Description <code>_instance</code> <code>Optional[SingletonPatternManager]</code> <p>The singleton instance</p> <code>_pattern_manager</code> <code>Optional[PatternManager]</code> <p>The wrapped PatternManager instance</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class LocalPatternManager:\n    \"\"\"\n    A simple singleton implementation of PatternManager that ensures only one instance\n    is created and reused throughout the application lifecycle.\n\n    This class wraps the PatternManager to provide efficient pattern loading by\n    maintaining a single reusable instance.\n\n    Attributes:\n        _instance (Optional[SingletonPatternManager]): The singleton instance\n        _pattern_manager (Optional[PatternManager]): The wrapped PatternManager instance\n    \"\"\"\n\n    _instance: Optional[\"LocalPatternManager\"] = None\n\n    def __new__(cls) -&gt; \"LocalPatternManager\":\n        \"\"\"\n        Create or return the singleton instance.\n\n        Returns:\n            SingletonPatternManager: The singleton instance\n        \"\"\"\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._pattern_manager = None\n        return cls._instance\n\n    @property\n    def pattern_manager(self) -&gt; \"PatternManager\":\n        \"\"\"\n        Lazy initialization of the PatternManager instance.\n\n        Returns:\n            PatternManager: The wrapped PatternManager instance\n\n        Raises:\n            RuntimeError: If PATTERN_REPO is not properly configured\n        \"\"\"\n        if self._pattern_manager is None:  # type: ignore\n            try:\n                load_dotenv()\n                if pattern_path_name := os.getenv(\"TNH_PATTERN_DIR\"):\n                    pattern_dir = Path(pattern_path_name)\n                    logger.debug(f\"pattern dir: {pattern_path_name}\")\n                else:\n                    pattern_dir = TNH_DEFAULT_PATTERN_DIR\n                self._pattern_manager = PatternManager(pattern_dir)\n            except ImportError as err:\n                raise RuntimeError(\n                    \"Failed to initialize PatternManager. Ensure pattern_manager \"\n                    f\"module and PATTERN_REPO are properly configured: {err}\"\n                ) from err\n        return self._pattern_manager\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LocalPatternManager.pattern_manager","title":"<code>pattern_manager</code>  <code>property</code>","text":"<p>Lazy initialization of the PatternManager instance.</p> <p>Returns:</p> Name Type Description <code>PatternManager</code> <code>PatternManager</code> <p>The wrapped PatternManager instance</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If PATTERN_REPO is not properly configured</p>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.LocalPatternManager.__new__","title":"<code>__new__()</code>","text":"<p>Create or return the singleton instance.</p> <p>Returns:</p> Name Type Description <code>SingletonPatternManager</code> <code>LocalPatternManager</code> <p>The singleton instance</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __new__(cls) -&gt; \"LocalPatternManager\":\n    \"\"\"\n    Create or return the singleton instance.\n\n    Returns:\n        SingletonPatternManager: The singleton instance\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n        cls._instance._pattern_manager = None\n    return cls._instance\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.OpenAIProcessor","title":"<code>OpenAIProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>OpenAI-based text processor implementation.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class OpenAIProcessor(TextProcessor):\n    \"\"\"OpenAI-based text processor implementation.\"\"\"\n\n    def __init__(self, model: Optional[str] = None, max_tokens: int = 0):\n        if not model:\n            model = DEFAULT_OPENAI_MODEL\n        self.model = model\n        self.max_tokens = max_tokens\n\n    def process_text(\n        self,\n        text: str,\n        instructions: str,\n        response_format: Optional[Type[ResponseFormat]] = None,\n        max_tokens: int = 0,\n        **kwargs,\n    ) -&gt; Union[str, ResponseFormat]:\n        \"\"\"Process text using OpenAI API with optional structured output.\"\"\"\n\n        if max_tokens == 0 and self.max_tokens &gt; 0:\n            max_tokens = self.max_tokens\n\n        return openai_process_text(\n            text,\n            instructions,\n            model=self.model,\n            max_tokens=max_tokens,\n            response_format=response_format,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.OpenAIProcessor.max_tokens","title":"<code>max_tokens = max_tokens</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.OpenAIProcessor.model","title":"<code>model = model</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.OpenAIProcessor.__init__","title":"<code>__init__(model=None, max_tokens=0)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(self, model: Optional[str] = None, max_tokens: int = 0):\n    if not model:\n        model = DEFAULT_OPENAI_MODEL\n    self.model = model\n    self.max_tokens = max_tokens\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.OpenAIProcessor.process_text","title":"<code>process_text(text, instructions, response_format=None, max_tokens=0, **kwargs)</code>","text":"<p>Process text using OpenAI API with optional structured output.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text(\n    self,\n    text: str,\n    instructions: str,\n    response_format: Optional[Type[ResponseFormat]] = None,\n    max_tokens: int = 0,\n    **kwargs,\n) -&gt; Union[str, ResponseFormat]:\n    \"\"\"Process text using OpenAI API with optional structured output.\"\"\"\n\n    if max_tokens == 0 and self.max_tokens &gt; 0:\n        max_tokens = self.max_tokens\n\n    return openai_process_text(\n        text,\n        instructions,\n        model=self.model,\n        max_tokens=max_tokens,\n        response_format=response_format,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection","title":"<code>ProcessedSection</code>  <code>dataclass</code>","text":"<p>Represents a processed section of text with its metadata.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>@dataclass\nclass ProcessedSection:\n    \"\"\"Represents a processed section of text with its metadata.\"\"\"\n\n    title: str\n    original_text: str\n    processed_text: str\n    start_line: int\n    end_line: int\n    metadata: Dict = field(default_factory=dict)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.end_line","title":"<code>end_line</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.metadata","title":"<code>metadata = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.original_text","title":"<code>original_text</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.processed_text","title":"<code>processed_text</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.start_line","title":"<code>start_line</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.title","title":"<code>title</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.__init__","title":"<code>__init__(title, original_text, processed_text, start_line, end_line, metadata=dict())</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser","title":"<code>SectionParser</code>","text":"<p>Generates structured section breakdowns of text content.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class SectionParser:\n    \"\"\"Generates structured section breakdowns of text content.\"\"\"\n\n    def __init__(\n        self,\n        section_scanner: TextProcessor,\n        section_pattern: Pattern,\n        review_count: int = DEFAULT_REVIEW_COUNT,\n    ):\n        \"\"\"\n        Initialize section generator.\n\n        Args:\n            processor: Implementation of TextProcessor\n            pattern: Pattern object containing section generation instructions\n            max_tokens: Maximum tokens for response\n            section_count: Target number of sections\n            review_count: Number of review passes\n        \"\"\"\n        self.section_scanner = section_scanner\n        self.section_pattern = section_pattern\n        self.review_count = review_count\n\n    def find_sections(\n        self,\n        text: str,\n        source_language: Optional[str] = None,\n        section_count_target: Optional[int] = None,\n        segment_size_target: Optional[int] = None,\n        template_dict: Optional[Dict[str, str]] = None,\n    ) -&gt; TextObject:\n        \"\"\"\n        Generate section breakdown of input text. The text must be split up by newlines.\n\n        Args:\n            text: Input text to process\n            source_language: ISO 639-1 language code, or None for autodetection\n            section_count_target: the target for the number of sections to find\n            segment_size_target: the target for the number of lines per section\n                (if section_count_target is specified, this value will be set to generate correct segments)\n            template_dict: Optional additional template variables\n\n        Returns:\n            TextObject containing section breakdown\n\n        Raises:\n            ValidationError: If response doesn't match TextObject schema\n        \"\"\"\n\n        # Prepare numbered text, each line is numbered\n        num_text = NumberedText(text)\n\n        if num_text.size &lt; SECTION_SEGMENT_SIZE_WARNING_LIMIT:\n            logger.warning(\n                f\"find_sections: Text has only {num_text.size} lines. This may lead to unexpected sectioning results.\"\n            )\n\n        # Get language if not specified\n        if not source_language:\n            source_language = get_language_name(text)\n\n        # determine section count if not specified\n        if not section_count_target:\n            segment_size_target, section_count_target = self._get_section_count_info(\n                text\n            )\n        elif not segment_size_target:\n            segment_size_target = round(num_text.size / section_count_target)\n\n        section_count_range = self._get_section_count_range(section_count_target)\n\n        # Prepare template variables\n        template_values = {\n            \"source_language\": source_language,\n            \"section_count\": section_count_range,\n            \"line_count\": segment_size_target,\n            \"review_count\": self.review_count,\n        }\n\n        if template_dict:\n            template_values |= template_dict\n\n        # Get and apply processing instructions\n        instructions = self.section_pattern.apply_template(template_values)\n        logger.debug(f\"Finding sections with pattern instructions:\\n {instructions}\")\n\n        logger.info(\n            f\"Finding sections for {source_language} text \"\n            f\"(target sections: {section_count_target})\"\n        )\n\n        # Process text with structured output\n        try:\n            result = self.section_scanner.process_text(\n                str(num_text), instructions, response_format=TextObject\n            )\n\n            # Validate section coverage\n            self._validate_sections(result.sections, num_text.size)\n\n            return result\n\n        except Exception as e:\n            logger.error(f\"Section generation failed: {e}\")\n            raise\n\n    def _get_section_count_info(self, text: str) -&gt; Tuple[int, int]:\n        num_text = NumberedText(text)\n        segment_size = _calculate_segment_size(num_text, DEFAULT_SECTION_TOKEN_SIZE)\n        section_count_target = round(num_text.size / segment_size)\n        return segment_size, section_count_target\n\n    def _get_section_count_range(\n        self,\n        section_count_target: int,\n        section_range_var: int = DEFAULT_SECTION_RANGE_VAR,\n    ) -&gt; str:\n        low = max(1, section_count_target - section_range_var)\n        high = section_count_target + section_range_var\n        return f\"{low}-{high}\"\n\n    def _validate_sections(\n        self, sections: List[LogicalSection], total_lines: int\n    ) -&gt; None:\n        \"\"\"\n        Validate section line coverage and ordering. Issues warnings for validation problems\n        instead of raising errors.\n\n        Args:\n            sections: List of generated sections\n            text: Original text\n        \"\"\"\n\n        covered_lines = set()\n        last_end = -1\n\n        for section in sections:\n            # Check line ordering\n            if section.start_line &lt;= last_end:\n                logger.warning(\n                    f\"Section lines should be sequential but found overlap: \"\n                    f\"section starting at {section.start_line} begins before or at \"\n                    f\"previous section end {last_end}\"\n                )\n\n            # Track line coverage\n            section_lines = set(range(section.start_line, section.end_line + 1))\n            if section_lines &amp; covered_lines:\n                logger.warning(\n                    f\"Found overlapping lines in section '{section.title_en}'. \"\n                    f\"Each line should belong to exactly one section.\"\n                )\n            covered_lines.update(section_lines)\n\n            last_end = section.end_line\n\n        # Check complete coverage\n        expected_lines = set(range(1, total_lines + 1))\n        if covered_lines != expected_lines:\n            missing = sorted(list(expected_lines - covered_lines))\n            logger.warning(\n                f\"Not all lines are covered by sections. \"\n                f\"Missing line numbers: {missing}\"\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser.review_count","title":"<code>review_count = review_count</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser.section_pattern","title":"<code>section_pattern = section_pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser.section_scanner","title":"<code>section_scanner = section_scanner</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser.__init__","title":"<code>__init__(section_scanner, section_pattern, review_count=DEFAULT_REVIEW_COUNT)</code>","text":"<p>Initialize section generator.</p> <p>Parameters:</p> Name Type Description Default <code>processor</code> <p>Implementation of TextProcessor</p> required <code>pattern</code> <p>Pattern object containing section generation instructions</p> required <code>max_tokens</code> <p>Maximum tokens for response</p> required <code>section_count</code> <p>Target number of sections</p> required <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(\n    self,\n    section_scanner: TextProcessor,\n    section_pattern: Pattern,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n):\n    \"\"\"\n    Initialize section generator.\n\n    Args:\n        processor: Implementation of TextProcessor\n        pattern: Pattern object containing section generation instructions\n        max_tokens: Maximum tokens for response\n        section_count: Target number of sections\n        review_count: Number of review passes\n    \"\"\"\n    self.section_scanner = section_scanner\n    self.section_pattern = section_pattern\n    self.review_count = review_count\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser.find_sections","title":"<code>find_sections(text, source_language=None, section_count_target=None, segment_size_target=None, template_dict=None)</code>","text":"<p>Generate section breakdown of input text. The text must be split up by newlines.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to process</p> required <code>source_language</code> <code>Optional[str]</code> <p>ISO 639-1 language code, or None for autodetection</p> <code>None</code> <code>section_count_target</code> <code>Optional[int]</code> <p>the target for the number of sections to find</p> <code>None</code> <code>segment_size_target</code> <code>Optional[int]</code> <p>the target for the number of lines per section (if section_count_target is specified, this value will be set to generate correct segments)</p> <code>None</code> <code>template_dict</code> <code>Optional[Dict[str, str]]</code> <p>Optional additional template variables</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject containing section breakdown</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If response doesn't match TextObject schema</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def find_sections(\n    self,\n    text: str,\n    source_language: Optional[str] = None,\n    section_count_target: Optional[int] = None,\n    segment_size_target: Optional[int] = None,\n    template_dict: Optional[Dict[str, str]] = None,\n) -&gt; TextObject:\n    \"\"\"\n    Generate section breakdown of input text. The text must be split up by newlines.\n\n    Args:\n        text: Input text to process\n        source_language: ISO 639-1 language code, or None for autodetection\n        section_count_target: the target for the number of sections to find\n        segment_size_target: the target for the number of lines per section\n            (if section_count_target is specified, this value will be set to generate correct segments)\n        template_dict: Optional additional template variables\n\n    Returns:\n        TextObject containing section breakdown\n\n    Raises:\n        ValidationError: If response doesn't match TextObject schema\n    \"\"\"\n\n    # Prepare numbered text, each line is numbered\n    num_text = NumberedText(text)\n\n    if num_text.size &lt; SECTION_SEGMENT_SIZE_WARNING_LIMIT:\n        logger.warning(\n            f\"find_sections: Text has only {num_text.size} lines. This may lead to unexpected sectioning results.\"\n        )\n\n    # Get language if not specified\n    if not source_language:\n        source_language = get_language_name(text)\n\n    # determine section count if not specified\n    if not section_count_target:\n        segment_size_target, section_count_target = self._get_section_count_info(\n            text\n        )\n    elif not segment_size_target:\n        segment_size_target = round(num_text.size / section_count_target)\n\n    section_count_range = self._get_section_count_range(section_count_target)\n\n    # Prepare template variables\n    template_values = {\n        \"source_language\": source_language,\n        \"section_count\": section_count_range,\n        \"line_count\": segment_size_target,\n        \"review_count\": self.review_count,\n    }\n\n    if template_dict:\n        template_values |= template_dict\n\n    # Get and apply processing instructions\n    instructions = self.section_pattern.apply_template(template_values)\n    logger.debug(f\"Finding sections with pattern instructions:\\n {instructions}\")\n\n    logger.info(\n        f\"Finding sections for {source_language} text \"\n        f\"(target sections: {section_count_target})\"\n    )\n\n    # Process text with structured output\n    try:\n        result = self.section_scanner.process_text(\n            str(num_text), instructions, response_format=TextObject\n        )\n\n        # Validate section coverage\n        self._validate_sections(result.sections, num_text.size)\n\n        return result\n\n    except Exception as e:\n        logger.error(f\"Section generation failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor","title":"<code>SectionProcessor</code>","text":"<p>Handles section-based XML text processing with configurable output handling.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class SectionProcessor:\n    \"\"\"Handles section-based XML text processing with configurable output handling.\"\"\"\n\n    def __init__(\n        self,\n        processor: TextProcessor,\n        pattern: Pattern,\n        template_dict: Dict,\n        wrap_in_document: bool = True,\n    ):\n        \"\"\"\n        Initialize the XML section processor.\n\n        Args:\n            processor: Implementation of TextProcessor to use\n            pattern: Pattern object containing processing instructions\n            template_dict: Dictionary for template substitution\n            wrap_in_document: Whether to wrap output in &lt;document&gt; tags\n        \"\"\"\n        self.processor = processor\n        self.pattern = pattern\n        self.template_dict = template_dict\n        self.wrap_in_document = wrap_in_document\n\n    def process_sections(\n        self,\n        transcript: str,\n        text_object: TextObject,\n    ) -&gt; Generator[ProcessedSection, None, None]:\n        \"\"\"\n        Process transcript sections and yield results one section at a time.\n\n        Args:\n            transcript: Text to process\n            text_object: Object containing section definitions\n\n        Yields:\n            ProcessedSection: One processed section at a time, containing:\n                - title: Section title (English or original language)\n                - original_text: Raw text segment\n                - processed_text: Processed text content\n                - start_line: Starting line number\n                - end_line: Ending line number\n        \"\"\"\n        numbered_transcript = NumberedText(transcript)\n        sections = text_object.sections\n\n        logger.info(\n            f\"Processing {len(sections)} sections with pattern: {self.pattern.name}\"\n        )\n\n        for i, section in enumerate(sections, 1):\n            logger.info(f\"Processing section {i}, '{section.title}':\")\n\n            # Get text segment for section\n            text_segment = numbered_transcript.get_segment(\n                section.start_line, end=section.end_line\n            )\n\n            # Prepare template variables\n            template_values = {\n                \"section_title\": section.title,\n                \"source_language\": text_object.language,\n                \"review_count\": DEFAULT_REVIEW_COUNT,\n            }\n\n            if self.template_dict:\n                template_values |= self.template_dict\n\n            # Get and apply processing instructions\n            instructions = self.pattern.apply_template(template_values)\n            if i &lt;= 1:\n                logger.debug(f\"Process instructions (first section):\\n{instructions}\")\n            processed_text = self.processor.process_text(text_segment, instructions)\n\n            yield ProcessedSection(\n                title=section.title,\n                original_text=text_segment,\n                processed_text=processed_text,\n                start_line=section.start_line,\n                end_line=section.end_line,\n            )\n\n    def process_paragraphs(\n        self,\n        transcript: str,\n    ) -&gt; Generator[str, None, None]:\n        \"\"\"\n        Process transcript by paragraphs (as sections) where paragraphs are assumed to be given as newline separated.\n\n        Args:\n            transcript: Text to process\n\n        Returns:\n            Generator of lines\n\n        Yields:\n            Processed lines as strings\n        \"\"\"\n        numbered_transcript = NumberedText(transcript)\n\n        logger.info(f\"Processing lines as paragraphs with pattern: {self.pattern.name}\")\n\n        for i, line in numbered_transcript:\n\n            # If line is empty or whitespace, continue\n            if not line.strip():\n                continue\n\n            # Otherwise get and apply processing instructions\n            instructions = self.pattern.apply_template(self.template_dict)\n\n            if i &lt;= 1:\n                logger.debug(f\"Process instructions (first paragraph):\\n{instructions}\")\n            yield self.processor.process_text(line, instructions)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.pattern","title":"<code>pattern = pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.processor","title":"<code>processor = processor</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.template_dict","title":"<code>template_dict = template_dict</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.wrap_in_document","title":"<code>wrap_in_document = wrap_in_document</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.__init__","title":"<code>__init__(processor, pattern, template_dict, wrap_in_document=True)</code>","text":"<p>Initialize the XML section processor.</p> <p>Parameters:</p> Name Type Description Default <code>processor</code> <code>TextProcessor</code> <p>Implementation of TextProcessor to use</p> required <code>pattern</code> <code>Pattern</code> <p>Pattern object containing processing instructions</p> required <code>template_dict</code> <code>Dict</code> <p>Dictionary for template substitution</p> required <code>wrap_in_document</code> <code>bool</code> <p>Whether to wrap output in  tags <code>True</code> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(\n    self,\n    processor: TextProcessor,\n    pattern: Pattern,\n    template_dict: Dict,\n    wrap_in_document: bool = True,\n):\n    \"\"\"\n    Initialize the XML section processor.\n\n    Args:\n        processor: Implementation of TextProcessor to use\n        pattern: Pattern object containing processing instructions\n        template_dict: Dictionary for template substitution\n        wrap_in_document: Whether to wrap output in &lt;document&gt; tags\n    \"\"\"\n    self.processor = processor\n    self.pattern = pattern\n    self.template_dict = template_dict\n    self.wrap_in_document = wrap_in_document\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.process_paragraphs","title":"<code>process_paragraphs(transcript)</code>","text":"<p>Process transcript by paragraphs (as sections) where paragraphs are assumed to be given as newline separated.</p> <p>Parameters:</p> Name Type Description Default <code>transcript</code> <code>str</code> <p>Text to process</p> required <p>Returns:</p> Type Description <code>None</code> <p>Generator of lines</p> <p>Yields:</p> Type Description <code>str</code> <p>Processed lines as strings</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_paragraphs(\n    self,\n    transcript: str,\n) -&gt; Generator[str, None, None]:\n    \"\"\"\n    Process transcript by paragraphs (as sections) where paragraphs are assumed to be given as newline separated.\n\n    Args:\n        transcript: Text to process\n\n    Returns:\n        Generator of lines\n\n    Yields:\n        Processed lines as strings\n    \"\"\"\n    numbered_transcript = NumberedText(transcript)\n\n    logger.info(f\"Processing lines as paragraphs with pattern: {self.pattern.name}\")\n\n    for i, line in numbered_transcript:\n\n        # If line is empty or whitespace, continue\n        if not line.strip():\n            continue\n\n        # Otherwise get and apply processing instructions\n        instructions = self.pattern.apply_template(self.template_dict)\n\n        if i &lt;= 1:\n            logger.debug(f\"Process instructions (first paragraph):\\n{instructions}\")\n        yield self.processor.process_text(line, instructions)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.process_sections","title":"<code>process_sections(transcript, text_object)</code>","text":"<p>Process transcript sections and yield results one section at a time.</p> <p>Parameters:</p> Name Type Description Default <code>transcript</code> <code>str</code> <p>Text to process</p> required <code>text_object</code> <code>TextObject</code> <p>Object containing section definitions</p> required <p>Yields:</p> Name Type Description <code>ProcessedSection</code> <code>ProcessedSection</code> <p>One processed section at a time, containing: - title: Section title (English or original language) - original_text: Raw text segment - processed_text: Processed text content - start_line: Starting line number - end_line: Ending line number</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_sections(\n    self,\n    transcript: str,\n    text_object: TextObject,\n) -&gt; Generator[ProcessedSection, None, None]:\n    \"\"\"\n    Process transcript sections and yield results one section at a time.\n\n    Args:\n        transcript: Text to process\n        text_object: Object containing section definitions\n\n    Yields:\n        ProcessedSection: One processed section at a time, containing:\n            - title: Section title (English or original language)\n            - original_text: Raw text segment\n            - processed_text: Processed text content\n            - start_line: Starting line number\n            - end_line: Ending line number\n    \"\"\"\n    numbered_transcript = NumberedText(transcript)\n    sections = text_object.sections\n\n    logger.info(\n        f\"Processing {len(sections)} sections with pattern: {self.pattern.name}\"\n    )\n\n    for i, section in enumerate(sections, 1):\n        logger.info(f\"Processing section {i}, '{section.title}':\")\n\n        # Get text segment for section\n        text_segment = numbered_transcript.get_segment(\n            section.start_line, end=section.end_line\n        )\n\n        # Prepare template variables\n        template_values = {\n            \"section_title\": section.title,\n            \"source_language\": text_object.language,\n            \"review_count\": DEFAULT_REVIEW_COUNT,\n        }\n\n        if self.template_dict:\n            template_values |= self.template_dict\n\n        # Get and apply processing instructions\n        instructions = self.pattern.apply_template(template_values)\n        if i &lt;= 1:\n            logger.debug(f\"Process instructions (first section):\\n{instructions}\")\n        processed_text = self.processor.process_text(text_segment, instructions)\n\n        yield ProcessedSection(\n            title=section.title,\n            original_text=text_segment,\n            processed_text=processed_text,\n            start_line=section.start_line,\n            end_line=section.end_line,\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextProcessor","title":"<code>TextProcessor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text processors that can return Pydantic objects.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class TextProcessor(ABC):\n    \"\"\"Abstract base class for text processors that can return Pydantic objects.\"\"\"\n\n    @abstractmethod\n    def process_text(\n        self,\n        text: str,\n        instructions: str,\n        response_format: Optional[Type[ResponseFormat]] = None,\n        **kwargs,\n    ) -&gt; Union[str, ResponseFormat]:\n        \"\"\"\n        Process text according to instructions.\n\n        Args:\n            text: Input text to process\n            instructions: Processing instructions\n            response_object: Optional Pydantic class for structured output\n            **kwargs: Additional processing parameters\n\n        Returns:\n            Either string or Pydantic model instance based on response_model\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextProcessor.process_text","title":"<code>process_text(text, instructions, response_format=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Process text according to instructions.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to process</p> required <code>instructions</code> <code>str</code> <p>Processing instructions</p> required <code>response_object</code> <p>Optional Pydantic class for structured output</p> required <code>**kwargs</code> <p>Additional processing parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, ResponseFormat]</code> <p>Either string or Pydantic model instance based on response_model</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>@abstractmethod\ndef process_text(\n    self,\n    text: str,\n    instructions: str,\n    response_format: Optional[Type[ResponseFormat]] = None,\n    **kwargs,\n) -&gt; Union[str, ResponseFormat]:\n    \"\"\"\n    Process text according to instructions.\n\n    Args:\n        text: Input text to process\n        instructions: Processing instructions\n        response_object: Optional Pydantic class for structured output\n        **kwargs: Additional processing parameters\n\n    Returns:\n        Either string or Pydantic model instance based on response_model\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextPunctuator","title":"<code>TextPunctuator</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class TextPunctuator:\n    def __init__(\n        self,\n        processor: TextProcessor,\n        punctuate_pattern: Pattern,\n        source_language: Optional[str] = None,\n        review_count: int = DEFAULT_REVIEW_COUNT,\n        style_convention=DEFAULT_PUNCTUATE_STYLE,\n    ):\n        \"\"\"\n        Initialize punctuation generator.\n\n        Args:\n            text_punctuator: Implementation of TextProcessor\n            punctuate_pattern: Pattern object containing punctuation instructions\n            section_count: Target number of sections\n            review_count: Number of review passes\n        \"\"\"\n\n        self.source_language = source_language\n        self.processor = processor\n        self.punctuate_pattern = punctuate_pattern\n        self.review_count = review_count\n        self.style_convention = style_convention\n\n    def punctuate_text(\n        self,\n        text: str,\n        source_language: Optional[str] = None,\n        template_dict: Optional[Dict] = None,\n    ) -&gt; str:\n        \"\"\"\n        punctuate a text based on a pattern and source language.\n        \"\"\"\n\n        if not source_language:\n            if self.source_language:\n                source_language = self.source_language\n            else:\n                source_language = get_language_name(text)\n\n        template_values = {\n            \"source_language\": source_language,\n            \"review_count\": self.review_count,\n            \"style_convention\": self.style_convention,\n        }\n\n        if template_dict:\n            template_values |= template_dict\n\n        logger.info(\"Punctuating text...\")\n        punctuate_instructions = self.punctuate_pattern.apply_template(template_values)\n        text = self.processor.process_text(text, punctuate_instructions)\n        logger.info(\"Punctuation completed.\")\n\n        # normalize newline spacing to two newline (default) between lines and return\n        # commented out to allow pattern to dictate newlines.\n        # return normalize_newlines(text)\n        return text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextPunctuator.processor","title":"<code>processor = processor</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextPunctuator.punctuate_pattern","title":"<code>punctuate_pattern = punctuate_pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextPunctuator.review_count","title":"<code>review_count = review_count</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextPunctuator.source_language","title":"<code>source_language = source_language</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextPunctuator.style_convention","title":"<code>style_convention = style_convention</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextPunctuator.__init__","title":"<code>__init__(processor, punctuate_pattern, source_language=None, review_count=DEFAULT_REVIEW_COUNT, style_convention=DEFAULT_PUNCTUATE_STYLE)</code>","text":"<p>Initialize punctuation generator.</p> <p>Parameters:</p> Name Type Description Default <code>text_punctuator</code> <p>Implementation of TextProcessor</p> required <code>punctuate_pattern</code> <code>Pattern</code> <p>Pattern object containing punctuation instructions</p> required <code>section_count</code> <p>Target number of sections</p> required <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(\n    self,\n    processor: TextProcessor,\n    punctuate_pattern: Pattern,\n    source_language: Optional[str] = None,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n    style_convention=DEFAULT_PUNCTUATE_STYLE,\n):\n    \"\"\"\n    Initialize punctuation generator.\n\n    Args:\n        text_punctuator: Implementation of TextProcessor\n        punctuate_pattern: Pattern object containing punctuation instructions\n        section_count: Target number of sections\n        review_count: Number of review passes\n    \"\"\"\n\n    self.source_language = source_language\n    self.processor = processor\n    self.punctuate_pattern = punctuate_pattern\n    self.review_count = review_count\n    self.style_convention = style_convention\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextPunctuator.punctuate_text","title":"<code>punctuate_text(text, source_language=None, template_dict=None)</code>","text":"<p>punctuate a text based on a pattern and source language.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def punctuate_text(\n    self,\n    text: str,\n    source_language: Optional[str] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; str:\n    \"\"\"\n    punctuate a text based on a pattern and source language.\n    \"\"\"\n\n    if not source_language:\n        if self.source_language:\n            source_language = self.source_language\n        else:\n            source_language = get_language_name(text)\n\n    template_values = {\n        \"source_language\": source_language,\n        \"review_count\": self.review_count,\n        \"style_convention\": self.style_convention,\n    }\n\n    if template_dict:\n        template_values |= template_dict\n\n    logger.info(\"Punctuating text...\")\n    punctuate_instructions = self.punctuate_pattern.apply_template(template_values)\n    text = self.processor.process_text(text, punctuate_instructions)\n    logger.info(\"Punctuation completed.\")\n\n    # normalize newline spacing to two newline (default) between lines and return\n    # commented out to allow pattern to dictate newlines.\n    # return normalize_newlines(text)\n    return text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.find_sections","title":"<code>find_sections(text, source_language=None, section_pattern=None, section_model=None, max_tokens=DEFAULT_SECTION_RESULT_MAX_SIZE, section_count=None, review_count=DEFAULT_REVIEW_COUNT, template_dict=None)</code>","text":"<p>High-level function for generating text sections.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <code>source_language</code> <code>Optional[str]</code> <p>ISO 639-1 language code</p> <code>None</code> <code>pattern</code> <p>Optional custom pattern (uses default if None)</p> required <code>model</code> <p>Optional model identifier</p> required <code>max_tokens</code> <code>int</code> <p>Maximum tokens for response</p> <code>DEFAULT_SECTION_RESULT_MAX_SIZE</code> <code>section_count</code> <code>Optional[int]</code> <p>Target number of sections</p> <code>None</code> <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> <code>template_dict</code> <code>Optional[Dict[str, str]]</code> <p>Optional additional template variables</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject containing section breakdown</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def find_sections(\n    text: str,\n    source_language: Optional[str] = None,\n    section_pattern: Optional[Pattern] = None,\n    section_model: Optional[str] = None,\n    max_tokens: int = DEFAULT_SECTION_RESULT_MAX_SIZE,\n    section_count: Optional[int] = None,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n    template_dict: Optional[Dict[str, str]] = None,\n) -&gt; TextObject:\n    \"\"\"\n    High-level function for generating text sections.\n\n    Args:\n        text: Input text\n        source_language: ISO 639-1 language code\n        pattern: Optional custom pattern (uses default if None)\n        model: Optional model identifier\n        max_tokens: Maximum tokens for response\n        section_count: Target number of sections\n        review_count: Number of review passes\n        template_dict: Optional additional template variables\n\n    Returns:\n        TextObject containing section breakdown\n    \"\"\"\n    if section_pattern is None:\n        section_pattern = get_default_pattern(DEFAULT_SECTION_PATTERN)\n        logger.debug(f\"Using default section pattern: {DEFAULT_SECTION_PATTERN}.\")\n\n    if source_language is None:\n        source_language = get_language_name(text)\n\n    section_scanner = OpenAIProcessor(model=section_model, max_tokens=max_tokens)\n    parser = SectionParser(\n        section_scanner=section_scanner,\n        section_pattern=section_pattern,\n        review_count=review_count,\n    )\n\n    return parser.find_sections(\n        text,\n        source_language=source_language,\n        section_count_target=section_count,\n        template_dict=template_dict,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.get_default_pattern","title":"<code>get_default_pattern(name)</code>","text":"<p>Get a pattern by name using the singleton PatternManager.</p> <p>This is a more efficient version that reuses a single PatternManager instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pattern to load</p> required <p>Returns:</p> Type Description <code>Pattern</code> <p>The loaded pattern</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If pattern name is invalid</p> <code>FileNotFoundError</code> <p>If pattern file doesn't exist</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def get_default_pattern(name: str) -&gt; Pattern:\n    \"\"\"\n    Get a pattern by name using the singleton PatternManager.\n\n    This is a more efficient version that reuses a single PatternManager instance.\n\n    Args:\n        name: Name of the pattern to load\n\n    Returns:\n        The loaded pattern\n\n    Raises:\n        ValueError: If pattern name is invalid\n        FileNotFoundError: If pattern file doesn't exist\n    \"\"\"\n    return LocalPatternManager().pattern_manager.load_pattern(name)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.process_text","title":"<code>process_text(text, pattern, source_language=None, model=None, template_dict=None)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text(\n    text: str,\n    pattern: Pattern,\n    source_language: Optional[str] = None,\n    model: Optional[str] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; str:\n\n    if not model:\n        model = DEFAULT_OPENAI_MODEL\n\n    processor = GeneralProcessor(\n        processor=OpenAIProcessor(model),\n        source_language=source_language,\n        pattern=pattern,\n    )\n\n    return processor.process_text(\n        text, source_language=source_language, template_dict=template_dict\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.process_text_by_paragraphs","title":"<code>process_text_by_paragraphs(transcript, template_dict, pattern=None, model=None)</code>","text":"<p>High-level function for processing text paragraphs. Assumes paragraphs are separated by newlines. Uses DEFAULT_XML_FORMAT_PATTERN as default pattern for text processing.</p> <p>Parameters:</p> Name Type Description Default <code>transcript</code> <code>str</code> <p>Text to process</p> required <code>pattern</code> <code>Optional[Pattern]</code> <p>Pattern object containing processing instructions</p> <code>None</code> <code>template_dict</code> <code>Dict[str, str]</code> <p>Dictionary for template substitution</p> required <code>model</code> <code>Optional[str]</code> <p>Optional model identifier for processor</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Generator for ProcessedSections</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text_by_paragraphs(\n    transcript: str,\n    template_dict: Dict[str, str],\n    pattern: Optional[Pattern] = None,\n    model: Optional[str] = None,\n) -&gt; Generator[str, None, None]:\n    \"\"\"\n    High-level function for processing text paragraphs. Assumes paragraphs are separated by newlines.\n    Uses DEFAULT_XML_FORMAT_PATTERN as default pattern for text processing.\n\n    Args:\n        transcript: Text to process\n        pattern: Pattern object containing processing instructions\n        template_dict: Dictionary for template substitution\n        model: Optional model identifier for processor\n\n\n    Returns:\n        Generator for ProcessedSections\n    \"\"\"\n    processor = OpenAIProcessor(model)\n\n    if not pattern:\n        pattern = get_default_pattern(DEFAULT_PARAGRAPH_FORMAT_PATTERN)\n\n    section_processor = SectionProcessor(processor, pattern, template_dict)\n\n    return section_processor.process_paragraphs(transcript)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.process_text_by_sections","title":"<code>process_text_by_sections(transcript, text_object, template_dict, pattern=None, model=None)</code>","text":"<p>High-level function for processing text sections with configurable output handling.</p> <p>Parameters:</p> Name Type Description Default <code>transcript</code> <code>str</code> <p>Text to process</p> required <code>text_object</code> <code>TextObject</code> <p>Object containing section definitions</p> required <code>pattern</code> <code>Optional[Pattern]</code> <p>Pattern object containing processing instructions</p> <code>None</code> <code>template_dict</code> <code>Dict</code> <p>Dictionary for template substitution</p> required <code>model</code> <code>Optional[str]</code> <p>Optional model identifier for processor</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Generator for ProcessedSections</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text_by_sections(\n    transcript: str,\n    text_object: TextObject,\n    template_dict: Dict,\n    pattern: Optional[Pattern] = None,\n    model: Optional[str] = None,\n) -&gt; Generator[ProcessedSection, None, None]:\n    \"\"\"\n    High-level function for processing text sections with configurable output handling.\n\n    Args:\n        transcript: Text to process\n        text_object: Object containing section definitions\n        pattern: Pattern object containing processing instructions\n        template_dict: Dictionary for template substitution\n        model: Optional model identifier for processor\n\n    Returns:\n        Generator for ProcessedSections\n    \"\"\"\n    processor = OpenAIProcessor(model)\n\n    if not pattern:\n        pattern = get_default_pattern(DEFAULT_XML_FORMAT_PATTERN)\n\n    section_processor = SectionProcessor(processor, pattern, template_dict)\n\n    return section_processor.process_sections(\n        transcript,\n        text_object,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.punctuate_text","title":"<code>punctuate_text(text, source_language=None, punctuate_pattern=None, punctuate_model=None, template_dict=None)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def punctuate_text(\n    text,\n    source_language: Optional[str] = None,\n    punctuate_pattern: Optional[Pattern] = None,\n    punctuate_model: Optional[str] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; str:\n\n    if not punctuate_model:\n        punctuate_model = DEFAULT_PUNCTUATE_MODEL\n\n    if not punctuate_pattern:\n        punctuate_pattern = get_default_pattern(DEFAULT_PUNCTUATE_PATTERN)\n\n    punctuator = TextPunctuator(\n        processor=OpenAIProcessor(punctuate_model),\n        source_language=source_language,\n        punctuate_pattern=punctuate_pattern,\n    )\n\n    return punctuator.punctuate_text(\n        text, source_language=source_language, template_dict=template_dict\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.translate_text_by_lines","title":"<code>translate_text_by_lines(text, source_language=None, target_language=None, pattern=None, model=None, style=None, segment_size=None, context_lines=None, review_count=None, template_dict=None)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def translate_text_by_lines(\n    text,\n    source_language: Optional[str] = None,\n    target_language: Optional[str] = None,\n    pattern: Optional[Pattern] = None,\n    model: Optional[str] = None,\n    style: Optional[str] = None,\n    segment_size: Optional[int] = None,\n    context_lines: Optional[int] = None,\n    review_count: Optional[int] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; str:\n\n    if pattern is None:\n        pattern = get_default_pattern(DEFAULT_TRANSLATION_PATTERN)\n\n    translator = LineTranslator(\n        processor=OpenAIProcessor(model),\n        pattern=pattern,\n        style=style or DEFAULT_TRANSLATE_STYLE,\n        context_lines=context_lines or DEFAULT_TRANSLATE_CONTEXT_LINES,\n        review_count=review_count or DEFAULT_REVIEW_COUNT,\n    )\n\n    return translator.translate_text(\n        text,\n        source_language=source_language,\n        target_language=target_language,\n        segment_size=segment_size,\n        template_dict=template_dict,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.openai_process_interface","title":"<code>openai_process_interface</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.openai_process_interface.TOKEN_BUFFER","title":"<code>TOKEN_BUFFER = 500</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.openai_process_interface.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.openai_process_interface.openai_process_text","title":"<code>openai_process_text(text_input, process_instructions, model=None, response_format=None, batch=False, max_tokens=0)</code>","text":"<p>postprocessing a transcription.</p> Source code in <code>src/tnh_scholar/ai_text_processing/openai_process_interface.py</code> <pre><code>def openai_process_text(\n    text_input: str,\n    process_instructions: str,\n    model: Optional[str] = None,\n    response_format: Optional[ResponseFormat] = None,\n    batch: bool = False,\n    max_tokens: int = 0,\n) -&gt; Union[ResponseFormat, str]:\n    \"\"\"postprocessing a transcription.\"\"\"\n\n    user_prompts = [text_input]\n    system_message = process_instructions\n\n    logger.debug(f\"OpenAI Process Text with process instructions:\\n{system_message}\")\n    if max_tokens == 0:\n        tokens = token_count(text_input)\n        max_tokens = tokens + TOKEN_BUFFER\n\n    model_name = model or \"default\"\n\n    logger.info(\n        f\"Open AI Text Processing{' as batch process' if batch else ''} with model '{model_name}' initiated. Requesting a maximum of {max_tokens} tokens.\"\n    )\n\n    if batch:\n        return _run_batch_process_text(\n            response_format, user_prompts, system_message, max_tokens\n        )\n    completion = run_immediate_completion_simple(\n        system_message,\n        text_input,\n        max_tokens=max_tokens,\n        response_format=response_format,\n    )\n    logger.debug(f\"Full completion:\\n{completion}\")\n    if response_format:\n        process_object = get_completion_object(completion)\n        logger.info(\"Processing completed.\")\n        return process_object\n    else:\n        process_text = get_completion_content(completion)\n        logger.info(\"Processing completed.\")\n        return process_text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns","title":"<code>patterns</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.MarkdownStr","title":"<code>MarkdownStr = NewType('MarkdownStr', str)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.SYSTEM_UPDATE_MESSAGE","title":"<code>SYSTEM_UPDATE_MESSAGE = 'PatternManager System Update:'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.ConcurrentAccessManager","title":"<code>ConcurrentAccessManager</code>","text":"<p>Manages concurrent access to pattern files.</p> <p>Provides: - File-level locking - Safe concurrent access patterns - Lock cleanup</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>class ConcurrentAccessManager:\n    \"\"\"\n    Manages concurrent access to pattern files.\n\n    Provides:\n    - File-level locking\n    - Safe concurrent access patterns\n    - Lock cleanup\n    \"\"\"\n\n    def __init__(self, lock_dir: Path):\n        \"\"\"\n        Initialize access manager.\n\n        Args:\n            lock_dir: Directory for lock files\n        \"\"\"\n        self.lock_dir = Path(lock_dir)\n        self._ensure_lock_dir()\n        self._cleanup_stale_locks()\n\n    def _ensure_lock_dir(self) -&gt; None:\n        \"\"\"Create lock directory if it doesn't exist.\"\"\"\n        self.lock_dir.mkdir(parents=True, exist_ok=True)\n\n    def _cleanup_stale_locks(self, max_age: timedelta = timedelta(hours=1)) -&gt; None:\n        \"\"\"\n        Remove stale lock files.\n\n        Args:\n            max_age: Maximum age for lock files before considered stale\n        \"\"\"\n        current_time = datetime.now()\n        for lock_file in self.lock_dir.glob(\"*.lock\"):\n            try:\n                mtime = datetime.fromtimestamp(lock_file.stat().st_mtime)\n                if current_time - mtime &gt; max_age:\n                    lock_file.unlink()\n                    logger.warning(f\"Removed stale lock file: {lock_file}\")\n            except FileNotFoundError:\n                # Lock was removed by another process\n                pass\n            except Exception as e:\n                logger.error(f\"Error cleaning up lock file {lock_file}: {e}\")\n\n    @contextmanager\n    def file_lock(self, file_path: Path):\n        \"\"\"\n        Context manager for safely accessing files.\n\n        Args:\n            file_path: Path to file to lock\n\n        Yields:\n            None when lock is acquired\n\n        Raises:\n            RuntimeError: If file is already locked\n            OSError: If lock file operations fail\n        \"\"\"\n        file_path = Path(file_path)\n        lock_file_path = self.lock_dir / f\"{file_path.stem}.lock\"\n        lock_fd = None\n\n        try:\n            # Open or create lock file\n            lock_fd = os.open(str(lock_file_path), os.O_WRONLY | os.O_CREAT)\n\n            try:\n                # Attempt to acquire lock\n                fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\n\n                # Write process info to lock file\n                pid = os.getpid()\n                timestamp = datetime.now().isoformat()\n                os.write(lock_fd, f\"{pid} {timestamp}\\n\".encode())\n\n                logger.debug(f\"Acquired lock for {file_path}\")\n                yield\n\n            except BlockingIOError as e:\n                raise RuntimeError(\n                    f\"File {file_path} is locked by another process\"\n                ) from e\n\n        except OSError as e:\n            logger.error(f\"Lock operation failed for {file_path}: {e}\")\n            raise\n\n        finally:\n            if lock_fd is not None:\n                try:\n                    # Release lock and close file descriptor\n                    fcntl.flock(lock_fd, fcntl.LOCK_UN)\n                    os.close(lock_fd)\n\n                    # Remove lock file\n                    lock_file_path.unlink(missing_ok=True)\n                    logger.debug(f\"Released lock for {file_path}\")\n\n                except Exception as e:\n                    logger.error(f\"Error cleaning up lock for {file_path}: {e}\")\n\n    def is_locked(self, file_path: Path) -&gt; bool:\n        \"\"\"\n        Check if a file is currently locked.\n\n        Args:\n            file_path: Path to file to check\n\n        Returns:\n            bool: True if file is locked\n        \"\"\"\n        lock_file_path = self.lock_dir / f\"{file_path.stem}.lock\"\n\n        if not lock_file_path.exists():\n            return False\n\n        try:\n            with open(lock_file_path, \"r\") as f:\n                # Try to acquire and immediately release lock\n                fcntl.flock(f, fcntl.LOCK_EX | fcntl.LOCK_NB)\n                fcntl.flock(f, fcntl.LOCK_UN)\n                return False\n        except BlockingIOError:\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.ConcurrentAccessManager.lock_dir","title":"<code>lock_dir = Path(lock_dir)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.ConcurrentAccessManager.__init__","title":"<code>__init__(lock_dir)</code>","text":"<p>Initialize access manager.</p> <p>Parameters:</p> Name Type Description Default <code>lock_dir</code> <code>Path</code> <p>Directory for lock files</p> required Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def __init__(self, lock_dir: Path):\n    \"\"\"\n    Initialize access manager.\n\n    Args:\n        lock_dir: Directory for lock files\n    \"\"\"\n    self.lock_dir = Path(lock_dir)\n    self._ensure_lock_dir()\n    self._cleanup_stale_locks()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.ConcurrentAccessManager.file_lock","title":"<code>file_lock(file_path)</code>","text":"<p>Context manager for safely accessing files.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file to lock</p> required <p>Yields:</p> Type Description <p>None when lock is acquired</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If file is already locked</p> <code>OSError</code> <p>If lock file operations fail</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>@contextmanager\ndef file_lock(self, file_path: Path):\n    \"\"\"\n    Context manager for safely accessing files.\n\n    Args:\n        file_path: Path to file to lock\n\n    Yields:\n        None when lock is acquired\n\n    Raises:\n        RuntimeError: If file is already locked\n        OSError: If lock file operations fail\n    \"\"\"\n    file_path = Path(file_path)\n    lock_file_path = self.lock_dir / f\"{file_path.stem}.lock\"\n    lock_fd = None\n\n    try:\n        # Open or create lock file\n        lock_fd = os.open(str(lock_file_path), os.O_WRONLY | os.O_CREAT)\n\n        try:\n            # Attempt to acquire lock\n            fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\n\n            # Write process info to lock file\n            pid = os.getpid()\n            timestamp = datetime.now().isoformat()\n            os.write(lock_fd, f\"{pid} {timestamp}\\n\".encode())\n\n            logger.debug(f\"Acquired lock for {file_path}\")\n            yield\n\n        except BlockingIOError as e:\n            raise RuntimeError(\n                f\"File {file_path} is locked by another process\"\n            ) from e\n\n    except OSError as e:\n        logger.error(f\"Lock operation failed for {file_path}: {e}\")\n        raise\n\n    finally:\n        if lock_fd is not None:\n            try:\n                # Release lock and close file descriptor\n                fcntl.flock(lock_fd, fcntl.LOCK_UN)\n                os.close(lock_fd)\n\n                # Remove lock file\n                lock_file_path.unlink(missing_ok=True)\n                logger.debug(f\"Released lock for {file_path}\")\n\n            except Exception as e:\n                logger.error(f\"Error cleaning up lock for {file_path}: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.ConcurrentAccessManager.is_locked","title":"<code>is_locked(file_path)</code>","text":"<p>Check if a file is currently locked.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if file is locked</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def is_locked(self, file_path: Path) -&gt; bool:\n    \"\"\"\n    Check if a file is currently locked.\n\n    Args:\n        file_path: Path to file to check\n\n    Returns:\n        bool: True if file is locked\n    \"\"\"\n    lock_file_path = self.lock_dir / f\"{file_path.stem}.lock\"\n\n    if not lock_file_path.exists():\n        return False\n\n    try:\n        with open(lock_file_path, \"r\") as f:\n            # Try to acquire and immediately release lock\n            fcntl.flock(f, fcntl.LOCK_EX | fcntl.LOCK_NB)\n            fcntl.flock(f, fcntl.LOCK_UN)\n            return False\n    except BlockingIOError:\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.GitBackedRepository","title":"<code>GitBackedRepository</code>","text":"<p>Manages versioned storage of patterns using Git.</p> <p>Provides basic Git operations while hiding complexity: - Automatic versioning of changes - Basic conflict resolution - History tracking</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>class GitBackedRepository:\n    \"\"\"\n    Manages versioned storage of patterns using Git.\n\n    Provides basic Git operations while hiding complexity:\n    - Automatic versioning of changes\n    - Basic conflict resolution\n    - History tracking\n    \"\"\"\n\n    def __init__(self, repo_path: Path):\n        \"\"\"\n        Initialize or connect to Git repository.\n\n        Args:\n            repo_path: Path to repository directory\n\n        Raises:\n            GitCommandError: If Git operations fail\n        \"\"\"\n        self.repo_path = repo_path\n\n        try:\n            # Try to connect to existing repository\n            self.repo = Repo(repo_path)\n            logger.debug(f\"Connected to existing Git repository at {repo_path}\")\n\n        except InvalidGitRepositoryError:\n            # Initialize new repository if none exists\n            logger.info(f\"Initializing new Git repository at {repo_path}\")\n            self.repo = Repo.init(repo_path)\n\n            # Create initial commit if repo is empty\n            if not self.repo.head.is_valid():\n                # Create and commit .gitignore\n                gitignore = repo_path / \".gitignore\"\n                gitignore.write_text(\"*.lock\\n.DS_Store\\n\")\n                self.repo.index.add([\".gitignore\"])\n                self.repo.index.commit(\"Initial repository setup\")\n\n    def update_file(self, file_path: Path) -&gt; str:\n        \"\"\"\n        Stage and commit changes to a file in the Git repository.\n\n        Args:\n            file_path: Absolute or relative path to the file.\n\n        Returns:\n            str: Commit hash if changes were made.\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n            ValueError: If the file is outside the repository.\n            GitCommandError: If Git operations fail.\n        \"\"\"\n        file_path = file_path.resolve()\n\n        # Ensure the file is within the repository\n        try:\n            rel_path = file_path.relative_to(self.repo_path)\n        except ValueError as e:\n            raise ValueError(\n                f\"File {file_path} is not under the repository root {self.repo_path}\"\n            ) from e\n\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File does not exist: {file_path}\")\n\n        try:\n            return self._commit_file_update(rel_path, file_path)\n        except GitCommandError as e:\n            logger.error(f\"Git operation failed: {e}\")\n            raise\n\n    def _commit_file_update(self, rel_path, file_path):\n        if self._is_file_clean(rel_path):\n            # Return the current commit hash if no changes\n            return self.repo.head.commit.hexsha\n\n        logger.info(f\"Detected changes in {rel_path}, updating version control.\")\n        self.repo.index.add([str(rel_path)])\n        commit = self.repo.index.commit(\n            f\"{SYSTEM_UPDATE_MESSAGE} {rel_path.stem}\",\n            author=Actor(\"PatternManager\", \"\"),\n        )\n        logger.info(f\"Committed changes to {file_path}: {commit.hexsha}\")\n        return commit.hexsha\n\n    def _get_file_revisions(self, file_path: Path) -&gt; List[Commit]:\n        \"\"\"\n        Get ordered list of commits that modified a file, most recent first.\n\n        Args:\n            file_path: Path to file relative to repository root\n\n        Returns:\n            List of Commit objects affecting this file\n\n        Raises:\n            GitCommandError: If Git operations fail\n        \"\"\"\n        rel_path = file_path.relative_to(self.repo_path)\n        try:\n            return list(self.repo.iter_commits(paths=str(rel_path)))\n        except GitCommandError as e:\n            logger.error(f\"Failed to get commits for {rel_path}: {e}\")\n            return []\n\n    def _get_commit_diff(\n        self, commit: Commit, file_path: Path, prev_commit: Optional[Commit] = None\n    ) -&gt; Tuple[str, str]:\n        \"\"\"\n        Get both stat and detailed diff for a commit.\n\n        Args:\n            commit: Commit to diff\n            file_path: Path relative to repository root\n            prev_commit: Previous commit for diff, defaults to commit's parent\n\n        Returns:\n            Tuple of (stat_diff, detailed_diff) where:\n                stat_diff: Summary of changes (files changed, insertions/deletions)\n                detailed_diff: Colored word-level diff with context\n\n        Raises:\n            GitCommandError: If Git operations fail\n        \"\"\"\n        prev_hash = prev_commit.hexsha if prev_commit else f\"{commit.hexsha}^\"\n        rel_path = file_path.relative_to(self.repo_path)\n\n        try:\n            # Get stats diff\n            stat = self.repo.git.diff(prev_hash, commit.hexsha, rel_path, stat=True)\n\n            # Get detailed diff\n            diff = self.repo.git.diff(\n                prev_hash,\n                commit.hexsha,\n                rel_path,\n                unified=2,\n                word_diff=\"plain\",\n                color=\"always\",\n                ignore_space_change=True,\n            )\n\n            return stat, diff\n        except GitCommandError as e:\n            logger.error(f\"Failed to get diff for {commit.hexsha}: {e}\")\n            return \"\", \"\"\n\n    def display_history(self, file_path: Path, max_versions: int = 0) -&gt; None:\n        \"\"\"\n        Display history of changes for a file with diffs between versions.\n\n        Shows most recent changes first, limited to max_versions entries.\n        For each change shows:\n        - Commit info and date\n        - Stats summary of changes\n        - Detailed color diff with 2 lines of context\n\n        Args:\n            file_path: Path to file in repository\n            max_versions: Maximum number of versions to show, if zero, shows all revisions.\n\n        Example:\n            &gt;&gt;&gt; repo.display_history(Path(\"patterns/format_dharma_talk.yaml\"))\n            Commit abc123def (2024-12-28 14:30:22):\n            1 file changed, 5 insertions(+), 2 deletions(-)\n\n            diff --git a/patterns/format_dharma_talk.yaml ...\n            ...\n        \"\"\"\n\n        try:\n            # Get commit history\n            commits = self._get_file_revisions(file_path)\n            if not commits:\n                print(f\"No history found for {file_path}\")\n                return\n\n            if max_versions == 0:\n                max_versions = len(commits)  # look at all commits.\n\n            # Display limited history with diffs\n            for i, commit in enumerate(commits[:max_versions]):\n                # Print commit header\n                date_str = commit.committed_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n                print(f\"\\nCommit {commit.hexsha[:8]} ({date_str}):\")\n                print(f\"Message: {commit.message.strip()}\")\n\n                # Get and display diffs\n                prev_commit = commits[i + 1] if i + 1 &lt; len(commits) else None\n                stat_diff, detailed_diff = self._get_commit_diff(\n                    commit, file_path, prev_commit\n                )\n\n                if stat_diff:\n                    print(\"\\nChanges:\")\n                    print(stat_diff)\n                if detailed_diff:\n                    print(\"\\nDetailed diff:\")\n                    print(detailed_diff)\n\n                print(\"\\033[0m\", end=\"\")\n                print(\"-\" * 80)  # Visual separator between commits\n\n        except Exception as e:\n            logger.error(f\"Failed to display history for {file_path}: {e}\")\n            print(f\"Error displaying history: {e}\")\n            raise\n\n    def _is_file_clean(self, rel_path: Path) -&gt; bool:\n        \"\"\"\n        Check if file has uncommitted changes.\n\n        Args:\n            rel_path: Path relative to repository root\n\n        Returns:\n            bool: True if file has no changes\n        \"\"\"\n        return str(rel_path) not in (\n            [item.a_path for item in self.repo.index.diff(None)]\n            + self.repo.untracked_files\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.GitBackedRepository.repo","title":"<code>repo = Repo(repo_path)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.GitBackedRepository.repo_path","title":"<code>repo_path = repo_path</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.GitBackedRepository.__init__","title":"<code>__init__(repo_path)</code>","text":"<p>Initialize or connect to Git repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to repository directory</p> required <p>Raises:</p> Type Description <code>GitCommandError</code> <p>If Git operations fail</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def __init__(self, repo_path: Path):\n    \"\"\"\n    Initialize or connect to Git repository.\n\n    Args:\n        repo_path: Path to repository directory\n\n    Raises:\n        GitCommandError: If Git operations fail\n    \"\"\"\n    self.repo_path = repo_path\n\n    try:\n        # Try to connect to existing repository\n        self.repo = Repo(repo_path)\n        logger.debug(f\"Connected to existing Git repository at {repo_path}\")\n\n    except InvalidGitRepositoryError:\n        # Initialize new repository if none exists\n        logger.info(f\"Initializing new Git repository at {repo_path}\")\n        self.repo = Repo.init(repo_path)\n\n        # Create initial commit if repo is empty\n        if not self.repo.head.is_valid():\n            # Create and commit .gitignore\n            gitignore = repo_path / \".gitignore\"\n            gitignore.write_text(\"*.lock\\n.DS_Store\\n\")\n            self.repo.index.add([\".gitignore\"])\n            self.repo.index.commit(\"Initial repository setup\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.GitBackedRepository.display_history","title":"<code>display_history(file_path, max_versions=0)</code>","text":"<p>Display history of changes for a file with diffs between versions.</p> <p>Shows most recent changes first, limited to max_versions entries. For each change shows: - Commit info and date - Stats summary of changes - Detailed color diff with 2 lines of context</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file in repository</p> required <code>max_versions</code> <code>int</code> <p>Maximum number of versions to show, if zero, shows all revisions.</p> <code>0</code> Example <p>repo.display_history(Path(\"patterns/format_dharma_talk.yaml\")) Commit abc123def (2024-12-28 14:30:22): 1 file changed, 5 insertions(+), 2 deletions(-)</p> <p>diff --git a/patterns/format_dharma_talk.yaml ... ...</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def display_history(self, file_path: Path, max_versions: int = 0) -&gt; None:\n    \"\"\"\n    Display history of changes for a file with diffs between versions.\n\n    Shows most recent changes first, limited to max_versions entries.\n    For each change shows:\n    - Commit info and date\n    - Stats summary of changes\n    - Detailed color diff with 2 lines of context\n\n    Args:\n        file_path: Path to file in repository\n        max_versions: Maximum number of versions to show, if zero, shows all revisions.\n\n    Example:\n        &gt;&gt;&gt; repo.display_history(Path(\"patterns/format_dharma_talk.yaml\"))\n        Commit abc123def (2024-12-28 14:30:22):\n        1 file changed, 5 insertions(+), 2 deletions(-)\n\n        diff --git a/patterns/format_dharma_talk.yaml ...\n        ...\n    \"\"\"\n\n    try:\n        # Get commit history\n        commits = self._get_file_revisions(file_path)\n        if not commits:\n            print(f\"No history found for {file_path}\")\n            return\n\n        if max_versions == 0:\n            max_versions = len(commits)  # look at all commits.\n\n        # Display limited history with diffs\n        for i, commit in enumerate(commits[:max_versions]):\n            # Print commit header\n            date_str = commit.committed_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n            print(f\"\\nCommit {commit.hexsha[:8]} ({date_str}):\")\n            print(f\"Message: {commit.message.strip()}\")\n\n            # Get and display diffs\n            prev_commit = commits[i + 1] if i + 1 &lt; len(commits) else None\n            stat_diff, detailed_diff = self._get_commit_diff(\n                commit, file_path, prev_commit\n            )\n\n            if stat_diff:\n                print(\"\\nChanges:\")\n                print(stat_diff)\n            if detailed_diff:\n                print(\"\\nDetailed diff:\")\n                print(detailed_diff)\n\n            print(\"\\033[0m\", end=\"\")\n            print(\"-\" * 80)  # Visual separator between commits\n\n    except Exception as e:\n        logger.error(f\"Failed to display history for {file_path}: {e}\")\n        print(f\"Error displaying history: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.GitBackedRepository.update_file","title":"<code>update_file(file_path)</code>","text":"<p>Stage and commit changes to a file in the Git repository.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Absolute or relative path to the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Commit hash if changes were made.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> <code>ValueError</code> <p>If the file is outside the repository.</p> <code>GitCommandError</code> <p>If Git operations fail.</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def update_file(self, file_path: Path) -&gt; str:\n    \"\"\"\n    Stage and commit changes to a file in the Git repository.\n\n    Args:\n        file_path: Absolute or relative path to the file.\n\n    Returns:\n        str: Commit hash if changes were made.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the file is outside the repository.\n        GitCommandError: If Git operations fail.\n    \"\"\"\n    file_path = file_path.resolve()\n\n    # Ensure the file is within the repository\n    try:\n        rel_path = file_path.relative_to(self.repo_path)\n    except ValueError as e:\n        raise ValueError(\n            f\"File {file_path} is not under the repository root {self.repo_path}\"\n        ) from e\n\n    if not file_path.exists():\n        raise FileNotFoundError(f\"File does not exist: {file_path}\")\n\n    try:\n        return self._commit_file_update(rel_path, file_path)\n    except GitCommandError as e:\n        logger.error(f\"Git operation failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern","title":"<code>Pattern</code>","text":"<p>Base Pattern class for version-controlled template patterns.</p> <p>Patterns contain: - Instructions: The main pattern instructions as a Jinja2 template.    Note: Instructions are intended to be saved in markdown format in a .md file. - Template fields: Default values for template variables - Metadata: Name and identifier information</p> <p>Version control is handled externally through Git, not in the pattern itself. Pattern identity is determined by the combination of identifiers.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the pattern</p> <code>instructions</code> <code>str</code> <p>The Jinja2 template string for this pattern</p> <code>default_template_fields</code> <code>Dict[str, str]</code> <p>Default values for template variables</p> <code>_allow_empty_vars</code> <code>bool</code> <p>Whether to allow undefined template variables</p> <code>_env</code> <code>Environment</code> <p>Configured Jinja2 environment instance</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>class Pattern:\n    \"\"\"\n    Base Pattern class for version-controlled template patterns.\n\n    Patterns contain:\n    - Instructions: The main pattern instructions as a Jinja2 template.\n       Note: Instructions are intended to be saved in markdown format in a .md file.\n    - Template fields: Default values for template variables\n    - Metadata: Name and identifier information\n\n    Version control is handled externally through Git, not in the pattern itself.\n    Pattern identity is determined by the combination of identifiers.\n\n    Attributes:\n        name (str): The name of the pattern\n        instructions (str): The Jinja2 template string for this pattern\n        default_template_fields (Dict[str, str]): Default values for template variables\n        _allow_empty_vars (bool): Whether to allow undefined template variables\n        _env (Environment): Configured Jinja2 environment instance\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        instructions: MarkdownStr,\n        default_template_fields: Optional[Dict[str, str]] = None,\n        allow_empty_vars: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a new Pattern instance.\n\n        Args:\n            name: Unique name identifying the pattern\n            instructions: Jinja2 template string containing the pattern\n            default_template_fields: Optional default values for template variables\n            allow_empty_vars: Whether to allow undefined template variables\n\n        Raises:\n            ValueError: If name or instructions are empty\n            TemplateError: If template syntax is invalid\n        \"\"\"\n        if not name or not instructions:\n            raise ValueError(\"Name and instructions must not be empty\")\n\n        self.name = name\n        self.instructions = instructions\n        self.default_template_fields = default_template_fields or {}\n        self._allow_empty_vars = allow_empty_vars\n        self._env = self._create_environment()\n\n        # Validate template syntax on initialization\n        self._validate_template()\n\n    @staticmethod\n    def _create_environment() -&gt; Environment:\n        \"\"\"\n        Create and configure a Jinja2 environment with optimal settings.\n\n        Returns:\n            Environment: Configured Jinja2 environment with security and formatting options\n        \"\"\"\n        return Environment(\n            undefined=StrictUndefined,  # Raise errors for undefined variables\n            trim_blocks=True,  # Remove first newline after a block\n            lstrip_blocks=True,  # Strip tabs and spaces from the start of lines\n            autoescape=True,  # Enable autoescaping for security\n        )\n\n    def _validate_template(self) -&gt; None:\n        \"\"\"\n        Validate the template syntax without rendering.\n\n        Raises:\n            TemplateError: If template syntax is invalid\n        \"\"\"\n        try:\n            self._env.parse(self.instructions)\n        except TemplateError as e:\n            raise TemplateError(\n                f\"Invalid template syntax in pattern '{self.name}': {str(e)}\"\n            ) from e\n\n    @lru_cache(maxsize=128)\n    def _get_template(self) -&gt; Template:\n        \"\"\"\n        Get or create a cached template instance.\n\n        Returns:\n            Template: Compiled Jinja2 template\n        \"\"\"\n        return self._env.from_string(self.instructions)\n\n    def apply_template(self, field_values: Optional[Dict[str, str]] = None) -&gt; str:\n        \"\"\"\n        Apply template values to pattern instructions using Jinja2.\n\n        Args:\n            field_values: Values to substitute into the template.\n                        If None, default_template_fields are used.\n\n        Returns:\n            str: Rendered instructions with template values applied.\n\n        Raises:\n            TemplateError: If template rendering fails\n            ValueError: If required template variables are missing\n        \"\"\"\n        # Combine default fields with provided fields, with provided taking precedence\n        template_values = {**self.default_template_fields, **(field_values or {})}\n\n        instructions = self.get_content_without_frontmatter()\n\n        try:\n            return self._render_template_with_values(instructions, template_values)\n        except TemplateError as e:\n            raise TemplateError(\n                f\"Template rendering failed for pattern '{self.name}': {str(e)}\"\n            ) from e\n\n    def _render_template_with_values(self, instructions, template_values):\n        # Parse template to find required variables\n\n        parsed_content = self._env.parse(instructions)\n        required_vars = find_undeclared_variables(parsed_content)\n\n        # Validate all required variables are provided\n        missing_vars = required_vars - set(template_values.keys())\n        if missing_vars and not self._allow_empty_vars:\n            raise ValueError(\n                f\"Missing required template variables in pattern '{self.name}': \"\n                f\"{', '.join(sorted(missing_vars))}\"\n            )\n\n        template = self._get_template()\n        return template.render(**template_values)\n\n    def extract_frontmatter(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Extract and validate YAML frontmatter from markdown instructions.\n\n        Returns:\n            Optional[Dict]: Frontmatter data if found and valid, None otherwise\n\n        Note:\n            Frontmatter must be at the very start of the file and properly formatted.\n        \"\"\"\n\n        # More precise pattern matching\n        pattern = r\"\\A---\\s*\\n(.*?)\\n---\\s*\\n\"\n        if match := re.match(pattern, self.instructions, re.DOTALL):\n            try:\n                frontmatter = yaml.safe_load(match[1])\n                if not isinstance(frontmatter, dict):\n                    logger.warning(\"Frontmatter must be a YAML dictionary\")\n                    return None\n                return frontmatter\n            except yaml.YAMLError as e:\n                logger.warning(f\"Invalid YAML in frontmatter: {e}\")\n                return None\n        return None\n\n    def get_content_without_frontmatter(self) -&gt; str:\n        \"\"\"\n        Get markdown content with frontmatter removed.\n\n        Returns:\n            str: Markdown content without frontmatter\n        \"\"\"\n        pattern = r\"\\A---\\s*\\n.*?\\n---\\s*\\n\"\n        return re.sub(pattern, \"\", self.instructions, flags=re.DOTALL)\n\n    def update_frontmatter(self, new_data: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Update or add frontmatter to the markdown content.\n\n        Args:\n            new_data: Dictionary of frontmatter fields to update\n        \"\"\"\n\n        current_frontmatter = self.extract_frontmatter() or {}\n        updated_frontmatter = {**current_frontmatter, **new_data}\n\n        # Create YAML string\n        yaml_str = yaml.dump(\n            updated_frontmatter, default_flow_style=False, allow_unicode=True\n        )\n\n        # Remove existing frontmatter if present\n        content = self.get_content_without_frontmatter()\n\n        # Combine new frontmatter with content\n        self.instructions = f\"---\\n{yaml_str}---\\n\\n{content}\"\n\n    def content_hash(self) -&gt; str:\n        \"\"\"\n        Generate a SHA-256 hash of the pattern content.\n\n        Useful for quick content comparison and change detection.\n\n        Returns:\n            str: Hexadecimal string of the SHA-256 hash\n        \"\"\"\n        content = f\"{self.name}{self.instructions}{sorted(self.default_template_fields.items())}\"\n        return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Convert pattern to dictionary for serialization.\n\n        Returns:\n            Dict containing all pattern data in serializable format\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"instructions\": self.instructions,\n            \"default_template_fields\": self.default_template_fields,\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"Pattern\":\n        \"\"\"\n        Create pattern instance from dictionary data.\n\n        Args:\n            data: Dictionary containing pattern data\n\n        Returns:\n            Pattern: New pattern instance\n\n        Raises:\n            ValueError: If required fields are missing\n        \"\"\"\n        required_fields = {\"name\", \"instructions\"}\n        if missing_fields := required_fields - set(data.keys()):\n            raise ValueError(f\"Missing required fields: {', '.join(missing_fields)}\")\n\n        return cls(\n            name=data[\"name\"],\n            instructions=data[\"instructions\"],\n            default_template_fields=data.get(\"default_template_fields\", {}),\n        )\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Compare patterns based on their content.\"\"\"\n        if not isinstance(other, Pattern):\n            return NotImplemented\n        return self.content_hash() == other.content_hash()\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Hash based on content hash for container operations.\"\"\"\n        return hash(self.content_hash())\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.default_template_fields","title":"<code>default_template_fields = default_template_fields or {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.instructions","title":"<code>instructions = instructions</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.name","title":"<code>name = name</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare patterns based on their content.</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Compare patterns based on their content.\"\"\"\n    if not isinstance(other, Pattern):\n        return NotImplemented\n    return self.content_hash() == other.content_hash()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash based on content hash for container operations.</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Hash based on content hash for container operations.\"\"\"\n    return hash(self.content_hash())\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.__init__","title":"<code>__init__(name, instructions, default_template_fields=None, allow_empty_vars=False)</code>","text":"<p>Initialize a new Pattern instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name identifying the pattern</p> required <code>instructions</code> <code>MarkdownStr</code> <p>Jinja2 template string containing the pattern</p> required <code>default_template_fields</code> <code>Optional[Dict[str, str]]</code> <p>Optional default values for template variables</p> <code>None</code> <code>allow_empty_vars</code> <code>bool</code> <p>Whether to allow undefined template variables</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If name or instructions are empty</p> <code>TemplateError</code> <p>If template syntax is invalid</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    instructions: MarkdownStr,\n    default_template_fields: Optional[Dict[str, str]] = None,\n    allow_empty_vars: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize a new Pattern instance.\n\n    Args:\n        name: Unique name identifying the pattern\n        instructions: Jinja2 template string containing the pattern\n        default_template_fields: Optional default values for template variables\n        allow_empty_vars: Whether to allow undefined template variables\n\n    Raises:\n        ValueError: If name or instructions are empty\n        TemplateError: If template syntax is invalid\n    \"\"\"\n    if not name or not instructions:\n        raise ValueError(\"Name and instructions must not be empty\")\n\n    self.name = name\n    self.instructions = instructions\n    self.default_template_fields = default_template_fields or {}\n    self._allow_empty_vars = allow_empty_vars\n    self._env = self._create_environment()\n\n    # Validate template syntax on initialization\n    self._validate_template()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.apply_template","title":"<code>apply_template(field_values=None)</code>","text":"<p>Apply template values to pattern instructions using Jinja2.</p> <p>Parameters:</p> Name Type Description Default <code>field_values</code> <code>Optional[Dict[str, str]]</code> <p>Values to substitute into the template.         If None, default_template_fields are used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered instructions with template values applied.</p> <p>Raises:</p> Type Description <code>TemplateError</code> <p>If template rendering fails</p> <code>ValueError</code> <p>If required template variables are missing</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def apply_template(self, field_values: Optional[Dict[str, str]] = None) -&gt; str:\n    \"\"\"\n    Apply template values to pattern instructions using Jinja2.\n\n    Args:\n        field_values: Values to substitute into the template.\n                    If None, default_template_fields are used.\n\n    Returns:\n        str: Rendered instructions with template values applied.\n\n    Raises:\n        TemplateError: If template rendering fails\n        ValueError: If required template variables are missing\n    \"\"\"\n    # Combine default fields with provided fields, with provided taking precedence\n    template_values = {**self.default_template_fields, **(field_values or {})}\n\n    instructions = self.get_content_without_frontmatter()\n\n    try:\n        return self._render_template_with_values(instructions, template_values)\n    except TemplateError as e:\n        raise TemplateError(\n            f\"Template rendering failed for pattern '{self.name}': {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.content_hash","title":"<code>content_hash()</code>","text":"<p>Generate a SHA-256 hash of the pattern content.</p> <p>Useful for quick content comparison and change detection.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Hexadecimal string of the SHA-256 hash</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def content_hash(self) -&gt; str:\n    \"\"\"\n    Generate a SHA-256 hash of the pattern content.\n\n    Useful for quick content comparison and change detection.\n\n    Returns:\n        str: Hexadecimal string of the SHA-256 hash\n    \"\"\"\n    content = f\"{self.name}{self.instructions}{sorted(self.default_template_fields.items())}\"\n    return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.extract_frontmatter","title":"<code>extract_frontmatter()</code>","text":"<p>Extract and validate YAML frontmatter from markdown instructions.</p> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict]: Frontmatter data if found and valid, None otherwise</p> Note <p>Frontmatter must be at the very start of the file and properly formatted.</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def extract_frontmatter(self) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract and validate YAML frontmatter from markdown instructions.\n\n    Returns:\n        Optional[Dict]: Frontmatter data if found and valid, None otherwise\n\n    Note:\n        Frontmatter must be at the very start of the file and properly formatted.\n    \"\"\"\n\n    # More precise pattern matching\n    pattern = r\"\\A---\\s*\\n(.*?)\\n---\\s*\\n\"\n    if match := re.match(pattern, self.instructions, re.DOTALL):\n        try:\n            frontmatter = yaml.safe_load(match[1])\n            if not isinstance(frontmatter, dict):\n                logger.warning(\"Frontmatter must be a YAML dictionary\")\n                return None\n            return frontmatter\n        except yaml.YAMLError as e:\n            logger.warning(f\"Invalid YAML in frontmatter: {e}\")\n            return None\n    return None\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create pattern instance from dictionary data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing pattern data</p> required <p>Returns:</p> Name Type Description <code>Pattern</code> <code>Pattern</code> <p>New pattern instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are missing</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"Pattern\":\n    \"\"\"\n    Create pattern instance from dictionary data.\n\n    Args:\n        data: Dictionary containing pattern data\n\n    Returns:\n        Pattern: New pattern instance\n\n    Raises:\n        ValueError: If required fields are missing\n    \"\"\"\n    required_fields = {\"name\", \"instructions\"}\n    if missing_fields := required_fields - set(data.keys()):\n        raise ValueError(f\"Missing required fields: {', '.join(missing_fields)}\")\n\n    return cls(\n        name=data[\"name\"],\n        instructions=data[\"instructions\"],\n        default_template_fields=data.get(\"default_template_fields\", {}),\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.get_content_without_frontmatter","title":"<code>get_content_without_frontmatter()</code>","text":"<p>Get markdown content with frontmatter removed.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Markdown content without frontmatter</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def get_content_without_frontmatter(self) -&gt; str:\n    \"\"\"\n    Get markdown content with frontmatter removed.\n\n    Returns:\n        str: Markdown content without frontmatter\n    \"\"\"\n    pattern = r\"\\A---\\s*\\n.*?\\n---\\s*\\n\"\n    return re.sub(pattern, \"\", self.instructions, flags=re.DOTALL)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert pattern to dictionary for serialization.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing all pattern data in serializable format</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert pattern to dictionary for serialization.\n\n    Returns:\n        Dict containing all pattern data in serializable format\n    \"\"\"\n    return {\n        \"name\": self.name,\n        \"instructions\": self.instructions,\n        \"default_template_fields\": self.default_template_fields,\n    }\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.Pattern.update_frontmatter","title":"<code>update_frontmatter(new_data)</code>","text":"<p>Update or add frontmatter to the markdown content.</p> <p>Parameters:</p> Name Type Description Default <code>new_data</code> <code>Dict[str, Any]</code> <p>Dictionary of frontmatter fields to update</p> required Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def update_frontmatter(self, new_data: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Update or add frontmatter to the markdown content.\n\n    Args:\n        new_data: Dictionary of frontmatter fields to update\n    \"\"\"\n\n    current_frontmatter = self.extract_frontmatter() or {}\n    updated_frontmatter = {**current_frontmatter, **new_data}\n\n    # Create YAML string\n    yaml_str = yaml.dump(\n        updated_frontmatter, default_flow_style=False, allow_unicode=True\n    )\n\n    # Remove existing frontmatter if present\n    content = self.get_content_without_frontmatter()\n\n    # Combine new frontmatter with content\n    self.instructions = f\"---\\n{yaml_str}---\\n\\n{content}\"\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.PatternManager","title":"<code>PatternManager</code>","text":"<p>Main interface for pattern management system.</p> <p>Provides high-level operations: - Pattern creation and loading - Automatic versioning - Safe concurrent access - Basic history tracking</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>class PatternManager:\n    \"\"\"\n    Main interface for pattern management system.\n\n    Provides high-level operations:\n    - Pattern creation and loading\n    - Automatic versioning\n    - Safe concurrent access\n    - Basic history tracking\n    \"\"\"\n\n    def __init__(self, base_path: Path):\n        \"\"\"\n        Initialize pattern management system.\n\n        Args:\n            base_path: Base directory for pattern storage\n        \"\"\"\n        self.base_path = Path(base_path).resolve()\n        self.base_path.mkdir(parents=True, exist_ok=True)\n\n        # Initialize subsystems\n        self.repo = GitBackedRepository(self.base_path)\n        self.access_manager = ConcurrentAccessManager(self.base_path / \".locks\")\n\n        logger.info(f\"Initialized pattern management system at {base_path}\")\n\n    def _normalize_path(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Normalize a path to be absolute under the repository base path.\n\n        Handles these cases to same result:\n        - \"my_file\" -&gt; &lt;base_path&gt;/my_file\n        - \"&lt;base_path&gt;/my_file\" -&gt; &lt;base_path&gt;/my_file\n\n        Args:\n            path: Input path as string or Path\n\n        Returns:\n            Path: Absolute path under base_path\n\n        Raises:\n            ValueError: If path would resolve outside repository\n        \"\"\"\n        path = Path(path)  # ensure we have a path\n\n        # Join with base_path as needed\n        if not path.is_absolute():\n            path = path if path.parent == self.base_path else self.base_path / path\n\n        # Safety check after resolution\n        resolved = path.resolve()\n        if not resolved.is_relative_to(self.base_path):\n            raise ValueError(\n                f\"Path {path} resolves outside repository: {self.base_path}\"\n            )\n\n        return resolved\n\n    def get_pattern_path(self, pattern_name: str) -&gt; Optional[Path]:\n        \"\"\"\n        Recursively search for a pattern file with the given name in base_path and all subdirectories.\n\n        Args:\n            pattern_id: pattern identifier to search for\n\n        Returns:\n            Optional[Path]: Full path to the found pattern file, or None if not found\n        \"\"\"\n        pattern = f\"{pattern_name}.md\"\n\n        try:\n            pattern_path = next(\n                path for path in self.base_path.rglob(pattern) if path.is_file()\n            )\n            logger.debug(f\"Found pattern file for ID {pattern_name} at: {pattern_path}\")\n            return self._normalize_path(pattern_path)\n\n        except StopIteration:\n            logger.debug(f\"No pattern file found with ID: {pattern_name}\")\n            return None\n\n    def save_pattern(self, pattern: Pattern, subdir: Optional[Path] = None) -&gt; Path:\n\n        pattern_name = pattern.name\n        instructions = pattern.instructions\n\n        if subdir is None:\n            path = self.base_path / f\"{pattern_name}.md\"\n        else:\n            path = self.base_path / subdir / f\"{pattern_name}.md\"\n\n        path = self._normalize_path(path)\n\n        # Check for existing pattern with same ID\n        existing_path = self.get_pattern_path(pattern_name)\n\n        if existing_path is not None and path != existing_path:\n            error_msg = (\n                f\"Existing pattern - {pattern_name} already exists at \"\n                f\"{existing_path.relative_to(self.base_path)}. \"\n                f\"Attempted to access at location: {path.relative_to(self.base_path)}\"\n            )\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        try:\n            with self.access_manager.file_lock(path):\n                write_text_to_file(path, instructions, overwrite=True)\n                self.repo.update_file(path)\n                logger.info(f\"Pattern saved at {path}\")\n                return path.relative_to(self.base_path)\n\n        except Exception as e:\n            logger.error(f\"Failed to save pattern {pattern.name}: {e}\")\n            raise\n\n    def load_pattern(self, pattern_name: str) -&gt; Pattern:\n        \"\"\"\n        Load the .md pattern file by name, extract placeholders, and\n        return a fully constructed Pattern object.\n\n        Args:\n            pattern_name: Name of the pattern (without .md extension).\n\n        Returns:\n            A new Pattern object whose 'instructions' is the file's text\n            and whose 'template_fields' are inferred from placeholders in\n            those instructions.\n        \"\"\"\n        # Locate the .md file; raise if missing\n        path = self.get_pattern_path(pattern_name)\n        if not path:\n            raise FileNotFoundError(f\"No pattern file named {pattern_name}.md found.\")\n\n        # Acquire lock before reading\n        with self.access_manager.file_lock(path):\n            instructions = get_text_from_file(path)\n\n        instructions = MarkdownStr(instructions)\n\n        # Create the pattern from the raw .md text\n        pattern = Pattern(name=pattern_name, instructions=instructions)\n\n        # Check for local uncommitted changes, updating file:\n        self.repo.update_file(path)\n\n        return pattern\n\n    def show_pattern_history(self, pattern_name: str) -&gt; None:\n        if path := self.get_pattern_path(pattern_name):\n            self.repo.display_history(path)\n        else:\n            logger.error(f\"Path to {pattern_name} not found.\")\n            return\n\n    # def get_pattern_history_from_path(self, path: Path) -&gt; List[Dict[str, Any]]:\n    #     \"\"\"\n    #     Get version history for a pattern.\n\n    #     Args:\n    #         path: Path to pattern file\n\n    #     Returns:\n    #         List of version information\n    #     \"\"\"\n    #     path = self._normalize_path(path)\n\n    #     return self.repo.get_history(path)\n\n    @classmethod\n    def verify_repository(cls, base_path: Path) -&gt; bool:\n        \"\"\"\n        Verify repository integrity and uniqueness of pattern names.\n\n        Performs the following checks:\n        1. Validates Git repository structure.\n        2. Ensures no duplicate pattern names exist.\n\n        Args:\n            base_path: Repository path to verify.\n\n        Returns:\n            bool: True if the repository is valid and contains no duplicate pattern files.\n        \"\"\"\n        try:\n            # Check if it's a valid Git repository\n            repo = Repo(base_path)\n\n            # Verify basic repository structure\n            basic_valid = (\n                repo.head.is_valid()\n                and not repo.bare\n                and (base_path / \".git\").is_dir()\n                and (base_path / \".locks\").is_dir()\n            )\n\n            if not basic_valid:\n                return False\n\n            # Check for duplicate pattern names\n            pattern_files = list(base_path.rglob(\"*.md\"))\n            seen_names = {}\n\n            for pattern_file in pattern_files:\n                # Skip files in .git directory\n                if \".git\" in pattern_file.parts:\n                    continue\n\n                # Get pattern name from the filename (without extension)\n                pattern_name = pattern_file.stem\n\n                if pattern_name in seen_names:\n                    logger.error(\n                        f\"Duplicate pattern file detected:\\n\"\n                        f\"  First occurrence: {seen_names[pattern_name]}\\n\"\n                        f\"  Second occurrence: {pattern_file}\"\n                    )\n                    return False\n\n                seen_names[pattern_name] = pattern_file\n\n            return True\n\n        except (InvalidGitRepositoryError, Exception) as e:\n            logger.error(f\"Repository verification failed: {e}\")\n            return False\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.PatternManager.access_manager","title":"<code>access_manager = ConcurrentAccessManager(self.base_path / '.locks')</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.PatternManager.base_path","title":"<code>base_path = Path(base_path).resolve()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.PatternManager.repo","title":"<code>repo = GitBackedRepository(self.base_path)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.patterns.PatternManager.__init__","title":"<code>__init__(base_path)</code>","text":"<p>Initialize pattern management system.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>Base directory for pattern storage</p> required Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def __init__(self, base_path: Path):\n    \"\"\"\n    Initialize pattern management system.\n\n    Args:\n        base_path: Base directory for pattern storage\n    \"\"\"\n    self.base_path = Path(base_path).resolve()\n    self.base_path.mkdir(parents=True, exist_ok=True)\n\n    # Initialize subsystems\n    self.repo = GitBackedRepository(self.base_path)\n    self.access_manager = ConcurrentAccessManager(self.base_path / \".locks\")\n\n    logger.info(f\"Initialized pattern management system at {base_path}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.PatternManager.get_pattern_path","title":"<code>get_pattern_path(pattern_name)</code>","text":"<p>Recursively search for a pattern file with the given name in base_path and all subdirectories.</p> <p>Parameters:</p> Name Type Description Default <code>pattern_id</code> <p>pattern identifier to search for</p> required <p>Returns:</p> Type Description <code>Optional[Path]</code> <p>Optional[Path]: Full path to the found pattern file, or None if not found</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def get_pattern_path(self, pattern_name: str) -&gt; Optional[Path]:\n    \"\"\"\n    Recursively search for a pattern file with the given name in base_path and all subdirectories.\n\n    Args:\n        pattern_id: pattern identifier to search for\n\n    Returns:\n        Optional[Path]: Full path to the found pattern file, or None if not found\n    \"\"\"\n    pattern = f\"{pattern_name}.md\"\n\n    try:\n        pattern_path = next(\n            path for path in self.base_path.rglob(pattern) if path.is_file()\n        )\n        logger.debug(f\"Found pattern file for ID {pattern_name} at: {pattern_path}\")\n        return self._normalize_path(pattern_path)\n\n    except StopIteration:\n        logger.debug(f\"No pattern file found with ID: {pattern_name}\")\n        return None\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.PatternManager.load_pattern","title":"<code>load_pattern(pattern_name)</code>","text":"<p>Load the .md pattern file by name, extract placeholders, and return a fully constructed Pattern object.</p> <p>Parameters:</p> Name Type Description Default <code>pattern_name</code> <code>str</code> <p>Name of the pattern (without .md extension).</p> required <p>Returns:</p> Type Description <code>Pattern</code> <p>A new Pattern object whose 'instructions' is the file's text</p> <code>Pattern</code> <p>and whose 'template_fields' are inferred from placeholders in</p> <code>Pattern</code> <p>those instructions.</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def load_pattern(self, pattern_name: str) -&gt; Pattern:\n    \"\"\"\n    Load the .md pattern file by name, extract placeholders, and\n    return a fully constructed Pattern object.\n\n    Args:\n        pattern_name: Name of the pattern (without .md extension).\n\n    Returns:\n        A new Pattern object whose 'instructions' is the file's text\n        and whose 'template_fields' are inferred from placeholders in\n        those instructions.\n    \"\"\"\n    # Locate the .md file; raise if missing\n    path = self.get_pattern_path(pattern_name)\n    if not path:\n        raise FileNotFoundError(f\"No pattern file named {pattern_name}.md found.\")\n\n    # Acquire lock before reading\n    with self.access_manager.file_lock(path):\n        instructions = get_text_from_file(path)\n\n    instructions = MarkdownStr(instructions)\n\n    # Create the pattern from the raw .md text\n    pattern = Pattern(name=pattern_name, instructions=instructions)\n\n    # Check for local uncommitted changes, updating file:\n    self.repo.update_file(path)\n\n    return pattern\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.PatternManager.save_pattern","title":"<code>save_pattern(pattern, subdir=None)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def save_pattern(self, pattern: Pattern, subdir: Optional[Path] = None) -&gt; Path:\n\n    pattern_name = pattern.name\n    instructions = pattern.instructions\n\n    if subdir is None:\n        path = self.base_path / f\"{pattern_name}.md\"\n    else:\n        path = self.base_path / subdir / f\"{pattern_name}.md\"\n\n    path = self._normalize_path(path)\n\n    # Check for existing pattern with same ID\n    existing_path = self.get_pattern_path(pattern_name)\n\n    if existing_path is not None and path != existing_path:\n        error_msg = (\n            f\"Existing pattern - {pattern_name} already exists at \"\n            f\"{existing_path.relative_to(self.base_path)}. \"\n            f\"Attempted to access at location: {path.relative_to(self.base_path)}\"\n        )\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n\n    try:\n        with self.access_manager.file_lock(path):\n            write_text_to_file(path, instructions, overwrite=True)\n            self.repo.update_file(path)\n            logger.info(f\"Pattern saved at {path}\")\n            return path.relative_to(self.base_path)\n\n    except Exception as e:\n        logger.error(f\"Failed to save pattern {pattern.name}: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.PatternManager.show_pattern_history","title":"<code>show_pattern_history(pattern_name)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>def show_pattern_history(self, pattern_name: str) -&gt; None:\n    if path := self.get_pattern_path(pattern_name):\n        self.repo.display_history(path)\n    else:\n        logger.error(f\"Path to {pattern_name} not found.\")\n        return\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.patterns.PatternManager.verify_repository","title":"<code>verify_repository(base_path)</code>  <code>classmethod</code>","text":"<p>Verify repository integrity and uniqueness of pattern names.</p> <p>Performs the following checks: 1. Validates Git repository structure. 2. Ensures no duplicate pattern names exist.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>Repository path to verify.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the repository is valid and contains no duplicate pattern files.</p> Source code in <code>src/tnh_scholar/ai_text_processing/patterns.py</code> <pre><code>@classmethod\ndef verify_repository(cls, base_path: Path) -&gt; bool:\n    \"\"\"\n    Verify repository integrity and uniqueness of pattern names.\n\n    Performs the following checks:\n    1. Validates Git repository structure.\n    2. Ensures no duplicate pattern names exist.\n\n    Args:\n        base_path: Repository path to verify.\n\n    Returns:\n        bool: True if the repository is valid and contains no duplicate pattern files.\n    \"\"\"\n    try:\n        # Check if it's a valid Git repository\n        repo = Repo(base_path)\n\n        # Verify basic repository structure\n        basic_valid = (\n            repo.head.is_valid()\n            and not repo.bare\n            and (base_path / \".git\").is_dir()\n            and (base_path / \".locks\").is_dir()\n        )\n\n        if not basic_valid:\n            return False\n\n        # Check for duplicate pattern names\n        pattern_files = list(base_path.rglob(\"*.md\"))\n        seen_names = {}\n\n        for pattern_file in pattern_files:\n            # Skip files in .git directory\n            if \".git\" in pattern_file.parts:\n                continue\n\n            # Get pattern name from the filename (without extension)\n            pattern_name = pattern_file.stem\n\n            if pattern_name in seen_names:\n                logger.error(\n                    f\"Duplicate pattern file detected:\\n\"\n                    f\"  First occurrence: {seen_names[pattern_name]}\\n\"\n                    f\"  Second occurrence: {pattern_file}\"\n                )\n                return False\n\n            seen_names[pattern_name] = pattern_file\n\n        return True\n\n    except (InvalidGitRepositoryError, Exception) as e:\n        logger.error(f\"Repository verification failed: {e}\")\n        return False\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.response_format","title":"<code>response_format</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.TEXT_SECTIONS_DESCRIPTION","title":"<code>TEXT_SECTIONS_DESCRIPTION = 'Ordered list of logical sections for the text. The sequence of line ranges for the sections must cover every line from start to finish without any overlaps or gaps.'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.LogicalSection","title":"<code>LogicalSection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A logically coherent section of text.</p> Source code in <code>src/tnh_scholar/ai_text_processing/response_format.py</code> <pre><code>class LogicalSection(BaseModel):\n    \"\"\"\n    A logically coherent section of text.\n    \"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"Meaningful title for the section in the original language of the section.\",\n    )\n    start_line: int = Field(\n        ..., description=\"Starting line number of the section (inclusive).\"\n    )\n    end_line: int = Field(\n        ..., description=\"Ending line number of the section (inclusive).\"\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.response_format.LogicalSection.end_line","title":"<code>end_line = Field(..., description='Ending line number of the section (inclusive).')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.LogicalSection.start_line","title":"<code>start_line = Field(..., description='Starting line number of the section (inclusive).')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.LogicalSection.title","title":"<code>title = Field(..., description='Meaningful title for the section in the original language of the section.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.TextObject","title":"<code>TextObject</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a text in any language broken into coherent logical sections.</p> Source code in <code>src/tnh_scholar/ai_text_processing/response_format.py</code> <pre><code>class TextObject(BaseModel):\n    \"\"\"\n    Represents a text in any language broken into coherent logical sections.\n    \"\"\"\n\n    language: str = Field(..., description=\"ISO 639-1 language code of the text.\")\n    sections: List[LogicalSection] = Field(..., description=TEXT_SECTIONS_DESCRIPTION)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.response_format.TextObject.language","title":"<code>language = Field(..., description='ISO 639-1 language code of the text.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.TextObject.sections","title":"<code>sections = Field(..., description=TEXT_SECTIONS_DESCRIPTION)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.typing","title":"<code>typing</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.typing.ResponseFormat","title":"<code>ResponseFormat = TypeVar('ResponseFormat', bound=BaseModel)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing","title":"<code>audio_processing</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio","title":"<code>audio</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.EXPECTED_TIME_FACTOR","title":"<code>EXPECTED_TIME_FACTOR = 0.45</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.MAX_DURATION","title":"<code>MAX_DURATION = 10 * 60</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.MAX_DURATION_MS","title":"<code>MAX_DURATION_MS = 10 * 60 * 1000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.MAX_INT16","title":"<code>MAX_INT16 = 32768.0</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.MIN_SILENCE_LENGTH","title":"<code>MIN_SILENCE_LENGTH = 1000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.SEEK_LENGTH","title":"<code>SEEK_LENGTH = 50</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.SILENCE_DBFS_THRESHOLD","title":"<code>SILENCE_DBFS_THRESHOLD = -30</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.logger","title":"<code>logger = get_child_logger('audio_processing')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.Boundary","title":"<code>Boundary</code>  <code>dataclass</code>","text":"<p>A data structure representing a detected audio boundary.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>float</code> <p>Start time of the segment in seconds.</p> <code>end</code> <code>float</code> <p>End time of the segment in seconds.</p> <code>text</code> <code>str</code> <p>Associated text (empty if silence-based).</p> Example <p>b = Boundary(start=0.0, end=30.0, text=\"Hello world\") b.start, b.end, b.text (0.0, 30.0, 'Hello world')</p> Source code in <code>src/tnh_scholar/audio_processing/audio.py</code> <pre><code>@dataclass\nclass Boundary:\n    \"\"\"A data structure representing a detected audio boundary.\n\n    Attributes:\n        start (float): Start time of the segment in seconds.\n        end (float): End time of the segment in seconds.\n        text (str): Associated text (empty if silence-based).\n\n    Example:\n        &gt;&gt;&gt; b = Boundary(start=0.0, end=30.0, text=\"Hello world\")\n        &gt;&gt;&gt; b.start, b.end, b.text\n        (0.0, 30.0, 'Hello world')\n    \"\"\"\n\n    start: float\n    end: float\n    text: str = \"\"\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio.Boundary.end","title":"<code>end</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.Boundary.start","title":"<code>start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.Boundary.text","title":"<code>text = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.Boundary.__init__","title":"<code>__init__(start, end, text='')</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio.audio_to_numpy","title":"<code>audio_to_numpy(audio_segment)</code>","text":"<p>Convert an AudioSegment object to a NumPy array suitable for Whisper.</p> <p>Parameters:</p> Name Type Description Default <code>audio_segment</code> <code>AudioSegment</code> <p>The input audio segment to convert.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A mono-channel NumPy array normalized to the range [-1, 1].</p> Example <p>audio = AudioSegment.from_file(\"example.mp3\") audio_numpy = audio_to_numpy(audio)</p> Source code in <code>src/tnh_scholar/audio_processing/audio.py</code> <pre><code>def audio_to_numpy(audio_segment: AudioSegment) -&gt; np.ndarray:\n    \"\"\"\n    Convert an AudioSegment object to a NumPy array suitable for Whisper.\n\n    Args:\n        audio_segment (AudioSegment): The input audio segment to convert.\n\n    Returns:\n        np.ndarray: A mono-channel NumPy array normalized to the range [-1, 1].\n\n    Example:\n        &gt;&gt;&gt; audio = AudioSegment.from_file(\"example.mp3\")\n        &gt;&gt;&gt; audio_numpy = audio_to_numpy(audio)\n    \"\"\"\n    # Convert the audio segment to raw sample data\n    raw_data = np.array(audio_segment.get_array_of_samples()).astype(np.float32)\n\n    # Normalize data to the range [-1, 1]\n    raw_data /= MAX_INT16\n\n    # Ensure mono-channel (use first channel if stereo)\n    if audio_segment.channels &gt; 1:\n        raw_data = raw_data.reshape(-1, audio_segment.channels)[:, 0]\n\n    return raw_data\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio.detect_silence_boundaries","title":"<code>detect_silence_boundaries(audio_file, min_silence_len=MIN_SILENCE_LENGTH, silence_thresh=SILENCE_DBFS_THRESHOLD, max_duration=MAX_DURATION_MS)</code>","text":"<p>Detect boundaries (start/end times) based on silence detection.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>Path to the audio file.</p> required <code>min_silence_len</code> <code>int</code> <p>Minimum silence length to consider for splitting (ms).</p> <code>MIN_SILENCE_LENGTH</code> <code>silence_thresh</code> <code>int</code> <p>Silence threshold in dBFS.</p> <code>SILENCE_DBFS_THRESHOLD</code> <code>max_duration</code> <code>int</code> <p>Maximum duration of any segment (ms).</p> <code>MAX_DURATION_MS</code> <p>Returns:</p> Type Description <code>Tuple[List[Boundary], Dict]</code> <p>List[Boundary]: A list of boundaries with empty text.</p> Example <p>boundaries = detect_silence_boundaries(Path(\"my_audio.mp3\")) for b in boundaries: ...     print(b.start, b.end)</p> Source code in <code>src/tnh_scholar/audio_processing/audio.py</code> <pre><code>def detect_silence_boundaries(\n    audio_file: Path,\n    min_silence_len: int = MIN_SILENCE_LENGTH,\n    silence_thresh: int = SILENCE_DBFS_THRESHOLD,\n    max_duration: int = MAX_DURATION_MS,\n) -&gt; Tuple[List[Boundary], Dict]:\n    \"\"\"\n    Detect boundaries (start/end times) based on silence detection.\n\n    Args:\n        audio_file (Path): Path to the audio file.\n        min_silence_len (int): Minimum silence length to consider for splitting (ms).\n        silence_thresh (int): Silence threshold in dBFS.\n        max_duration (int): Maximum duration of any segment (ms).\n\n    Returns:\n        List[Boundary]: A list of boundaries with empty text.\n\n    Example:\n        &gt;&gt;&gt; boundaries = detect_silence_boundaries(Path(\"my_audio.mp3\"))\n        &gt;&gt;&gt; for b in boundaries:\n        ...     print(b.start, b.end)\n    \"\"\"\n    logger.debug(\n        f\"Detecting silence boundaries with min_silence={min_silence_len}, silence_thresh={silence_thresh}\"\n    )\n\n    audio = AudioSegment.from_file(audio_file)\n    nonsilent_ranges = detect_nonsilent(\n        audio,\n        min_silence_len=min_silence_len,\n        silence_thresh=silence_thresh,\n        seek_step=SEEK_LENGTH,\n    )\n\n    # Combine ranges to enforce max_duration\n    if not nonsilent_ranges:\n        # If no nonsilent segments found, return entire file as one boundary\n        duration_s = len(audio) / 1000.0\n        return [Boundary(start=0.0, end=duration_s, text=\"\")]\n\n    combined_ranges = []\n    current_start, current_end = nonsilent_ranges[0]\n    for start, end in nonsilent_ranges[1:]:\n        if (current_end - current_start) + (end - start) &lt;= max_duration:\n            # Extend the current segment\n            current_end = end\n        else:\n            combined_ranges.append((current_start, current_end))\n            current_start, current_end = start, end\n    combined_ranges.append((current_start, current_end))\n\n    return [\n        Boundary(start=start_ms / 1000.0, end=end_ms / 1000.0, text=\"\")\n        for start_ms, end_ms in combined_ranges\n    ]\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio.detect_whisper_boundaries","title":"<code>detect_whisper_boundaries(audio_file, model_size='tiny', language=None)</code>","text":"<p>Detect sentence boundaries using a Whisper model.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>Path to the audio file.</p> required <code>model_size</code> <code>str</code> <p>Whisper model size.</p> <code>'tiny'</code> <code>language</code> <code>str</code> <p>Language to force for transcription (e.g. 'en', 'vi'), or None for auto.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Boundary]</code> <p>List[Boundary]: A list of sentence boundaries with text.</p> Example <p>boundaries = detect_whisper_boundaries(Path(\"my_audio.mp3\"), model_size=\"tiny\") for b in boundaries: ...     print(b.start, b.end, b.text)</p> Source code in <code>src/tnh_scholar/audio_processing/audio.py</code> <pre><code>def detect_whisper_boundaries(\n    audio_file: Path, model_size: str = \"tiny\", language: str = None\n) -&gt; List[Boundary]:\n    \"\"\"\n    Detect sentence boundaries using a Whisper model.\n\n    Args:\n        audio_file (Path): Path to the audio file.\n        model_size (str): Whisper model size.\n        language (str): Language to force for transcription (e.g. 'en', 'vi'), or None for auto.\n\n    Returns:\n        List[Boundary]: A list of sentence boundaries with text.\n\n    Example:\n        &gt;&gt;&gt; boundaries = detect_whisper_boundaries(Path(\"my_audio.mp3\"), model_size=\"tiny\")\n        &gt;&gt;&gt; for b in boundaries:\n        ...     print(b.start, b.end, b.text)\n    \"\"\"\n\n    os.environ[\"KMP_WARNINGS\"] = \"0\"  # Turn of OMP warning message\n\n    # Load model\n    logger.info(\"Loading Whisper model...\")\n    model = load_whisper_model(model_size)\n    logger.info(f\"Model '{model_size}' loaded.\")\n\n    if language:\n        logger.info(f\"Language for boundaries set to '{language}'\")\n    else:\n        logger.info(\"Language not set. Autodetect will be used in Whisper model.\")\n\n    # with TimeProgress(expected_time=expected_time, desc=\"Generating transcription boundaries\"):\n    boundary_transcription = whisper_model_transcribe(\n        model,\n        str(audio_file),\n        task=\"transcribe\",\n        word_timestamps=True,\n        language=language,\n        verbose=False,\n    )\n\n    sentence_boundaries = [\n        Boundary(start=segment[\"start\"], end=segment[\"end\"], text=segment[\"text\"])\n        for segment in boundary_transcription[\"segments\"]\n    ]\n    return sentence_boundaries, boundary_transcription\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio.split_audio","title":"<code>split_audio(audio_file, method='whisper', output_dir=None, model_size='tiny', language=None, min_silence_len=MIN_SILENCE_LENGTH, silence_thresh=SILENCE_DBFS_THRESHOLD, max_duration=MAX_DURATION)</code>","text":"<p>High-level function to split an audio file into chunks based on a chosen method.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>The input audio file.</p> required <code>method</code> <code>str</code> <p>Splitting method, \"silence\" or \"whisper\".</p> <code>'whisper'</code> <code>output_dir</code> <code>Path</code> <p>Directory to store output.</p> <code>None</code> <code>model_size</code> <code>str</code> <p>Whisper model size if method='whisper'.</p> <code>'tiny'</code> <code>language</code> <code>str</code> <p>Language for whisper transcription if method='whisper'.</p> <code>None</code> <code>min_silence_len</code> <code>int</code> <p>For silence-based detection, min silence length in ms.</p> <code>MIN_SILENCE_LENGTH</code> <code>silence_thresh</code> <code>int</code> <p>Silence threshold in dBFS.</p> <code>SILENCE_DBFS_THRESHOLD</code> <code>max_duration_s</code> <code>int</code> <p>Max chunk length in seconds.</p> required <code>max_duration_ms</code> <code>int</code> <p>Max chunk length in ms (for silence detection combination).</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Directory containing the resulting chunks.</p> Example Source code in <code>src/tnh_scholar/audio_processing/audio.py</code> <pre><code>def split_audio(\n    audio_file: Path,\n    method: str = \"whisper\",\n    output_dir: Path = None,\n    model_size: str = \"tiny\",\n    language: str = None,\n    min_silence_len: int = MIN_SILENCE_LENGTH,\n    silence_thresh: int = SILENCE_DBFS_THRESHOLD,\n    max_duration: int = MAX_DURATION,\n) -&gt; Path:\n    \"\"\"\n    High-level function to split an audio file into chunks based on a chosen method.\n\n    Args:\n        audio_file (Path): The input audio file.\n        method (str): Splitting method, \"silence\" or \"whisper\".\n        output_dir (Path): Directory to store output.\n        model_size (str): Whisper model size if method='whisper'.\n        language (str): Language for whisper transcription if method='whisper'.\n        min_silence_len (int): For silence-based detection, min silence length in ms.\n        silence_thresh (int): Silence threshold in dBFS.\n        max_duration_s (int): Max chunk length in seconds.\n        max_duration_ms (int): Max chunk length in ms (for silence detection combination).\n\n    Returns:\n        Path: Directory containing the resulting chunks.\n\n    Example:\n        &gt;&gt;&gt; # Split using silence detection\n        &gt;&gt;&gt; split_audio(Path(\"my_audio.mp3\"), method=\"silence\")\n\n        &gt;&gt;&gt; # Split using whisper-based sentence boundaries\n        &gt;&gt;&gt; split_audio(Path(\"my_audio.mp3\"), method=\"whisper\", model_size=\"base\", language=\"en\")\n    \"\"\"\n\n    logger.info(f\"Splitting audio with max_duration={max_duration} seconds\")\n\n    if method == \"whisper\":\n        boundaries, _ = detect_whisper_boundaries(\n            audio_file, model_size=model_size, language=language\n        )\n\n    elif method == \"silence\":\n        max_duration_ms = (\n            max_duration * 1000\n        )  # convert duration in seconds to milliseconds\n        boundaries = detect_silence_boundaries(\n            audio_file,\n            min_silence_len=min_silence_len,\n            silence_thresh=silence_thresh,\n            max_duration=max_duration_ms,\n        )\n    else:\n        raise ValueError(f\"Unknown method: {method}. Must be 'silence' or 'whisper'.\")\n\n    # delete all files in the output_dir (this is useful for reprocessing)\n\n    return split_audio_at_boundaries(\n        audio_file, boundaries, output_dir=output_dir, max_duration=max_duration\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio.split_audio--split-using-silence-detection","title":"Split using silence detection","text":"<p>split_audio(Path(\"my_audio.mp3\"), method=\"silence\")</p>"},{"location":"api/#tnh_scholar.audio_processing.audio.split_audio--split-using-whisper-based-sentence-boundaries","title":"Split using whisper-based sentence boundaries","text":"<p>split_audio(Path(\"my_audio.mp3\"), method=\"whisper\", model_size=\"base\", language=\"en\")</p>"},{"location":"api/#tnh_scholar.audio_processing.audio.split_audio_at_boundaries","title":"<code>split_audio_at_boundaries(audio_file, boundaries, output_dir=None, max_duration=MAX_DURATION)</code>","text":"<p>Split the audio file into chunks based on provided boundaries, ensuring all audio is included and boundaries align with the start of Whisper segments.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>The input audio file.</p> required <code>boundaries</code> <code>List[Boundary]</code> <p>Detected boundaries.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to store the resulting chunks.</p> <code>None</code> <code>max_duration</code> <code>int</code> <p>Maximum chunk length in seconds.</p> <code>MAX_DURATION</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Directory containing the chunked audio files.</p> Example <p>boundaries = [Boundary(34.02, 37.26, \"...\"), Boundary(38.0, 41.18, \"...\")] out_dir = split_audio_at_boundaries(Path(\"my_audio.mp3\"), boundaries)</p> Source code in <code>src/tnh_scholar/audio_processing/audio.py</code> <pre><code>def split_audio_at_boundaries(\n    audio_file: Path,\n    boundaries: List[Boundary],\n    output_dir: Path = None,\n    max_duration: int = MAX_DURATION,\n) -&gt; Path:\n    \"\"\"\n    Split the audio file into chunks based on provided boundaries, ensuring all audio is included\n    and boundaries align with the start of Whisper segments.\n\n    Args:\n        audio_file (Path): The input audio file.\n        boundaries (List[Boundary]): Detected boundaries.\n        output_dir (Path): Directory to store the resulting chunks.\n        max_duration (int): Maximum chunk length in seconds.\n\n    Returns:\n        Path: Directory containing the chunked audio files.\n\n    Example:\n        &gt;&gt;&gt; boundaries = [Boundary(34.02, 37.26, \"...\"), Boundary(38.0, 41.18, \"...\")]\n        &gt;&gt;&gt; out_dir = split_audio_at_boundaries(Path(\"my_audio.mp3\"), boundaries)\n    \"\"\"\n    logger.info(f\"Splitting audio with max_duration={max_duration} seconds\")\n\n    # Load the audio file\n    audio = AudioSegment.from_file(audio_file)\n\n    # Create output directory based on filename\n    if output_dir is None:\n        output_dir = audio_file.parent / f\"{audio_file.stem}_chunks\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Clean up the output directory\n    for file in output_dir.iterdir():\n        if file.is_file():\n            logger.info(f\"Deleting existing file: {file}\")\n            file.unlink()\n\n    chunk_start = 0  # Start time for the first chunk in ms\n    chunk_count = 1\n    current_chunk = AudioSegment.empty()\n\n    for idx, boundary in enumerate(boundaries):\n        segment_start_ms = int(boundary.start * 1000)\n        if idx + 1 &lt; len(boundaries):\n            segment_end_ms = int(\n                boundaries[idx + 1].start * 1000\n            )  # Next boundary's start\n        else:\n            segment_end_ms = len(audio)  # End of the audio for the last boundary\n\n        # Adjust for the first segment starting at 0\n        if idx == 0 and segment_start_ms &gt; 0:\n            segment_start_ms = 0  # Ensure we include the very beginning of the audio\n\n        segment = audio[segment_start_ms:segment_end_ms]\n\n        logger.debug(\n            f\"Boundary index: {idx}, segment_start: {segment_start_ms / 1000}, segment_end: {segment_end_ms / 1000}, duration: {segment.duration_seconds}\"\n        )\n        logger.debug(f\"Current chunk Duration (s): {current_chunk.duration_seconds}\")\n\n        if len(current_chunk) + len(segment) &lt;= max_duration * 1000:\n            # Add segment to the current chunk\n            current_chunk += segment\n        else:\n            # Export current chunk\n            chunk_path = output_dir / f\"chunk_{chunk_count}.mp3\"\n            current_chunk.export(chunk_path, format=\"mp3\")\n            logger.info(f\"Exported: {chunk_path}\")\n            chunk_count += 1\n\n            # Start a new chunk with the current segment\n            current_chunk = segment\n\n    # Export the final chunk if any audio remains\n    if len(current_chunk) &gt; 0:\n        chunk_path = output_dir / f\"chunk_{chunk_count}.mp3\"\n        current_chunk.export(chunk_path, format=\"mp3\")\n        logger.info(f\"Exported: {chunk_path}\")\n\n    return output_dir\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio.whisper_model_transcribe","title":"<code>whisper_model_transcribe(model, input_source, *args, **kwargs)</code>","text":"<p>Wrapper around model.transcribe that suppresses the known 'FP16 is not supported on CPU; using FP32 instead' UserWarning and redirects unwanted 'OMP' messages to prevent interference.</p> <p>This function accepts all args and kwargs that model.transcribe normally does, and supports input sources as file paths (str or Path) or in-memory audio arrays.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The Whisper model instance.</p> required <code>input_source</code> <code>Union[str, Path, ndarray]</code> <p>Input audio file path, URL, or in-memory audio array.</p> required <code>*args</code> <p>Additional positional arguments for model.transcribe.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments for model.transcribe.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Transcription result from model.transcribe.</p> Example Source code in <code>src/tnh_scholar/audio_processing/audio.py</code> <pre><code>def whisper_model_transcribe(\n    model: Any,\n    input_source: Any,\n    *args,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Wrapper around model.transcribe that suppresses the known\n    'FP16 is not supported on CPU; using FP32 instead' UserWarning\n    and redirects unwanted 'OMP' messages to prevent interference.\n\n    This function accepts all args and kwargs that model.transcribe normally does,\n    and supports input sources as file paths (str or Path) or in-memory audio arrays.\n\n    Parameters:\n        model (Any): The Whisper model instance.\n        input_source (Union[str, Path, np.ndarray]): Input audio file path, URL, or in-memory audio array.\n        *args: Additional positional arguments for model.transcribe.\n        **kwargs: Additional keyword arguments for model.transcribe.\n\n    Returns:\n        Dict[str, Any]: Transcription result from model.transcribe.\n\n    Example:\n        # Using a file path\n        result = whisper_model_transcribe(my_model, \"sample_audio.mp3\", verbose=True)\n\n        # Using an audio array\n        result = whisper_model_transcribe(my_model, audio_array, language=\"en\")\n    \"\"\"\n\n    # class StdoutFilter(io.StringIO):\n    #     def __init__(self, original_stdout):\n    #         super().__init__()\n    #         self.original_stdout = original_stdout\n\n    #     def write(self, message):\n    #         # Suppress specific messages like 'OMP:' while allowing others\n    #         if \"OMP:\" not in message:\n    #             self.original_stdout.write(message)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            message=\"FP16 is not supported on CPU; using FP32 instead\",\n            category=UserWarning,\n        )\n\n        # Redirect stdout to suppress OMP messages\n        # original_stdout = sys.stdout\n        # sys.stdout = filtered_stdout\n\n        try:\n            # Convert Path to str if needed\n            if isinstance(input_source, Path):\n                input_source = str(input_source)\n\n            # Call the original transcribe function\n            return model.transcribe(input_source, *args, **kwargs)\n        finally:\n            # Restore original stdout\n            # sys.stdout = original_stdout\n            pass\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio.whisper_model_transcribe--using-a-file-path","title":"Using a file path","text":"<p>result = whisper_model_transcribe(my_model, \"sample_audio.mp3\", verbose=True)</p>"},{"location":"api/#tnh_scholar.audio_processing.audio.whisper_model_transcribe--using-an-audio-array","title":"Using an audio array","text":"<p>result = whisper_model_transcribe(my_model, audio_array, language=\"en\")</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription","title":"<code>transcription</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.custom_to_json","title":"<code>custom_to_json(transcript)</code>","text":"<p>Custom JSON conversion function to handle problematic float values from Open AI API interface.</p> <p>Parameters:</p> Name Type Description Default <code>transcript</code> <code>Any</code> <p>Object from OpenAI API's transcription.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>JSON string with problematic values fixed.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription.py</code> <pre><code>def custom_to_json(transcript: TranscriptionVerbose) -&gt; str:\n    \"\"\"\n    Custom JSON conversion function to handle problematic float values from Open AI API interface.\n\n    Args:\n        transcript (Any): Object from OpenAI API's transcription.\n\n    Returns:\n        str: JSON string with problematic values fixed.\n    \"\"\"\n    logger.debug(\"Entered custom_to_json function.\")\n    try:\n        # Use warnings.catch_warnings to catch specific warnings\n        with warnings.catch_warnings(record=True) as caught_warnings:\n            warnings.simplefilter(\"always\", UserWarning)  # Catch all UserWarnings\n            data = transcript.to_dict()\n\n            # Check if any warnings were caught\n            for warning in caught_warnings:\n                if issubclass(warning.category, UserWarning):\n                    warning_msg = str(warning.message)\n                    if \"Expected `str` but got `float`\" in warning_msg:\n                        logger.debug(\n                            \"Known UserWarning in OPENAI .to_dict() float serialization caught and ignored.\"\n                        )\n                    else:\n                        logger.warning(\n                            f\"Unexpected warning during to_dict(): {warning_msg}\"\n                        )\n    except Exception as e:\n        logger.error(f\"Error during to_dict(): {e}\", exc_info=True)\n        return json.dumps({})  # Return an empty JSON as a fallback\n\n    # Traverse the dictionary to convert problematic floats to strings\n    for key, value in data.items():\n        if isinstance(value, float):  # Handle floats\n            data[key] = float(f\"{value:.18f}\")\n\n    # Serialize the cleaned dictionary back to JSON\n    logger.debug(\"Dumping json in custom_to_json...\")\n    return json.dumps(data)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.get_text_from_transcript","title":"<code>get_text_from_transcript(transcript)</code>","text":"<pre><code>Extracts and combines text from all segments of a transcription.\n\nArgs:\n    transcript (TranscriptionVerbose): A transcription object containing segments of text.\n\nReturns:\n    str: A single string with all segment texts concatenated, separated by newlines.\n\nRaises:\n    ValueError: If the transcript object is invalid or missing required attributes.\n\nExample:\n    &gt;&gt;&gt; from openai.types.audio.transcription_verbose import TranscriptionVerbose\n    &gt;&gt;&gt; transcript = TranscriptionVerbose(segments=[{\"text\": \"Hello\"}, {\"text\": \"world\"}])\n    &gt;&gt;&gt; get_text_from_transcript(transcript)\n    'Hello\n</code></pre> <p>world'</p> Source code in <code>src/tnh_scholar/audio_processing/transcription.py</code> <pre><code>def get_text_from_transcript(transcript: TranscriptionVerbose) -&gt; str:\n    \"\"\"\n    Extracts and combines text from all segments of a transcription.\n\n    Args:\n        transcript (TranscriptionVerbose): A transcription object containing segments of text.\n\n    Returns:\n        str: A single string with all segment texts concatenated, separated by newlines.\n\n    Raises:\n        ValueError: If the transcript object is invalid or missing required attributes.\n\n    Example:\n        &gt;&gt;&gt; from openai.types.audio.transcription_verbose import TranscriptionVerbose\n        &gt;&gt;&gt; transcript = TranscriptionVerbose(segments=[{\"text\": \"Hello\"}, {\"text\": \"world\"}])\n        &gt;&gt;&gt; get_text_from_transcript(transcript)\n        'Hello\\nworld'\n    \"\"\"\n    logger.debug(f\"transcript is type: {type(transcript)}\")\n\n    return \"\\n\".join(segment.text.strip() for segment in transcript.segments)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.get_transcription","title":"<code>get_transcription(file, model, prompt, jsonl_out, mode='transcribe')</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/transcription.py</code> <pre><code>def get_transcription(\n    file: Path, model: str, prompt: str, jsonl_out, mode=\"transcribe\"\n):\n    logger.info(\n        f\"Speech transcript parameters: file={file}, model={model}, response_format=verbose_json, mode={mode}\\n\\tprompt='{prompt}'\"\n    )\n    transcript = run_transcription_speech(\n        file, model=model, response_format=\"verbose_json\", prompt=prompt, mode=mode\n    )\n\n    # Use the custom_to_json function\n    json_output = custom_to_json(transcript)\n    logger.debug(f\"Serialized JSON output excerpt: {json_output[:1000]}...\")\n\n    # Write the serialized JSON to the JSONL file\n    jsonl_out.write(json_output + \"\\n\")\n\n    return get_text_from_transcript(transcript)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.process_audio_chunks","title":"<code>process_audio_chunks(directory, output_file, jsonl_file, model='whisper-1', prompt='', translate=False)</code>","text":"<p>Processes all audio chunks in the specified directory using OpenAI's transcription API, saves the transcription objects into a JSONL file, and stitches the transcriptions into a single text file.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path to the directory containing audio chunks.</p> required <code>output_file</code> <code>Path</code> <p>Path to the output file to save the stitched transcription.</p> required <code>jsonl_file</code> <code>Path</code> <p>Path to save the transcription objects as a JSONL file.</p> required <code>model</code> <code>str</code> <p>The transcription model to use (default is \"whisper-1\").</p> <code>'whisper-1'</code> <code>prompt</code> <code>str</code> <p>Optional prompt to provide context for better transcription.</p> <code>''</code> <code>translate</code> <code>bool</code> <p>Optional flag to translate speech to English (useful if the audio input is not English)</p> <code>False</code> <p>Raises:     FileNotFoundError: If no audio chunks are found in the directory.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription.py</code> <pre><code>def process_audio_chunks(\n    directory: Path,\n    output_file: Path,\n    jsonl_file: Path,\n    model: str = \"whisper-1\",\n    prompt: str = \"\",\n    translate: bool = False,\n) -&gt; None:\n    \"\"\"\n    Processes all audio chunks in the specified directory using OpenAI's transcription API,\n    saves the transcription objects into a JSONL file, and stitches the transcriptions\n    into a single text file.\n\n    Args:\n        directory (Path): Path to the directory containing audio chunks.\n        output_file (Path): Path to the output file to save the stitched transcription.\n        jsonl_file (Path): Path to save the transcription objects as a JSONL file.\n        model (str): The transcription model to use (default is \"whisper-1\").\n        prompt (str): Optional prompt to provide context for better transcription.\n        translate (bool): Optional flag to translate speech to English (useful if the audio input is not English)\n    Raises:\n        FileNotFoundError: If no audio chunks are found in the directory.\n    \"\"\"\n\n    # Ensure the output directory exists\n    output_file.parent.mkdir(parents=True, exist_ok=True)\n    jsonl_file.parent.mkdir(parents=True, exist_ok=True)\n\n    # Collect all audio chunks in the directory, sorting numerically by chunk number\n    audio_files = sorted(\n        directory.glob(\"*.mp3\"),\n        key=lambda f: int(f.stem.split(\"_\")[1]),  # Extract the number from 'chunk_X'\n    )\n\n    if not audio_files:\n        raise FileNotFoundError(f\"No audio files found in the directory: {directory}\")\n\n    # log files to process:\n    audio_file_names = [file.name for file in audio_files]  # get strings for logging\n    audio_file_name_str = \"\\n\\t\".join(audio_file_names)\n    audio_file_count = len(audio_file_names)\n    logger.info(\n        f\"{audio_file_count} audio files found in {directory}:\\n\\t{audio_file_name_str}\"\n    )\n\n    # Initialize the output content\n    stitched_transcription = []\n\n    # Open the JSONL file for writing\n    with jsonl_file.open(\"w\", encoding=\"utf-8\") as jsonl_out:\n        # Process each audio chunk\n        for audio_file in audio_files:\n            logger.info(f\"Processing {audio_file.name}...\")\n            try:\n                if translate:\n                    text = get_transcription(\n                        audio_file, model, prompt, jsonl_out, mode=\"translate\"\n                    )\n                else:\n                    text = get_transcription(\n                        audio_file, model, prompt, jsonl_out, mode=\"transcribe\"\n                    )\n\n                stitched_transcription.append(text)\n\n            except Exception as e:\n                logger.error(f\"Error processing {audio_file.name}: {e}\", exc_info=True)\n                raise e\n\n    # Write the stitched transcription to the output file\n    with output_file.open(\"w\", encoding=\"utf-8\") as out_file:\n        out_file.write(\" \".join(stitched_transcription))\n\n    logger.info(f\"Stitched transcription saved to {output_file}\")\n    logger.info(f\"Full transcript objects saved to {jsonl_file}\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.process_audio_file","title":"<code>process_audio_file(audio_file, output_file, jsonl_file, model='whisper-1', prompt='', translate=False)</code>","text":"<p>Processes a single audio file using OpenAI's transcription API, saves the transcription objects into a JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>Path to the the audio file for processing</p> required <code>output_file</code> <code>Path</code> <p>Path to the output file to save the stitched transcription.</p> required <code>jsonl_file</code> <code>Path</code> <p>Path to save the transcription objects as a JSONL file.</p> required <code>model</code> <code>str</code> <p>The transcription model to use (default is \"whisper-1\").</p> <code>'whisper-1'</code> <code>prompt</code> <code>str</code> <p>Optional prompt to provide context for better transcription.</p> <code>''</code> <code>translate</code> <code>bool</code> <p>Optional flag to translate speech to English (useful if the audio input is not English)</p> <code>False</code> <p>Raises:     FileNotFoundError: If no audio chunks are found in the directory.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription.py</code> <pre><code>def process_audio_file(\n    audio_file: Path,\n    output_file: Path,\n    jsonl_file: Path,\n    model: str = \"whisper-1\",\n    prompt: str = \"\",\n    translate: bool = False,\n) -&gt; None:\n    \"\"\"\n    Processes a single audio file using OpenAI's transcription API,\n    saves the transcription objects into a JSONL file.\n\n    Args:\n        audio_file (Path): Path to the the audio file for processing\n        output_file (Path): Path to the output file to save the stitched transcription.\n        jsonl_file (Path): Path to save the transcription objects as a JSONL file.\n        model (str): The transcription model to use (default is \"whisper-1\").\n        prompt (str): Optional prompt to provide context for better transcription.\n        translate (bool): Optional flag to translate speech to English (useful if the audio input is not English)\n    Raises:\n        FileNotFoundError: If no audio chunks are found in the directory.\n    \"\"\"\n\n    # Ensure the output directory exists\n    output_file.parent.mkdir(parents=True, exist_ok=True)\n    jsonl_file.parent.mkdir(parents=True, exist_ok=True)\n\n    if not audio_file.exists():\n        raise FileNotFoundError(f\"Audio file {audio_file} not found.\")\n    else:\n        logger.info(f\"Audio file found: {audio_file}\")\n\n    # Open the JSONL file for writing\n    with jsonl_file.open(\"w\", encoding=\"utf-8\") as jsonl_out:\n        logger.info(f\"Processing {audio_file.name}...\")\n        try:\n            if translate:\n                text = get_transcription(\n                    audio_file, model, prompt, jsonl_out, mode=\"translate\"\n                )\n            else:\n                text = get_transcription(\n                    audio_file, model, prompt, jsonl_out, mode=\"transcribe\"\n                )\n        except Exception as e:\n            logger.error(f\"Error processing {audio_file.name}: {e}\", exc_info=True)\n            raise e\n\n    # Write the stitched transcription to the output file\n    with output_file.open(\"w\", encoding=\"utf-8\") as out_file:\n        out_file.write(text)\n\n    logger.info(f\"Transcription saved to {output_file}\")\n    logger.info(f\"Full transcript objects saved to {jsonl_file}\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.whisper_security","title":"<code>whisper_security</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.whisper_security.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.whisper_security.load_whisper_model","title":"<code>load_whisper_model(model_name)</code>","text":"<p>Safely load a Whisper model with security best practices.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the Whisper model to load (e.g., \"tiny\", \"base\", \"small\")</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Loaded Whisper model</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If model loading fails</p> Source code in <code>src/tnh_scholar/audio_processing/whisper_security.py</code> <pre><code>def load_whisper_model(model_name: str) -&gt; Any:\n    \"\"\"\n    Safely load a Whisper model with security best practices.\n\n    Args:\n        model_name: Name of the Whisper model to load (e.g., \"tiny\", \"base\", \"small\")\n\n    Returns:\n        Loaded Whisper model\n\n    Raises:\n        RuntimeError: If model loading fails\n    \"\"\"\n    import whisper\n\n    try:\n        with safe_torch_load():\n            model = whisper.load_model(model_name)\n        return model\n    except Exception as e:\n        logger.error(\"Failed to load Whisper model %r: %s\", model_name, e)\n        raise RuntimeError(f\"Failed to load Whisper model: {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.whisper_security.safe_torch_load","title":"<code>safe_torch_load(weights_only=True)</code>","text":"<p>Context manager that temporarily modifies torch.load to use weights_only=True by default.</p> <p>This addresses the FutureWarning in PyTorch regarding pickle security: https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models</p> <p>Parameters:</p> Name Type Description Default <code>weights_only</code> <code>bool</code> <p>If True, limits unpickling to tensor data only.</p> <code>True</code> <p>Yields:</p> Type Description <code>None</code> <p>None</p> Example <p>with safe_torch_load(): ...     model = whisper.load_model(\"tiny\")</p> Source code in <code>src/tnh_scholar/audio_processing/whisper_security.py</code> <pre><code>@contextlib.contextmanager\ndef safe_torch_load(weights_only: bool = True) -&gt; Generator[None, None, None]:\n    \"\"\"\n    Context manager that temporarily modifies torch.load to use weights_only=True by default.\n\n    This addresses the FutureWarning in PyTorch regarding pickle security:\n    https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models\n\n    Args:\n        weights_only: If True, limits unpickling to tensor data only.\n\n    Yields:\n        None\n\n    Example:\n        &gt;&gt;&gt; with safe_torch_load():\n        ...     model = whisper.load_model(\"tiny\")\n    \"\"\"\n    original_torch_load = torch.load\n    try:\n        torch.load = partial(original_torch_load, weights_only=weights_only)\n        logger.debug(\"Modified torch.load to use weights_only=%s\", weights_only)\n        yield\n    finally:\n        torch.load = original_torch_load\n        logger.debug(\"Restored original torch.load\")\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools","title":"<code>cli_tools</code>","text":"<p>TNH Scholar CLI Tools</p> <p>Command-line interface tools for the TNH Scholar project:</p> <pre><code>audio-transcribe:\n    Audio processing pipeline that handles downloading, segmentation,\n    and transcription of Buddhist teachings.\n\ntnh-fab:\n    Text processing tool for texts, providing functionality for\n    punctuation, sectioning, translation, and pattern-based processing.\n</code></pre> <p>See individual tool documentation for usage details and examples.</p>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe","title":"<code>audio_transcribe</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe","title":"<code>audio_transcribe</code>","text":"<p>This module provides a command line interface for handling audio transcription tasks. It can optionally:  - Download audio from a YouTube URL.  - Split existing audio into chunks.  - Transcribe audio chunks to text.</p> Usage Example <p>In a production environment, this CLI tool would be installed as part of the <code>tnh-scholar</code> package.</p>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe--download-split-and-transcribe-from-a-single-youtube-url","title":"Download, split, and transcribe from a single YouTube URL","text":"<p>audio-transcribe         --yt_download         --yt_process_url \"https://www.youtube.com/watch?v=EXAMPLE\"         --split         --transcribe         --output_dir ./processed         --prompt \"Dharma, Deer Park...\"</p>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_CHUNK_DURATION_MIN","title":"<code>DEFAULT_CHUNK_DURATION_MIN = 7</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_CHUNK_DURATION_SEC","title":"<code>DEFAULT_CHUNK_DURATION_SEC = DEFAULT_CHUNK_DURATION_MIN * 60</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_OUTPUT_DIR","title":"<code>DEFAULT_OUTPUT_DIR = './audio_transcriptions'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_PROMPT","title":"<code>DEFAULT_PROMPT = 'Dharma, Deer Park, Thay, Thich Nhat Hanh, Bodhicitta, Bodhisattva, Mahayana'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.REQUIREMENTS_PATH","title":"<code>REQUIREMENTS_PATH = TNH_CLI_TOOLS_DIR / 'audio_transcribe' / 'environment' / 'requirements.txt'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.RE_DOWNLOAD_CONFIRMATION_STR","title":"<code>RE_DOWNLOAD_CONFIRMATION_STR = 'An mp3 file corresponding to {url} already exists in the output path:\\n\\t{output_dir}.\\nSKIP download ([Y]/n)?'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.logger","title":"<code>logger = get_child_logger('audio_transcribe')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.audio_transcribe","title":"<code>audio_transcribe(split, transcribe, yt_url, yt_url_csv, file, chunk_dir, output_dir, chunk_duration, no_chunks, start_time, translate, prompt, silence_boundaries, whisper_boundaries, language)</code>","text":"<p>Entry point for the audio transcription pipeline. Depending on the provided flags and arguments, it can download audio from YouTube, split the audio into chunks, and/or transcribe the chunks.</p> <p>Steps are:</p> <ol> <li> <p>Download (if requested)</p> </li> <li> <p>Split (if requested)</p> </li> <li> <p>Transcribe (if requested)</p> </li> </ol> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/audio_transcribe.py</code> <pre><code>@click.command()\n@click.option(\n    \"-s\", \"--split\", is_flag=True, help=\"Split downloaded/local audio into chunks.\"\n)\n@click.option(\"-t\", \"--transcribe\", is_flag=True, help=\"Transcribe the audio chunks.\")\n@click.option(\"-y\", \"--yt_url\", type=str, help=\"Single YouTube URL to process.\")\n@click.option(\n    \"-v\",\n    \"--yt_url_csv\",\n    type=click.Path(exists=True),\n    help=\"A CSV File containing multiple YouTube URLs. The first column of the file must be the URL and Second column a start time (if specified).\",\n)\n@click.option(\n    \"-f\", \"--file\", type=click.Path(exists=True), help=\"Path to a local audio file.\"\n)\n@click.option(\n    \"-c\",\n    \"--chunk_dir\",\n    type=click.Path(),\n    help=\"Directory for pre-existing chunks or where to store new chunks.\",\n)\n@click.option(\n    \"-o\",\n    \"--output_dir\",\n    type=click.Path(),\n    default=DEFAULT_OUTPUT_DIR,\n    help=f\"Base output directory. DEFAULT: '{DEFAULT_OUTPUT_DIR}' \",\n)\n@click.option(\n    \"-d\",\n    \"--chunk_duration\",\n    type=int,\n    default=DEFAULT_CHUNK_DURATION_SEC,\n    help=f\"Max chunk duration in seconds (default: {DEFAULT_CHUNK_DURATION_MIN} minutes).\",\n)\n@click.option(\n    \"-x\",\n    \"--no_chunks\",\n    is_flag=True,\n    help=\"Run transcription directly on the audio files source(s). WARNING: for files &gt; 10 minutes in Length, the Open AI transcription API may fail.\",\n)\n@click.option(\n    \"-b\",\n    \"--start\",\n    \"start_time\",\n    type=str,\n    help=\"Start time (beginning) offset for the input media (HH:MM:SS).\",\n)\n@click.option(\n    \"-a\",\n    \"--translate\",\n    is_flag=True,\n    help=\"Include translation in the transcription if set.\",\n)\n@click.option(\n    \"-p\",\n    \"--prompt\",\n    type=str,\n    default=DEFAULT_PROMPT,\n    help=\"Prompt or keywords to guide the transcription.\",\n)\n@click.option(\n    \"-i\",\n    \"--silence_boundaries\",\n    is_flag=True,\n    help=\"Use silence detection to split audio file(s)\",\n)\n@click.option(\n    \"-w\",\n    \"--whisper_boundaries\",\n    is_flag=True,\n    help=\"(DEFAULT) Use a whisper based model to audio at sentence boundaries.\",\n)\n@click.option(\n    \"-l\",\n    \"--language\",\n    type=str,\n    help=\"The two letter language code. e.g. 'vi' for Vietnamese. Used for splitting only. DEFAULT: English ('en').\",\n)\ndef audio_transcribe(\n    split: bool,\n    transcribe: bool,\n    yt_url: str | None,\n    yt_url_csv: str | None,\n    file: str | None,\n    chunk_dir: str | None,\n    output_dir: str,\n    chunk_duration: int,\n    no_chunks: bool,\n    start_time: str | None,\n    translate: bool,\n    prompt: str,\n    silence_boundaries: bool,\n    whisper_boundaries: bool,\n    language: str | None,\n) -&gt; None:\n    \"\"\"\n    Entry point for the audio transcription pipeline.\n    Depending on the provided flags and arguments, it can download audio from YouTube,\n    split the audio into chunks, and/or transcribe the chunks.\n\n    Steps are:\n\n    1. Download (if requested)\n\n    2. Split (if requested)\n\n    3. Transcribe (if requested)\n    \"\"\"\n\n    check_ytd_version()  # Do a version check on startup. Version issues can cause yt-dlp to fail.\n\n    logger.info(\"Starting audio transcription pipeline...\")\n\n    # initial parameter processing\n    if not split and not transcribe:  # if neither set, we assume both.\n        split = True\n        transcribe = True\n\n    is_download = bool(yt_url or yt_url_csv)\n    if not language:\n        language = \"en\"\n\n    # default logic for splitting boundaries\n    if not whisper_boundaries and not silence_boundaries:\n        whisper_boundaries = True\n\n    try:\n        # Validate input arguments\n        audio_file: Path | None = Path(file) if file else None\n        chunk_directory: Path | None = Path(chunk_dir) if chunk_dir else None\n        out_dir = Path(output_dir)\n\n        validate_inputs(\n            is_download=is_download,\n            yt_url=yt_url,\n            yt_url_list=Path(yt_url_csv) if yt_url_csv else None,\n            audio_file=audio_file,\n            split=split,\n            transcribe=transcribe,\n            chunk_dir=chunk_directory,\n            no_chunks=no_chunks,\n            silence_boundaries=silence_boundaries,\n            whisper_boundaries=whisper_boundaries,\n        )\n\n        # Determine the list of URLs if we are downloading from YouTube\n        urls: list[str] = []\n        if yt_url_csv:\n            if is_download:\n                urls = get_youtube_urls_from_csv(Path(yt_url_csv))\n        elif yt_url:\n            if is_download:\n                urls = [yt_url]\n\n        # If we are downloading from YouTube, handle that\n        downloaded_files: list[Path] = []\n        if is_download:\n            for url in urls:\n                download_path = get_video_download_path_yt(out_dir, url)\n                if download_path.exists():\n                    if get_user_confirmation(\n                        RE_DOWNLOAD_CONFIRMATION_STR.format(url=url, output_dir=out_dir)\n                    ):\n                        logger.info(f\"Skipping download for {url}.\")\n                    else:\n                        logger.info(f\"Re-downloading {url}:\")\n                        download_path = download_audio_yt(\n                            url, out_dir, start_time=start_time\n                        )\n                        logger.info(f\"Successfully downloaded {url} to {download_path}\")\n                else:\n                    logger.info(f\"Downloading from YouTube: {url}\")\n                    ensure_directory_exists(out_dir)\n                    download_path = download_audio_yt(\n                        url, out_dir, start_time=start_time\n                    )\n                    logger.info(f\"Successfully downloaded {url} to {download_path}\")\n\n                downloaded_files.append(download_path)\n\n        # If we have a local audio file specified (no yt_download), treat that as our input\n        if audio_file and not is_download:\n            downloaded_files = [audio_file]\n\n        # If splitting is requested, split either the downloaded files or the provided audio\n        if split:\n            for audio_file in downloaded_files:\n                audio_name = audio_file.stem\n                audio_output_dir = out_dir / audio_name\n                ensure_directory_exists(audio_output_dir)\n                chunk_output_dir = chunk_directory or audio_output_dir / \"chunks\"\n                ensure_directory_exists(chunk_output_dir)\n\n                logger.info(f\"Splitting audio into chunks for {audio_file}\")\n\n                if (\n                    not whisper_boundaries\n                    and not silence_boundaries\n                    or not silence_boundaries\n                ):\n                    detection_method = \"whisper\"\n                else:\n                    detection_method = \"silence\"\n                split_audio(\n                    audio_file=audio_file,\n                    method=detection_method,\n                    output_dir=chunk_output_dir,\n                    max_duration=chunk_duration,\n                    language=language,\n                )\n\n        # If transcribe is requested, we must have a chunk directory to transcribe from\n        if transcribe:\n            for audio_file in downloaded_files:\n                audio_name = audio_file.stem\n                audio_output_dir = out_dir / audio_name\n                transcript_file = audio_output_dir / f\"{audio_name}.txt\"\n                if no_chunks:\n                    jsonl_file = audio_output_dir / f\"{audio_name}.jsonl\"\n                    logger.info(\n                        f\"Transcribing {audio_name} directly without chunking...\"\n                    )\n                    process_audio_file(\n                        audio_file=audio_file,\n                        output_file=transcript_file,\n                        jsonl_file=jsonl_file,\n                        prompt=prompt,\n                        translate=translate,\n                    )\n\n                else:\n                    chunk_output_dir = chunk_directory or audio_output_dir / \"chunks\"\n                    jsonl_file = audio_output_dir / f\"{audio_name}.jsonl\"\n                    logger.info(f\"Transcribing chunks from {chunk_output_dir}\")\n                    process_audio_chunks(\n                        directory=chunk_output_dir,\n                        output_file=transcript_file,\n                        jsonl_file=jsonl_file,\n                        prompt=prompt,\n                        translate=translate,\n                    )\n\n        logger.info(\"Audio transcription pipeline completed successfully.\")\n    except Exception as e:\n        logger.error(f\"An error occurred: {e}\")\n        logger.debug(\"traceback info\", exc_info=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.main","title":"<code>main()</code>","text":"<p>Entry point for AUDIO-TRANSCRIBE CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/audio_transcribe.py</code> <pre><code>def main():\n    \"\"\"Entry point for AUDIO-TRANSCRIBE CLI tool.\"\"\"\n    audio_transcribe()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.environment","title":"<code>environment</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.environment.env","title":"<code>env</code>","text":"<code>logger = get_child_logger(__name__)</code> <code>module-attribute</code> <code>check_env()</code> <p>Check the environment for necessary conditions: 1. Check OpenAI key is available. 2. Check that all requirements from requirements.txt are importable.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/environment/env.py</code> <pre><code>def check_env() -&gt; bool:\n    \"\"\"\n    Check the environment for necessary conditions:\n    1. Check OpenAI key is available.\n    2. Check that all requirements from requirements.txt are importable.\n    \"\"\"\n    logger.debug(\"checking environment.\")\n\n    if not check_openai_env():\n        return False\n\n    if shutil.which(\"ffmpeg\") is None:\n        logger.error(\"ffmpeg not found in PATH. ffmpeg required for audio processing.\")\n        return False\n\n    return True\n</code></pre> <code>check_requirements(requirements_file)</code> <p>Check that all requirements listed in requirements.txt can be imported. If any cannot be imported, print a warning.</p> <p>This is a heuristic check. Some packages may not share the same name as their importable module. Adjust the name mappings below as needed.</p> Example <p>check_requirements(Path(\"./requirements.txt\"))</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/environment/env.py</code> <pre><code>def check_requirements(requirements_file: Path) -&gt; None:\n    \"\"\"\n    Check that all requirements listed in requirements.txt can be imported.\n    If any cannot be imported, print a warning.\n\n    This is a heuristic check. Some packages may not share the same name as their importable module.\n    Adjust the name mappings below as needed.\n\n    Example:\n        &gt;&gt;&gt; check_requirements(Path(\"./requirements.txt\"))\n        # Prints warnings if imports fail, otherwise silent.\n    \"\"\"\n    # Map requirement names to their importable module names if they differ\n    name_map = {\n        \"python-dotenv\": \"dotenv\",\n        \"openai_whisper\": \"whisper\",\n        \"protobuf\": \"google.protobuf\",\n        # Add other mappings if needed\n    }\n\n    # Parse requirements.txt to get a list of package names\n    packages = []\n    with requirements_file.open(\"r\") as req_file:\n        for line in req_file:\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                continue\n            # Each line generally looks like 'package==version'\n            pkg_name = line.split(\"==\")[0].strip()\n            packages.append(pkg_name)\n\n    # Try importing each package\n    for pkg in packages:\n        mod_name = name_map.get(pkg, pkg)\n        try:\n            __import__(mod_name)\n        except ImportError:\n            print(\n                f\"WARNING: Could not import '{mod_name}' from '{pkg}'. Check that it is correctly installed.\"\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.environment.env.check_requirements--prints-warnings-if-imports-fail-otherwise-silent","title":"Prints warnings if imports fail, otherwise silent.","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.validate","title":"<code>validate</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.validate.validate_inputs","title":"<code>validate_inputs(is_download, yt_url, yt_url_list, audio_file, split, transcribe, chunk_dir, no_chunks, silence_boundaries, whisper_boundaries)</code>","text":"<p>Validate the CLI inputs to ensure logical consistency given all the flags.</p> <p>Conditions &amp; Requirements: 1. At least one action (yt_download, split, transcribe) should be requested.    Otherwise, nothing is done, so raise an error.</p> <ol> <li>If yt_download is True:</li> <li> <p>Must specify either yt_process_url OR yt_process_url_list (not both, not none).</p> </li> <li> <p>If yt_download is False:</p> </li> <li>If split is requested, we need a local audio file (since no download will occur).</li> <li> <p>If transcribe is requested without split and without yt_download:</p> <ul> <li>If no_chunks = False, we must have chunk_dir to read existing chunks.</li> <li>If no_chunks = True, we must have a local audio file (direct transcription) or previously downloaded file    (but since yt_download=False, previously downloaded file scenario doesn't apply here,    so effectively we need local audio in that scenario).</li> </ul> </li> <li> <p>no_chunks flag:</p> </li> <li> <p>If no_chunks = True, we are doing direct transcription on entire audio without chunking.</p> <ul> <li>Cannot use split if no_chunks = True. (Mutually exclusive)</li> <li>chunk_dir is irrelevant if no_chunks = True; since we don't split into chunks,    requiring a chunk_dir doesn't make sense. If provided, it's not useful, but let's allow it silently    or raise an error for clarity. It's safer to raise an error to prevent user confusion.</li> </ul> </li> <li> <p>Boundaries flags (silence_boundaries, whisper_boundaries):</p> </li> <li>These flags control how splitting is done.</li> <li>If split = False, these are irrelevant. Not necessarily an error, but could be a no-op.      For robustness, raise an error if user specifies these without split, to avoid confusion.</li> <li>If split = True and no_chunks = True, that\u2019s contradictory already, so no need for boundary logic there.</li> <li>If split = True, exactly one method should be chosen:      If both silence_boundaries and whisper_boundaries are True simultaneously or both are False simultaneously,      we need a clear default or raise an error. By the code snippet logic, whisper_boundaries is default True      if not stated otherwise. To keep it robust:<ul> <li>If both are True, raise error.</li> <li>If both are False, that means user explicitly turned them off or never turned on whisper.      The code snippet sets whisper_boundaries True by default. If user sets it False somehow,      we can then default to silence. Just ensure at run-time we have a deterministic method:      If both are False, we can default to whisper or silence. Let's default to whisper if no flags given.      However, given the code snippet, whisper_boundaries has a default of True.      If the user sets whisper_boundaries to False and also does not set silence_boundaries,      then no method is chosen. Let's then raise an error if both ended up False to avoid ambiguity.</li> </ul> </li> </ol> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arguments are not logically consistent.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/validate.py</code> <pre><code>def validate_inputs(\n    is_download: bool,\n    yt_url: str | None,\n    yt_url_list: Path | None,\n    audio_file: Path | None,\n    split: bool,\n    transcribe: bool,\n    chunk_dir: Path | None,\n    no_chunks: bool,\n    silence_boundaries: bool,\n    whisper_boundaries: bool,\n) -&gt; None:\n    \"\"\"\n    Validate the CLI inputs to ensure logical consistency given all the flags.\n\n    Conditions &amp; Requirements:\n    1. At least one action (yt_download, split, transcribe) should be requested.\n       Otherwise, nothing is done, so raise an error.\n\n    2. If yt_download is True:\n       - Must specify either yt_process_url OR yt_process_url_list (not both, not none).\n\n    3. If yt_download is False:\n       - If split is requested, we need a local audio file (since no download will occur).\n       - If transcribe is requested without split and without yt_download:\n         - If no_chunks = False, we must have chunk_dir to read existing chunks.\n         - If no_chunks = True, we must have a local audio file (direct transcription) or previously downloaded file\n           (but since yt_download=False, previously downloaded file scenario doesn't apply here,\n           so effectively we need local audio in that scenario).\n\n    4. no_chunks flag:\n       - If no_chunks = True, we are doing direct transcription on entire audio without chunking.\n         - Cannot use split if no_chunks = True. (Mutually exclusive)\n         - chunk_dir is irrelevant if no_chunks = True; since we don't split into chunks,\n           requiring a chunk_dir doesn't make sense. If provided, it's not useful, but let's allow it silently\n           or raise an error for clarity. It's safer to raise an error to prevent user confusion.\n\n    5. Boundaries flags (silence_boundaries, whisper_boundaries):\n       - These flags control how splitting is done.\n       - If split = False, these are irrelevant. Not necessarily an error, but could be a no-op.\n         For robustness, raise an error if user specifies these without split, to avoid confusion.\n       - If split = True and no_chunks = True, that\u2019s contradictory already, so no need for boundary logic there.\n       - If split = True, exactly one method should be chosen:\n         If both silence_boundaries and whisper_boundaries are True simultaneously or both are False simultaneously,\n         we need a clear default or raise an error. By the code snippet logic, whisper_boundaries is default True\n         if not stated otherwise. To keep it robust:\n           - If both are True, raise error.\n           - If both are False, that means user explicitly turned them off or never turned on whisper.\n             The code snippet sets whisper_boundaries True by default. If user sets it False somehow,\n             we can then default to silence. Just ensure at run-time we have a deterministic method:\n             If both are False, we can default to whisper or silence. Let's default to whisper if no flags given.\n             However, given the code snippet, whisper_boundaries has a default of True.\n             If the user sets whisper_boundaries to False and also does not set silence_boundaries,\n             then no method is chosen. Let's then raise an error if both ended up False to avoid ambiguity.\n\n    Raises:\n        ValueError: If the input arguments are not logically consistent.\n    \"\"\"\n\n    # 1. Check that we have at least one action\n    if not is_download and not split and not transcribe:\n        raise ValueError(\n            \"No actions requested. At least one of --yt_download, --split, --transcribe, or --full must be set.\"\n        )\n\n    # 2. Validate YouTube download logic\n    if is_download:\n        if yt_url and yt_url_list:\n            raise ValueError(\n                \"Both --yt_process_url and --yt_process_url_list provided. Only one allowed.\"\n            )\n        if not yt_url and not yt_url_list:\n            raise ValueError(\n                \"When --yt_download is specified, you must provide --yt_process_url or --yt_process_url_list.\"\n            )\n\n    # 3. Logic when no YouTube download:\n    if not is_download:\n        # If splitting but no download, need an audio file\n        if split and audio_file is None:\n            raise ValueError(\n                \"Splitting requested but no audio file provided and no YouTube download source available.\"\n            )\n\n        if transcribe and not split:\n            if no_chunks:\n                # Direct transcription, need an audio file\n                if audio_file is None:\n                    raise ValueError(\n                        \"Transcription requested with no_chunks=True but no audio file provided.\"\n                    )\n            elif chunk_dir is None:\n                raise ValueError(\n                    \"Transcription requested without splitting or downloading and no_chunks=False. Must provide --chunk_dir with pre-split chunks.\"\n                )\n\n    # Check no_chunks scenario:\n    # no_chunks and split are mutually exclusive\n    # If transcribing but not splitting or downloading:\n    # If no_chunks and chunk_dir provided, it doesn't make sense since we won't use chunks at all.\n    # 4. no_chunks flag validation:\n    # no_chunks=False, we need chunks from chunk_dir\n    if no_chunks:\n        if split:\n            raise ValueError(\n                \"Cannot use --no_chunks and --split together. Choose one option.\"\n            )\n        if chunk_dir is not None:\n            raise ValueError(\"Cannot specify --chunk_dir when --no_chunks is set.\")\n\n    # 5. Boundaries flags:\n    # If splitting is not requested but boundaries flags are set, it's meaningless.\n    # The code snippet defaults whisper_boundaries to True, so if user tries to turn it off and sets silence?\n    # We'll require that boundaries only matter if split is True.\n    if not split and (silence_boundaries or whisper_boundaries):\n        raise ValueError(\n            \"Boundary detection flags given but splitting is not requested. Remove these flags or enable --split.\"\n        )\n\n    # If split is True, we must have a consistent boundary method:\n    if split:\n        # If both whisper and silence are somehow True:\n        if silence_boundaries and whisper_boundaries:\n            raise ValueError(\n                \"Cannot use both --silence_boundaries and --whisper_boundaries simultaneously.\"\n            )\n\n        # If both are False:\n        # Given the original snippet, whisper_boundaries is True by default.\n        # For the sake of robustness, let's say if user sets both off, we can't proceed:\n        if not silence_boundaries and not whisper_boundaries:\n            raise ValueError(\n                \"No boundary method selected for splitting. Enable either whisper or silence boundaries.\"\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.version_check","title":"<code>version_check</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.version_check.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.version_check.YTDVersionChecker","title":"<code>YTDVersionChecker</code>","text":"<p>Simple version checker for yt-dlp with robust version comparison.</p> <p>This is a prototype implementation may need expansion in these areas: - Caching to prevent frequent PyPI calls - More comprehensive error handling for:     - Missing/uninstalled packages     - Network timeouts     - JSON parsing errors     - Invalid version strings - Environment detection (virtualenv, conda, system Python) - Configuration options for version pinning - Proxy support for network requests</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/version_check.py</code> <pre><code>class YTDVersionChecker:\n    \"\"\"\n    Simple version checker for yt-dlp with robust version comparison.\n\n    This is a prototype implementation may need expansion in these areas:\n    - Caching to prevent frequent PyPI calls\n    - More comprehensive error handling for:\n        - Missing/uninstalled packages\n        - Network timeouts\n        - JSON parsing errors\n        - Invalid version strings\n    - Environment detection (virtualenv, conda, system Python)\n    - Configuration options for version pinning\n    - Proxy support for network requests\n    \"\"\"\n\n    PYPI_URL = \"https://pypi.org/pypi/yt-dlp/json\"\n    NETWORK_TIMEOUT = 5  # seconds\n\n    def _get_installed_version(self) -&gt; Version:\n        \"\"\"\n        Get installed yt-dlp version.\n\n        Returns:\n            Version object representing installed version\n\n        Raises:\n            ImportError: If yt-dlp is not installed\n            InvalidVersion: If installed version string is invalid\n        \"\"\"\n        try:\n            if version_str := str(importlib.metadata.version(\"yt-dlp\")):\n                return Version(version_str)\n            else:\n                raise InvalidVersion(\"yt-dlp version string is empty\")\n        except importlib.metadata.PackageNotFoundError as e:\n            raise ImportError(\"yt-dlp is not installed\") from e\n        except InvalidVersion:\n            raise\n\n    def _get_latest_version(self) -&gt; Version:\n        \"\"\"\n        Get latest version from PyPI.\n\n        Returns:\n            Version object representing latest available version\n\n        Raises:\n            requests.RequestException: For any network-related errors\n            InvalidVersion: If PyPI version string is invalid\n            KeyError: If PyPI response JSON is malformed\n        \"\"\"\n        try:\n            response = requests.get(self.PYPI_URL, timeout=self.NETWORK_TIMEOUT)\n            response.raise_for_status()\n            version_str = response.json()[\"info\"][\"version\"]\n            return Version(version_str)\n        except requests.RequestException as e:\n            raise requests.RequestException(\n                \"Failed to fetch version from PyPI. Check network connection.\"\n            ) from e\n\n    def check_version(self) -&gt; Tuple[bool, Version, Version]:\n        \"\"\"\n        Check if yt-dlp needs updating.\n\n        Returns:\n            Tuple of (needs_update, installed_version, latest_version)\n\n        Raises:\n            ImportError: If yt-dlp is not installed\n            requests.RequestException: For network-related errors\n            InvalidVersion: If version strings are invalid\n        \"\"\"\n        installed_version = self._get_installed_version()\n        latest_version = self._get_latest_version()\n\n        needs_update = installed_version &lt; latest_version\n        return needs_update, installed_version, latest_version\n</code></pre> <code>NETWORK_TIMEOUT = 5</code> <code>class-attribute</code> <code>instance-attribute</code> <code>PYPI_URL = 'https://pypi.org/pypi/yt-dlp/json'</code> <code>class-attribute</code> <code>instance-attribute</code> <code>check_version()</code> <p>Check if yt-dlp needs updating.</p> <p>Returns:</p> Type Description <code>Tuple[bool, Version, Version]</code> <p>Tuple of (needs_update, installed_version, latest_version)</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If yt-dlp is not installed</p> <code>RequestException</code> <p>For network-related errors</p> <code>InvalidVersion</code> <p>If version strings are invalid</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/version_check.py</code> <pre><code>def check_version(self) -&gt; Tuple[bool, Version, Version]:\n    \"\"\"\n    Check if yt-dlp needs updating.\n\n    Returns:\n        Tuple of (needs_update, installed_version, latest_version)\n\n    Raises:\n        ImportError: If yt-dlp is not installed\n        requests.RequestException: For network-related errors\n        InvalidVersion: If version strings are invalid\n    \"\"\"\n    installed_version = self._get_installed_version()\n    latest_version = self._get_latest_version()\n\n    needs_update = installed_version &lt; latest_version\n    return needs_update, installed_version, latest_version\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.version_check.check_ytd_version","title":"<code>check_ytd_version()</code>","text":"<p>Check if yt-dlp needs updating and log appropriate messages.</p> <p>This function checks the installed version of yt-dlp against the latest version on PyPI and logs informational or error messages as appropriate. It handles network errors, missing packages, and version parsing issues gracefully.</p> <p>The function does not raise exceptions but logs them using the application's logging system.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/version_check.py</code> <pre><code>def check_ytd_version() -&gt; None:\n    \"\"\"\n    Check if yt-dlp needs updating and log appropriate messages.\n\n    This function checks the installed version of yt-dlp against the latest version\n    on PyPI and logs informational or error messages as appropriate. It handles\n    network errors, missing packages, and version parsing issues gracefully.\n\n    The function does not raise exceptions but logs them using the application's\n    logging system.\n    \"\"\"\n    checker = YTDVersionChecker()\n    try:\n        needs_update, current, latest = checker.check_version()\n        if needs_update:\n            logger.info(f\"Update available: {current} -&gt; {latest}\")\n            logger.info(\"Please run the appropriate upgrade in your environment.\")\n            logger.info(\"   For example: pip install --upgrade yt-dlp \")\n        else:\n            logger.info(f\"yt-dlp is up to date (version {current})\")\n\n    except ImportError as e:\n        logger.error(f\"In yt-dlp version check: Package error: {e}\")\n    except requests.RequestException as e:\n        logger.error(f\"In yt-dlp version check: Network error: {e}\")\n    except InvalidVersion as e:\n        logger.error(f\"In yt-dlp version check: Version parsing error: {e}\")\n    except Exception as e:\n        logger.error(f\"In yt-dlp version check: Unexpected error: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.nfmt","title":"<code>nfmt</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.nfmt.nfmt","title":"<code>nfmt</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.nfmt.nfmt.main","title":"<code>main()</code>","text":"<p>Entry point for the nfmt CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/nfmt/nfmt.py</code> <pre><code>def main():\n    \"\"\"Entry point for the nfmt CLI tool.\"\"\"\n    nfmt()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.nfmt.nfmt.nfmt","title":"<code>nfmt(input_file, output, spacing)</code>","text":"<p>Normalize the number of newlines in a text file.</p> Source code in <code>src/tnh_scholar/cli_tools/nfmt/nfmt.py</code> <pre><code>@click.command()\n@click.argument(\"input_file\", type=click.File(\"r\"), default=\"-\")\n@click.option(\n    \"-o\",\n    \"--output\",\n    type=click.File(\"w\"),\n    default=\"-\",\n    help=\"Output file (default: stdout)\",\n)\n@click.option(\n    \"-s\", \"--spacing\", default=2, help=\"Number of newlines between blocks (default: 2)\"\n)\ndef nfmt(input_file, output, spacing):\n    \"\"\"Normalize the number of newlines in a text file.\"\"\"\n    text = input_file.read()\n    result = normalize_newlines(text, spacing)\n    output.write(result)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split","title":"<code>sent_split</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split","title":"<code>sent_split</code>","text":"<p>Simple CLI tool for sentence splitting.</p> <p>This module provides a command line interface for splitting text into sentences. Uses NLTK for robust sentence tokenization. Reads from stdin and writes to stdout by default, with optional file input/output.</p>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.ensure_nltk_data","title":"<code>ensure_nltk_data()</code>","text":"<p>Ensure NLTK punkt tokenizer is available.</p> Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>def ensure_nltk_data():\n    \"\"\"Ensure NLTK punkt tokenizer is available.\"\"\"\n    try:\n        # Try to find the resource\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        # If not found, try downloading\n        try:\n            nltk.download('punkt', quiet=True)\n            # Verify download\n            nltk.data.find('tokenizers/punkt')\n        except Exception as e:\n            raise RuntimeError(\n                \"Failed to download required NLTK data. \"\n                \"Please run 'python -m nltk.downloader punkt' \"\n                f\"to install manually. Error: {e}\"\n            ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.main","title":"<code>main()</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>def main():\n    sent_split()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.process_text","title":"<code>process_text(text, newline=True)</code>","text":"<p>Split text into sentences using NLTK.</p> Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>def process_text(text: str, newline: bool = True) -&gt; str:\n    \"\"\"Split text into sentences using NLTK.\"\"\"\n    ensure_nltk_data()\n    sentences = sent_tokenize(text)\n    return \"\\n\".join(sentences) if newline else \" \".join(sentences)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.sent_split","title":"<code>sent_split(input_file, output, space)</code>","text":"<p>Split text into sentences using NLTK's sentence tokenizer.</p> <p>Reads from stdin if no input file is specified. Writes to stdout if no output file is specified.</p> Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>@click.command()\n@click.argument('input_file', type=click.File('r'), required=False)\n@click.option('-o', '--output', type=click.File('w'),\n              help='Output file (default: stdout)')\n@click.option('-s', '--space', is_flag=True,\n              help='Separate sentences with spaces instead of newlines')\ndef sent_split(input_file: Optional[TextIO],\n               output: Optional[TextIO],\n               space: bool) -&gt; None:\n    \"\"\"Split text into sentences using NLTK's sentence tokenizer.\n\n    Reads from stdin if no input file is specified.\n    Writes to stdout if no output file is specified.\n    \"\"\"\n    try:\n        # Read from file or stdin\n        input_text = input_file.read() if input_file else sys.stdin.read()\n\n        # Process the text\n        result = process_text(input_text, newline=not space)\n\n        # Write to file or stdout\n        output_file = output or sys.stdout\n        output_file.write(result)\n\n        if output:\n            click.echo(f\"Output written to: {output.name}\")\n\n    except Exception as e:\n        click.echo(f\"Error processing text: {e}\", err=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab","title":"<code>tnh_fab</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab","title":"<code>tnh_fab</code>","text":"<p>TNH-FAB Command Line Interface</p> <p>Part of the THICH NHAT HANH SCHOLAR (TNH_SCHOLAR) project. A rapid prototype implementation of the TNH-FAB command-line tool for Open AI based text processing. Provides core functionality for text punctuation, sectioning, translation, and processing.</p>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.DEFAULT_SECTION_PATTERN","title":"<code>DEFAULT_SECTION_PATTERN = 'default_section'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.pass_config","title":"<code>pass_config = click.make_pass_decorator(TNHFabConfig, ensure=True)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.TNHFabConfig","title":"<code>TNHFabConfig</code>","text":"<p>Holds configuration for the TNH-FAB CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>class TNHFabConfig:\n    \"\"\"Holds configuration for the TNH-FAB CLI tool.\"\"\"\n\n    def __init__(self):\n        self.verbose: bool = False\n        self.debug: bool = False\n        self.quiet: bool = False\n        # Initialize pattern manager with directory set in .env file or default.\n\n        load_dotenv()\n\n        if pattern_path_name := os.getenv(\"TNH_PATTERN_DIR\"):\n            pattern_dir = Path(pattern_path_name)\n            logger.debug(f\"pattern dir: {pattern_path_name}\")\n        else:\n            pattern_dir = TNH_DEFAULT_PATTERN_DIR\n\n        pattern_dir.mkdir(parents=True, exist_ok=True)\n        self.pattern_manager = PatternManager(pattern_dir)\n</code></pre> <code>debug = False</code> <code>instance-attribute</code> <code>pattern_manager = PatternManager(pattern_dir)</code> <code>instance-attribute</code> <code>quiet = False</code> <code>instance-attribute</code> <code>verbose = False</code> <code>instance-attribute</code> <code>__init__()</code> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>def __init__(self):\n    self.verbose: bool = False\n    self.debug: bool = False\n    self.quiet: bool = False\n    # Initialize pattern manager with directory set in .env file or default.\n\n    load_dotenv()\n\n    if pattern_path_name := os.getenv(\"TNH_PATTERN_DIR\"):\n        pattern_dir = Path(pattern_path_name)\n        logger.debug(f\"pattern dir: {pattern_path_name}\")\n    else:\n        pattern_dir = TNH_DEFAULT_PATTERN_DIR\n\n    pattern_dir.mkdir(parents=True, exist_ok=True)\n    self.pattern_manager = PatternManager(pattern_dir)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.get_pattern","title":"<code>get_pattern(pattern_manager, pattern_name)</code>","text":"<p>Get pattern from the pattern manager.</p> <p>Parameters:</p> Name Type Description Default <code>pattern_manager</code> <code>PatternManager</code> <p>Initialized PatternManager instance</p> required <code>pattern_name</code> <code>str</code> <p>Name of the pattern to load</p> required <p>Returns:</p> Name Type Description <code>Pattern</code> <code>Pattern</code> <p>Loaded pattern object</p> <p>Raises:</p> Type Description <code>ClickException</code> <p>If pattern cannot be loaded</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>def get_pattern(pattern_manager: PatternManager, pattern_name: str) -&gt; Pattern:\n    \"\"\"\n    Get pattern from the pattern manager.\n\n    Args:\n        pattern_manager: Initialized PatternManager instance\n        pattern_name: Name of the pattern to load\n\n    Returns:\n        Pattern: Loaded pattern object\n\n    Raises:\n        click.ClickException: If pattern cannot be loaded\n    \"\"\"\n    try:\n        return pattern_manager.load_pattern(pattern_name)\n    except FileNotFoundError as e:\n        raise click.ClickException(\n            f\"Pattern '{pattern_name}' not found in {pattern_manager.base_path}\"\n        ) from e\n    except Exception as e:\n        raise click.ClickException(f\"Error loading pattern: {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.main","title":"<code>main()</code>","text":"<p>Entry point for TNH-FAB CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>def main():\n    \"\"\"Entry point for TNH-FAB CLI tool.\"\"\"\n    tnh_fab()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.process","title":"<code>process(config, input_file, pattern, section, paragraph, template)</code>","text":"<p>Apply custom pattern-based processing to text with flexible structuring options.</p> <p>This command provides flexible text processing using customizable patterns. It can process text either by sections (defined in a JSON file or auto-detected), by paragraphs, or can be used to process a text as a whole (this is the default). This is particularly useful for formatting, restructuring, or applying consistent transformations to text.</p> <p>Examples:</p> <pre><code>\b\n# Process using a specific pattern\n$ tnh-fab process -p format_xml input.txt\n\n\b\n# Process using paragraph mode\n$ tnh-fab process -p format_xml -g input.txt\n\n\b\n# Process with custom sections\n$ tnh-fab process -p format_xml -s sections.json input.txt\n\n\b\n# Process with template values\n$ tnh-fab process -p format_xml -t template.yaml input.txt\n</code></pre> <p>Processing Modes:</p> <pre><code>\b\n1. Single Input Mode (default)\n    - Processes entire input.\n\n\b\n2. Section Mode (-s):\n    - Uses sections from JSON file if provided (-s)\n    - If no section file is provided, sections are auto-generated.\n    - Processes each section according to pattern\n\n\b\n3. Paragraph Mode (-g):\n    - Treats each line/paragraph as a separate unit\n    - Useful for simpler processing tasks\n    - More memory efficient for large files\n</code></pre> <p>\b Notes:     - Required pattern must exist in pattern directory     - Template values can customize pattern behavior</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>@tnh_fab.command()\n@click.argument(\n    \"input_file\", type=click.Path(exists=True, path_type=Path), required=False\n)\n@click.option(\"-p\", \"--pattern\", required=True, help=\"Pattern name for processing\")\n@click.option(\n    \"-s\",\n    \"--section\",\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Process using sections from JSON file, or auto-generate if no file provided\",\n)\n@click.option(\"-g\", \"--paragraph\", is_flag=True, help=\"Process text by paragraphs\")\n@click.option(\n    \"-t\",\n    \"--template\",\n    type=click.Path(exists=True, path_type=Path),\n    help=\"YAML file containing template values\",\n)\n@pass_config\ndef process(\n    config: TNHFabConfig,\n    input_file: Optional[Path],\n    pattern: str,\n    section: Optional[Path],\n    paragraph: bool,\n    template: Optional[Path],\n):\n    \"\"\"Apply custom pattern-based processing to text with flexible structuring options.\n\n    This command provides flexible text processing using customizable patterns. It can\n    process text either by sections (defined in a JSON file or auto-detected), by\n    paragraphs, or can be used to process a text as a whole (this is the default).\n    This is particularly useful for formatting, restructuring, or applying\n    consistent transformations to text.\n\n    Examples:\n\n        \\b\n        # Process using a specific pattern\n        $ tnh-fab process -p format_xml input.txt\n\n        \\b\n        # Process using paragraph mode\n        $ tnh-fab process -p format_xml -g input.txt\n\n        \\b\n        # Process with custom sections\n        $ tnh-fab process -p format_xml -s sections.json input.txt\n\n        \\b\n        # Process with template values\n        $ tnh-fab process -p format_xml -t template.yaml input.txt\n\n\n    Processing Modes:\n\n        \\b\n        1. Single Input Mode (default)\n            - Processes entire input.\n\n        \\b\n        2. Section Mode (-s):\n            - Uses sections from JSON file if provided (-s)\n            - If no section file is provided, sections are auto-generated.\n            - Processes each section according to pattern\n\n        \\b\n        3. Paragraph Mode (-g):\n            - Treats each line/paragraph as a separate unit\n            - Useful for simpler processing tasks\n            - More memory efficient for large files\n\n    \\b\n    Notes:\n        - Required pattern must exist in pattern directory\n        - Template values can customize pattern behavior\n\n    \"\"\"\n    text = read_input(click, input_file)  # type: ignore\n    process_pattern = get_pattern(config.pattern_manager, pattern)\n\n    template_dict: Dict[str, str] = {}\n\n    if paragraph:\n        result = process_text_by_paragraphs(\n            text, template_dict, pattern=process_pattern\n        )\n        for processed in result:\n            click.echo(processed)\n    elif section is not None:  # Section mode (either file or auto-generate)\n        if isinstance(section, Path):  # Section file provided\n            sections_json = Path(section).read_text()\n            text_obj = TextObject.model_validate_json(sections_json)\n\n        else:  # Auto-generate sections\n            default_section_pattern = get_pattern(\n                config.pattern_manager, DEFAULT_SECTION_PATTERN\n            )\n            text_obj = find_sections(text, section_pattern=default_section_pattern)\n\n        result = process_text_by_sections(\n            text, text_obj, template_dict, pattern=process_pattern\n        )\n        for processed_section in result:\n            click.echo(processed_section.processed_text)\n    else:\n        result = process_text(\n            text, pattern=process_pattern, template_dict=template_dict\n        )\n        click.echo(result)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.punctuate","title":"<code>punctuate(config, input_file, language, style, review_count, pattern)</code>","text":"<p>Add punctuation and structure to text based on language-specific rules.</p> <p>This command processes input text to add or correct punctuation, spacing, and basic structural elements. It is particularly useful for texts that lack proper punctuation or need standardization.</p> <p>Examples:</p> <pre><code>\b\n# Process a file using default settings\n$ tnh-fab punctuate input.txt\n\n\b\n# Process Vietnamese text with custom style\n$ tnh-fab punctuate -l vi -y \"Modern\" input.txt\n\n\b\n# Process from stdin with increased review passes\n$ cat input.txt | tnh-fab punctuate -c 5\n</code></pre> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>@tnh_fab.command()\n@click.argument(\n    \"input_file\", type=click.Path(exists=True, path_type=Path), required=False\n)\n@click.option(\n    \"-l\",\n    \"--language\",\n    help=\"Source language code (e.g., 'en', 'vi'). Auto-detected if not specified.\",\n)\n@click.option(\n    \"-y\", \"--style\", default=\"APA\", help=\"Punctuation style to apply (default: 'APA')\"\n)\n@click.option(\n    \"-c\",\n    \"--review-count\",\n    type=int,\n    default=3,\n    help=\"Number of review passes (default: 3)\",\n)\n@click.option(\n    \"-p\",\n    \"--pattern\",\n    default=\"default_punctuate\",\n    help=\"Pattern name for punctuation rules (default: 'default_punctuate')\",\n)\n@pass_config\ndef punctuate(\n    config: TNHFabConfig,\n    input_file: Optional[Path],\n    language: Optional[str],\n    style: str,\n    review_count: int,\n    pattern: str,\n):\n    \"\"\"Add punctuation and structure to text based on language-specific rules.\n\n    This command processes input text to add or correct punctuation, spacing, and basic\n    structural elements. It is particularly useful for texts that lack proper punctuation\n    or need standardization.\n\n\n    Examples:\n\n        \\b\n        # Process a file using default settings\n        $ tnh-fab punctuate input.txt\n\n        \\b\n        # Process Vietnamese text with custom style\n        $ tnh-fab punctuate -l vi -y \"Modern\" input.txt\n\n        \\b\n        # Process from stdin with increased review passes\n        $ cat input.txt | tnh-fab punctuate -c 5\n\n    \"\"\"\n    text = read_input(click, input_file)  # type: ignore\n    punctuate_pattern = get_pattern(config.pattern_manager, pattern)\n    result = punctuate_text(\n        text,\n        source_language=language,\n        punctuate_pattern=punctuate_pattern,\n        template_dict={\"style_convention\": style, \"review_count\": review_count},\n    )\n    click.echo(result)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.read_input","title":"<code>read_input(ctx, input_file)</code>","text":"<p>Read input from file or stdin.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>def read_input(ctx: Context, input_file: Optional[Path]) -&gt; str:\n    \"\"\"Read input from file or stdin.\"\"\"\n    if input_file:\n        return input_file.read_text()\n    if not sys.stdin.isatty():\n        return sys.stdin.read()\n    ctx.fail(\"No input provided\")\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.section","title":"<code>section(config, input_file, language, num_sections, review_count, pattern)</code>","text":"<p>Analyze and divide text into logical sections based on content.</p> <p>This command processes the input text to identify coherent sections based on content analysis. It generates a structured representation of the text with sections that maintain logical continuity. Each section includes metadata such as title and line range.</p> <p>Examples:</p> <pre><code>\b\n# Auto-detect sections in a file\n$ tnh-fab section input.txt\n\n\b\n# Specify desired number of sections\n$ tnh-fab section -n 5 input.txt\n\n\b\n# Process Vietnamese text with custom pattern\n$ tnh-fab section -l vi -p custom_section_pattern input.txt\n\n\b\n# Section text from stdin with increased review\n$ cat input.txt | tnh-fab section -c 5\n</code></pre> <p>\b Output Format:     JSON object containing:     - language: Detected or specified language code     - sections: Array of section objects, each with:         - title: Section title in original language         - start_line: Starting line number (inclusive)         - end_line: Ending line number (inclusive)</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>@tnh_fab.command()\n@click.argument(\n    \"input_file\", type=click.Path(exists=True, path_type=Path), required=False\n)\n@click.option(\n    \"-l\",\n    \"--language\",\n    help=\"Source language code (e.g., 'en', 'vi'). Auto-detected if not specified.\",\n)\n@click.option(\n    \"-n\",\n    \"--num-sections\",\n    type=int,\n    help=\"Target number of sections (auto-calculated if not specified)\",\n)\n@click.option(\n    \"-c\",\n    \"--review-count\",\n    type=int,\n    default=3,\n    help=\"Number of review passes (default: 3)\",\n)\n@click.option(\n    \"-p\",\n    \"--pattern\",\n    default=\"default_section\",\n    help=\"Pattern name for section analysis (default: 'default_section')\",\n)\n@pass_config\ndef section(\n    config: TNHFabConfig,\n    input_file: Optional[Path],\n    language: Optional[str],\n    num_sections: Optional[int],\n    review_count: int,\n    pattern: str,\n):\n    \"\"\"Analyze and divide text into logical sections based on content.\n\n    This command processes the input text to identify coherent sections based on content\n    analysis. It generates a structured representation of the text with sections that\n    maintain logical continuity. Each section includes metadata such as title and line\n    range.\n\n    Examples:\n\n        \\b\n        # Auto-detect sections in a file\n        $ tnh-fab section input.txt\n\n        \\b\n        # Specify desired number of sections\n        $ tnh-fab section -n 5 input.txt\n\n        \\b\n        # Process Vietnamese text with custom pattern\n        $ tnh-fab section -l vi -p custom_section_pattern input.txt\n\n        \\b\n        # Section text from stdin with increased review\n        $ cat input.txt | tnh-fab section -c 5\n\n    \\b\n    Output Format:\n        JSON object containing:\n        - language: Detected or specified language code\n        - sections: Array of section objects, each with:\n            - title: Section title in original language\n            - start_line: Starting line number (inclusive)\n            - end_line: Ending line number (inclusive)\n    \"\"\"\n    text = read_input(click, input_file)  # type: ignore\n    section_pattern = get_pattern(config.pattern_manager, pattern)\n    result = find_sections(\n        text,\n        source_language=language,\n        section_pattern=section_pattern,\n        section_count=num_sections,\n        review_count=review_count,\n    )\n    # For prototype, just output the JSON representation\n    click.echo(result.model_dump_json(indent=2))\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.tnh_fab","title":"<code>tnh_fab(ctx, verbose, debug, quiet)</code>","text":"<p>TNH-FAB: Thich Nhat Hanh Scholar Text processing command-line tool.</p> <p>CORE COMMANDS: punctuate, section, translate, process</p> <p>To Get help on any command and see its options:</p> <p>tnh-fab [COMMAND] --help</p> <p>Provides specialized processing for multi-lingual Dharma content.</p> <p>Offers functionalities for punctuation, sectioning, line-based translation, and general text processing based on predefined patterns. Input text can be provided either via a file or standard input.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>@click.group()\n@click.option(\"-v\", \"--verbose\", is_flag=True, help=\"Enable detailed logging. (NOT implemented)\")\n@click.option(\"--debug\", is_flag=True, help=\"Enable debug output\")\n@click.option(\"--quiet\", is_flag=True, help=\"Suppress all non-error output\")\n@click.pass_context\ndef tnh_fab(ctx: Context, verbose: bool, debug: bool, quiet: bool):\n    \"\"\"TNH-FAB: Thich Nhat Hanh Scholar Text processing command-line tool.\n\n    CORE COMMANDS: punctuate, section, translate, process\n\n    To Get help on any command and see its options:\n\n    tnh-fab [COMMAND] --help\n\n    Provides specialized processing for multi-lingual Dharma content.\n\n    Offers functionalities for punctuation, sectioning, line-based translation,\n    and general text processing based on predefined patterns.\n    Input text can be provided either via a file or standard input.\n    \"\"\"        \n    config = ctx.ensure_object(TNHFabConfig)\n\n    if not check_openai_env():\n\n        ctx.fail(\"Missing OpenAI Credentials.\")\n\n    config.verbose = verbose\n    config.debug = debug\n    config.quiet = quiet\n\n    if not quiet:\n        if debug:\n            setup_logging(log_level=logging.DEBUG)\n        else:\n            setup_logging(log_level=logging.INFO)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.translate","title":"<code>translate(config, input_file, language, target, style, context_lines, segment_size, pattern)</code>","text":"<p>Translate text while preserving line numbers and contextual understanding.</p> <p>This command performs intelligent translation that maintains line number correspondence between source and translated text. It uses surrounding context to improve translation accuracy and consistency, particularly important for Buddhist texts where terminology and context are crucial.</p> <p>Examples:</p> <pre><code>\b\n# Translate Vietnamese text to English\n$ tnh-fab translate -l vi input.txt\n\n\b\n# Translate to French with specific style\n$ tnh-fab translate -l vi -r fr -y \"Formal\" input.txt\n\n\b\n# Translate with increased context\n$ tnh-fab translate --context-lines 5 input.txt\n\n\b\n# Translate using custom segment size\n$ tnh-fab translate --segment-size 10 input.txt\n</code></pre> <p>\b Notes:     - Line numbers are preserved in the output     - Context lines are used to improve translation accuracy     - Segment size affects processing speed and memory usage</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>@tnh_fab.command()\n@click.argument(\n    \"input_file\", type=click.Path(exists=True, path_type=Path), required=False\n)\n@click.option(\n    \"-l\", \"--language\", help=\"Source language code. Auto-detected if not specified.\"\n)\n@click.option(\n    \"-r\", \"--target\", default=\"en\", help=\"Target language code (default: 'en')\"\n)\n@click.option(\n    \"-y\", \"--style\", help=\"Translation style (e.g., 'American Dharma Teaching')\"\n)\n@click.option(\n    \"--context-lines\",\n    type=int,\n    default=3,\n    help=\"Number of context lines to consider (default: 3)\",\n)\n@click.option(\n    \"--segment-size\",\n    type=int,\n    help=\"Lines per translation segment (auto-calculated if not specified)\",\n)\n@click.option(\n    \"-p\",\n    \"--pattern\",\n    default=\"default_line_translation\",\n    help=\"Pattern name for translation (default: 'default_line_translation')\",\n)\n@pass_config\ndef translate(\n    config: TNHFabConfig,\n    input_file: Optional[Path],\n    language: Optional[str],\n    target: str,\n    style: Optional[str],\n    context_lines: int,\n    segment_size: Optional[int],\n    pattern: str,\n):\n    \"\"\"Translate text while preserving line numbers and contextual understanding.\n\n    This command performs intelligent translation that maintains line number correspondence\n    between source and translated text. It uses surrounding context to improve translation\n    accuracy and consistency, particularly important for Buddhist texts where terminology\n    and context are crucial.\n\n    Examples:\n\n        \\b\n        # Translate Vietnamese text to English\n        $ tnh-fab translate -l vi input.txt\n\n        \\b\n        # Translate to French with specific style\n        $ tnh-fab translate -l vi -r fr -y \"Formal\" input.txt\n\n        \\b\n        # Translate with increased context\n        $ tnh-fab translate --context-lines 5 input.txt\n\n        \\b\n        # Translate using custom segment size\n        $ tnh-fab translate --segment-size 10 input.txt\n\n    \\b\n    Notes:\n        - Line numbers are preserved in the output\n        - Context lines are used to improve translation accuracy\n        - Segment size affects processing speed and memory usage\n    \"\"\"\n    text = read_input(click, input_file)  # type: ignore\n    translation_pattern = get_pattern(config.pattern_manager, pattern)\n    result = translate_text_by_lines(\n        text,\n        source_language=language,\n        target_language=target,\n        pattern=translation_pattern,\n        style=style,\n        context_lines=context_lines,\n        segment_size=segment_size,\n    )\n    click.echo(result)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_setup","title":"<code>tnh_setup</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup","title":"<code>tnh_setup</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.OPENAI_ENV_HELP_MSG","title":"<code>OPENAI_ENV_HELP_MSG = \"\\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; OpenAI API key not found in environment. &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\\n\\nFor AI processing with TNH-scholar:\\n\\n1. Get an API key from https://platform.openai.com/api-keys\\n2. Set the OPENAI_API_KEY environment variable:\\n\\n   export OPENAI_API_KEY='your-api-key-here'  # Linux/Mac\\n   set OPENAI_API_KEY=your-api-key-here       # Windows\\n\\nFor OpenAI API access help: https://platform.openai.com/\\n\\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; -- &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\\n\"</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.PATTERNS_URL","title":"<code>PATTERNS_URL = 'https://github.com/aaronksolomon/patterns/archive/main.zip'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.create_config_dirs","title":"<code>create_config_dirs()</code>","text":"<p>Create required configuration directories.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_setup/tnh_setup.py</code> <pre><code>def create_config_dirs():\n    \"\"\"Create required configuration directories.\"\"\"\n    TNH_CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n    TNH_LOG_DIR.mkdir(exist_ok=True)\n    TNH_DEFAULT_PATTERN_DIR.mkdir(exist_ok=True)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.download_patterns","title":"<code>download_patterns()</code>","text":"<p>Download and extract pattern files from GitHub.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_setup/tnh_setup.py</code> <pre><code>def download_patterns() -&gt; bool:\n    \"\"\"Download and extract pattern files from GitHub.\"\"\"\n    try:\n        response = requests.get(PATTERNS_URL)\n        response.raise_for_status()\n\n        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n            root_dir = zip_ref.filelist[0].filename.split('/')[0]\n\n            for zip_info in zip_ref.filelist:\n                if zip_info.filename.endswith('.md'):\n                    rel_path = Path(zip_info.filename).relative_to(root_dir)\n                    target_path = TNH_DEFAULT_PATTERN_DIR / rel_path\n\n                    target_path.parent.mkdir(parents=True, exist_ok=True)\n\n                    with zip_ref.open(zip_info) as source, open(target_path, 'wb') as target:\n                        target.write(source.read())\n        return True\n\n    except Exception as e:\n        click.echo(f\"Pattern download failed: {e}\", err=True)\n        return False\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.main","title":"<code>main()</code>","text":"<p>Entry point for setup CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_setup/tnh_setup.py</code> <pre><code>def main():\n    \"\"\"Entry point for setup CLI tool.\"\"\"\n    tnh_setup()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.tnh_setup","title":"<code>tnh_setup(skip_env, skip_patterns)</code>","text":"<p>Set up TNH Scholar configuration.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_setup/tnh_setup.py</code> <pre><code>@click.command()\n@click.option('--skip-env', is_flag=True, help='Skip API key setup')\n@click.option('--skip-patterns', is_flag=True, help='Skip pattern download')\ndef tnh_setup(skip_env: bool, skip_patterns: bool):\n    \"\"\"Set up TNH Scholar configuration.\"\"\"\n    click.echo(\"Setting up TNH Scholar...\")\n\n    # Create config directories\n    create_config_dirs()\n    click.echo(f\"Created config directory: {TNH_CONFIG_DIR}\")\n\n    # Pattern download\n    if not skip_patterns and click.confirm(\n                \"\\nDownload pattern (markdown text) files from GitHub?\\n\"\n                f\"Source: {PATTERNS_URL}\\n\"\n                f\"Target: {TNH_DEFAULT_PATTERN_DIR}\"\n            ):\n        if download_patterns():\n            click.echo(\"Pattern files downloaded successfully\")\n        else:\n            click.echo(\"Pattern download failed\", err=True)\n\n    # Environment test:\n    if not skip_env:\n        load_dotenv()  # for development\n        if not check_openai_env(output=False):\n            print(OPENAI_ENV_HELP_MSG)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.token_count","title":"<code>token_count</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.token_count.token_count","title":"<code>token_count</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.token_count.token_count.main","title":"<code>main()</code>","text":"<p>Entry point for the token-count CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/token_count/token_count.py</code> <pre><code>def main():\n    \"\"\"Entry point for the token-count CLI tool.\"\"\"\n    token_count_cli()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.token_count.token_count.token_count_cli","title":"<code>token_count_cli(input_file)</code>","text":"<p>Return the Open AI API token count of a text file. Based on gpt-4o.</p> Source code in <code>src/tnh_scholar/cli_tools/token_count/token_count.py</code> <pre><code>@click.command()\n@click.argument(\"input_file\", type=click.File(\"r\"), default=\"-\")\ndef token_count_cli(input_file):\n    \"\"\"Return the Open AI API token count of a text file. Based on gpt-4o.\"\"\"\n    text = input_file.read()\n    result = token_count(text)\n    click.echo(result)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch","title":"<code>ytt_fetch</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch","title":"<code>ytt_fetch</code>","text":"<p>Simple CLI tool for retrieving video transcripts.</p> <p>This module provides a command line interface for downloading video transcripts in specified languages. It uses yt-dlp for video info extraction.</p>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.main","title":"<code>main()</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>def main():\n    ytt_fetch()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.ytt_fetch","title":"<code>ytt_fetch(url, lang, output)</code>","text":"<p>Youtube Transcript Fetch: Retrieve and save transcript for a Youtube video using yt-dlp.</p> Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>@click.command()\n@click.argument(\"url\")\n@click.option(\n    \"-l\", \"--lang\", default=\"en\", help=\"Language code for transcript (default: en)\"\n)\n@click.option(\n    \"-o\",\n    \"--output\",\n    type=click.Path(),\n    help=\"Save transcript text to file instead of printing\",\n)\ndef ytt_fetch(url: str, lang: str, output: Optional[str]) -&gt; None:\n    \"\"\"\n    Youtube Transcript Fetch: Retrieve and save transcript for a Youtube video using yt-dlp.\n    \"\"\"\n\n    try:\n        transcript_text = get_transcript(url, lang)\n\n    except TranscriptNotFoundError as e:\n        click.echo(e, err=True)\n        sys.exit(1)\n    except yt_dlp.utils.DownloadError as e:\n        click.echo(f\"Failed to extract video transcript: {e}\", err=True)\n        sys.exit(1)\n\n    try:\n        if output:\n            output_path = Path(output)\n            write_text_to_file(output_path, transcript_text, overwrite=True)\n            click.echo(f\"Transcript written to: {output_path}\")\n        else:\n            click.echo(transcript_text)\n\n    except FileNotFoundError as e:\n        click.echo(f\"File not found error: {e}\", err=True)\n        sys.exit(1)\n    except (IOError, OSError) as e:\n        click.echo(f\"Error writing transcript to file: {e}\", err=True)\n        sys.exit(1)\n    except TypeError as e:\n        click.echo(f\"Unexpected type error: {e}\", err=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.dev_tools","title":"<code>dev_tools</code>","text":""},{"location":"api/#tnh_scholar.dev_tools.generate_tree","title":"<code>generate_tree</code>","text":""},{"location":"api/#tnh_scholar.dev_tools.generate_tree.ignore_list","title":"<code>ignore_list = ['__pycache__', '*.pyc', '*.pyo', '*.pyd', '.git*', '.pytest_cache', '*.egg-info', 'dist', 'build', 'data', 'processed_data', 'sandbox', 'patterns', '.vscode', 'tmp', 'site']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.dev_tools.generate_tree.ignore_str","title":"<code>ignore_str = '|'.join(ignore_list)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.dev_tools.generate_tree.output_file","title":"<code>output_file = sys.argv[2] if len(sys.argv) &gt; 2 else 'project_directory_tree.txt'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.dev_tools.generate_tree.root_dir","title":"<code>root_dir = sys.argv[1] if len(sys.argv) &gt; 1 else '.'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.dev_tools.generate_tree.generate_tree","title":"<code>generate_tree(root_dir='.', output_file='project_directory_tree.txt')</code>","text":"Source code in <code>src/tnh_scholar/dev_tools/generate_tree.py</code> <pre><code>def generate_tree(root_dir='.', output_file='project_directory_tree.txt'):\n    root = Path(root_dir)\n    output = Path(output_file)\n\n    try:\n        # Try using tree command first\n        subprocess.run([\"tree\", \"-I\", \n            ignore_str,\n            \"-o\", str(output)], check=True)\n    except (subprocess.CalledProcessError, FileNotFoundError):\n        # Fallback to pathlib implementation\n        with open(output, \"w\") as f:\n            f.write(\".\\n\")\n            for path in sorted(root.rglob(\"*\")):\n                if any(p.startswith(\".\") for p in path.parts):\n                    continue\n                if path.name in {\"__pycache__\", \"*.pyc\", \"*.pyo\", \"*.pyd\"}:\n                    continue\n                rel_path = path.relative_to(root)\n                f.write(f\"{'    ' * (len(path.parts)-1)}\u251c\u2500\u2500 {path.name}\\n\")\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing","title":"<code>journal_processing</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process","title":"<code>journal_process</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.BATCH_RETRY_DELAY","title":"<code>BATCH_RETRY_DELAY = 5</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.MAX_BATCH_RETRIES","title":"<code>MAX_BATCH_RETRIES = 40</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.MAX_TOKEN_LIMIT","title":"<code>MAX_TOKEN_LIMIT = 60000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.journal_schema","title":"<code>journal_schema = {'type': 'object', 'properties': {'journal_summary': {'type': 'string'}, 'sections': {'type': 'array', 'items': {'type': 'object', 'properties': {'title_vi': {'type': 'string'}, 'title_en': {'type': 'string'}, 'author': {'type': ['string', 'null']}, 'summary': {'type': 'string'}, 'keywords': {'type': 'array', 'items': {'type': 'string'}}, 'start_page': {'type': 'integer', 'minimum': 1}, 'end_page': {'type': 'integer', 'minimum': 1}}, 'required': ['title_vi', 'title_en', 'summary', 'keywords', 'start_page', 'end_page']}}}, 'required': ['journal_summary', 'sections']}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.logger","title":"<code>logger = logging.getLogger('journal_process')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.batch_section","title":"<code>batch_section(input_xml_path, batch_jsonl, system_message, journal_name)</code>","text":"<p>Splits the journal content into sections using GPT, with retries for both starting and completing the batch.</p> <p>Parameters:</p> Name Type Description Default <code>input_xml_path</code> <code>str</code> <p>Path to the input XML file.</p> required <code>output_json_path</code> <code>str</code> <p>Path to save validated metadata JSON.</p> required <code>raw_output_path</code> <code>str</code> <p>Path to save the raw batch results.</p> required <code>journal_name</code> <code>str</code> <p>Name of the journal being processed.</p> required <code>max_retries</code> <code>int</code> <p>Maximum number of retries for batch processing.</p> required <code>retry_delay</code> <code>int</code> <p>Delay in seconds between retries.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>the result of the batch sectioning process as a serialized json object.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def batch_section(\n    input_xml_path: Path, batch_jsonl: Path, system_message, journal_name\n):\n    \"\"\"\n    Splits the journal content into sections using GPT, with retries for both starting and completing the batch.\n\n    Args:\n        input_xml_path (str): Path to the input XML file.\n        output_json_path (str): Path to save validated metadata JSON.\n        raw_output_path (str): Path to save the raw batch results.\n        journal_name (str): Name of the journal being processed.\n        max_retries (int): Maximum number of retries for batch processing.\n        retry_delay (int): Delay in seconds between retries.\n\n    Returns:\n        str: the result of the batch sectioning process as a serialized json object.\n    \"\"\"\n    try:\n        logger.info(\n            f\"Starting sectioning batch for {journal_name} with file:\\n\\t{input_xml_path}\"\n        )\n        # Load journal content\n        journal_pages = get_text_from_file(input_xml_path)\n\n        # Create GPT messages for sectioning\n        user_message_wrapper = lambda text: f\"{text}\"\n        messages = generate_messages(\n            system_message, user_message_wrapper, [journal_pages]\n        )\n\n        # Create JSONL file for batch processing\n        jsonl_file = create_jsonl_file_for_batch(messages, batch_jsonl, json_mode=True)\n\n    except Exception as e:\n        logger.error(\n            f\"Failed to initialize batch sectioning data for journal '{journal_name}'.\",\n            extra={\"input_xml_path\": input_xml_path},\n            exc_info=True,\n        )\n        raise RuntimeError(\n            f\"Error initializing batch sectioning data for journal '{journal_name}'.\"\n        ) from e\n\n    response = start_batch_with_retries(\n        jsonl_file,\n        description=f\"Batch for sectioning journal: {journal_name} | input file: {input_xml_path}\",\n    )\n\n    if response:\n        json_result = response[\n            0\n        ]  # should return json, just one batch so first response\n        # Log success and return output json\n        logger.info(\n            f\"Successfully batch sectioned journal '{journal_name}' with input file: {input_xml_path}.\"\n        )\n        return json_result\n    else:\n        logger.error(\"Section batch failed to get response.\")\n        return \"\"\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.batch_translate","title":"<code>batch_translate(input_xml_path, batch_json_path, metadata_path, system_message, journal_name)</code>","text":"<p>Translates the journal sections using the GPT model. Saves the translated content back to XML.</p> <p>Parameters:</p> Name Type Description Default <code>input_xml_path</code> <code>str</code> <p>Path to the input XML file.</p> required <code>metadata_path</code> <code>str</code> <p>Path to the metadata JSON file.</p> required <code>journal_name</code> <code>str</code> <p>Name of the journal.</p> required <code>xml_output_path</code> <code>str</code> <p>Path to save the translated XML.</p> required <code>max_retries</code> <code>int</code> <p>Maximum number of retries for batch operations.</p> required <code>retry_delay</code> <code>int</code> <p>Delay in seconds between retries.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the process succeeds, False otherwise.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def batch_translate(\n    input_xml_path: Path,\n    batch_json_path: Path,\n    metadata_path: Path,\n    system_message,\n    journal_name: str,\n):\n    \"\"\"\n    Translates the journal sections using the GPT model.\n    Saves the translated content back to XML.\n\n    Args:\n        input_xml_path (str): Path to the input XML file.\n        metadata_path (str): Path to the metadata JSON file.\n        journal_name (str): Name of the journal.\n        xml_output_path (str): Path to save the translated XML.\n        max_retries (int): Maximum number of retries for batch operations.\n        retry_delay (int): Delay in seconds between retries.\n\n    Returns:\n        bool: True if the process succeeds, False otherwise.\n    \"\"\"\n    logger.info(\n        f\"Starting translation batch for journal '{journal_name}':\\n\\twith file: {input_xml_path}\\n\\tmetadata: {metadata_path}\"\n    )\n\n    # Data initialization:\n    try:\n        # load metadata\n        serial_json = get_text_from_file(metadata_path)\n\n        section_metadata = deserialize_json(serial_json)\n        if not section_metadata:\n            raise RuntimeError(f\"Metadata could not be loaded from {metadata_path}.\")\n\n        # Extract page groups and split XML content\n        page_groups = extract_page_groups_from_metadata(section_metadata)\n        xml_content = get_text_from_file(input_xml_path)\n        section_contents = split_xml_on_pagebreaks(xml_content, page_groups)\n\n        if section_contents:\n            logger.debug(f\"section_contents[0]:\\n{section_contents[0]}\")\n        else:\n            logger.error(\"No sectin contents.\")\n\n    except Exception as e:\n        logger.error(\n            f\"Failed to initialize data for translation batching for journal '{journal_name}'.\",\n            exc_info=True,\n        )\n        raise RuntimeError(\n            f\"Error during data initialization for journal '{journal_name}'.\"\n        ) from e\n\n    translation_data = translate_sections(\n        batch_json_path,\n        system_message,\n        section_contents,\n        section_metadata,\n        journal_name,\n    )\n    return translation_data\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.deserialize_json","title":"<code>deserialize_json(serialized_data)</code>","text":"<p>Converts a serialized JSON string into a Python dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>serialized_data</code> <code>str</code> <p>The JSON string to deserialize.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The deserialized Python dictionary.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def deserialize_json(serialized_data: str):\n    \"\"\"\n    Converts a serialized JSON string into a Python dictionary.\n\n    Args:\n        serialized_data (str): The JSON string to deserialize.\n\n    Returns:\n        dict: The deserialized Python dictionary.\n    \"\"\"\n    if not isinstance(serialized_data, str):\n        logger.error(\n            f\"String input required for deserialize_json. Received: {type(serialized_data)}\"\n        )\n        raise ValueError(\"String input required.\")\n\n    try:\n        # Convert the JSON string into a dictionary\n        return json.loads(serialized_data)\n    except json.JSONDecodeError as e:\n        logger.error(f\"Failed to deserialize JSON: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.extract_page_groups_from_metadata","title":"<code>extract_page_groups_from_metadata(metadata)</code>","text":"<p>Extracts page groups from the section metadata for use with <code>split_xml_pages</code>.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The section metadata containing sections with start and end pages.</p> required <p>Returns:</p> Type Description <p>List[Tuple[int, int]]: A list of tuples, each representing a page range (start_page, end_page).</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def extract_page_groups_from_metadata(metadata):\n    \"\"\"\n    Extracts page groups from the section metadata for use with `split_xml_pages`.\n\n    Parameters:\n        metadata (dict): The section metadata containing sections with start and end pages.\n\n    Returns:\n        List[Tuple[int, int]]: A list of tuples, each representing a page range (start_page, end_page).\n    \"\"\"\n    page_groups = []\n\n    # Ensure metadata contains sections\n    if \"sections\" not in metadata or not isinstance(metadata[\"sections\"], list):\n        raise ValueError(\n            \"Metadata does not contain a valid 'sections' key with a list of sections.\"\n        )\n\n    for section in metadata[\"sections\"]:\n        try:\n            # Extract start and end pages\n            start_page = section.get(\"start_page\")\n            end_page = section.get(\"end_page\")\n\n            # Ensure both start_page and end_page are integers\n            if not isinstance(start_page, int) or not isinstance(end_page, int):\n                raise ValueError(f\"Invalid page range in section: {section}\")\n\n            # Add the tuple to the page groups list\n            page_groups.append((start_page, end_page))\n\n        except KeyError as e:\n            print(f\"Missing key in section metadata: {e}\")\n        except ValueError as e:\n            print(f\"Error processing section metadata: {e}\")\n\n    logger.debug(f\"page groups found: {page_groups}\")\n\n    return page_groups\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.generate_all_batches","title":"<code>generate_all_batches(processed_document_dir, system_message, user_wrap_function, file_regex='.*\\\\.xml')</code>","text":"<p>Generate cleaning batches for all journals in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>processed_journals_dir</code> <code>str</code> <p>Path to the directory containing processed journal data.</p> required <code>system_message</code> <code>str</code> <p>System message template for batch processing.</p> required <code>user_wrap_function</code> <code>callable</code> <p>Function to wrap user input for processing pages.</p> required <code>file_regex</code> <code>str</code> <p>Regex pattern to identify target files (default: \".*.xml\").</p> <code>'.*\\\\.xml'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def generate_all_batches(\n    processed_document_dir: str,\n    system_message: str,\n    user_wrap_function,\n    file_regex: str = r\".*\\.xml\",\n):\n    \"\"\"\n    Generate cleaning batches for all journals in the specified directory.\n\n    Parameters:\n        processed_journals_dir (str): Path to the directory containing processed journal data.\n        system_message (str): System message template for batch processing.\n        user_wrap_function (callable): Function to wrap user input for processing pages.\n        file_regex (str): Regex pattern to identify target files (default: \".*\\\\.xml\").\n\n    Returns:\n        None\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    document_dir = Path(processed_document_dir)\n    regex = re.compile(file_regex)\n\n    for journal_file in document_dir.iterdir():\n        if journal_file.is_file() and regex.search(journal_file.name):\n            try:\n                # Derive output file path\n                output_file = journal_file.with_suffix(\".jsonl\")\n                logger.info(f\"Generating batch for {journal_file}...\")\n\n                # Call single batch function\n                generate_single_oa_batch_from_pages(\n                    input_xml_file=str(journal_file),\n                    output_file=str(output_file),\n                    system_message=system_message,\n                    user_wrap_function=user_wrap_function,\n                )\n            except Exception as e:\n                logger.error(f\"Failed to process {journal_file}: {e}\")\n                continue\n\n    logger.info(\"Batch generation completed.\")\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.generate_clean_batch","title":"<code>generate_clean_batch(input_xml_file, output_file, system_message, user_wrap_function)</code>","text":"<p>Generate a batch file for the OpenAI (OA) API using a single input XML file.</p> <p>Parameters:</p> Name Type Description Default <code>batch_file</code> <code>str</code> <p>Full path to the input XML file to process.</p> required <code>output_file</code> <code>str</code> <p>Full path to the output batch JSONL file.</p> required <code>system_message</code> <code>str</code> <p>System message template for batch processing.</p> required <code>user_wrap_function</code> <code>callable</code> <p>Function to wrap user input for processing pages.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Path to the created batch file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during file processing.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def generate_clean_batch(\n    input_xml_file: str, output_file: str, system_message: str, user_wrap_function\n):\n    \"\"\"\n    Generate a batch file for the OpenAI (OA) API using a single input XML file.\n\n    Parameters:\n        batch_file (str): Full path to the input XML file to process.\n        output_file (str): Full path to the output batch JSONL file.\n        system_message (str): System message template for batch processing.\n        user_wrap_function (callable): Function to wrap user input for processing pages.\n\n    Returns:\n        str: Path to the created batch file.\n\n    Raises:\n        Exception: If an error occurs during file processing.\n    \"\"\"\n\n    try:\n        # Read the OCR text from the batch file\n        text = get_text_from_file(input_xml_file)\n        logger.info(f\"Processing file: {input_xml_file}\")\n\n        # Split the text into pages for processing\n        pages = split_xml_on_pagebreaks(text)\n        pages = wrap_all_lines(pages)  # wrap lines with brackets.\n        if not pages:\n            raise ValueError(f\"No pages found in XML file: {input_xml_file}\")\n        logger.info(f\"Found {len(pages)} pages in {input_xml_file}.\")\n\n        max_tokens = [_get_max_tokens_for_clean(page) for page in pages]\n\n        # Generate messages for the pages\n        batch_message_seq = generate_messages(system_message, user_wrap_function, pages)\n\n        # Save the batch file\n        create_jsonl_file_for_batch(\n            batch_message_seq, output_file, max_token_list=max_tokens\n        )\n        logger.info(f\"Batch file created successfully: {output_file}\")\n\n        return output_file\n\n    except FileNotFoundError:\n        logger.error(\"File not found.\")\n        raise\n    except ValueError as e:\n        logger.error(f\"Value error: {e}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error while processing {input_xml_file}: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.generate_single_oa_batch_from_pages","title":"<code>generate_single_oa_batch_from_pages(input_xml_file, output_file, system_message, user_wrap_function)</code>","text":"<p>*** Depricated *** Generate a batch file for the OpenAI (OA) API using a single input XML file.</p> <p>Parameters:</p> Name Type Description Default <code>batch_file</code> <code>str</code> <p>Full path to the input XML file to process.</p> required <code>output_file</code> <code>str</code> <p>Full path to the output batch JSONL file.</p> required <code>system_message</code> <code>str</code> <p>System message template for batch processing.</p> required <code>user_wrap_function</code> <code>callable</code> <p>Function to wrap user input for processing pages.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Path to the created batch file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during file processing.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def generate_single_oa_batch_from_pages(\n    input_xml_file: str,\n    output_file: str,\n    system_message: str,\n    user_wrap_function,\n):\n    \"\"\"\n    *** Depricated ***\n    Generate a batch file for the OpenAI (OA) API using a single input XML file.\n\n    Parameters:\n        batch_file (str): Full path to the input XML file to process.\n        output_file (str): Full path to the output batch JSONL file.\n        system_message (str): System message template for batch processing.\n        user_wrap_function (callable): Function to wrap user input for processing pages.\n\n    Returns:\n        str: Path to the created batch file.\n\n    Raises:\n        Exception: If an error occurs during file processing.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    try:\n        # Read the OCR text from the batch file\n        text = get_text_from_file(input_xml_file)\n        logger.info(f\"Processing file: {input_xml_file}\")\n\n        # Split the text into pages for processing\n        pages = split_xml_pages(text)\n        if not pages:\n            raise ValueError(f\"No pages found in XML file: {input_xml_file}\")\n        logger.info(f\"Found {len(pages)} pages in {input_xml_file}.\")\n\n        # Generate messages for the pages\n        batch_message_seq = generate_messages(system_message, user_wrap_function, pages)\n\n        # Save the batch file\n        create_jsonl_file_for_batch(batch_message_seq, output_file)\n        logger.info(f\"Batch file created successfully: {output_file}\")\n\n        return output_file\n\n    except FileNotFoundError:\n        logger.error(f\"File not found: {input_xml_file}\")\n        raise\n    except ValueError as e:\n        logger.error(f\"Value error: {e}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error while processing {input_xml_file}: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.save_cleaned_data","title":"<code>save_cleaned_data(cleaned_xml_path, cleaned_wrapped_pages, journal_name)</code>","text":"Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def save_cleaned_data(\n    cleaned_xml_path: Path, cleaned_wrapped_pages: List[str], journal_name\n):\n    try:\n        logger.info(f\"Saving cleaned content to XML for journal '{journal_name}'.\")\n        cleaned_wrapped_pages = unwrap_all_lines(cleaned_wrapped_pages)\n        save_pages_to_xml(cleaned_xml_path, cleaned_wrapped_pages, overwrite=True)\n        logger.info(f\"Cleaned journal saved successfully to:\\n\\t{cleaned_xml_path}\")\n    except Exception as e:\n        logger.error(\n            f\"Failed to save cleaned data for journal '{journal_name}'.\",\n            extra={\"cleaned_xml_path\": cleaned_xml_path},\n            exc_info=True,\n        )\n        raise RuntimeError(\n            f\"Failed to save cleaned data for journal '{journal_name}'.\"\n        ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.save_sectioning_data","title":"<code>save_sectioning_data(output_json_path, raw_output_path, serial_json, journal_name)</code>","text":"Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def save_sectioning_data(\n    output_json_path: Path, raw_output_path: Path, serial_json: str, journal_name\n):\n    try:\n        raw_output_path.write_text(serial_json, encoding=\"utf-8\")\n    except Exception as e:\n        logger.error(\n            f\"Failed to write raw response file for journal '{journal_name}'.\",\n            extra={\"raw_output_path\": raw_output_path},\n            exc_info=True,\n        )\n        raise RuntimeError(\n            f\"Failed to write raw response file for journal '{journal_name}'.\"\n        ) from e\n\n    # Validate and save metadata\n    try:\n        valid = validate_and_save_metadata(\n            output_json_path, serial_json, journal_schema\n        )\n        if not valid:\n            raise RuntimeError(\n                f\"Validation failed for metadata of journal '{journal_name}'.\"\n            )\n    except Exception as e:\n        logger.error(\n            f\"Error occurred while validating and saving metadata for journal '{journal_name}'.\",\n            extra={\"output_json_path\": output_json_path},\n            exc_info=True,\n        )\n        raise RuntimeError(f\"Validation error for journal '{journal_name}'.\") from e\n\n    return output_json_path\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.save_translation_data","title":"<code>save_translation_data(xml_output_path, translation_data, journal_name)</code>","text":"Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def save_translation_data(xml_output_path: Path, translation_data, journal_name):\n    # Save translated content back to XML\n    try:\n        logger.info(f\"Saving translated content to XML for journal '{journal_name}'.\")\n        join_xml_data_to_doc(xml_output_path, translation_data, overwrite=True)\n        logger.info(f\"Translated journal saved successfully to:\\n\\t{xml_output_path}\")\n\n    except Exception as e:\n        logger.error(\n            f\"Failed to save translation data for journal '{journal_name}'.\",\n            extra={\"xml_output_path\": xml_output_path},\n            exc_info=True,\n        )\n        raise RuntimeError(\n            f\"Failed to save translation data for journal '{journal_name}'.\"\n        ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.send_data_for_tx_batch","title":"<code>send_data_for_tx_batch(batch_jsonl_path, section_data_to_send, system_message, max_token_list, journal_name, immediate=False)</code>","text":"<p>Sends data for translation batch or immediate processing.</p> <p>Parameters:</p> Name Type Description Default <code>batch_jsonl_path</code> <code>Path</code> <p>Path for the JSONL file to save batch data.</p> required <code>section_data_to_send</code> <code>List</code> <p>List of section data to translate.</p> required <code>system_message</code> <code>str</code> <p>System message for the translation process.</p> required <code>max_token_list</code> <code>List</code> <p>List of max tokens for each section.</p> required <code>journal_name</code> <code>str</code> <p>Name of the journal being processed.</p> required <code>immediate</code> <code>bool</code> <p>If True, run immediate chat processing instead of batch.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>List</code> <p>Translated data from the batch or immediate process.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def send_data_for_tx_batch(\n    batch_jsonl_path: Path,\n    section_data_to_send: List,\n    system_message,\n    max_token_list: List,\n    journal_name,\n    immediate=False,\n):\n    \"\"\"\n    Sends data for translation batch or immediate processing.\n\n    Args:\n        batch_jsonl_path (Path): Path for the JSONL file to save batch data.\n        section_data_to_send (List): List of section data to translate.\n        system_message (str): System message for the translation process.\n        max_token_list (List): List of max tokens for each section.\n        journal_name (str): Name of the journal being processed.\n        immediate (bool): If True, run immediate chat processing instead of batch.\n\n    Returns:\n        List: Translated data from the batch or immediate process.\n    \"\"\"\n    try:\n        # Generate all messages using the generate_messages function\n        user_message_wrapper = (\n            lambda section_info: f\"Translate this section with title '{section_info.title}':\\n{section_info.content}\"\n        )\n        messages = generate_messages(\n            system_message, user_message_wrapper, section_data_to_send\n        )\n\n        if immediate:\n            logger.info(f\"Running immediate chat process for journal '{journal_name}'.\")\n            translated_data = []\n            for i, message in enumerate(messages):\n                max_tokens = max_token_list[i]\n                response = run_immediate_chat_process(message, max_tokens=max_tokens)\n                translated_data.append(response)\n            logger.info(\n                f\"Immediate translation completed for journal '{journal_name}'.\"\n            )\n            return translated_data\n        else:\n            logger.info(f\"Running batch processing for journal '{journal_name}'.\")\n            # Create batch file for batch processing\n            jsonl_file = create_jsonl_file_for_batch(\n                messages, batch_jsonl_path, max_token_list=max_token_list\n            )\n            if not jsonl_file:\n                raise RuntimeError(\"Failed to create JSONL file for translation batch.\")\n\n            # Process batch and return the result\n            translation_data = start_batch_with_retries(\n                jsonl_file,\n                description=f\"Batch for translating journal '{journal_name}'\",\n            )\n            logger.info(f\"Batch translation completed for journal '{journal_name}'.\")\n            return translation_data\n\n    except Exception as e:\n        logger.error(\n            f\"Error during translation processing for journal '{journal_name}'.\",\n            exc_info=True,\n        )\n        raise RuntimeError(\"Error in translation process.\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.setup_logger","title":"<code>setup_logger(log_file_path)</code>","text":"<p>Configures the logger to write to a log file and the console. Adds a custom \"PRIORITY_INFO\" logging level for important messages.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def setup_logger(log_file_path):\n    \"\"\"\n    Configures the logger to write to a log file and the console.\n    Adds a custom \"PRIORITY_INFO\" logging level for important messages.\n    \"\"\"\n    # Remove existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",  # Include logger name\n        handlers=[\n            logging.FileHandler(log_file_path, encoding=\"utf-8\"),\n            logging.StreamHandler(),  # Optional: to log to the console as well\n        ],\n    )\n\n    # Suppress DEBUG/INFO logs for specific noisy modules\n    modules_to_suppress = [\"httpx\", \"httpcore\", \"urllib3\", \"openai\", \"google\"]\n    for module in modules_to_suppress:\n        logger = logging.getLogger(module)\n        logger.setLevel(logging.WARNING)  # Suppress DEBUG and INFO logs\n\n    # Add a custom \"PRIORITY_INFO\" level\n    PRIORITY_INFO_LEVEL = 25  # Between INFO (20) and WARNING (30)\n    logging.addLevelName(PRIORITY_INFO_LEVEL, \"PRIORITY_INFO\")\n\n    def priority_info(self, message, *args, **kwargs):\n        if self.isEnabledFor(PRIORITY_INFO_LEVEL):\n            self._log(PRIORITY_INFO_LEVEL, f\"\\033[93m{message}\\033[0m\", args, **kwargs)\n\n    logging.Logger.priority_info = priority_info\n\n    return logging.getLogger(__name__)\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.translate_sections","title":"<code>translate_sections(batch_jsonl_path, system_message, section_contents, section_metadata, journal_name, immediate=False)</code>","text":"<p>build up sections in batches to translate</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def translate_sections(\n    batch_jsonl_path: Path,\n    system_message,\n    section_contents,\n    section_metadata,\n    journal_name,\n    immediate=False,\n):\n    \"\"\"build up sections in batches to translate\"\"\"\n\n    section_mdata = section_metadata[\"sections\"]\n    if len(section_contents) != len(section_mdata):\n        raise RuntimeError(\"Section length mismatch.\")\n\n    # collate metadata and section content, calculate max_tokens per section:\n    section_data_to_send = []\n    max_token_list = []\n    current_token_count = 0\n    collected_translations = []\n    section_last_index = len(section_mdata) - 1\n\n    for i, section_info in enumerate(section_mdata):\n        section_content = section_contents[i]\n        max_tokens = floor(token_count(section_content) * 1.3) + 1000\n        max_token_list.append(max_tokens)\n        current_token_count += max_tokens\n        section_data = SimpleNamespace(\n            title=section_info[\"title_en\"], content=section_content\n        )\n        section_data_to_send.append(section_data)\n        logger.debug(f\"section {i}: {section_data.title} added for batch processing.\")\n\n        if current_token_count &gt;= MAX_TOKEN_LIMIT or i == section_last_index:\n            # send sections for batch processing since token limit reached.\n            batch_result = send_data_for_tx_batch(\n                batch_jsonl_path,\n                section_data_to_send,\n                system_message,\n                max_token_list,\n                journal_name,\n                immediate,\n            )\n            collected_translations.extend(batch_result)\n\n            # reset containers to start building up next batch.\n            section_data_to_send = []\n            max_token_list = []\n            current_token_count = 0\n\n    return collected_translations\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.unwrap_all_lines","title":"<code>unwrap_all_lines(pages)</code>","text":"Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def unwrap_all_lines(pages):\n    result = []\n    for page in pages:\n        if page == \"blank page\":\n            result.append(page)\n        else:\n            result.append(unwrap_lines(page))\n    return result\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.unwrap_lines","title":"<code>unwrap_lines(text)</code>","text":"<pre><code>Removes angle brackets (&lt; &gt;) from encapsulated lines and merges them into\na newline-separated string.\n\nParameters:\n    text (str): The input string with encapsulated lines.\n\nReturns:\n    str: A newline-separated string with the encapsulation removed.\n\nExample:\n    &gt;&gt;&gt; merge_encapsulated_lines(\"&lt;Line 1&gt; &lt;Line 2&gt; &lt;Line 3&gt;\")\n    'Line 1\n</code></pre> <p>Line 2 Line 3'         &gt;&gt;&gt; merge_encapsulated_lines(\" \")         'Line 1 Line 2 Line 3' Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def unwrap_lines(text: str) -&gt; str:\n    \"\"\"\n    Removes angle brackets (&lt; &gt;) from encapsulated lines and merges them into\n    a newline-separated string.\n\n    Parameters:\n        text (str): The input string with encapsulated lines.\n\n    Returns:\n        str: A newline-separated string with the encapsulation removed.\n\n    Example:\n        &gt;&gt;&gt; merge_encapsulated_lines(\"&lt;Line 1&gt; &lt;Line 2&gt; &lt;Line 3&gt;\")\n        'Line 1\\nLine 2\\nLine 3'\n        &gt;&gt;&gt; merge_encapsulated_lines(\"&lt;Line 1&gt;\\n&lt;Line 2&gt;\\n&lt;Line 3&gt;\")\n        'Line 1\\nLine 2\\nLine 3'\n    \"\"\"\n    # Find all content between &lt; and &gt; using regex\n    matches = re.findall(r\"&lt;(.*?)&gt;\", text)\n    # Join the extracted content with newlines\n    return \"\\n\".join(matches)\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.validate_and_clean_data","title":"<code>validate_and_clean_data(data, schema)</code>","text":"<p>Recursively validate and clean AI-generated data to fit the given schema. Any missing fields are filled with defaults, and extra fields are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The AI-generated data to validate and clean.</p> required <code>schema</code> <code>dict</code> <p>The schema defining the required structure.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The cleaned data adhering to the schema.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def validate_and_clean_data(data, schema):\n    \"\"\"\n    Recursively validate and clean AI-generated data to fit the given schema.\n    Any missing fields are filled with defaults, and extra fields are ignored.\n\n    Args:\n        data (dict): The AI-generated data to validate and clean.\n        schema (dict): The schema defining the required structure.\n\n    Returns:\n        dict: The cleaned data adhering to the schema.\n    \"\"\"\n\n    def clean_value(value, field_schema):\n        \"\"\"\n        Clean a single value based on its schema, attempting type conversions where necessary.\n        \"\"\"\n        field_type = field_schema[\"type\"]\n\n        # Handle type: string\n        if field_type == \"string\":\n            if isinstance(value, str):\n                return value\n            elif value is not None:\n                return str(value)\n            return \"unset\"\n\n        # Handle type: integer\n        elif field_type == \"integer\":\n            if isinstance(value, int):\n                return value\n            elif isinstance(value, str) and value.isdigit():\n                return int(value)\n            try:\n                return int(float(value))  # Handle cases like \"2.0\"\n            except (ValueError, TypeError):\n                return 0\n\n        # Handle type: array\n        elif field_type == \"array\":\n            if isinstance(value, list):\n                item_schema = field_schema.get(\"items\", {})\n                return [clean_value(item, item_schema) for item in value]\n            elif isinstance(value, str):\n                # Try splitting comma-separated strings into a list\n                return [v.strip() for v in value.split(\",\")]\n            return []\n\n        # Handle type: object\n        elif field_type == \"object\":\n            if isinstance(value, dict):\n                return validate_and_clean_data(value, field_schema)\n            return {}\n\n        # Handle nullable strings\n        elif field_type == [\"string\", \"null\"]:\n            if value is None or isinstance(value, str):\n                return value\n            return str(value)\n\n        # Default case for unknown or unsupported types\n        return \"unset\"\n\n    def clean_object(obj, obj_schema):\n        \"\"\"\n        Clean a dictionary object based on its schema.\n        \"\"\"\n        if not isinstance(obj, dict):\n            print(\n                f\"Expected dict but got: \\n{type(obj)}: {obj}\\nResetting to empty dict.\"\n            )\n            return {}\n        cleaned = {}\n        properties = obj_schema.get(\"properties\", {})\n        for key, field_schema in properties.items():\n            # Set default value for missing fields\n            cleaned[key] = clean_value(obj.get(key), field_schema)\n        return cleaned\n\n    # Handle the top-level object\n    if schema[\"type\"] == \"object\":\n        cleaned_data = clean_object(data, schema)\n        return cleaned_data\n    else:\n        raise ValueError(\"Top-level schema must be of type 'object'.\")\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.validate_and_save_metadata","title":"<code>validate_and_save_metadata(output_file_path, json_metadata_serial, schema)</code>","text":"<p>Validates and cleans journal data against the schema, then writes it to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The journal data as a serialized JSON string to validate and clean.</p> required <code>schema</code> <code>dict</code> <p>The schema defining the required structure.</p> required <code>output_file_path</code> <code>str</code> <p>Path to the output JSON file.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if successfully written to the file, False otherwise.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def validate_and_save_metadata(\n    output_file_path: Path, json_metadata_serial: str, schema\n):\n    \"\"\"\n    Validates and cleans journal data against the schema, then writes it to a JSON file.\n\n    Args:\n        data (str): The journal data as a serialized JSON string to validate and clean.\n        schema (dict): The schema defining the required structure.\n        output_file_path (str): Path to the output JSON file.\n\n    Returns:\n        bool: True if successfully written to the file, False otherwise.\n    \"\"\"\n    try:\n        # Clean the data to fit the schema\n        data = deserialize_json(json_metadata_serial)\n        cleaned_data = validate_and_clean_data(data, schema)\n\n        # Write the parsed data to the specified JSON file\n        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n        logger.info(\n            f\"Parsed and validated metadata successfully written to {output_file_path}\"\n        )\n        return True\n    except Exception as e:\n        logger.error(f\"An error occurred during validation or writing: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.wrap_all_lines","title":"<code>wrap_all_lines(pages)</code>","text":"Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def wrap_all_lines(pages):\n    return [wrap_lines(page) for page in pages]\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.wrap_lines","title":"<code>wrap_lines(text)</code>","text":"<pre><code>Encloses each line of the input text with angle brackets.\n\nArgs:\n    text (str): The input string containing lines separated by '\n</code></pre> <p>'.</p> <pre><code>Returns:\n    str: A string where each line is enclosed in angle brackets.\n\nExample:\n    &gt;&gt;&gt; enclose_lines(\"This is a string with\n</code></pre> <p>two lines.\")         ' &lt;    two lines.&gt;' Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def wrap_lines(text: str) -&gt; str:\n    \"\"\"\n    Encloses each line of the input text with angle brackets.\n\n    Args:\n        text (str): The input string containing lines separated by '\\n'.\n\n    Returns:\n        str: A string where each line is enclosed in angle brackets.\n\n    Example:\n        &gt;&gt;&gt; enclose_lines(\"This is a string with   \\n   two lines.\")\n        '&lt;This is a string with  &gt;\\n&lt;    two lines.&gt;'\n    \"\"\"\n    return \"\\n\".join(f\"&lt;{line}&gt;\" for line in text.split(\"\\n\"))\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config","title":"<code>logging_config</code>","text":""},{"location":"api/#tnh_scholar.logging_config.BASE_LOG_DIR","title":"<code>BASE_LOG_DIR = Path('./logs')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.BASE_LOG_NAME","title":"<code>BASE_LOG_NAME = 'tnh'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.DEFAULT_CONSOLE_FORMAT_STRING","title":"<code>DEFAULT_CONSOLE_FORMAT_STRING = '%(asctime)s - %(name)s - %(log_color)s%(levelname)s%(reset)s - %(message)s'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.DEFAULT_FILE_FORMAT_STRING","title":"<code>DEFAULT_FILE_FORMAT_STRING = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.DEFAULT_LOG_FILEPATH","title":"<code>DEFAULT_LOG_FILEPATH = Path('main.log')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LOG_COLORS","title":"<code>LOG_COLORS = {'DEBUG': 'bold_green', 'INFO': 'cyan', 'PRIORITY_INFO': 'bold_cyan', 'WARNING': 'bold_yellow', 'ERROR': 'bold_red', 'CRITICAL': 'bold_red'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.MAX_FILE_SIZE","title":"<code>MAX_FILE_SIZE = 10 * 1024 * 1024</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.PRIORITY_INFO_LEVEL","title":"<code>PRIORITY_INFO_LEVEL = 25</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.OMPFilter","title":"<code>OMPFilter</code>","text":"<p>               Bases: <code>Filter</code></p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>class OMPFilter(logging.Filter):\n    def filter(self, record):\n        # Suppress messages containing \"OMP:\"\n        return \"OMP:\" not in record.getMessage()\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.OMPFilter.filter","title":"<code>filter(record)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def filter(self, record):\n    # Suppress messages containing \"OMP:\"\n    return \"OMP:\" not in record.getMessage()\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.get_child_logger","title":"<code>get_child_logger(name, console=None, separate_file=False)</code>","text":"<p>Get a child logger that writes logs to a console or a specified file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the child logger (e.g., module name).</p> required <code>console</code> <code>bool</code> <p>If True, log to the console. If False, do not log to the console.                       If None, inherit console behavior from the parent logger.</p> <code>None</code> <code>file</code> <code>Path</code> <p>A string specifying a logfile to log to. will be placed under existing root logs directory. If provided,                    a rotating file handler will be added.</p> required <p>Returns:</p> Type Description <p>logging.Logger: Configured child logger.</p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def get_child_logger(name: str, console: bool = None, separate_file: bool = False):\n    \"\"\"\n    Get a child logger that writes logs to a console or a specified file.\n\n    Args:\n        name (str): The name of the child logger (e.g., module name).\n        console (bool, optional): If True, log to the console. If False, do not log to the console.\n                                  If None, inherit console behavior from the parent logger.\n        file (Path, optional): A string specifying a logfile to log to. will be placed under existing root logs directory. If provided,\n                               a rotating file handler will be added.\n\n    Returns:\n        logging.Logger: Configured child logger.\n    \"\"\"\n    # Create the fully qualified child logger name\n    full_name = f\"{BASE_LOG_NAME}.{name}\"\n    logger = logging.getLogger(full_name)\n    logger.debug(f\"Created logger with name: {logger.name}\")\n\n    # Check if the logger already has handlers to avoid duplication\n    if not logger.handlers:\n        # Add console handler if specified\n        if console:\n            console_handler = colorlog.StreamHandler()\n            console_formatter = colorlog.ColoredFormatter(\n                DEFAULT_CONSOLE_FORMAT_STRING,\n                log_colors=LOG_COLORS,\n            )\n            console_handler.setFormatter(console_formatter)\n            logger.addHandler(console_handler)\n\n        # Add file handler if a file path is provided\n        if separate_file:\n            logfile = BASE_LOG_DIR / f\"{name}.log\"\n            logfile.parent.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n            file_handler = RotatingFileHandler(\n                filename=logfile,\n                maxBytes=MAX_FILE_SIZE,  # Use the global MAX_FILE_SIZE\n                backupCount=5,\n            )\n            file_formatter = logging.Formatter(DEFAULT_FILE_FORMAT_STRING)\n            file_handler.setFormatter(file_formatter)\n            logger.addHandler(file_handler)\n\n        # Ensure the logger inherits handlers and settings from the base logger\n        logger.propagate = True\n\n    return logger\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.priority_info","title":"<code>priority_info(self, message, *args, **kwargs)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def priority_info(self, message, *args, **kwargs):\n    if self.isEnabledFor(PRIORITY_INFO_LEVEL):\n        self._log(PRIORITY_INFO_LEVEL, message, args, **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.setup_logging","title":"<code>setup_logging(log_level=logging.INFO, log_filepath=DEFAULT_LOG_FILEPATH, max_log_file_size=MAX_FILE_SIZE, backup_count=5, console_format=DEFAULT_CONSOLE_FORMAT_STRING, file_format=DEFAULT_FILE_FORMAT_STRING, console=True, suppressed_modules=None)</code>","text":"<p>Configure the base logger with handlers, including the custom PRIORITY_INFO level.</p> <p>Parameters:</p> Name Type Description Default <code>log_level</code> <code>int</code> <p>Logging level for the base logger.</p> <code>INFO</code> <code>log_file_path</code> <code>Path</code> <p>Path to the log file.</p> required <code>max_log_file_size</code> <code>int</code> <p>Maximum log file size in bytes.</p> <code>MAX_FILE_SIZE</code> <code>backup_count</code> <code>int</code> <p>Number of backup log files to keep.</p> <code>5</code> <code>console_format</code> <code>str</code> <p>Format string for console logs.</p> <code>DEFAULT_CONSOLE_FORMAT_STRING</code> <code>file_format</code> <code>str</code> <p>Format string for file logs.</p> <code>DEFAULT_FILE_FORMAT_STRING</code> <code>suppressed_modules</code> <code>list</code> <p>List of third-party modules to suppress logs for.</p> <code>None</code> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def setup_logging(\n    log_level=logging.INFO,\n    log_filepath=DEFAULT_LOG_FILEPATH,\n    max_log_file_size=MAX_FILE_SIZE,  # 10MB\n    backup_count=5,\n    console_format=DEFAULT_CONSOLE_FORMAT_STRING,\n    file_format=DEFAULT_FILE_FORMAT_STRING,\n    console=True,\n    suppressed_modules=None,\n):\n    \"\"\"\n    Configure the base logger with handlers, including the custom PRIORITY_INFO level.\n\n    Args:\n        log_level (int): Logging level for the base logger.\n        log_file_path (Path): Path to the log file.\n        max_log_file_size (int): Maximum log file size in bytes.\n        backup_count (int): Number of backup log files to keep.\n        console_format (str): Format string for console logs.\n        file_format (str): Format string for file logs.\n        suppressed_modules (list): List of third-party modules to suppress logs for.\n    \"\"\"\n    # Create the base logger\n    log_file_path = BASE_LOG_DIR / log_filepath\n    base_logger = logging.getLogger(BASE_LOG_NAME)\n    base_logger.setLevel(log_level)\n\n    # Clear existing handlers to avoid duplication\n    base_logger.handlers.clear()\n\n    # Colorized console handler\n    if console:\n        console_handler = colorlog.StreamHandler()\n        console_formatter = colorlog.ColoredFormatter(\n            console_format,\n            log_colors=LOG_COLORS,\n        )\n        console_handler.setFormatter(console_formatter)\n        # Add the OMP filter to the console handler\n        console_handler.addFilter(OMPFilter())\n        base_logger.addHandler(console_handler)\n\n    # Plain file handler\n    log_file_path.parent.mkdir(parents=True, exist_ok=True)\n    file_handler = RotatingFileHandler(\n        filename=log_file_path,\n        maxBytes=max_log_file_size,\n        backupCount=backup_count,\n    )\n    file_formatter = logging.Formatter(file_format)\n    file_handler.setFormatter(file_formatter)\n    base_logger.addHandler(file_handler)\n\n    # Suppress noisy third-party logs\n    if suppressed_modules:\n        for module in suppressed_modules:\n            logging.getLogger(module).setLevel(logging.WARNING)\n\n    # Prevent propagation to the root logger\n    base_logger.propagate = False\n\n    return base_logger\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing","title":"<code>ocr_processing</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.DEFAULT_ANNOTATION_FONT_PATH","title":"<code>DEFAULT_ANNOTATION_FONT_PATH = Path('/System/Library/Fonts/Supplemental/Arial.ttf')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.DEFAULT_ANNOTATION_FONT_SIZE","title":"<code>DEFAULT_ANNOTATION_FONT_SIZE = 12</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.DEFAULT_ANNOTATION_LANGUAGE_HINTS","title":"<code>DEFAULT_ANNOTATION_LANGUAGE_HINTS = ['vi']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.DEFAULT_ANNOTATION_METHOD","title":"<code>DEFAULT_ANNOTATION_METHOD = 'DOCUMENT_TEXT_DETECTION'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.DEFAULT_ANNOTATION_OFFSET","title":"<code>DEFAULT_ANNOTATION_OFFSET = 2</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.logger","title":"<code>logger = logging.getLogger('ocr_processing')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.PDFParseWarning","title":"<code>PDFParseWarning</code>","text":"<p>               Bases: <code>Warning</code></p> <p>Custom warning class for PDF parsing issues. Encapsulates minimal logic for displaying warnings with a custom format.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>class PDFParseWarning(Warning):\n    \"\"\"\n    Custom warning class for PDF parsing issues.\n    Encapsulates minimal logic for displaying warnings with a custom format.\n    \"\"\"\n\n    @staticmethod\n    def warn(message: str):\n        \"\"\"\n        Display a warning message with custom formatting.\n\n        Parameters:\n            message (str): The warning message to display.\n        \"\"\"\n        formatted_message = f\"\\033[93mPDFParseWarning: {message}\\033[0m\"\n        print(formatted_message)  # Simply prints the warning\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.PDFParseWarning.warn","title":"<code>warn(message)</code>  <code>staticmethod</code>","text":"<p>Display a warning message with custom formatting.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The warning message to display.</p> required Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>@staticmethod\ndef warn(message: str):\n    \"\"\"\n    Display a warning message with custom formatting.\n\n    Parameters:\n        message (str): The warning message to display.\n    \"\"\"\n    formatted_message = f\"\\033[93mPDFParseWarning: {message}\\033[0m\"\n    print(formatted_message)  # Simply prints the warning\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.annotate_image_with_text","title":"<code>annotate_image_with_text(image, text_annotations, annotation_font_path, font_size=12)</code>","text":"<p>Annotates a PIL image with bounding boxes and text descriptions from OCR results.</p> <p>Parameters:</p> Name Type Description Default <code>pil_image</code> <code>Image</code> <p>The input PIL image to annotate.</p> required <code>text_annotations</code> <code>List[EntityAnnotation]</code> <p>OCR results containing bounding boxes and text.</p> required <code>annotation_font_path</code> <code>str</code> <p>Path to the font file for text annotations.</p> required <code>font_size</code> <code>int</code> <p>Font size for text annotations.</p> <code>12</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The annotated PIL image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input image is None.</p> <code>IOError</code> <p>If the font file cannot be loaded.</p> <code>Exception</code> <p>For any other unexpected errors.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def annotate_image_with_text(\n    image: Image.Image,\n    text_annotations: List[EntityAnnotation],\n    annotation_font_path: str,\n    font_size: int = 12,\n) -&gt; Image.Image:\n    \"\"\"\n    Annotates a PIL image with bounding boxes and text descriptions from OCR results.\n\n    Parameters:\n        pil_image (Image.Image): The input PIL image to annotate.\n        text_annotations (List[EntityAnnotation]): OCR results containing bounding boxes and text.\n        annotation_font_path (str): Path to the font file for text annotations.\n        font_size (int): Font size for text annotations.\n\n    Returns:\n        Image.Image: The annotated PIL image.\n\n    Raises:\n        ValueError: If the input image is None.\n        IOError: If the font file cannot be loaded.\n        Exception: For any other unexpected errors.\n    \"\"\"\n    if image is None:\n        raise ValueError(\"The input image is None.\")\n\n    try:\n        font = ImageFont.truetype(annotation_font_path, font_size)\n    except IOError as e:\n        raise IOError(f\"Failed to load the font from '{annotation_font_path}': {e}\")\n\n    draw = ImageDraw.Draw(image)\n\n    try:\n        for i, text_obj in enumerate(text_annotations):\n            vertices = [\n                (vertex.x, vertex.y) for vertex in text_obj.bounding_poly.vertices\n            ]\n            if (\n                len(vertices) == 4\n            ):  # Ensure there are exactly 4 vertices for a rectangle\n                # Draw the bounding box\n                draw.polygon(vertices, outline=\"red\", width=2)\n\n                # Skip the first bounding box (whole text region)\n                if i &gt; 0:\n                    # Offset the text position slightly for clarity\n                    text_position = (vertices[0][0] + 2, vertices[0][1] + 2)\n                    draw.text(\n                        text_position, text_obj.description, fill=\"red\", font=font\n                    )\n\n    except AttributeError as e:\n        raise ValueError(f\"Invalid text annotation structure: {e}\")\n    except Exception as e:\n        raise Exception(f\"An error occurred during image annotation: {e}\")\n\n    return image\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.build_processed_pdf","title":"<code>build_processed_pdf(pdf_path, client, preprocessor=None, annotation_font_path=DEFAULT_ANNOTATION_FONT_PATH)</code>","text":"<p>Processes a PDF document, extracting text, word locations, annotated images, and unannotated images.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>Path</code> <p>Path to the PDF file.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>annotation_font_path</code> <code>Path</code> <p>Path to the font file for annotations.</p> <code>DEFAULT_ANNOTATION_FONT_PATH</code> <p>Returns:</p> Type Description <code>Tuple[List[str], List[List[EntityAnnotation]], List[Image], List[Image]]</code> <p>Tuple[List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]]: - List of extracted full-page texts (one entry per page). - List of word locations (list of <code>vision.EntityAnnotation</code> objects for each page). - List of annotated images (with bounding boxes and text annotations). - List of unannotated images (raw page images).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified PDF file does not exist.</p> <code>ValueError</code> <p>If the PDF file is invalid or contains no pages.</p> <code>Exception</code> <p>For any unexpected errors during processing.</p> Example <p>from pathlib import Path from google.cloud import vision pdf_path = Path(\"/path/to/example.pdf\") font_path = Path(\"/path/to/fonts/Arial.ttf\") client = vision.ImageAnnotatorClient() try:     text_pages, word_locations_list, annotated_images, unannotated_images = build_processed_pdf(         pdf_path, client, font_path     )     print(f\"Processed {len(text_pages)} pages successfully!\") except Exception as e:     print(f\"Error processing PDF: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def build_processed_pdf(\n    pdf_path: Path,\n    client: vision.ImageAnnotatorClient,\n    preprocessor: Callable = None,\n    annotation_font_path: Path = DEFAULT_ANNOTATION_FONT_PATH,\n) -&gt; Tuple[\n    List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]\n]:\n    \"\"\"\n    Processes a PDF document, extracting text, word locations, annotated images, and unannotated images.\n\n    Parameters:\n        pdf_path (Path): Path to the PDF file.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        annotation_font_path (Path): Path to the font file for annotations.\n\n    Returns:\n        Tuple[List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]]:\n            - List of extracted full-page texts (one entry per page).\n            - List of word locations (list of `vision.EntityAnnotation` objects for each page).\n            - List of annotated images (with bounding boxes and text annotations).\n            - List of unannotated images (raw page images).\n\n    Raises:\n        FileNotFoundError: If the specified PDF file does not exist.\n        ValueError: If the PDF file is invalid or contains no pages.\n        Exception: For any unexpected errors during processing.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from google.cloud import vision\n        &gt;&gt;&gt; pdf_path = Path(\"/path/to/example.pdf\")\n        &gt;&gt;&gt; font_path = Path(\"/path/to/fonts/Arial.ttf\")\n        &gt;&gt;&gt; client = vision.ImageAnnotatorClient()\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     text_pages, word_locations_list, annotated_images, unannotated_images = build_processed_pdf(\n        &gt;&gt;&gt;         pdf_path, client, font_path\n        &gt;&gt;&gt;     )\n        &gt;&gt;&gt;     print(f\"Processed {len(text_pages)} pages successfully!\")\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error processing PDF: {e}\")\n    \"\"\"\n    try:\n        doc = load_pdf_pages(pdf_path)\n    except FileNotFoundError as fnf_error:\n        raise FileNotFoundError(f\"Error loading PDF: {fnf_error}\")\n    except ValueError as ve:\n        raise ValueError(f\"Invalid PDF file: {ve}\")\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred while loading the PDF: {e}\")\n\n    if doc.page_count == 0:\n        raise ValueError(f\"The PDF file '{pdf_path}' contains no pages.\")\n\n    logger.info(f\"Processing file with {doc.page_count} pages:\\n\\t{pdf_path}\")\n\n    text_pages = []\n    word_locations_list = []\n    annotated_images = []\n    unannotated_images = []\n    first_page_dimensions = None\n\n    for page_num in range(doc.page_count):\n        logger.info(f\"Processing page {page_num + 1}/{doc.page_count}...\")\n\n        try:\n            page = doc.load_page(page_num)\n            (\n                full_page_text,\n                word_locations,\n                annotated_image,\n                unannotated_image,\n                page_dimensions,\n            ) = process_page(page, client, annotation_font_path, preprocessor)\n\n            if full_page_text:  # this is not an empty page\n\n                if page_num == 0:  # save first page info\n                    first_page_dimensions = page_dimensions\n                elif (\n                    page_dimensions != first_page_dimensions\n                ):  # verify page dimensions are consistent\n                    PDFParseWarning.warn(\n                        f\"Page {page_num + 1} has different dimensions than page 1.\"\n                        f\"({page_dimensions}) compared to the first page: ({first_page_dimensions}).\"\n                    )\n\n                text_pages.append(full_page_text)\n                word_locations_list.append(word_locations)\n                annotated_images.append(annotated_image)\n                unannotated_images.append(unannotated_image)\n            else:\n                PDFParseWarning.warn(\n                    f\"Page {page_num + 1} empty, added empty datastructures...\\n\"\n                    # f\"  (Note that total document length will be reduced.)\"\n                )\n\n        except ValueError as ve:\n            print(f\"ValueError on page {page_num + 1}: {ve}\")\n        except OSError as oe:\n            print(f\"OSError on page {page_num + 1}: {oe}\")\n        except Exception as e:\n            print(f\"Unexpected error on page {page_num + 1}: {e}\")\n\n    print(f\"page dimensions: {page_dimensions}\")\n    return text_pages, word_locations_list, annotated_images, unannotated_images\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.deserialize_entity_annotations_from_json","title":"<code>deserialize_entity_annotations_from_json(data)</code>","text":"<p>Deserializes JSON data into a nested list of EntityAnnotation objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The JSON string containing serialized annotations.</p> required <p>Returns:</p> Type Description <code>List[List[EntityAnnotation]]</code> <p>List[List[EntityAnnotation]]: The reconstructed nested list of EntityAnnotation objects.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def deserialize_entity_annotations_from_json(data: str) -&gt; List[List[EntityAnnotation]]:\n    \"\"\"\n    Deserializes JSON data into a nested list of EntityAnnotation objects.\n\n    Parameters:\n        data (str): The JSON string containing serialized annotations.\n\n    Returns:\n        List[List[EntityAnnotation]]: The reconstructed nested list of EntityAnnotation objects.\n    \"\"\"\n    serialized_data = json.loads(data)\n    deserialized_data = []\n\n    for serialized_page in serialized_data:\n        page_annotations = [\n            EntityAnnotation.deserialize(base64.b64decode(serialized_annotation))\n            for serialized_annotation in serialized_page\n        ]\n        deserialized_data.append(page_annotations)\n\n    return deserialized_data\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.extract_image_from_page","title":"<code>extract_image_from_page(page)</code>","text":"<p>Extracts the first image from the given PDF page and returns it as a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The PDF page object.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The first image on the page as a Pillow Image object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no images are found on the page or the image data is incomplete.</p> <code>Exception</code> <p>For unexpected errors during image extraction.</p> Example <p>import fitz from PIL import Image doc = fitz.open(\"/path/to/document.pdf\") page = doc.load_page(0)  # Load the first page try:     image = extract_image_from_page(page)     image.show()  # Display the image except Exception as e:     print(f\"Error extracting image: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def extract_image_from_page(page: fitz.Page) -&gt; Image.Image:\n    \"\"\"\n    Extracts the first image from the given PDF page and returns it as a PIL Image.\n\n    Parameters:\n        page (fitz.Page): The PDF page object.\n\n    Returns:\n        Image.Image: The first image on the page as a Pillow Image object.\n\n    Raises:\n        ValueError: If no images are found on the page or the image data is incomplete.\n        Exception: For unexpected errors during image extraction.\n\n    Example:\n        &gt;&gt;&gt; import fitz\n        &gt;&gt;&gt; from PIL import Image\n        &gt;&gt;&gt; doc = fitz.open(\"/path/to/document.pdf\")\n        &gt;&gt;&gt; page = doc.load_page(0)  # Load the first page\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     image = extract_image_from_page(page)\n        &gt;&gt;&gt;     image.show()  # Display the image\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error extracting image: {e}\")\n    \"\"\"\n    try:\n        # Get images from the page\n        images = page.get_images(full=True)\n        if not images:\n            raise ValueError(\"No images found on the page.\")\n\n        # Extract the first image reference\n        xref = images[0][0]  # Get the first image's xref\n        base_image = page.parent.extract_image(xref)\n\n        # Validate the extracted image data\n        if (\n            \"image\" not in base_image\n            or \"width\" not in base_image\n            or \"height\" not in base_image\n        ):\n            raise ValueError(\"The extracted image data is incomplete.\")\n\n        # Convert the raw image bytes into a Pillow image\n        image_bytes = base_image[\"image\"]\n        pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n\n        return pil_image\n\n    except ValueError as ve:\n        raise ve  # Re-raise for calling functions to handle\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred during image extraction: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.get_page_dimensions","title":"<code>get_page_dimensions(page)</code>","text":"<p>Extracts the width and height of a single PDF page in both inches and pixels.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>A single PDF page object from PyMuPDF.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the width and height of the page in inches and pixels.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def get_page_dimensions(page: fitz.Page) -&gt; dict:\n    \"\"\"\n    Extracts the width and height of a single PDF page in both inches and pixels.\n\n    Args:\n        page (fitz.Page): A single PDF page object from PyMuPDF.\n\n    Returns:\n        dict: A dictionary containing the width and height of the page in inches and pixels.\n    \"\"\"\n    # Get page dimensions in points and convert to inches\n    page_width_pts, page_height_pts = page.rect.width, page.rect.height\n    page_width_in = page_width_pts / 72  # Convert points to inches\n    page_height_in = page_height_pts / 72\n\n    # Extract the first image on the page (if any) to get pixel dimensions\n    images = page.get_images(full=True)\n    if images:\n        xref = images[0][0]\n        base_image = page.parent.extract_image(xref)\n        width_px = base_image[\"width\"]\n        height_px = base_image[\"height\"]\n    else:\n        width_px, height_px = None, None  # No image found on the page\n\n    # Return dimensions\n    return {\n        \"width_in\": page_width_in,\n        \"height_in\": page_height_in,\n        \"width_px\": width_px,\n        \"height_px\": height_px,\n    }\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.load_pdf_pages","title":"<code>load_pdf_pages(pdf_path)</code>","text":"<p>Opens the PDF document and returns the fitz Document object.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>Path</code> <p>The path to the PDF file.</p> required <p>Returns:</p> Type Description <code>Document</code> <p>fitz.Document: The loaded PDF document.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>ValueError</code> <p>If the file is not a valid PDF document.</p> <code>Exception</code> <p>For any unexpected error.</p> Example <p>from pathlib import Path pdf_path = Path(\"/path/to/example.pdf\") try:     pdf_doc = load_pdf_pages(pdf_path)     print(f\"PDF contains {pdf_doc.page_count} pages.\") except Exception as e:     print(f\"Error loading PDF: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def load_pdf_pages(pdf_path: Path) -&gt; fitz.Document:\n    \"\"\"\n    Opens the PDF document and returns the fitz Document object.\n\n    Parameters:\n        pdf_path (Path): The path to the PDF file.\n\n    Returns:\n        fitz.Document: The loaded PDF document.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        ValueError: If the file is not a valid PDF document.\n        Exception: For any unexpected error.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; pdf_path = Path(\"/path/to/example.pdf\")\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     pdf_doc = load_pdf_pages(pdf_path)\n        &gt;&gt;&gt;     print(f\"PDF contains {pdf_doc.page_count} pages.\")\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error loading PDF: {e}\")\n    \"\"\"\n    if not pdf_path.exists():\n        raise FileNotFoundError(f\"The file '{pdf_path}' does not exist.\")\n\n    if not pdf_path.suffix.lower() == \".pdf\":\n        raise ValueError(\n            f\"The file '{pdf_path}' is not a valid PDF document (expected '.pdf').\"\n        )\n\n    try:\n        return fitz.open(str(pdf_path))  # PyMuPDF expects a string path\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred while opening the PDF: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.load_processed_PDF_data","title":"<code>load_processed_PDF_data(base_path)</code>","text":"<p>Loads processed PDF data from files using metadata for file references.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>Directory where the data is stored (as a Path object).</p> required <code>base_name</code> <code>str</code> <p>Base name of the processed directory.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[List[EntityAnnotation]], List[Image], List[Image]]</code> <p>Tuple[List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]]: - Loaded text pages. - Word locations (list of <code>EntityAnnotation</code> objects for each page). - Annotated images. - Unannotated images.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If any required files are missing.</p> <code>ValueError</code> <p>If the metadata file is incomplete or invalid.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def load_processed_PDF_data(\n    base_path: Path,\n) -&gt; Tuple[\n    List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]\n]:\n    \"\"\"\n    Loads processed PDF data from files using metadata for file references.\n\n    Parameters:\n        output_dir (Path): Directory where the data is stored (as a Path object).\n        base_name (str): Base name of the processed directory.\n\n    Returns:\n        Tuple[List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]]:\n            - Loaded text pages.\n            - Word locations (list of `EntityAnnotation` objects for each page).\n            - Annotated images.\n            - Unannotated images.\n\n    Raises:\n        FileNotFoundError: If any required files are missing.\n        ValueError: If the metadata file is incomplete or invalid.\n    \"\"\"\n    metadata_file = base_path / \"metadata.json\"\n\n    # Load metadata\n    try:\n        with metadata_file.open(\"r\", encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Metadata file '{metadata_file}' not found.\")\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid metadata file format: {e}\")\n\n    # Extract file paths from metadata\n    text_pages_file = base_path / metadata.get(\"files\", {}).get(\n        \"text_pages\", \"text_pages.json\"\n    )\n    word_locations_file = base_path / metadata.get(\"files\", {}).get(\n        \"word_locations\", \"word_locations.json\"\n    )\n    images_dir = Path(metadata.get(\"images_directory\", base_path / \"images\"))\n\n    # Validate file paths\n    if not text_pages_file.exists():\n        raise FileNotFoundError(f\"Text pages file '{text_pages_file}' not found.\")\n    if not word_locations_file.exists():\n        raise FileNotFoundError(\n            f\"Word locations file '{word_locations_file}' not found.\"\n        )\n    if not images_dir.exists() or not images_dir.is_dir():\n        raise FileNotFoundError(f\"Images directory '{images_dir}' not found.\")\n\n    # Load text pages\n    with text_pages_file.open(\"r\", encoding=\"utf-8\") as f:\n        text_pages = json.load(f)\n\n    # Load word locations\n    with word_locations_file.open(\"r\", encoding=\"utf-8\") as f:\n        serialized_word_locations = f.read()\n        word_locations = deserialize_entity_annotations_from_json(\n            serialized_word_locations\n        )\n\n    # Load images\n    annotated_images = []\n    unannotated_images = []\n    for file in sorted(\n        images_dir.iterdir()\n    ):  # Iterate over files in the images directory\n        if file.name.startswith(\"annotated_page_\") and file.suffix == \".png\":\n            annotated_images.append(Image.open(file))\n        elif file.name.startswith(\"unannotated_page_\") and file.suffix == \".png\":\n            unannotated_images.append(Image.open(file))\n\n    # Ensure images were loaded correctly\n    if not annotated_images or not unannotated_images:\n        raise ValueError(f\"No images found in the directory '{images_dir}'.\")\n\n    return text_pages, word_locations, annotated_images, unannotated_images\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.make_image_preprocess_mask","title":"<code>make_image_preprocess_mask(mask_height)</code>","text":"<p>Creates a preprocessing function that masks a specified height at the bottom of the image.</p> <p>Parameters:</p> Name Type Description Default <code>mask_height</code> <code>float</code> <p>The proportion of the image height to mask at the bottom (0.0 to 1.0).</p> required <p>Returns:</p> Type Description <code>Callable[[Image, int], Image]</code> <p>Callable[[Image.Image, int], Image.Image]: A preprocessing function that takes an image</p> <code>Callable[[Image, int], Image]</code> <p>and page number as input and returns the processed image.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def make_image_preprocess_mask(\n    mask_height: float,\n) -&gt; Callable[[Image.Image, int], Image.Image]:\n    \"\"\"\n    Creates a preprocessing function that masks a specified height at the bottom of the image.\n\n    Parameters:\n        mask_height (float): The proportion of the image height to mask at the bottom (0.0 to 1.0).\n\n    Returns:\n        Callable[[Image.Image, int], Image.Image]: A preprocessing function that takes an image\n        and page number as input and returns the processed image.\n    \"\"\"\n\n    def pre_process_image(image: Image.Image, page_number: int) -&gt; Image.Image:\n        \"\"\"\n        Preprocesses the image by masking the bottom region or performing other preprocessing steps.\n\n        Parameters:\n            image (Image.Image): The input image as a Pillow object.\n            page_number (int): The page number of the image (useful for conditional preprocessing).\n\n        Returns:\n            Image.Image: The preprocessed image.\n        \"\"\"\n\n        if page_number &gt; 0:  # don't apply mask to cover page.\n            # Make a copy of the image to avoid modifying the original\n            draw = ImageDraw.Draw(image)\n\n            # Get image dimensions\n            width, height = image.size\n\n            # Mask the bottom region based on the specified height proportion\n            mask_pixels = int(height * mask_height)\n            draw.rectangle([(0, height - mask_pixels), (width, height)], fill=\"black\")\n\n        return image\n\n    return pre_process_image\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.pil_to_bytes","title":"<code>pil_to_bytes(image, format='PNG')</code>","text":"<p>Converts a Pillow image to raw bytes.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The Pillow image object to convert.</p> required <code>format</code> <code>str</code> <p>The format to save the image as (e.g., \"PNG\", \"JPEG\"). Default is \"PNG\".</p> <code>'PNG'</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The raw bytes of the image.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def pil_to_bytes(image: Image.Image, format: str = \"PNG\") -&gt; bytes:\n    \"\"\"\n    Converts a Pillow image to raw bytes.\n\n    Parameters:\n        image (Image.Image): The Pillow image object to convert.\n        format (str): The format to save the image as (e.g., \"PNG\", \"JPEG\"). Default is \"PNG\".\n\n    Returns:\n        bytes: The raw bytes of the image.\n    \"\"\"\n    with io.BytesIO() as output:\n        image.save(output, format=format)\n        return output.getvalue()\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.process_page","title":"<code>process_page(page, client, annotation_font_path, preprocessor=None)</code>","text":"<p>Processes a single PDF page, extracting text, word locations, and annotated images.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The PDF page object.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>pre_processor</code> <code>Callable[[Image, int], Image]</code> <p>Preprocessing function for the image.</p> required <code>annotation_font_path</code> <code>str</code> <p>Path to the font file for annotations.</p> required <p>Returns:</p> Type Description <code>Tuple[str, List[EntityAnnotation], Image, Image, dict]</code> <p>Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]: - Full page text (str) - Word locations (List of vision.EntityAnnotation) - Annotated image (Pillow Image object) - Original unprocessed image (Pillow Image object) - Page dimensions (dict)</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def process_page(\n    page: fitz.Page,\n    client: vision.ImageAnnotatorClient,\n    annotation_font_path: str,\n    preprocessor: Callable[[Image.Image, int], Image.Image] = None,\n) -&gt; Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]:\n    \"\"\"\n    Processes a single PDF page, extracting text, word locations, and annotated images.\n\n    Parameters:\n        page (fitz.Page): The PDF page object.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        pre_processor (Callable[[Image.Image, int], Image.Image]): Preprocessing function for the image.\n        annotation_font_path (str): Path to the font file for annotations.\n\n    Returns:\n        Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]:\n            - Full page text (str)\n            - Word locations (List of vision.EntityAnnotation)\n            - Annotated image (Pillow Image object)\n            - Original unprocessed image (Pillow Image object)\n            - Page dimensions (dict)\n    \"\"\"\n    # Extract the original image from the PDF page\n    original_image = extract_image_from_page(page)\n\n    # Make a copy of the original image for processing\n    processed_image = original_image.copy()\n\n    # Apply the preprocessing function (if provided)\n    if preprocessor:\n        # print(\"preprocessing...\") # debug\n        processed_image = preprocessor(processed_image, page.number)\n        # processed_image.show() # debug\n\n    # Annotate the processed image using the Vision API\n    response = process_single_image(processed_image, client)\n\n    if response:\n        text_annotations = response.text_annotations\n        # Extract full text and word locations\n        full_page_text = text_annotations[0].description if text_annotations else \"\"\n        word_locations = text_annotations[1:] if len(text_annotations) &gt; 1 else []\n    else:\n        # return empty data\n        full_page_text = \"\"\n        word_locations = [EntityAnnotation()]\n        text_annotations = [\n            EntityAnnotation()\n        ]  # create empty data structures to allow storing to proceed.\n\n    # Create an annotated image with bounding boxes and labels\n    annotated_image = annotate_image_with_text(\n        processed_image, text_annotations, annotation_font_path\n    )\n\n    # Get page dimensions (from the original PDF page, not the image)\n    page_dimensions = get_page_dimensions(page)\n\n    return (\n        full_page_text,\n        word_locations,\n        annotated_image,\n        original_image,\n        page_dimensions,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.process_single_image","title":"<code>process_single_image(image, client, feature_type=DEFAULT_ANNOTATION_METHOD, language_hints=DEFAULT_ANNOTATION_LANGUAGE_HINTS)</code>","text":"<p>Processes a single image with the Google Vision API and returns text annotations.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The preprocessed Pillow image object.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>feature_type</code> <code>str</code> <p>Type of text detection to use ('TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION').</p> <code>DEFAULT_ANNOTATION_METHOD</code> <code>language_hints</code> <code>List</code> <p>Language hints for OCR.</p> <code>DEFAULT_ANNOTATION_LANGUAGE_HINTS</code> <p>Returns:</p> Type Description <code>List[EntityAnnotation]</code> <p>List[vision.EntityAnnotation]: Text annotations from the Vision API response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no text is detected.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def process_single_image(\n    image: Image.Image,\n    client: vision.ImageAnnotatorClient,\n    feature_type: str = DEFAULT_ANNOTATION_METHOD,\n    language_hints: List = DEFAULT_ANNOTATION_LANGUAGE_HINTS,\n) -&gt; List[vision.EntityAnnotation]:\n    \"\"\"\n    Processes a single image with the Google Vision API and returns text annotations.\n\n    Parameters:\n        image (Image.Image): The preprocessed Pillow image object.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        feature_type (str): Type of text detection to use ('TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION').\n        language_hints (List): Language hints for OCR.\n\n    Returns:\n        List[vision.EntityAnnotation]: Text annotations from the Vision API response.\n\n    Raises:\n        ValueError: If no text is detected.\n    \"\"\"\n    # Convert the Pillow image to bytes\n    image_bytes = pil_to_bytes(image, format=\"PNG\")\n\n    # Map feature type\n    feature_map = {\n        \"TEXT_DETECTION\": vision.Feature.Type.TEXT_DETECTION,\n        \"DOCUMENT_TEXT_DETECTION\": vision.Feature.Type.DOCUMENT_TEXT_DETECTION,\n    }\n    if feature_type not in feature_map:\n        raise ValueError(\n            f\"Invalid feature type '{feature_type}'. Use 'TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION'.\"\n        )\n\n    # Prepare Vision API request\n    vision_image = vision.Image(content=image_bytes)\n    features = [vision.Feature(type=feature_map[feature_type])]\n    image_context = vision.ImageContext(language_hints=language_hints)\n\n    # Make the API call\n    response = client.annotate_image(\n        {\"image\": vision_image, \"features\": features, \"image_context\": image_context}\n    )\n\n    return response\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.save_processed_pdf_data","title":"<code>save_processed_pdf_data(output_dir, journal_name, text_pages, word_locations, annotated_images, unannotated_images)</code>","text":"<p>Saves processed PDF data to files for later reloading.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>Directory to save the data (as a Path object).</p> required <code>base_name</code> <code>str</code> <p>Base name for the output directory (usually the PDF name without extension).</p> required <code>text_pages</code> <code>List[str]</code> <p>Extracted full-page text.</p> required <code>word_locations</code> <code>List[List[EntityAnnotation]]</code> <p>Word locations and annotations from Vision API.</p> required <code>annotated_images</code> <code>List[Image]</code> <p>Annotated images with bounding boxes.</p> required <code>unannotated_images</code> <code>List[Image]</code> <p>Raw unannotated images.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def save_processed_pdf_data(\n    output_dir: Path,\n    journal_name: str,\n    text_pages: List[str],\n    word_locations: List[List[EntityAnnotation]],\n    annotated_images: List[Image.Image],\n    unannotated_images: List[Image.Image],\n):\n    \"\"\"\n    Saves processed PDF data to files for later reloading.\n\n    Parameters:\n        output_dir (Path): Directory to save the data (as a Path object).\n        base_name (str): Base name for the output directory (usually the PDF name without extension).\n        text_pages (List[str]): Extracted full-page text.\n        word_locations (List[List[EntityAnnotation]]): Word locations and annotations from Vision API.\n        annotated_images (List[PIL.Image.Image]): Annotated images with bounding boxes.\n        unannotated_images (List[PIL.Image.Image]): Raw unannotated images.\n\n    Returns:\n        None\n    \"\"\"\n    # Create output directories\n    base_path = output_dir / journal_name / \"ocr_data\"\n    images_dir = base_path / \"images\"\n\n    base_path.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save text data\n    text_pages_file = base_path / \"text_pages.json\"\n    with text_pages_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(text_pages, f, indent=4, ensure_ascii=False)\n\n    # Save word locations as JSON\n    word_locations_file = base_path / \"word_locations.json\"\n    serialized_word_locations = serialize_entity_annotations_to_json(word_locations)\n    with word_locations_file.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(serialized_word_locations)\n\n    # Save images\n    for i, annotated_image in enumerate(annotated_images):\n        annotated_file = images_dir / f\"annotated_page_{i + 1}.png\"\n        annotated_image.save(annotated_file)\n    for i, unannotated_image in enumerate(unannotated_images):\n        unannotated_file = images_dir / f\"unannotated_page_{i + 1}.png\"\n        unannotated_image.save(unannotated_file)\n\n    # Save metadata\n    metadata = {\n        \"source_pdf\": journal_name,\n        \"page_count\": len(text_pages),\n        \"images_directory\": str(\n            images_dir\n        ),  # Convert Path to string for JSON serialization\n        \"files\": {\n            \"text_pages\": \"text_pages.json\",\n            \"word_locations\": \"word_locations.json\",\n        },\n    }\n    metadata_file = base_path / \"metadata.json\"\n    with metadata_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(metadata, f, indent=4)\n\n    print(f\"Processed data saved in: {base_path}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.serialize_entity_annotations_to_json","title":"<code>serialize_entity_annotations_to_json(annotations)</code>","text":"<p>Serializes a nested list of EntityAnnotation objects into a JSON-compatible format using Base64 encoding.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>List[List[EntityAnnotation]]</code> <p>The nested list of EntityAnnotation objects.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The serialized data in JSON format as a string.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def serialize_entity_annotations_to_json(\n    annotations: List[List[EntityAnnotation]],\n) -&gt; str:\n    \"\"\"\n    Serializes a nested list of EntityAnnotation objects into a JSON-compatible format using Base64 encoding.\n\n    Parameters:\n        annotations (List[List[EntityAnnotation]]): The nested list of EntityAnnotation objects.\n\n    Returns:\n        str: The serialized data in JSON format as a string.\n    \"\"\"\n    serialized_data = []\n    for page_annotations in annotations:\n        serialized_page = [\n            base64.b64encode(annotation.SerializeToString()).decode(\"utf-8\")\n            for annotation in page_annotations\n        ]\n        serialized_data.append(serialized_page)\n\n    # Convert to a JSON string\n    return json.dumps(serialized_data, indent=4)\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.start_image_annotator_client","title":"<code>start_image_annotator_client(credentials_file=None, api_endpoint='vision.googleapis.com', timeout=(10, 30), enable_logging=False)</code>","text":"<p>Starts and returns a Google Vision API ImageAnnotatorClient with optional configuration.</p> <p>Parameters:</p> Name Type Description Default <code>credentials_file</code> <code>str</code> <p>Path to the credentials JSON file. If None, uses the default environment variable.</p> <code>None</code> <code>api_endpoint</code> <code>str</code> <p>Custom API endpoint for the Vision API. Default is the global endpoint.</p> <code>'vision.googleapis.com'</code> <code>timeout</code> <code>Tuple[int, int]</code> <p>Connection and read timeouts in seconds. Default is (10, 30).</p> <code>(10, 30)</code> <code>enable_logging</code> <code>bool</code> <p>Enable detailed logging for debugging. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ImageAnnotatorClient</code> <p>vision.ImageAnnotatorClient: Configured Vision API client.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified credentials file is not found.</p> <code>Exception</code> <p>For unexpected errors during client setup.</p> Example <p>client = start_image_annotator_client(     credentials_file=\"/path/to/credentials.json\",     api_endpoint=\"vision.googleapis.com\",     timeout=(10, 30),     enable_logging=True ) print(\"Google Vision API client initialized.\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def start_image_annotator_client(\n    credentials_file: str = None,\n    api_endpoint: str = \"vision.googleapis.com\",\n    timeout: Tuple[int, int] = (10, 30),\n    enable_logging: bool = False,\n) -&gt; vision.ImageAnnotatorClient:\n    \"\"\"\n    Starts and returns a Google Vision API ImageAnnotatorClient with optional configuration.\n\n    Parameters:\n        credentials_file (str): Path to the credentials JSON file. If None, uses the default environment variable.\n        api_endpoint (str): Custom API endpoint for the Vision API. Default is the global endpoint.\n        timeout (Tuple[int, int]): Connection and read timeouts in seconds. Default is (10, 30).\n        enable_logging (bool): Enable detailed logging for debugging. Default is False.\n\n    Returns:\n        vision.ImageAnnotatorClient: Configured Vision API client.\n\n    Raises:\n        FileNotFoundError: If the specified credentials file is not found.\n        Exception: For unexpected errors during client setup.\n\n    Example:\n        &gt;&gt;&gt; client = start_image_annotator_client(\n        &gt;&gt;&gt;     credentials_file=\"/path/to/credentials.json\",\n        &gt;&gt;&gt;     api_endpoint=\"vision.googleapis.com\",\n        &gt;&gt;&gt;     timeout=(10, 30),\n        &gt;&gt;&gt;     enable_logging=True\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; print(\"Google Vision API client initialized.\")\n    \"\"\"\n    try:\n        # Set up credentials\n        if credentials_file:\n            if not os.path.exists(credentials_file):\n                raise FileNotFoundError(\n                    f\"Credentials file '{credentials_file}' not found.\"\n                )\n            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_file\n\n        # Configure client options\n        client_options = {\"api_endpoint\": api_endpoint}\n        client = vision.ImageAnnotatorClient(client_options=client_options)\n\n        # Optionally enable logging\n        if enable_logging:\n            print(f\"Vision API Client started with endpoint: {api_endpoint}\")\n            print(f\"Timeout settings: Connect={timeout[0]}s, Read={timeout[1]}s\")\n\n        return client\n\n    except Exception as e:\n        raise Exception(f\"Failed to initialize ImageAnnotatorClient: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor","title":"<code>ocr_editor</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.current_image","title":"<code>current_image = st.session_state.current_image</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.current_page_index","title":"<code>current_page_index = st.session_state.current_page_index</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.current_text","title":"<code>current_text = pages[current_page_index]</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.edited_text","title":"<code>edited_text = st.text_area('Edit OCR Text', value=st.session_state.current_text, key=f'text_area_{st.session_state.current_page_index}', height=400)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.image_directory","title":"<code>image_directory = st.sidebar.text_input('Image Directory', value='./images')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.ocr_text_directory","title":"<code>ocr_text_directory = st.sidebar.text_input('OCR Text Directory', value='./ocr_text')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.pages","title":"<code>pages = st.session_state.pages</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.save_path","title":"<code>save_path = os.path.join(ocr_text_directory, 'updated_ocr.xml')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.tree","title":"<code>tree = st.session_state.tree</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.uploaded_image_file","title":"<code>uploaded_image_file = st.sidebar.file_uploader('Upload an Image', type=['jpg', 'jpeg', 'png', 'pdf'])</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.uploaded_text_file","title":"<code>uploaded_text_file = st.sidebar.file_uploader('Upload OCR Text File', type=['xml'])</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.extract_pages","title":"<code>extract_pages(tree)</code>","text":"<p>Extract page data from the XML tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>ElementTree</code> <p>Parsed XML tree.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries containing 'number' and 'text' for each page.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_editor.py</code> <pre><code>def extract_pages(tree):\n    \"\"\"\n    Extract page data from the XML tree.\n\n    Args:\n        tree (etree.ElementTree): Parsed XML tree.\n\n    Returns:\n        list: A list of dictionaries containing 'number' and 'text' for each page.\n    \"\"\"\n    pages = []\n    for page in tree.xpath(\"//page\"):\n        page_number = page.get(\"page\")\n        ocr_text = page.text.strip() if page.text else \"\"\n        pages.append({\"number\": page_number, \"text\": ocr_text})\n    return pages\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.load_xml","title":"<code>load_xml(file_obj)</code>","text":"<p>Load an XML file from a file-like object.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_editor.py</code> <pre><code>def load_xml(file_obj):\n    \"\"\"\n    Load an XML file from a file-like object.\n    \"\"\"\n    try:\n        tree = etree.parse(file_obj)  # Directly parse the file-like object\n        return tree\n    except etree.XMLSyntaxError as e:\n        st.error(f\"Error parsing XML file: {e}\")\n        return None\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.save_xml","title":"<code>save_xml(tree, file_path)</code>","text":"<p>Save the modified XML tree to a file.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_editor.py</code> <pre><code>def save_xml(tree, file_path):\n    \"\"\"\n    Save the modified XML tree to a file.\n    \"\"\"\n    with open(file_path, \"wb\") as file:\n        tree.write(file, pretty_print=True, encoding=\"utf-8\", xml_declaration=True)\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing","title":"<code>ocr_processing</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.DEFAULT_ANNOTATION_FONT_PATH","title":"<code>DEFAULT_ANNOTATION_FONT_PATH = Path('/System/Library/Fonts/Supplemental/Arial.ttf')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.DEFAULT_ANNOTATION_FONT_SIZE","title":"<code>DEFAULT_ANNOTATION_FONT_SIZE = 12</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.DEFAULT_ANNOTATION_LANGUAGE_HINTS","title":"<code>DEFAULT_ANNOTATION_LANGUAGE_HINTS = ['vi']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.DEFAULT_ANNOTATION_METHOD","title":"<code>DEFAULT_ANNOTATION_METHOD = 'DOCUMENT_TEXT_DETECTION'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.DEFAULT_ANNOTATION_OFFSET","title":"<code>DEFAULT_ANNOTATION_OFFSET = 2</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.logger","title":"<code>logger = logging.getLogger('ocr_processing')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.PDFParseWarning","title":"<code>PDFParseWarning</code>","text":"<p>               Bases: <code>Warning</code></p> <p>Custom warning class for PDF parsing issues. Encapsulates minimal logic for displaying warnings with a custom format.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>class PDFParseWarning(Warning):\n    \"\"\"\n    Custom warning class for PDF parsing issues.\n    Encapsulates minimal logic for displaying warnings with a custom format.\n    \"\"\"\n\n    @staticmethod\n    def warn(message: str):\n        \"\"\"\n        Display a warning message with custom formatting.\n\n        Parameters:\n            message (str): The warning message to display.\n        \"\"\"\n        formatted_message = f\"\\033[93mPDFParseWarning: {message}\\033[0m\"\n        print(formatted_message)  # Simply prints the warning\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.PDFParseWarning.warn","title":"<code>warn(message)</code>  <code>staticmethod</code>","text":"<p>Display a warning message with custom formatting.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The warning message to display.</p> required Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>@staticmethod\ndef warn(message: str):\n    \"\"\"\n    Display a warning message with custom formatting.\n\n    Parameters:\n        message (str): The warning message to display.\n    \"\"\"\n    formatted_message = f\"\\033[93mPDFParseWarning: {message}\\033[0m\"\n    print(formatted_message)  # Simply prints the warning\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.annotate_image_with_text","title":"<code>annotate_image_with_text(image, text_annotations, annotation_font_path, font_size=12)</code>","text":"<p>Annotates a PIL image with bounding boxes and text descriptions from OCR results.</p> <p>Parameters:</p> Name Type Description Default <code>pil_image</code> <code>Image</code> <p>The input PIL image to annotate.</p> required <code>text_annotations</code> <code>List[EntityAnnotation]</code> <p>OCR results containing bounding boxes and text.</p> required <code>annotation_font_path</code> <code>str</code> <p>Path to the font file for text annotations.</p> required <code>font_size</code> <code>int</code> <p>Font size for text annotations.</p> <code>12</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The annotated PIL image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input image is None.</p> <code>IOError</code> <p>If the font file cannot be loaded.</p> <code>Exception</code> <p>For any other unexpected errors.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def annotate_image_with_text(\n    image: Image.Image,\n    text_annotations: List[EntityAnnotation],\n    annotation_font_path: str,\n    font_size: int = 12,\n) -&gt; Image.Image:\n    \"\"\"\n    Annotates a PIL image with bounding boxes and text descriptions from OCR results.\n\n    Parameters:\n        pil_image (Image.Image): The input PIL image to annotate.\n        text_annotations (List[EntityAnnotation]): OCR results containing bounding boxes and text.\n        annotation_font_path (str): Path to the font file for text annotations.\n        font_size (int): Font size for text annotations.\n\n    Returns:\n        Image.Image: The annotated PIL image.\n\n    Raises:\n        ValueError: If the input image is None.\n        IOError: If the font file cannot be loaded.\n        Exception: For any other unexpected errors.\n    \"\"\"\n    if image is None:\n        raise ValueError(\"The input image is None.\")\n\n    try:\n        font = ImageFont.truetype(annotation_font_path, font_size)\n    except IOError as e:\n        raise IOError(f\"Failed to load the font from '{annotation_font_path}': {e}\")\n\n    draw = ImageDraw.Draw(image)\n\n    try:\n        for i, text_obj in enumerate(text_annotations):\n            vertices = [\n                (vertex.x, vertex.y) for vertex in text_obj.bounding_poly.vertices\n            ]\n            if (\n                len(vertices) == 4\n            ):  # Ensure there are exactly 4 vertices for a rectangle\n                # Draw the bounding box\n                draw.polygon(vertices, outline=\"red\", width=2)\n\n                # Skip the first bounding box (whole text region)\n                if i &gt; 0:\n                    # Offset the text position slightly for clarity\n                    text_position = (vertices[0][0] + 2, vertices[0][1] + 2)\n                    draw.text(\n                        text_position, text_obj.description, fill=\"red\", font=font\n                    )\n\n    except AttributeError as e:\n        raise ValueError(f\"Invalid text annotation structure: {e}\")\n    except Exception as e:\n        raise Exception(f\"An error occurred during image annotation: {e}\")\n\n    return image\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.build_processed_pdf","title":"<code>build_processed_pdf(pdf_path, client, preprocessor=None, annotation_font_path=DEFAULT_ANNOTATION_FONT_PATH)</code>","text":"<p>Processes a PDF document, extracting text, word locations, annotated images, and unannotated images.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>Path</code> <p>Path to the PDF file.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>annotation_font_path</code> <code>Path</code> <p>Path to the font file for annotations.</p> <code>DEFAULT_ANNOTATION_FONT_PATH</code> <p>Returns:</p> Type Description <code>Tuple[List[str], List[List[EntityAnnotation]], List[Image], List[Image]]</code> <p>Tuple[List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]]: - List of extracted full-page texts (one entry per page). - List of word locations (list of <code>vision.EntityAnnotation</code> objects for each page). - List of annotated images (with bounding boxes and text annotations). - List of unannotated images (raw page images).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified PDF file does not exist.</p> <code>ValueError</code> <p>If the PDF file is invalid or contains no pages.</p> <code>Exception</code> <p>For any unexpected errors during processing.</p> Example <p>from pathlib import Path from google.cloud import vision pdf_path = Path(\"/path/to/example.pdf\") font_path = Path(\"/path/to/fonts/Arial.ttf\") client = vision.ImageAnnotatorClient() try:     text_pages, word_locations_list, annotated_images, unannotated_images = build_processed_pdf(         pdf_path, client, font_path     )     print(f\"Processed {len(text_pages)} pages successfully!\") except Exception as e:     print(f\"Error processing PDF: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def build_processed_pdf(\n    pdf_path: Path,\n    client: vision.ImageAnnotatorClient,\n    preprocessor: Callable = None,\n    annotation_font_path: Path = DEFAULT_ANNOTATION_FONT_PATH,\n) -&gt; Tuple[\n    List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]\n]:\n    \"\"\"\n    Processes a PDF document, extracting text, word locations, annotated images, and unannotated images.\n\n    Parameters:\n        pdf_path (Path): Path to the PDF file.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        annotation_font_path (Path): Path to the font file for annotations.\n\n    Returns:\n        Tuple[List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]]:\n            - List of extracted full-page texts (one entry per page).\n            - List of word locations (list of `vision.EntityAnnotation` objects for each page).\n            - List of annotated images (with bounding boxes and text annotations).\n            - List of unannotated images (raw page images).\n\n    Raises:\n        FileNotFoundError: If the specified PDF file does not exist.\n        ValueError: If the PDF file is invalid or contains no pages.\n        Exception: For any unexpected errors during processing.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from google.cloud import vision\n        &gt;&gt;&gt; pdf_path = Path(\"/path/to/example.pdf\")\n        &gt;&gt;&gt; font_path = Path(\"/path/to/fonts/Arial.ttf\")\n        &gt;&gt;&gt; client = vision.ImageAnnotatorClient()\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     text_pages, word_locations_list, annotated_images, unannotated_images = build_processed_pdf(\n        &gt;&gt;&gt;         pdf_path, client, font_path\n        &gt;&gt;&gt;     )\n        &gt;&gt;&gt;     print(f\"Processed {len(text_pages)} pages successfully!\")\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error processing PDF: {e}\")\n    \"\"\"\n    try:\n        doc = load_pdf_pages(pdf_path)\n    except FileNotFoundError as fnf_error:\n        raise FileNotFoundError(f\"Error loading PDF: {fnf_error}\")\n    except ValueError as ve:\n        raise ValueError(f\"Invalid PDF file: {ve}\")\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred while loading the PDF: {e}\")\n\n    if doc.page_count == 0:\n        raise ValueError(f\"The PDF file '{pdf_path}' contains no pages.\")\n\n    logger.info(f\"Processing file with {doc.page_count} pages:\\n\\t{pdf_path}\")\n\n    text_pages = []\n    word_locations_list = []\n    annotated_images = []\n    unannotated_images = []\n    first_page_dimensions = None\n\n    for page_num in range(doc.page_count):\n        logger.info(f\"Processing page {page_num + 1}/{doc.page_count}...\")\n\n        try:\n            page = doc.load_page(page_num)\n            (\n                full_page_text,\n                word_locations,\n                annotated_image,\n                unannotated_image,\n                page_dimensions,\n            ) = process_page(page, client, annotation_font_path, preprocessor)\n\n            if full_page_text:  # this is not an empty page\n\n                if page_num == 0:  # save first page info\n                    first_page_dimensions = page_dimensions\n                elif (\n                    page_dimensions != first_page_dimensions\n                ):  # verify page dimensions are consistent\n                    PDFParseWarning.warn(\n                        f\"Page {page_num + 1} has different dimensions than page 1.\"\n                        f\"({page_dimensions}) compared to the first page: ({first_page_dimensions}).\"\n                    )\n\n                text_pages.append(full_page_text)\n                word_locations_list.append(word_locations)\n                annotated_images.append(annotated_image)\n                unannotated_images.append(unannotated_image)\n            else:\n                PDFParseWarning.warn(\n                    f\"Page {page_num + 1} empty, added empty datastructures...\\n\"\n                    # f\"  (Note that total document length will be reduced.)\"\n                )\n\n        except ValueError as ve:\n            print(f\"ValueError on page {page_num + 1}: {ve}\")\n        except OSError as oe:\n            print(f\"OSError on page {page_num + 1}: {oe}\")\n        except Exception as e:\n            print(f\"Unexpected error on page {page_num + 1}: {e}\")\n\n    print(f\"page dimensions: {page_dimensions}\")\n    return text_pages, word_locations_list, annotated_images, unannotated_images\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.deserialize_entity_annotations_from_json","title":"<code>deserialize_entity_annotations_from_json(data)</code>","text":"<p>Deserializes JSON data into a nested list of EntityAnnotation objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The JSON string containing serialized annotations.</p> required <p>Returns:</p> Type Description <code>List[List[EntityAnnotation]]</code> <p>List[List[EntityAnnotation]]: The reconstructed nested list of EntityAnnotation objects.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def deserialize_entity_annotations_from_json(data: str) -&gt; List[List[EntityAnnotation]]:\n    \"\"\"\n    Deserializes JSON data into a nested list of EntityAnnotation objects.\n\n    Parameters:\n        data (str): The JSON string containing serialized annotations.\n\n    Returns:\n        List[List[EntityAnnotation]]: The reconstructed nested list of EntityAnnotation objects.\n    \"\"\"\n    serialized_data = json.loads(data)\n    deserialized_data = []\n\n    for serialized_page in serialized_data:\n        page_annotations = [\n            EntityAnnotation.deserialize(base64.b64decode(serialized_annotation))\n            for serialized_annotation in serialized_page\n        ]\n        deserialized_data.append(page_annotations)\n\n    return deserialized_data\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.extract_image_from_page","title":"<code>extract_image_from_page(page)</code>","text":"<p>Extracts the first image from the given PDF page and returns it as a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The PDF page object.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The first image on the page as a Pillow Image object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no images are found on the page or the image data is incomplete.</p> <code>Exception</code> <p>For unexpected errors during image extraction.</p> Example <p>import fitz from PIL import Image doc = fitz.open(\"/path/to/document.pdf\") page = doc.load_page(0)  # Load the first page try:     image = extract_image_from_page(page)     image.show()  # Display the image except Exception as e:     print(f\"Error extracting image: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def extract_image_from_page(page: fitz.Page) -&gt; Image.Image:\n    \"\"\"\n    Extracts the first image from the given PDF page and returns it as a PIL Image.\n\n    Parameters:\n        page (fitz.Page): The PDF page object.\n\n    Returns:\n        Image.Image: The first image on the page as a Pillow Image object.\n\n    Raises:\n        ValueError: If no images are found on the page or the image data is incomplete.\n        Exception: For unexpected errors during image extraction.\n\n    Example:\n        &gt;&gt;&gt; import fitz\n        &gt;&gt;&gt; from PIL import Image\n        &gt;&gt;&gt; doc = fitz.open(\"/path/to/document.pdf\")\n        &gt;&gt;&gt; page = doc.load_page(0)  # Load the first page\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     image = extract_image_from_page(page)\n        &gt;&gt;&gt;     image.show()  # Display the image\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error extracting image: {e}\")\n    \"\"\"\n    try:\n        # Get images from the page\n        images = page.get_images(full=True)\n        if not images:\n            raise ValueError(\"No images found on the page.\")\n\n        # Extract the first image reference\n        xref = images[0][0]  # Get the first image's xref\n        base_image = page.parent.extract_image(xref)\n\n        # Validate the extracted image data\n        if (\n            \"image\" not in base_image\n            or \"width\" not in base_image\n            or \"height\" not in base_image\n        ):\n            raise ValueError(\"The extracted image data is incomplete.\")\n\n        # Convert the raw image bytes into a Pillow image\n        image_bytes = base_image[\"image\"]\n        pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n\n        return pil_image\n\n    except ValueError as ve:\n        raise ve  # Re-raise for calling functions to handle\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred during image extraction: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.get_page_dimensions","title":"<code>get_page_dimensions(page)</code>","text":"<p>Extracts the width and height of a single PDF page in both inches and pixels.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>A single PDF page object from PyMuPDF.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the width and height of the page in inches and pixels.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def get_page_dimensions(page: fitz.Page) -&gt; dict:\n    \"\"\"\n    Extracts the width and height of a single PDF page in both inches and pixels.\n\n    Args:\n        page (fitz.Page): A single PDF page object from PyMuPDF.\n\n    Returns:\n        dict: A dictionary containing the width and height of the page in inches and pixels.\n    \"\"\"\n    # Get page dimensions in points and convert to inches\n    page_width_pts, page_height_pts = page.rect.width, page.rect.height\n    page_width_in = page_width_pts / 72  # Convert points to inches\n    page_height_in = page_height_pts / 72\n\n    # Extract the first image on the page (if any) to get pixel dimensions\n    images = page.get_images(full=True)\n    if images:\n        xref = images[0][0]\n        base_image = page.parent.extract_image(xref)\n        width_px = base_image[\"width\"]\n        height_px = base_image[\"height\"]\n    else:\n        width_px, height_px = None, None  # No image found on the page\n\n    # Return dimensions\n    return {\n        \"width_in\": page_width_in,\n        \"height_in\": page_height_in,\n        \"width_px\": width_px,\n        \"height_px\": height_px,\n    }\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.load_pdf_pages","title":"<code>load_pdf_pages(pdf_path)</code>","text":"<p>Opens the PDF document and returns the fitz Document object.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>Path</code> <p>The path to the PDF file.</p> required <p>Returns:</p> Type Description <code>Document</code> <p>fitz.Document: The loaded PDF document.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>ValueError</code> <p>If the file is not a valid PDF document.</p> <code>Exception</code> <p>For any unexpected error.</p> Example <p>from pathlib import Path pdf_path = Path(\"/path/to/example.pdf\") try:     pdf_doc = load_pdf_pages(pdf_path)     print(f\"PDF contains {pdf_doc.page_count} pages.\") except Exception as e:     print(f\"Error loading PDF: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def load_pdf_pages(pdf_path: Path) -&gt; fitz.Document:\n    \"\"\"\n    Opens the PDF document and returns the fitz Document object.\n\n    Parameters:\n        pdf_path (Path): The path to the PDF file.\n\n    Returns:\n        fitz.Document: The loaded PDF document.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        ValueError: If the file is not a valid PDF document.\n        Exception: For any unexpected error.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; pdf_path = Path(\"/path/to/example.pdf\")\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     pdf_doc = load_pdf_pages(pdf_path)\n        &gt;&gt;&gt;     print(f\"PDF contains {pdf_doc.page_count} pages.\")\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error loading PDF: {e}\")\n    \"\"\"\n    if not pdf_path.exists():\n        raise FileNotFoundError(f\"The file '{pdf_path}' does not exist.\")\n\n    if not pdf_path.suffix.lower() == \".pdf\":\n        raise ValueError(\n            f\"The file '{pdf_path}' is not a valid PDF document (expected '.pdf').\"\n        )\n\n    try:\n        return fitz.open(str(pdf_path))  # PyMuPDF expects a string path\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred while opening the PDF: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.load_processed_PDF_data","title":"<code>load_processed_PDF_data(base_path)</code>","text":"<p>Loads processed PDF data from files using metadata for file references.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>Directory where the data is stored (as a Path object).</p> required <code>base_name</code> <code>str</code> <p>Base name of the processed directory.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[List[EntityAnnotation]], List[Image], List[Image]]</code> <p>Tuple[List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]]: - Loaded text pages. - Word locations (list of <code>EntityAnnotation</code> objects for each page). - Annotated images. - Unannotated images.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If any required files are missing.</p> <code>ValueError</code> <p>If the metadata file is incomplete or invalid.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def load_processed_PDF_data(\n    base_path: Path,\n) -&gt; Tuple[\n    List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]\n]:\n    \"\"\"\n    Loads processed PDF data from files using metadata for file references.\n\n    Parameters:\n        output_dir (Path): Directory where the data is stored (as a Path object).\n        base_name (str): Base name of the processed directory.\n\n    Returns:\n        Tuple[List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]]:\n            - Loaded text pages.\n            - Word locations (list of `EntityAnnotation` objects for each page).\n            - Annotated images.\n            - Unannotated images.\n\n    Raises:\n        FileNotFoundError: If any required files are missing.\n        ValueError: If the metadata file is incomplete or invalid.\n    \"\"\"\n    metadata_file = base_path / \"metadata.json\"\n\n    # Load metadata\n    try:\n        with metadata_file.open(\"r\", encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Metadata file '{metadata_file}' not found.\")\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid metadata file format: {e}\")\n\n    # Extract file paths from metadata\n    text_pages_file = base_path / metadata.get(\"files\", {}).get(\n        \"text_pages\", \"text_pages.json\"\n    )\n    word_locations_file = base_path / metadata.get(\"files\", {}).get(\n        \"word_locations\", \"word_locations.json\"\n    )\n    images_dir = Path(metadata.get(\"images_directory\", base_path / \"images\"))\n\n    # Validate file paths\n    if not text_pages_file.exists():\n        raise FileNotFoundError(f\"Text pages file '{text_pages_file}' not found.\")\n    if not word_locations_file.exists():\n        raise FileNotFoundError(\n            f\"Word locations file '{word_locations_file}' not found.\"\n        )\n    if not images_dir.exists() or not images_dir.is_dir():\n        raise FileNotFoundError(f\"Images directory '{images_dir}' not found.\")\n\n    # Load text pages\n    with text_pages_file.open(\"r\", encoding=\"utf-8\") as f:\n        text_pages = json.load(f)\n\n    # Load word locations\n    with word_locations_file.open(\"r\", encoding=\"utf-8\") as f:\n        serialized_word_locations = f.read()\n        word_locations = deserialize_entity_annotations_from_json(\n            serialized_word_locations\n        )\n\n    # Load images\n    annotated_images = []\n    unannotated_images = []\n    for file in sorted(\n        images_dir.iterdir()\n    ):  # Iterate over files in the images directory\n        if file.name.startswith(\"annotated_page_\") and file.suffix == \".png\":\n            annotated_images.append(Image.open(file))\n        elif file.name.startswith(\"unannotated_page_\") and file.suffix == \".png\":\n            unannotated_images.append(Image.open(file))\n\n    # Ensure images were loaded correctly\n    if not annotated_images or not unannotated_images:\n        raise ValueError(f\"No images found in the directory '{images_dir}'.\")\n\n    return text_pages, word_locations, annotated_images, unannotated_images\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.make_image_preprocess_mask","title":"<code>make_image_preprocess_mask(mask_height)</code>","text":"<p>Creates a preprocessing function that masks a specified height at the bottom of the image.</p> <p>Parameters:</p> Name Type Description Default <code>mask_height</code> <code>float</code> <p>The proportion of the image height to mask at the bottom (0.0 to 1.0).</p> required <p>Returns:</p> Type Description <code>Callable[[Image, int], Image]</code> <p>Callable[[Image.Image, int], Image.Image]: A preprocessing function that takes an image</p> <code>Callable[[Image, int], Image]</code> <p>and page number as input and returns the processed image.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def make_image_preprocess_mask(\n    mask_height: float,\n) -&gt; Callable[[Image.Image, int], Image.Image]:\n    \"\"\"\n    Creates a preprocessing function that masks a specified height at the bottom of the image.\n\n    Parameters:\n        mask_height (float): The proportion of the image height to mask at the bottom (0.0 to 1.0).\n\n    Returns:\n        Callable[[Image.Image, int], Image.Image]: A preprocessing function that takes an image\n        and page number as input and returns the processed image.\n    \"\"\"\n\n    def pre_process_image(image: Image.Image, page_number: int) -&gt; Image.Image:\n        \"\"\"\n        Preprocesses the image by masking the bottom region or performing other preprocessing steps.\n\n        Parameters:\n            image (Image.Image): The input image as a Pillow object.\n            page_number (int): The page number of the image (useful for conditional preprocessing).\n\n        Returns:\n            Image.Image: The preprocessed image.\n        \"\"\"\n\n        if page_number &gt; 0:  # don't apply mask to cover page.\n            # Make a copy of the image to avoid modifying the original\n            draw = ImageDraw.Draw(image)\n\n            # Get image dimensions\n            width, height = image.size\n\n            # Mask the bottom region based on the specified height proportion\n            mask_pixels = int(height * mask_height)\n            draw.rectangle([(0, height - mask_pixels), (width, height)], fill=\"black\")\n\n        return image\n\n    return pre_process_image\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.pil_to_bytes","title":"<code>pil_to_bytes(image, format='PNG')</code>","text":"<p>Converts a Pillow image to raw bytes.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The Pillow image object to convert.</p> required <code>format</code> <code>str</code> <p>The format to save the image as (e.g., \"PNG\", \"JPEG\"). Default is \"PNG\".</p> <code>'PNG'</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The raw bytes of the image.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def pil_to_bytes(image: Image.Image, format: str = \"PNG\") -&gt; bytes:\n    \"\"\"\n    Converts a Pillow image to raw bytes.\n\n    Parameters:\n        image (Image.Image): The Pillow image object to convert.\n        format (str): The format to save the image as (e.g., \"PNG\", \"JPEG\"). Default is \"PNG\".\n\n    Returns:\n        bytes: The raw bytes of the image.\n    \"\"\"\n    with io.BytesIO() as output:\n        image.save(output, format=format)\n        return output.getvalue()\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.process_page","title":"<code>process_page(page, client, annotation_font_path, preprocessor=None)</code>","text":"<p>Processes a single PDF page, extracting text, word locations, and annotated images.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The PDF page object.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>pre_processor</code> <code>Callable[[Image, int], Image]</code> <p>Preprocessing function for the image.</p> required <code>annotation_font_path</code> <code>str</code> <p>Path to the font file for annotations.</p> required <p>Returns:</p> Type Description <code>Tuple[str, List[EntityAnnotation], Image, Image, dict]</code> <p>Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]: - Full page text (str) - Word locations (List of vision.EntityAnnotation) - Annotated image (Pillow Image object) - Original unprocessed image (Pillow Image object) - Page dimensions (dict)</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def process_page(\n    page: fitz.Page,\n    client: vision.ImageAnnotatorClient,\n    annotation_font_path: str,\n    preprocessor: Callable[[Image.Image, int], Image.Image] = None,\n) -&gt; Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]:\n    \"\"\"\n    Processes a single PDF page, extracting text, word locations, and annotated images.\n\n    Parameters:\n        page (fitz.Page): The PDF page object.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        pre_processor (Callable[[Image.Image, int], Image.Image]): Preprocessing function for the image.\n        annotation_font_path (str): Path to the font file for annotations.\n\n    Returns:\n        Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]:\n            - Full page text (str)\n            - Word locations (List of vision.EntityAnnotation)\n            - Annotated image (Pillow Image object)\n            - Original unprocessed image (Pillow Image object)\n            - Page dimensions (dict)\n    \"\"\"\n    # Extract the original image from the PDF page\n    original_image = extract_image_from_page(page)\n\n    # Make a copy of the original image for processing\n    processed_image = original_image.copy()\n\n    # Apply the preprocessing function (if provided)\n    if preprocessor:\n        # print(\"preprocessing...\") # debug\n        processed_image = preprocessor(processed_image, page.number)\n        # processed_image.show() # debug\n\n    # Annotate the processed image using the Vision API\n    response = process_single_image(processed_image, client)\n\n    if response:\n        text_annotations = response.text_annotations\n        # Extract full text and word locations\n        full_page_text = text_annotations[0].description if text_annotations else \"\"\n        word_locations = text_annotations[1:] if len(text_annotations) &gt; 1 else []\n    else:\n        # return empty data\n        full_page_text = \"\"\n        word_locations = [EntityAnnotation()]\n        text_annotations = [\n            EntityAnnotation()\n        ]  # create empty data structures to allow storing to proceed.\n\n    # Create an annotated image with bounding boxes and labels\n    annotated_image = annotate_image_with_text(\n        processed_image, text_annotations, annotation_font_path\n    )\n\n    # Get page dimensions (from the original PDF page, not the image)\n    page_dimensions = get_page_dimensions(page)\n\n    return (\n        full_page_text,\n        word_locations,\n        annotated_image,\n        original_image,\n        page_dimensions,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.process_single_image","title":"<code>process_single_image(image, client, feature_type=DEFAULT_ANNOTATION_METHOD, language_hints=DEFAULT_ANNOTATION_LANGUAGE_HINTS)</code>","text":"<p>Processes a single image with the Google Vision API and returns text annotations.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The preprocessed Pillow image object.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>feature_type</code> <code>str</code> <p>Type of text detection to use ('TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION').</p> <code>DEFAULT_ANNOTATION_METHOD</code> <code>language_hints</code> <code>List</code> <p>Language hints for OCR.</p> <code>DEFAULT_ANNOTATION_LANGUAGE_HINTS</code> <p>Returns:</p> Type Description <code>List[EntityAnnotation]</code> <p>List[vision.EntityAnnotation]: Text annotations from the Vision API response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no text is detected.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def process_single_image(\n    image: Image.Image,\n    client: vision.ImageAnnotatorClient,\n    feature_type: str = DEFAULT_ANNOTATION_METHOD,\n    language_hints: List = DEFAULT_ANNOTATION_LANGUAGE_HINTS,\n) -&gt; List[vision.EntityAnnotation]:\n    \"\"\"\n    Processes a single image with the Google Vision API and returns text annotations.\n\n    Parameters:\n        image (Image.Image): The preprocessed Pillow image object.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        feature_type (str): Type of text detection to use ('TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION').\n        language_hints (List): Language hints for OCR.\n\n    Returns:\n        List[vision.EntityAnnotation]: Text annotations from the Vision API response.\n\n    Raises:\n        ValueError: If no text is detected.\n    \"\"\"\n    # Convert the Pillow image to bytes\n    image_bytes = pil_to_bytes(image, format=\"PNG\")\n\n    # Map feature type\n    feature_map = {\n        \"TEXT_DETECTION\": vision.Feature.Type.TEXT_DETECTION,\n        \"DOCUMENT_TEXT_DETECTION\": vision.Feature.Type.DOCUMENT_TEXT_DETECTION,\n    }\n    if feature_type not in feature_map:\n        raise ValueError(\n            f\"Invalid feature type '{feature_type}'. Use 'TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION'.\"\n        )\n\n    # Prepare Vision API request\n    vision_image = vision.Image(content=image_bytes)\n    features = [vision.Feature(type=feature_map[feature_type])]\n    image_context = vision.ImageContext(language_hints=language_hints)\n\n    # Make the API call\n    response = client.annotate_image(\n        {\"image\": vision_image, \"features\": features, \"image_context\": image_context}\n    )\n\n    return response\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.save_processed_pdf_data","title":"<code>save_processed_pdf_data(output_dir, journal_name, text_pages, word_locations, annotated_images, unannotated_images)</code>","text":"<p>Saves processed PDF data to files for later reloading.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>Directory to save the data (as a Path object).</p> required <code>base_name</code> <code>str</code> <p>Base name for the output directory (usually the PDF name without extension).</p> required <code>text_pages</code> <code>List[str]</code> <p>Extracted full-page text.</p> required <code>word_locations</code> <code>List[List[EntityAnnotation]]</code> <p>Word locations and annotations from Vision API.</p> required <code>annotated_images</code> <code>List[Image]</code> <p>Annotated images with bounding boxes.</p> required <code>unannotated_images</code> <code>List[Image]</code> <p>Raw unannotated images.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def save_processed_pdf_data(\n    output_dir: Path,\n    journal_name: str,\n    text_pages: List[str],\n    word_locations: List[List[EntityAnnotation]],\n    annotated_images: List[Image.Image],\n    unannotated_images: List[Image.Image],\n):\n    \"\"\"\n    Saves processed PDF data to files for later reloading.\n\n    Parameters:\n        output_dir (Path): Directory to save the data (as a Path object).\n        base_name (str): Base name for the output directory (usually the PDF name without extension).\n        text_pages (List[str]): Extracted full-page text.\n        word_locations (List[List[EntityAnnotation]]): Word locations and annotations from Vision API.\n        annotated_images (List[PIL.Image.Image]): Annotated images with bounding boxes.\n        unannotated_images (List[PIL.Image.Image]): Raw unannotated images.\n\n    Returns:\n        None\n    \"\"\"\n    # Create output directories\n    base_path = output_dir / journal_name / \"ocr_data\"\n    images_dir = base_path / \"images\"\n\n    base_path.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save text data\n    text_pages_file = base_path / \"text_pages.json\"\n    with text_pages_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(text_pages, f, indent=4, ensure_ascii=False)\n\n    # Save word locations as JSON\n    word_locations_file = base_path / \"word_locations.json\"\n    serialized_word_locations = serialize_entity_annotations_to_json(word_locations)\n    with word_locations_file.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(serialized_word_locations)\n\n    # Save images\n    for i, annotated_image in enumerate(annotated_images):\n        annotated_file = images_dir / f\"annotated_page_{i + 1}.png\"\n        annotated_image.save(annotated_file)\n    for i, unannotated_image in enumerate(unannotated_images):\n        unannotated_file = images_dir / f\"unannotated_page_{i + 1}.png\"\n        unannotated_image.save(unannotated_file)\n\n    # Save metadata\n    metadata = {\n        \"source_pdf\": journal_name,\n        \"page_count\": len(text_pages),\n        \"images_directory\": str(\n            images_dir\n        ),  # Convert Path to string for JSON serialization\n        \"files\": {\n            \"text_pages\": \"text_pages.json\",\n            \"word_locations\": \"word_locations.json\",\n        },\n    }\n    metadata_file = base_path / \"metadata.json\"\n    with metadata_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(metadata, f, indent=4)\n\n    print(f\"Processed data saved in: {base_path}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.serialize_entity_annotations_to_json","title":"<code>serialize_entity_annotations_to_json(annotations)</code>","text":"<p>Serializes a nested list of EntityAnnotation objects into a JSON-compatible format using Base64 encoding.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>List[List[EntityAnnotation]]</code> <p>The nested list of EntityAnnotation objects.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The serialized data in JSON format as a string.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def serialize_entity_annotations_to_json(\n    annotations: List[List[EntityAnnotation]],\n) -&gt; str:\n    \"\"\"\n    Serializes a nested list of EntityAnnotation objects into a JSON-compatible format using Base64 encoding.\n\n    Parameters:\n        annotations (List[List[EntityAnnotation]]): The nested list of EntityAnnotation objects.\n\n    Returns:\n        str: The serialized data in JSON format as a string.\n    \"\"\"\n    serialized_data = []\n    for page_annotations in annotations:\n        serialized_page = [\n            base64.b64encode(annotation.SerializeToString()).decode(\"utf-8\")\n            for annotation in page_annotations\n        ]\n        serialized_data.append(serialized_page)\n\n    # Convert to a JSON string\n    return json.dumps(serialized_data, indent=4)\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.start_image_annotator_client","title":"<code>start_image_annotator_client(credentials_file=None, api_endpoint='vision.googleapis.com', timeout=(10, 30), enable_logging=False)</code>","text":"<p>Starts and returns a Google Vision API ImageAnnotatorClient with optional configuration.</p> <p>Parameters:</p> Name Type Description Default <code>credentials_file</code> <code>str</code> <p>Path to the credentials JSON file. If None, uses the default environment variable.</p> <code>None</code> <code>api_endpoint</code> <code>str</code> <p>Custom API endpoint for the Vision API. Default is the global endpoint.</p> <code>'vision.googleapis.com'</code> <code>timeout</code> <code>Tuple[int, int]</code> <p>Connection and read timeouts in seconds. Default is (10, 30).</p> <code>(10, 30)</code> <code>enable_logging</code> <code>bool</code> <p>Enable detailed logging for debugging. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ImageAnnotatorClient</code> <p>vision.ImageAnnotatorClient: Configured Vision API client.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified credentials file is not found.</p> <code>Exception</code> <p>For unexpected errors during client setup.</p> Example <p>client = start_image_annotator_client(     credentials_file=\"/path/to/credentials.json\",     api_endpoint=\"vision.googleapis.com\",     timeout=(10, 30),     enable_logging=True ) print(\"Google Vision API client initialized.\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def start_image_annotator_client(\n    credentials_file: str = None,\n    api_endpoint: str = \"vision.googleapis.com\",\n    timeout: Tuple[int, int] = (10, 30),\n    enable_logging: bool = False,\n) -&gt; vision.ImageAnnotatorClient:\n    \"\"\"\n    Starts and returns a Google Vision API ImageAnnotatorClient with optional configuration.\n\n    Parameters:\n        credentials_file (str): Path to the credentials JSON file. If None, uses the default environment variable.\n        api_endpoint (str): Custom API endpoint for the Vision API. Default is the global endpoint.\n        timeout (Tuple[int, int]): Connection and read timeouts in seconds. Default is (10, 30).\n        enable_logging (bool): Enable detailed logging for debugging. Default is False.\n\n    Returns:\n        vision.ImageAnnotatorClient: Configured Vision API client.\n\n    Raises:\n        FileNotFoundError: If the specified credentials file is not found.\n        Exception: For unexpected errors during client setup.\n\n    Example:\n        &gt;&gt;&gt; client = start_image_annotator_client(\n        &gt;&gt;&gt;     credentials_file=\"/path/to/credentials.json\",\n        &gt;&gt;&gt;     api_endpoint=\"vision.googleapis.com\",\n        &gt;&gt;&gt;     timeout=(10, 30),\n        &gt;&gt;&gt;     enable_logging=True\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; print(\"Google Vision API client initialized.\")\n    \"\"\"\n    try:\n        # Set up credentials\n        if credentials_file:\n            if not os.path.exists(credentials_file):\n                raise FileNotFoundError(\n                    f\"Credentials file '{credentials_file}' not found.\"\n                )\n            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_file\n\n        # Configure client options\n        client_options = {\"api_endpoint\": api_endpoint}\n        client = vision.ImageAnnotatorClient(client_options=client_options)\n\n        # Optionally enable logging\n        if enable_logging:\n            print(f\"Vision API Client started with endpoint: {api_endpoint}\")\n            print(f\"Timeout settings: Connect={timeout[0]}s, Read={timeout[1]}s\")\n\n        return client\n\n    except Exception as e:\n        raise Exception(f\"Failed to initialize ImageAnnotatorClient: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface","title":"<code>openai_interface</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface","title":"<code>openai_interface</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface.DEBUG_DISPLAY_BUFFER","title":"<code>DEBUG_DISPLAY_BUFFER = 1000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface.DEFAULT_MAX_BATCH_RETRY","title":"<code>DEFAULT_MAX_BATCH_RETRY = 60</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface.DEFAULT_MODEL_SETTINGS","title":"<code>DEFAULT_MODEL_SETTINGS = {'gpt-4o': {'max_tokens': 16000, 'context_limit': 128000, 'temperature': 1.0}, 'gpt-3.5-turbo': {'max_tokens': 4096, 'context_limit': 16384, 'temperature': 1.0}, 'gpt-4o-mini': {'max_tokens': 16000, 'context_limit': 128000, 'temperature': 1.0}}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface.MAX_BATCH_LIST","title":"<code>MAX_BATCH_LIST = 30</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface.OPEN_AI_DEFAULT_MODEL","title":"<code>OPEN_AI_DEFAULT_MODEL = 'gpt-4o'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface.open_ai_encoding","title":"<code>open_ai_encoding = tiktoken.encoding_for_model(OPEN_AI_DEFAULT_MODEL)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface.open_ai_model_settings","title":"<code>open_ai_model_settings = DEFAULT_MODEL_SETTINGS</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface.ClientNotInitializedError","title":"<code>ClientNotInitializedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the OpenAI client is not initialized.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>class ClientNotInitializedError(Exception):\n    \"\"\"Exception raised when the OpenAI client is not initialized.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.OpenAIClient","title":"<code>OpenAIClient</code>","text":"<p>Singleton class for managing the OpenAI client.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>class OpenAIClient:\n    \"\"\"Singleton class for managing the OpenAI client.\"\"\"\n\n    _instance = None\n\n    def __init__(self, api_key: str):\n        \"\"\"Initialize the OpenAI client.\"\"\"\n        self.client = OpenAI(api_key=api_key)\n\n    @classmethod\n    def get_instance(cls):\n        \"\"\"\n        Get or initialize the OpenAI client.\n\n        Returns:\n            OpenAI: The singleton OpenAI client instance.\n        \"\"\"\n        if cls._instance is None:\n            # Load the .env file\n            load_dotenv()\n\n            if api_key := os.getenv(\"OPENAI_API_KEY\"):\n                # Initialize the singleton instance\n                cls._instance = cls(api_key)\n            else:\n                raise ValueError(\n                    \"API key not found. Set it in the .env file with the key 'OPENAI_API_KEY'.\"\n                )\n\n        return cls._instance.client\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.OpenAIClient.client","title":"<code>client = OpenAI(api_key=api_key)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.openai_interface.OpenAIClient.__init__","title":"<code>__init__(api_key)</code>","text":"<p>Initialize the OpenAI client.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def __init__(self, api_key: str):\n    \"\"\"Initialize the OpenAI client.\"\"\"\n    self.client = OpenAI(api_key=api_key)\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.OpenAIClient.get_instance","title":"<code>get_instance()</code>  <code>classmethod</code>","text":"<p>Get or initialize the OpenAI client.</p> <p>Returns:</p> Name Type Description <code>OpenAI</code> <p>The singleton OpenAI client instance.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>@classmethod\ndef get_instance(cls):\n    \"\"\"\n    Get or initialize the OpenAI client.\n\n    Returns:\n        OpenAI: The singleton OpenAI client instance.\n    \"\"\"\n    if cls._instance is None:\n        # Load the .env file\n        load_dotenv()\n\n        if api_key := os.getenv(\"OPENAI_API_KEY\"):\n            # Initialize the singleton instance\n            cls._instance = cls(api_key)\n        else:\n            raise ValueError(\n                \"API key not found. Set it in the .env file with the key 'OPENAI_API_KEY'.\"\n            )\n\n    return cls._instance.client\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.create_jsonl_file_for_batch","title":"<code>create_jsonl_file_for_batch(messages, output_file_path=None, max_token_list=None, model=OPEN_AI_DEFAULT_MODEL, tools=None, json_mode=False)</code>","text":"<p>Creates a JSONL file for batch processing, with each request using the same system message, user messages, and optional function schema for function calling.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[str]</code> <p>List of message objects to be sent for completion.</p> required <code>output_file_path</code> <code>str</code> <p>The path where the .jsonl file will be saved.</p> <code>None</code> <code>model</code> <code>str</code> <p>The model to use (default is set globally).</p> <code>OPEN_AI_DEFAULT_MODEL</code> <code>functions</code> <code>list</code> <p>List of function schemas to enable function calling.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The path to the generated .jsonl file.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def create_jsonl_file_for_batch(\n    messages: List[str],\n    output_file_path: Optional[Path] = None,\n    max_token_list: Optional[List[int]] = None,\n    model: str = OPEN_AI_DEFAULT_MODEL,\n    tools=None,\n    json_mode: Optional[bool] = False,\n):\n    \"\"\"\n    Creates a JSONL file for batch processing, with each request using the same system message, user messages,\n    and optional function schema for function calling.\n\n    Args:\n        messages: List of message objects to be sent for completion.\n        output_file_path (str): The path where the .jsonl file will be saved.\n        model (str): The model to use (default is set globally).\n        functions (list, optional): List of function schemas to enable function calling.\n\n    Returns:\n        str: The path to the generated .jsonl file.\n    \"\"\"\n    global open_ai_model_settings\n\n    total_tokens = 0\n\n    if not max_token_list:\n        max_tokens = open_ai_model_settings[model][\"max_tokens\"]\n        message_count = len(messages)\n        max_token_list = [max_tokens] * message_count\n\n    temperature = open_ai_model_settings[model][\"temperature\"]\n    total_tokens = sum(max_token_list)\n\n    if output_file_path is None:\n        date_str = datetime.now().strftime(\"%m%d%Y\")\n        output_file_path = Path(f\"batch_requests_{date_str}.jsonl\")\n\n    # Ensure the directory for the output file exists\n    output_dir = Path(output_file_path).parent\n    if not output_dir.exists():\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n    requests = []\n    for i, message in enumerate(messages):\n\n        # get max_tokens\n        max_tokens = max_token_list[i]\n\n        request_obj = {\n            \"custom_id\": f\"request-{i+1}\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": model,\n                \"messages\": message,\n                \"max_tokens\": max_tokens,\n                \"temperature\": temperature,\n            },\n        }\n        if json_mode:\n            request_obj[\"body\"][\"response_format\"] = {\"type\": \"json_object\"}\n        if tools:\n            request_obj[\"body\"][\"tools\"] = tools\n\n        if i == 0:  # log first iteration only.\n            _log_batch_creation_info(output_file_path, request_obj, total_tokens)\n\n        requests.append(request_obj)\n\n    # Write requests to JSONL file\n    with open(output_file_path, \"w\") as f:\n        for request in requests:\n            json.dump(request, f)\n            f.write(\"\\n\")\n\n    logger.info(f\"JSONL file created at: {output_file_path}\")\n    return output_file_path\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.delete_api_files","title":"<code>delete_api_files(cutoff_date)</code>","text":"<p>Delete all files on OpenAI's storage older than a given date at midnight.</p> <p>Parameters: - cutoff_date (datetime): The cutoff date. Files older than this date will be deleted.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def delete_api_files(cutoff_date: datetime):\n    \"\"\"\n    Delete all files on OpenAI's storage older than a given date at midnight.\n\n    Parameters:\n    - cutoff_date (datetime): The cutoff date. Files older than this date will be deleted.\n    \"\"\"\n    # Set the OpenAI API key\n    client = get_api_client()\n\n    # Get a list of all files\n    files = client.files.list()\n\n    for file in files.data:\n        # Parse the file creation timestamp\n        file_created_at = datetime.fromtimestamp(file.created_at)\n        # Check if the file is older than the cutoff date\n        if file_created_at &lt; cutoff_date:\n            try:\n                # Delete the file\n                client.files.delete(file.id)\n                print(f\"Deleted file: {file.id} (created on {file_created_at})\")\n            except Exception as e:\n                logger.error(f\"Failed to delete file {file.id}: {e}\", exc_info=True)\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.generate_messages","title":"<code>generate_messages(system_message, user_message_wrapper, data_list_to_process, log_system_message=True)</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def generate_messages(\n    system_message: str,\n    user_message_wrapper: callable,\n    data_list_to_process: List,\n    log_system_message=True,\n):\n    messages = []\n    for data_element in data_list_to_process:\n        message_block = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\n                \"role\": \"user\",\n                \"content\": user_message_wrapper(data_element),\n            },\n        ]\n        messages.append(message_block)\n    return messages\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.get_active_batches","title":"<code>get_active_batches()</code>","text":"<p>Retrieve the list of active batches using the OpenAI API.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def get_active_batches() -&gt; List[Dict]:\n    \"\"\"\n    Retrieve the list of active batches using the OpenAI API.\n    \"\"\"\n    client = get_api_client()\n\n    try:\n        batches = client.batches.list(limit=MAX_BATCH_LIST)\n        batch_list = []\n        for batch in batches:\n            if batch.status == \"in_progress\":\n                batch_info = {\n                    \"id\": batch.id,\n                    \"status\": batch.status,\n                    \"created_at\": batch.created_at,\n                    # Add other relevant attributes as needed\n                }\n                batch_list.append(batch_info)\n        return batch_list\n    except Exception as e:\n        logger.error(f\"Error fetching active batches: {e}\")\n        return []\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.get_all_batch_info","title":"<code>get_all_batch_info()</code>","text":"<p>Retrieve the list of batches up to MAX_BATCH_LIST using the OpenAI API.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def get_all_batch_info():\n    \"\"\"\n    Retrieve the list of batches up to MAX_BATCH_LIST using the OpenAI API.\n    \"\"\"\n    client = get_api_client()\n\n    try:\n        batches = client.batches.list(limit=MAX_BATCH_LIST)\n        batch_list = []\n        for batch in batches:\n            batch_info = {\n                \"id\": batch.id,\n                \"status\": batch.status,\n                \"created_at\": batch.created_at,\n                \"output_file_id\": batch.output_file_id,\n                \"metadata\": batch.metadata,\n                # Add other relevant attributes as needed\n            }\n            batch_list.append(batch_info)\n        return batch_list\n    except Exception as e:\n        logger.error(f\"Error fetching active batches: {e}\", exc_info=True)\n        return []\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.get_api_client","title":"<code>get_api_client()</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def get_api_client():\n    return OpenAIClient.get_instance()\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.get_batch_response","title":"<code>get_batch_response(batch_id)</code>","text":"<p>Retrieves the status of a batch job and returns the result if completed. Parses the JSON result file, collects the output messages, and returns them as a Python list.</p> <p>Args: - batch_id : The batch_id string to retrieve status and results for.</p> <p>Returns: - If completed: A list containing the message content for each response of the batch process. - If not completed: A string with the batch status.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def get_batch_response(batch_id: str) -&gt; List[str]:\n    \"\"\"\n    Retrieves the status of a batch job and returns the result if completed.\n    Parses the JSON result file, collects the output messages,\n    and returns them as a Python list.\n\n    Args:\n    - batch_id : The batch_id string to retrieve status and results for.\n\n    Returns:\n    - If completed: A list containing the message content for each response of the batch process.\n    - If not completed: A string with the batch status.\n    \"\"\"\n    client = get_api_client()\n\n    # Check the batch status\n    batch_status = client.batches.retrieve(batch_id)\n    if batch_status.status != \"completed\":\n        logger.info(f\"Batch status for {batch_id}: {batch_status.status}\")\n        return batch_status.status\n\n    # Retrieve the output file contents\n    file_id = batch_status.output_file_id\n    file_response = client.files.content(file_id)\n\n    # Parse the JSON lines in the output file\n    results = []\n    for line in file_response.text.splitlines():\n        data = json.loads(line)  # Parse each line as JSON\n        if response_body := data.get(\"response\", {}).get(\"body\", {}):\n            content = response_body[\"choices\"][0][\"message\"][\"content\"]\n            results.append(content)\n\n    return results\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.get_batch_status","title":"<code>get_batch_status(batch_id)</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def get_batch_status(batch_id):\n    client = get_api_client()\n\n    batch = client.batches.retrieve(batch_id)\n    return batch.status\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.get_completed_batches","title":"<code>get_completed_batches()</code>","text":"<p>Retrieve the list of active batches using the OpenAI API.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def get_completed_batches() -&gt; List[Dict]:\n    \"\"\"\n    Retrieve the list of active batches using the OpenAI API.\n    \"\"\"\n    client = get_api_client()\n\n    try:\n        batches = client.batches.list(limit=MAX_BATCH_LIST)\n        batch_list = []\n        for batch in batches:\n            if batch.status == \"completed\":\n                batch_info = {\n                    \"id\": batch.id,\n                    \"status\": batch.status,\n                    \"created_at\": batch.created_at,\n                    \"output_file_id\": batch.output_file_id,\n                    \"metadata\": batch.metadata,\n                    # Add other relevant attributes as needed\n                }\n                batch_list.append(batch_info)\n        return batch_list\n    except Exception as e:\n        logger.error(f\"Error fetching active batches: {e}\", exc_info=True)\n        return []\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.get_completion_content","title":"<code>get_completion_content(chat_completion)</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def get_completion_content(chat_completion):\n    return chat_completion.choices[0].message.content\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.get_completion_object","title":"<code>get_completion_object(chat_completion)</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def get_completion_object(chat_completion):\n    return chat_completion.choices[0].message.parsed\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.get_last_batch_response","title":"<code>get_last_batch_response(n=0)</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def get_last_batch_response(n: int = 0):\n    assert n &lt; MAX_BATCH_LIST\n    completed = get_completed_batches()\n    return get_batch_response(completed[n][\"id\"])\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.get_model_settings","title":"<code>get_model_settings(model, parameter)</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def get_model_settings(model, parameter):\n    return open_ai_model_settings[model][parameter]\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.poll_batch_for_response","title":"<code>poll_batch_for_response(batch_id, interval=10, timeout=3600, backoff_factor=1.3, max_interval=600)</code>","text":"<p>Poll the batch status until it completes, fails, or expires.</p> <p>Parameters:</p> Name Type Description Default <code>batch_id</code> <code>str</code> <p>The ID of the batch to poll.</p> required <code>interval</code> <code>int</code> <p>Initial time (in seconds) to wait between polls. Default is 10 seconds.</p> <code>10</code> <code>timeout</code> <code>int</code> <p>Maximum duration (in seconds) to poll before timing out. Use 1 hour as default.</p> <code>3600</code> <code>backoff_factor</code> <code>int</code> <p>Factor by which the interval increases after each poll.</p> <code>1.3</code> <code>max_interval</code> <code>int</code> <p>Maximum polling interval in seconds.</p> <code>600</code> <p>Returns:</p> Name Type Description <code>list</code> <code>bool | list</code> <p>The batch response if successful.</p> <code>bool</code> <code>bool | list</code> <p>Returns False if the batch fails, times out, or expires.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the batch ID is not found or if an unexpected error occurs.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def poll_batch_for_response(\n    batch_id: str,\n    interval: int = 10,\n    timeout: int = 3600,\n    backoff_factor: float = 1.3,\n    max_interval: int = 600,\n) -&gt; bool | list:\n    \"\"\"\n    Poll the batch status until it completes, fails, or expires.\n\n    Args:\n        batch_id (str): The ID of the batch to poll.\n        interval (int): Initial time (in seconds) to wait between polls. Default is 10 seconds.\n        timeout (int): Maximum duration (in seconds) to poll before timing out. Use 1 hour as default.\n        backoff_factor (int): Factor by which the interval increases after each poll.\n        max_interval (int): Maximum polling interval in seconds.\n\n    Returns:\n        list: The batch response if successful.\n        bool: Returns False if the batch fails, times out, or expires.\n\n    Raises:\n        RuntimeError: If the batch ID is not found or if an unexpected error occurs.\n    \"\"\"\n    start_time = time.time()\n    logger.info(f\"Polling batch status for batch ID {batch_id} ...\")\n\n    attempts = 0\n    while True:\n        try:\n            time.sleep(interval)\n            elapsed_time = time.time() - start_time\n\n            # Check for timeout\n            if elapsed_time &gt; timeout:\n                logger.error(\n                    f\"Polling timed out after {timeout} seconds for batch ID {batch_id}.\"\n                )\n                return False\n\n            # Get batch status\n            batch_status = get_batch_status(batch_id)\n            logger.debug(f\"Batch ID {batch_id} status: {batch_status}\")\n\n            if not batch_status:\n                raise RuntimeError(\n                    f\"Batch ID {batch_id} not found or invalid response from `get_batch_status`.\"\n                )\n\n            # Handle completed batch\n            if batch_status == \"completed\":\n                logger.info(\n                    f\"Batch processing for ID {batch_id} completed successfully.\"\n                )\n                try:\n                    return get_batch_response(batch_id)\n                except Exception as e:\n                    logger.error(\n                        f\"Error retrieving response for batch ID {batch_id}: {e}\",\n                        exc_info=True,\n                    )\n                    raise RuntimeError(\n                        f\"Failed to retrieve response for batch ID {batch_id}.\"\n                    ) from e\n\n            # Handle failed batch\n            elif batch_status == \"failed\":\n                logger.error(f\"Batch processing for ID {batch_id} failed.\")\n                return False\n\n            # Log ongoing status and adjust interval\n            logger.info(\n                f\"Batch status: {batch_status}. Retrying in {interval} seconds...\"\n            )\n            attempts += 1\n            interval = min(floor(interval * backoff_factor), max_interval)\n\n        except Exception as e:\n            logger.error(\n                f\"Unexpected error while polling batch ID {batch_id}: {e}\",\n                exc_info=True,\n            )\n            raise RuntimeError(f\"Error during polling for batch ID {batch_id}.\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.run_immediate_chat_process","title":"<code>run_immediate_chat_process(messages, max_tokens=0, response_format=None, model=OPEN_AI_DEFAULT_MODEL)</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def run_immediate_chat_process(\n    messages, max_tokens: int = 0, response_format=None, model=OPEN_AI_DEFAULT_MODEL\n):\n    client = get_api_client()\n\n    max_model_tokens = open_ai_model_settings[model][\"max_tokens\"]\n    if max_tokens == 0:\n        max_tokens = max_model_tokens\n\n    if max_tokens &gt; max_model_tokens:\n        logger.warning(\n            \"Maximum token request exceeded: {max_tokens} for model: {model}\"\n        )\n        logger.warning(f\"Setting max_tokens to model maximum: {max_model_tokens}\")\n        max_tokens = max_model_tokens\n\n    try:\n        return (\n            client.beta.chat.completions.parse(\n                messages=messages,\n                model=model,\n                response_format=response_format,\n                max_completion_tokens=max_tokens,\n            )\n            if response_format\n            else client.chat.completions.create(\n                messages=messages,\n                model=model,\n                max_completion_tokens=max_tokens,\n            )\n        )\n    except Exception as e:\n        logger.error(f\"Error running immediate chat: {e}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.run_immediate_completion_simple","title":"<code>run_immediate_completion_simple(system_message, user_message, model=None, max_tokens=0, response_format=None)</code>","text":"<p>Runs a single chat completion with a system message and user message.</p> <p>This function simplifies the process of running a single chat completion with the OpenAI API by handling model selection, token limits, and logging. It allows for specifying a response format and handles potential <code>ValueError</code> exceptions during the API call.</p> <p>Parameters:</p> Name Type Description Default <code>system_message</code> <code>str</code> <p>The system message to guide the conversation.</p> required <code>user_message</code> <code>str</code> <p>The user's message as input for the chat completion.</p> required <code>model</code> <code>str</code> <p>The OpenAI model to use. Defaults to None, which uses the default model.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens for the completion. Defaults to 0, which uses the model's maximum.</p> <code>0</code> <code>response_format</code> <code>dict</code> <p>The desired response format. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>OpenAIObject | None: The chat completion response if successful, or None if a <code>ValueError</code> occurs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if max_tokens exceeds the model's maximum token limit.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def run_immediate_completion_simple(\n    system_message: str,\n    user_message: str,\n    model=None,\n    max_tokens: int = 0,\n    response_format=None,\n):\n    \"\"\"Runs a single chat completion with a system message and user message.\n\n    This function simplifies the process of running a single chat completion with the OpenAI API by handling\n    model selection, token limits, and logging. It allows for specifying a response format and handles potential\n    `ValueError` exceptions during the API call.\n\n    Args:\n        system_message (str): The system message to guide the conversation.\n        user_message (str): The user's message as input for the chat completion.\n        model (str, optional): The OpenAI model to use. Defaults to None, which uses the default model.\n        max_tokens (int, optional): The maximum number of tokens for the completion. Defaults to 0, which uses the model's maximum.\n        response_format (dict, optional): The desired response format. Defaults to None.\n\n    Returns:\n        OpenAIObject | None: The chat completion response if successful, or None if a `ValueError` occurs.\n\n    Raises:\n        ValueError: if max_tokens exceeds the model's maximum token limit.\n    \"\"\"\n\n    client = get_api_client()\n\n    if not model:\n        model = OPEN_AI_DEFAULT_MODEL\n\n    max_model_tokens = open_ai_model_settings[model][\"max_tokens\"]\n    if max_tokens == 0:\n        max_tokens = max_model_tokens\n\n    if max_tokens &gt; max_model_tokens:\n        logger.warning(\n            \"Maximum token request exceeded: {max_tokens} for model: {model}\"\n        )\n        logger.warning(f\"Setting max_tokens to model maximum: {max_model_tokens}\")\n        max_tokens = max_model_tokens\n\n    logger.debug(f\"User message content:\\n{user_message[:DEBUG_DISPLAY_BUFFER]} ...\")\n    message_block = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": user_message},\n    ]\n\n    try:\n        logger.debug(\n            f\"Starting chat completion with response_format={response_format} and max_tokens={max_tokens}...\"\n        )\n\n        return (\n            client.beta.chat.completions.parse(\n                messages=message_block,  # type: ignore\n                model=model,\n                response_format=response_format,\n                max_completion_tokens=max_tokens,\n            )\n            if response_format\n            else client.chat.completions.create(\n                messages=message_block,  # type: ignore\n                model=model,\n                max_completion_tokens=max_tokens,\n            )\n        )\n    except ValueError as e:\n        logger.error(f\"Value Error running immediate chat: {e}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.run_single_batch","title":"<code>run_single_batch(user_prompts, system_message, user_wrap_function=None, max_token_list=None, description='')</code>","text":"<p>Generate a batch file for the OpenAI (OA) API and send it.</p> <p>Parameters:</p> Name Type Description Default <code>system_message</code> <code>str</code> <p>System message template for batch processing.</p> required <code>user_wrap_function</code> <code>callable</code> <p>Function to wrap user input for processing pages.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>List[str]</code> <p>Path to the created batch file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during file processing.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def run_single_batch(\n    user_prompts: List,\n    system_message: str,\n    user_wrap_function: callable = None,\n    max_token_list: List[int] = None,\n    description=\"\",\n) -&gt; List[str]:\n    \"\"\"\n    Generate a batch file for the OpenAI (OA) API and send it.\n\n    Parameters:\n        system_message (str): System message template for batch processing.\n        user_wrap_function (callable): Function to wrap user input for processing pages.\n\n    Returns:\n        str: Path to the created batch file.\n\n    Raises:\n        Exception: If an error occurs during file processing.\n    \"\"\"\n\n    if max_token_list is None:\n        max_token_list = []\n    try:\n        if not user_wrap_function:\n            user_wrap_function = lambda x: x\n\n        # Generate messages for the pages\n        batch_message_seq = generate_messages(\n            system_message, user_wrap_function, user_prompts\n        )\n\n        batch_file = Path(\"./temp_batch_run.jsonl\")\n\n        # Save the batch file\n        create_jsonl_file_for_batch(\n            batch_message_seq, batch_file, max_token_list=max_token_list\n        )\n        # logger.info(f\"Batch file created successfully: {output_file}\")\n\n    except Exception as e:\n        logger.error(f\"Error while creating immediate batch file {batch_file}: {e}\")\n        raise\n\n    try:\n\n        if not description:\n            description = (\n                f\"Single batch process: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n            )\n        response_list = start_batch_with_retries(batch_file, description=description)\n\n    except Exception as e:\n        logger.error(f\"Failed to complete batch process: {e}\", exc_info=True)\n        raise\n\n    return response_list\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.run_transcription_speech","title":"<code>run_transcription_speech(audio_file, model=OPEN_AI_DEFAULT_MODEL, response_format='verbose_json', prompt='', mode='transcribe')</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def run_transcription_speech(\n    audio_file: Path,\n    model: str = OPEN_AI_DEFAULT_MODEL,\n    response_format=\"verbose_json\",\n    prompt=\"\",\n    mode: str = \"transcribe\",\n):  # mode can be \"transcribe\" or \"translate\"\n\n    client = get_api_client()\n\n    with audio_file.open(\"rb\") as file:\n        if mode == \"transcribe\":\n            transcript = client.audio.transcriptions.create(\n                model=model, response_format=response_format, prompt=prompt, file=file\n            )\n        elif mode == \"translate\":\n            transcript = client.audio.translations.create(\n                model=model, response_format=response_format, prompt=prompt, file=file\n            )\n        else:\n            logger.error(f\"Invalid mode: {mode}, in speech transcription generation.\")\n            raise ValueError(f\"'translate' or 'transcribe' expected, not {mode}.\")\n\n    return transcript\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.set_model_settings","title":"<code>set_model_settings(model_settings_dict)</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def set_model_settings(model_settings_dict):\n    global open_ai_model_settings\n    open_ai_model_settings = model_settings_dict\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.start_batch","title":"<code>start_batch(jsonl_file, description='')</code>","text":"<p>Starts a batch process using OpenAI's client with an optional description and JSONL batch file.</p> <p>Parameters:</p> Name Type Description Default <code>jsonl_file</code> <code>Path</code> <p>Path to the .jsonl batch file to be used as input. Must be a pathlib.Path object.</p> required <code>description</code> <code>str</code> <p>A description for metadata to label the batch job.                          If None, a default description is generated with the                          current date-time and file name.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the batch object if successful, or an error message if failed.</p> Example <p>jsonl_file = Path(\"batch_requests.jsonl\") start_batch(jsonl_file)</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def start_batch(jsonl_file: Path, description=\"\"):\n    \"\"\"\n    Starts a batch process using OpenAI's client with an optional description and JSONL batch file.\n\n    Args:\n        jsonl_file (Path): Path to the .jsonl batch file to be used as input. Must be a pathlib.Path object.\n        description (str, optional): A description for metadata to label the batch job.\n                                     If None, a default description is generated with the\n                                     current date-time and file name.\n\n    Returns:\n        dict: A dictionary containing the batch object if successful, or an error message if failed.\n\n    Example:\n        jsonl_file = Path(\"batch_requests.jsonl\")\n        start_batch(jsonl_file)\n    \"\"\"\n    client = get_api_client()\n\n    if not isinstance(jsonl_file, Path):\n        raise TypeError(\"The 'jsonl_file' argument must be a pathlib.Path object.\")\n\n    if not jsonl_file.exists():\n        raise FileNotFoundError(f\"The file {jsonl_file} does not exist.\")\n\n    basename = jsonl_file.stem\n\n    # Generate description:\n    current_time = datetime.now().astimezone().strftime(\"%m-%d-%Y %H:%M:%S %Z\")\n    description = f\"{current_time} | {jsonl_file.name} | {description}\"\n\n    try:\n        # Attempt to create the input file for the batch process\n        with jsonl_file.open(\"rb\") as file:\n            batch_input_file = client.files.create(file=file, purpose=\"batch\")\n        batch_input_file_id = batch_input_file.id\n    except Exception as e:\n        return {\"error\": f\"File upload failed: {e}\"}\n\n    try:\n        # Attempt to create the batch with specified input file and metadata description\n        batch = client.batches.create(\n            input_file_id=batch_input_file_id,\n            endpoint=\"/v1/chat/completions\",\n            completion_window=\"24h\",\n            metadata={\"description\": description, \"basename\": basename},\n        )\n\n        # log the batch\n        _log_batch_start_info(batch, description)\n        return batch\n\n    except Exception as e:\n        return {\"error\": f\"Batch creation failed: {e}\"}\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.start_batch_with_retries","title":"<code>start_batch_with_retries(jsonl_file, description='', max_retries=DEFAULT_MAX_BATCH_RETRY, retry_delay=5, poll_interval=10, timeout=3600)</code>","text":"<p>Starts a batch with retries and polls for its completion.</p> <p>Parameters:</p> Name Type Description Default <code>jsonl_file</code> <code>Path</code> <p>Path to the JSONL file for batch input.</p> required <code>description</code> <code>str</code> <p>A description for the batch job (optional).</p> <code>''</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries to start and complete the batch (default: 3).</p> <code>DEFAULT_MAX_BATCH_RETRY</code> <code>retry_delay</code> <code>int</code> <p>Delay in seconds between retries (default: 60).</p> <code>5</code> <code>poll_interval</code> <code>int</code> <p>Interval in seconds for polling batch status (default: 10).</p> <code>10</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds for polling (default: 23 hours).</p> <code>3600</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>The batch response if completed successfully.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the batch fails after all retries or encounters an error.</p> Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def start_batch_with_retries(\n    jsonl_file: Path,\n    description: str = \"\",\n    max_retries: int = DEFAULT_MAX_BATCH_RETRY,\n    retry_delay: int = 5,\n    poll_interval: int = 10,\n    timeout: int = 3600,\n) -&gt; list[str]:\n    \"\"\"\n    Starts a batch with retries and polls for its completion.\n\n    Args:\n        jsonl_file (Path): Path to the JSONL file for batch input.\n        description (str): A description for the batch job (optional).\n        max_retries (int): Maximum number of retries to start and complete the batch (default: 3).\n        retry_delay (int): Delay in seconds between retries (default: 60).\n        poll_interval (int): Interval in seconds for polling batch status (default: 10).\n        timeout (int): Timeout in seconds for polling (default: 23 hours).\n\n    Returns:\n        list: The batch response if completed successfully.\n\n    Raises:\n        RuntimeError: If the batch fails after all retries or encounters an error.\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            # Start the batch\n            batch = start_batch(jsonl_file, description=description)\n            if not batch or \"error\" in batch:\n                raise RuntimeError(\n                    f\"Failed to start batch: {batch.get('error', 'Unknown error')}\"\n                )\n\n            batch_id = batch.id\n            if not batch_id:\n                raise RuntimeError(\"Batch started but no ID was returned.\")\n\n            logger.info(\n                f\"Batch started: attempt {attempt + 1}.\",\n                extra={\"batch_id\": batch_id, \"description\": description},\n            )\n\n            # Poll for batch completion\n            response_list = poll_batch_for_response(\n                batch_id, interval=poll_interval, timeout=timeout\n            )\n\n            # Check for a response\n            if response_list:\n                logger.info(\n                    f\"Batch completed successfully after {attempt + 1} attempts.\",\n                    extra={\"batch_id\": batch_id, \"description\": description},\n                )\n                break  # exit for loop\n\n            else:  # No response means batch failed. Retry.\n                logger.error(\n                    f\"Attempt {attempt + 1} failed. Retrying batch process in {retry_delay} seconds...\",\n                    extra={\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"description\": description,\n                    },\n                )\n                time.sleep(retry_delay)\n\n        except Exception as e:\n            logger.error(\n                f\"Batch start and polling failed on attempt {attempt + 1}: {e}\",\n                exc_info=True,\n                extra={\"attempt\": attempt + 1, \"description\": description},\n            )\n            time.sleep(retry_delay)\n\n    else:  # else the loop completed before succesful result\n        logger.error(\n            f\"Failed to complete batch after {max_retries} retries.\",\n            extra={\"description\": description},\n        )\n        raise RuntimeError(\n            f\"Error: Failed to complete batch after {max_retries} retries.\"\n        )\n\n    return response_list\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.token_count","title":"<code>token_count(text)</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def token_count(text):\n    return len(open_ai_encoding.encode(text))\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.openai_interface.token_count_file","title":"<code>token_count_file(text_file)</code>","text":"Source code in <code>src/tnh_scholar/openai_interface/openai_interface.py</code> <pre><code>def token_count_file(text_file: Path):\n    text = get_text_from_file(text_file)\n    return token_count(text)\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs","title":"<code>run_oa_batch_jobs</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.BATCH_JOB_PATH","title":"<code>BATCH_JOB_PATH = Path('UNSET')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.CHECK_INTERVAL_SECONDS","title":"<code>CHECK_INTERVAL_SECONDS = 60</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.ENQUEUED_BATCH_TOKEN_LIMIT","title":"<code>ENQUEUED_BATCH_TOKEN_LIMIT = 90000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.enqueued_tokens","title":"<code>enqueued_tokens = 0</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.sent_batches","title":"<code>sent_batches = {}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.calculate_enqueued_tokens","title":"<code>calculate_enqueued_tokens(active_batches)</code>","text":"<p>Calculate the total number of enqueued tokens from active batches.</p> Source code in <code>src/tnh_scholar/openai_interface/run_oa_batch_jobs.py</code> <pre><code>def calculate_enqueued_tokens(active_batches: List[Dict]) -&gt; int:\n    \"\"\"\n    Calculate the total number of enqueued tokens from active batches.\n    \"\"\"\n    total_tokens = 0\n    for batch in active_batches:\n        total_tokens += batch.get(\"input_tokens\", 0)\n    return total_tokens\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.download_batch_result","title":"<code>download_batch_result(client, batch_id)</code>","text":"<p>Download the result of a completed batch.</p> Source code in <code>src/tnh_scholar/openai_interface/run_oa_batch_jobs.py</code> <pre><code>def download_batch_result(client, batch_id):\n    \"\"\"\n    Download the result of a completed batch.\n    \"\"\"\n    try:\n        response = client.get(f\"/v1/batches/{batch_id}\")\n        result = response.get(\"result\", {})\n        output_file = f\"batch_results_{batch_id}.json\"\n        with open(output_file, \"w\") as file:\n            json.dump(result, file, indent=4)\n        print(f\"Batch {batch_id} completed. Result saved to {output_file}.\")\n    except Exception as e:\n        print(f\"Error downloading result for batch {batch_id}: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.get_active_batches","title":"<code>get_active_batches(client)</code>","text":"<p>Retrieve the list of active batches using the OpenAI API.</p> Source code in <code>src/tnh_scholar/openai_interface/run_oa_batch_jobs.py</code> <pre><code>def get_active_batches(client) -&gt; List[Dict]:\n    \"\"\"\n    Retrieve the list of active batches using the OpenAI API.\n    \"\"\"\n    try:\n        response = client.get(\"/v1/batches\")\n        return response.get(\"data\", [])\n    except Exception as e:\n        print(f\"Error fetching active batches: {e}\")\n        return []\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.main","title":"<code>main()</code>","text":"<p>Main function to manage and monitor batch jobs.</p> Source code in <code>src/tnh_scholar/openai_interface/run_oa_batch_jobs.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to manage and monitor batch jobs.\n    \"\"\"\n    global enqueued_tokens, sent_batches\n    client = set_api_client()\n    if not client:\n        print(\"Failed to initialize API client. Exiting.\")\n        return\n\n    batch_file_directory = \"./journal_cleaning_batches\"\n\n    while True:\n        # Poll for completed batches\n        print(\"Polling for completed batches...\")\n        poll_batches(client)\n\n        # Calculate remaining tokens\n        remaining_tokens = ENQUEUED_BATCH_TOKEN_LIMIT - enqueued_tokens\n        print(f\"Remaining tokens: {remaining_tokens}\")\n\n        # Enqueue new batches if there's space\n        print(\"Processing batch files...\")\n        process_batch_files(client, batch_file_directory, remaining_tokens)\n\n        # Wait for the next polling cycle\n        print(f\"Waiting for {CHECK_INTERVAL_SECONDS} seconds before next check...\")\n        time.sleep(CHECK_INTERVAL_SECONDS)\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.poll_batches","title":"<code>poll_batches(client)</code>","text":"<p>Poll for completed batches and update global enqueued_tokens.</p> Source code in <code>src/tnh_scholar/openai_interface/run_oa_batch_jobs.py</code> <pre><code>def poll_batches(client):\n    \"\"\"\n    Poll for completed batches and update global enqueued_tokens.\n    \"\"\"\n    global enqueued_tokens, sent_batches\n    completed_batches = []\n\n    for batch_id, info in sent_batches.items():\n        batch = info[\"batch\"]\n        try:\n            response = client.get(f\"/v1/batches/{batch_id}\")\n            status = response.get(\"status\")\n            if status == \"completed\":\n                download_batch_result(client, batch_id)\n                enqueued_tokens -= info[\"token_size\"]\n                completed_batches.append(batch_id)\n            elif status == \"failed\":\n                print(f\"Batch {batch_id} failed. Removing from tracking.\")\n                enqueued_tokens -= info[\"token_size\"]\n                completed_batches.append(batch_id)\n        except Exception as e:\n            print(f\"Error checking status for batch {batch_id}: {e}\")\n\n    # Remove completed batches from sent_batches\n    for batch_id in completed_batches:\n        del sent_batches[batch_id]\n</code></pre>"},{"location":"api/#tnh_scholar.openai_interface.run_oa_batch_jobs.process_batch_files","title":"<code>process_batch_files(client, batch_file_directory, remaining_tokens)</code>","text":"<p>Process batch files in the batch job directory, enqueue new batches if space permits.</p> Source code in <code>src/tnh_scholar/openai_interface/run_oa_batch_jobs.py</code> <pre><code>def process_batch_files(client, batch_file_directory, remaining_tokens):\n    \"\"\"\n    Process batch files in the batch job directory, enqueue new batches if space permits.\n    \"\"\"\n    global enqueued_tokens, sent_batches\n    batch_info = []\n\n    for path_obj in Path(batch_file_directory).iterdir():\n        regex = re.compile(r\"^.*\\.jsonl$\")\n        if path_obj.is_file() and regex.search(path_obj.name):\n            batch_file = Path(batch_file_directory) / path_obj.name\n            print(f\"Found batch file: {batch_file}\")\n\n            # Calculate the token count for this batch\n            try:\n                with open(batch_file, \"r\") as file:\n                    data = file.read()\n                    batch_tokens = token_count(data)\n            except Exception as e:\n                print(f\"Failed to calculate token count for {batch_file}: {e}\")\n                continue\n\n            # Enqueue batch if there's space\n            if batch_tokens &lt;= remaining_tokens:\n                try:\n                    batch = start_batch(client, batch_file)\n                    sent_batches[batch[\"id\"]] = {\n                        \"batch\": batch,\n                        \"token_size\": batch_tokens,\n                    }\n                    enqueued_tokens += batch_tokens\n                    remaining_tokens -= batch_tokens\n                    print(f\"Batch enqueued: {batch['id']}\")\n                except Exception as e:\n                    print(f\"Failed to enqueue batch {batch_file}: {e}\")\n            else:\n                print(f\"Insufficient token space for {batch_file}. Skipping.\")\n    return batch_info\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing","title":"<code>text_processing</code>","text":""},{"location":"api/#tnh_scholar.text_processing.__all__","title":"<code>__all__ = ['bracket_lines', 'unbracket_lines', 'lines_from_bracketed_text', 'NumberedText', 'normalize_newlines', 'clean_text']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText","title":"<code>NumberedText</code>","text":"<p>Represents a text document with numbered lines for easy reference and manipulation.</p> <p>Provides utilities for working with line-numbered text including reading, writing, accessing lines by number, and iterating over numbered lines.</p> <p>Attributes:</p> Name Type Description <code>lines</code> <code>List[str]</code> <p>List of text lines</p> <code>start</code> <code>int</code> <p>Starting line number (default: 1)</p> <code>separator</code> <code>str</code> <p>Separator between line number and content (default: \": \")</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text = \"First line\\nSecond line\\n\\nFourth line\"\n&gt;&gt;&gt; doc = NumberedText(text)\n&gt;&gt;&gt; print(doc)\n1: First line\n2: Second line\n3:\n4: Fourth line\n</code></pre> <pre><code>&gt;&gt;&gt; print(doc.get_line(2))\nSecond line\n</code></pre> <pre><code>&gt;&gt;&gt; for num, line in doc:\n...     print(f\"Line {num}: {len(line)} chars\")\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>class NumberedText:\n    \"\"\"\n    Represents a text document with numbered lines for easy reference and manipulation.\n\n    Provides utilities for working with line-numbered text including reading,\n    writing, accessing lines by number, and iterating over numbered lines.\n\n    Attributes:\n        lines (List[str]): List of text lines\n        start (int): Starting line number (default: 1)\n        separator (str): Separator between line number and content (default: \": \")\n\n    Examples:\n        &gt;&gt;&gt; text = \"First line\\\\nSecond line\\\\n\\\\nFourth line\"\n        &gt;&gt;&gt; doc = NumberedText(text)\n        &gt;&gt;&gt; print(doc)\n        1: First line\n        2: Second line\n        3:\n        4: Fourth line\n\n        &gt;&gt;&gt; print(doc.get_line(2))\n        Second line\n\n        &gt;&gt;&gt; for num, line in doc:\n        ...     print(f\"Line {num}: {len(line)} chars\")\n    \"\"\"\n\n    @dataclass\n    class LineSegment:\n        \"\"\"\n        Represents a segment of lines with start and end indices in 1-based indexing.\n\n        The segment follows Python range conventions where start is inclusive and\n        end is exclusive. However, indexing is 1-based to match NumberedText.\n\n        Attributes:\n            start: Starting line number (inclusive, 1-based)\n            end: Ending line number (exclusive, 1-based)\n        \"\"\"\n\n        start: int\n        end: int\n\n        def __iter__(self):\n            \"\"\"Allow unpacking into start, end pairs.\"\"\"\n            yield self.start\n            yield self.end\n\n    class SegmentIterator:\n        \"\"\"\n        Iterator for generating line segments of specified size.\n\n        Produces segments of lines with start/end indices following 1-based indexing.\n        The final segment may be smaller than the specified segment size.\n\n        Attributes:\n            total_lines: Total number of lines in text\n            segment_size: Number of lines per segment\n            start_line: Starting line number (1-based)\n            min_segment_size: Minimum size for the final segment\n        \"\"\"\n\n        def __init__(\n            self,\n            total_lines: int,\n            segment_size: int,\n            start_line: int = 1,\n            min_segment_size: Optional[int] = None,\n        ):\n            \"\"\"\n            Initialize the segment iterator.\n\n            Args:\n                total_lines: Total number of lines to iterate over\n                segment_size: Desired size of each segment\n                start_line: First line number (default: 1)\n                min_segment_size: Minimum size for final segment (default: None)\n                    If specified, the last segment will be merged with the previous one\n                    if it would be smaller than this size.\n\n            Raises:\n                ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n                ValueError: If start_line &lt; 1 (must use 1-based indexing)\n                ValueError: If min_segment_size &gt;= segment_size\n            \"\"\"\n            if segment_size &lt; 1:\n                raise ValueError(\"Segment size must be at least 1\")\n            if total_lines &lt; 1:\n                raise ValueError(\"Total lines must be at least 1\")\n            if start_line &lt; 1:\n                raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n            if min_segment_size is not None and min_segment_size &gt;= segment_size:\n                raise ValueError(\"Minimum segment size must be less than segment size\")\n\n            self.total_lines = total_lines\n            self.segment_size = segment_size\n            self.start_line = start_line\n            self.min_segment_size = min_segment_size\n\n            # Calculate number of segments\n            remaining_lines = total_lines - start_line + 1\n            self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n\n        def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n            \"\"\"\n            Iterate over line segments.\n\n            Yields:\n                LineSegment containing start (inclusive) and end (exclusive) indices\n            \"\"\"\n            current = self.start_line\n\n            for i in range(self.num_segments):\n                is_last_segment = i == self.num_segments - 1\n                segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n                # Handle minimum segment size for last segment\n                if (\n                    is_last_segment\n                    and self.min_segment_size is not None\n                    and segment_end - current &lt; self.min_segment_size\n                    and i &gt; 0\n                ):\n                    # Merge with previous segment by not yielding\n                    break\n\n                yield NumberedText.LineSegment(current, segment_end)\n                current = segment_end\n\n    def __init__(\n        self, content: Optional[str] = None, start: int = 1, separator: str = \":\"\n    ) -&gt; None:\n        \"\"\"\n        Initialize a numbered text document, detecting and preserving existing numbering.\n\n        Valid numbered text must have:\n        - Sequential line numbers\n        - Consistent separator character(s)\n        - Every non-empty line must follow the numbering pattern\n\n        Args:\n            content: Initial text content, if any\n            start: Starting line number (used only if content isn't already numbered)\n            separator: Separator between line numbers and content (only if content isn't numbered)\n\n        Examples:\n            &gt;&gt;&gt; # Custom separators\n            &gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\\\n2\u2192Second line\")\n            &gt;&gt;&gt; doc.separator == \"\u2192\"\n            True\n\n            &gt;&gt;&gt; # Preserves starting number\n            &gt;&gt;&gt; doc = NumberedText(\"5#First\\\\n6#Second\")\n            &gt;&gt;&gt; doc.start == 5\n            True\n\n            &gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n            &gt;&gt;&gt; doc = NumberedText(\"1. First item\\\\n2. Second item\")\n            &gt;&gt;&gt; doc.numbered_lines\n            ['1: 1. First item', '2: 2. Second item']\n        \"\"\"\n\n        self.lines: List[str] = []  # Declare lines here\n        self.start: int = start  # Declare start with its type\n        self.separator: str = separator  # and separator\n\n        if not isinstance(content, str):\n            raise ValueError(\"NumberedText requires string input.\")\n\n        if start &lt; 1:  # enforce 1 based indexing.\n            raise IndexError(\n                \"NumberedText: Numbered lines must begin on an integer great or equal to 1.\"\n            )\n\n        if not content:\n            return\n\n        # Analyze the text format\n        is_numbered, detected_sep, start_num = get_numbered_format(content)\n\n        format_info = get_numbered_format(content)\n\n        if format_info.is_numbered:\n            self.start = format_info.start_num  # type: ignore\n            self.separator = format_info.separator  # type: ignore\n\n            # Extract content by removing number and separator\n            pattern = re.compile(rf\"^\\d+{re.escape(detected_sep)}\")\n            self.lines = []\n\n            for line in content.splitlines():\n                if line.strip():\n                    self.lines.append(pattern.sub(\"\", line))\n                else:\n                    self.lines.append(line)\n        else:\n            self.lines = content.splitlines()\n            self.start = start\n            self.separator = separator\n\n    @classmethod\n    def from_file(cls, path: Path, **kwargs) -&gt; \"NumberedText\":\n        \"\"\"Create a NumberedText instance from a file.\"\"\"\n        return cls(Path(path).read_text(), **kwargs)\n\n    def _format_line(self, line_num: int, line: str) -&gt; str:\n        return f\"{line_num}{self.separator}{line}\"\n\n    def _to_internal_index(self, idx: int) -&gt; int:\n        \"\"\"return the index into the lines object in Python 0-based indexing.\"\"\"\n        if idx &gt; 0:\n            return idx - self.start\n        elif idx &lt; 0:  # allow negative indexing to index from end\n            if abs(idx) &gt; self.size:\n                raise IndexError(f\"NumberedText: negative index out of range: {idx}\")\n            return self.end + idx  # convert to logical positive location for reference.\n        else:\n            raise IndexError(\"NumberedText: Index cannot be zero in 1-based indexing.\")\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the numbered text representation.\"\"\"\n        return \"\\n\".join(\n            self._format_line(i, line) for i, line in enumerate(self.lines, self.start)\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of lines.\"\"\"\n        return len(self.lines)\n\n    def __iter__(self) -&gt; Iterator[tuple[int, str]]:\n        \"\"\"Iterate over (line_number, line_content) pairs.\"\"\"\n        return iter((i, line) for i, line in enumerate(self.lines, self.start))\n\n    def __getitem__(self, index: int) -&gt; str:\n        \"\"\"Get line content by line number (1-based indexing).\"\"\"\n        return self.lines[self._to_internal_index(index)]\n\n    def get_line(self, line_num: int) -&gt; str:\n        \"\"\"Get content of specified line number.\"\"\"\n        return self[line_num]\n\n    def _to_line_index(self, internal_index: int) -&gt; int:\n        return self.start + self._to_internal_index(internal_index)\n\n    def get_numbered_line(self, line_num: int) -&gt; str:\n        \"\"\"Get specified line with line number.\"\"\"\n        idx = self._to_line_index(line_num)\n        return self._format_line(idx, self[idx])\n\n    def get_lines(self, start: int, end: int) -&gt; List[str]:\n        \"\"\"Get content of line range, not inclusive of end line.\"\"\"\n        return self.lines[self._to_internal_index(start) : self._to_internal_index(end)]\n\n    def get_numbered_lines(self, start: int, end: int) -&gt; List[str]:\n        return [\n            self._format_line(i + self._to_internal_index(start) + 1, line)\n            for i, line in enumerate(self.get_lines(start, end))\n        ]\n\n    def get_segment(self, start: int, end: int) -&gt; str:\n        if start &lt; self.start:\n            raise IndexError(f\"Start index {start} is before first line {self.start}\")\n        if end &gt; len(self) + 1:\n            raise IndexError(f\"End index {end} is past last line {len(self)}\")\n        if start &gt;= end:\n            raise IndexError(f\"Start index {start} must be less than end index {end}\")\n        return \"\\n\".join(self.get_lines(start, end))\n\n    def iter_segments(\n        self, segment_size: int, min_segment_size: Optional[int] = None\n    ) -&gt; Iterator[LineSegment]:\n        \"\"\"\n        Iterate over segments of the text with specified size.\n\n        Args:\n            segment_size: Number of lines per segment\n            min_segment_size: Optional minimum size for final segment.\n                If specified, last segment will be merged with previous one\n                if it would be smaller than this size.\n\n        Yields:\n            LineSegment objects containing start and end line numbers\n\n        Example:\n            &gt;&gt;&gt; text = NumberedText(\"line1\\\\nline2\\\\nline3\\\\nline4\\\\nline5\")\n            &gt;&gt;&gt; for segment in text.iter_segments(2):\n            ...     print(f\"Lines {segment.start}-{segment.end}\")\n            Lines 1-3\n            Lines 3-5\n            Lines 5-6\n        \"\"\"\n        iterator = self.SegmentIterator(\n            len(self), segment_size, self.start, min_segment_size\n        )\n        return iter(iterator)\n\n    def get_numbered_segment(self, start: int, end: int) -&gt; str:\n        return \"\\n\".join(self.get_numbered_lines(start, end))\n\n    def save(self, path: Path, numbered: bool = True) -&gt; None:\n        \"\"\"\n        Save document to file.\n\n        Args:\n            path: Output file path\n            numbered: Whether to save with line numbers (default: True)\n        \"\"\"\n        content = str(self) if numbered else \"\\n\".join(self.lines)\n        Path(path).write_text(content)\n\n    def append(self, text: str) -&gt; None:\n        \"\"\"Append text, splitting into lines if needed.\"\"\"\n        self.lines.extend(text.splitlines())\n\n    def insert(self, line_num: int, text: str) -&gt; None:\n        \"\"\"Insert text at specified line number. Assumes text is not empty.\"\"\"\n        new_lines = text.splitlines()\n        internal_idx = self._to_internal_index(line_num)\n        self.lines[internal_idx:internal_idx] = new_lines\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Get original text without line numbers.\"\"\"\n        return \"\\n\".join(self.lines)\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Get the number of lines.\"\"\"\n        return len(self.lines)\n\n    @property\n    def numbered_lines(self) -&gt; List[str]:\n        \"\"\"\n        Get list of lines with line numbers included.\n\n        Returns:\n            List[str]: Lines with numbers and separator prefixed\n\n        Examples:\n            &gt;&gt;&gt; doc = NumberedText(\"First line\\\\nSecond line\")\n            &gt;&gt;&gt; doc.numbered_lines\n            ['1: First line', '2: Second line']\n\n        Note:\n            - Unlike str(self), this returns a list rather than joined string\n            - Maintains consistent formatting with separator\n            - Useful for processing or displaying individual numbered lines\n        \"\"\"\n        return [\n            f\"{i}{self.separator}{line}\"\n            for i, line in enumerate(self.lines, self.start)\n        ]\n\n    @property\n    def end(self) -&gt; int:\n        return self.start + len(self.lines) - 1\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.content","title":"<code>content</code>  <code>property</code>","text":"<p>Get original text without line numbers.</p>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.end","title":"<code>end</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.lines","title":"<code>lines = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.numbered_lines","title":"<code>numbered_lines</code>  <code>property</code>","text":"<p>Get list of lines with line numbers included.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Lines with numbers and separator prefixed</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; doc = NumberedText(\"First line\\nSecond line\")\n&gt;&gt;&gt; doc.numbered_lines\n['1: First line', '2: Second line']\n</code></pre> Note <ul> <li>Unlike str(self), this returns a list rather than joined string</li> <li>Maintains consistent formatting with separator</li> <li>Useful for processing or displaying individual numbered lines</li> </ul>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.separator","title":"<code>separator = separator</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.size","title":"<code>size</code>  <code>property</code>","text":"<p>Get the number of lines.</p>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.start","title":"<code>start = start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.LineSegment","title":"<code>LineSegment</code>  <code>dataclass</code>","text":"<p>Represents a segment of lines with start and end indices in 1-based indexing.</p> <p>The segment follows Python range conventions where start is inclusive and end is exclusive. However, indexing is 1-based to match NumberedText.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>int</code> <p>Starting line number (inclusive, 1-based)</p> <code>end</code> <code>int</code> <p>Ending line number (exclusive, 1-based)</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>@dataclass\nclass LineSegment:\n    \"\"\"\n    Represents a segment of lines with start and end indices in 1-based indexing.\n\n    The segment follows Python range conventions where start is inclusive and\n    end is exclusive. However, indexing is 1-based to match NumberedText.\n\n    Attributes:\n        start: Starting line number (inclusive, 1-based)\n        end: Ending line number (exclusive, 1-based)\n    \"\"\"\n\n    start: int\n    end: int\n\n    def __iter__(self):\n        \"\"\"Allow unpacking into start, end pairs.\"\"\"\n        yield self.start\n        yield self.end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.LineSegment.end","title":"<code>end</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.LineSegment.start","title":"<code>start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.LineSegment.__init__","title":"<code>__init__(start, end)</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.LineSegment.__iter__","title":"<code>__iter__()</code>","text":"<p>Allow unpacking into start, end pairs.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self):\n    \"\"\"Allow unpacking into start, end pairs.\"\"\"\n    yield self.start\n    yield self.end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator","title":"<code>SegmentIterator</code>","text":"<p>Iterator for generating line segments of specified size.</p> <p>Produces segments of lines with start/end indices following 1-based indexing. The final segment may be smaller than the specified segment size.</p> <p>Attributes:</p> Name Type Description <code>total_lines</code> <p>Total number of lines in text</p> <code>segment_size</code> <p>Number of lines per segment</p> <code>start_line</code> <p>Starting line number (1-based)</p> <code>min_segment_size</code> <p>Minimum size for the final segment</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>class SegmentIterator:\n    \"\"\"\n    Iterator for generating line segments of specified size.\n\n    Produces segments of lines with start/end indices following 1-based indexing.\n    The final segment may be smaller than the specified segment size.\n\n    Attributes:\n        total_lines: Total number of lines in text\n        segment_size: Number of lines per segment\n        start_line: Starting line number (1-based)\n        min_segment_size: Minimum size for the final segment\n    \"\"\"\n\n    def __init__(\n        self,\n        total_lines: int,\n        segment_size: int,\n        start_line: int = 1,\n        min_segment_size: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize the segment iterator.\n\n        Args:\n            total_lines: Total number of lines to iterate over\n            segment_size: Desired size of each segment\n            start_line: First line number (default: 1)\n            min_segment_size: Minimum size for final segment (default: None)\n                If specified, the last segment will be merged with the previous one\n                if it would be smaller than this size.\n\n        Raises:\n            ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n            ValueError: If start_line &lt; 1 (must use 1-based indexing)\n            ValueError: If min_segment_size &gt;= segment_size\n        \"\"\"\n        if segment_size &lt; 1:\n            raise ValueError(\"Segment size must be at least 1\")\n        if total_lines &lt; 1:\n            raise ValueError(\"Total lines must be at least 1\")\n        if start_line &lt; 1:\n            raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n        if min_segment_size is not None and min_segment_size &gt;= segment_size:\n            raise ValueError(\"Minimum segment size must be less than segment size\")\n\n        self.total_lines = total_lines\n        self.segment_size = segment_size\n        self.start_line = start_line\n        self.min_segment_size = min_segment_size\n\n        # Calculate number of segments\n        remaining_lines = total_lines - start_line + 1\n        self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n\n    def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n        \"\"\"\n        Iterate over line segments.\n\n        Yields:\n            LineSegment containing start (inclusive) and end (exclusive) indices\n        \"\"\"\n        current = self.start_line\n\n        for i in range(self.num_segments):\n            is_last_segment = i == self.num_segments - 1\n            segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n            # Handle minimum segment size for last segment\n            if (\n                is_last_segment\n                and self.min_segment_size is not None\n                and segment_end - current &lt; self.min_segment_size\n                and i &gt; 0\n            ):\n                # Merge with previous segment by not yielding\n                break\n\n            yield NumberedText.LineSegment(current, segment_end)\n            current = segment_end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.min_segment_size","title":"<code>min_segment_size = min_segment_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.num_segments","title":"<code>num_segments = remaining_lines + segment_size - 1 // segment_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.segment_size","title":"<code>segment_size = segment_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.start_line","title":"<code>start_line = start_line</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.total_lines","title":"<code>total_lines = total_lines</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.__init__","title":"<code>__init__(total_lines, segment_size, start_line=1, min_segment_size=None)</code>","text":"<p>Initialize the segment iterator.</p> <p>Parameters:</p> Name Type Description Default <code>total_lines</code> <code>int</code> <p>Total number of lines to iterate over</p> required <code>segment_size</code> <code>int</code> <p>Desired size of each segment</p> required <code>start_line</code> <code>int</code> <p>First line number (default: 1)</p> <code>1</code> <code>min_segment_size</code> <code>Optional[int]</code> <p>Minimum size for final segment (default: None) If specified, the last segment will be merged with the previous one if it would be smaller than this size.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If segment_size &lt; 1 or total_lines &lt; 1</p> <code>ValueError</code> <p>If start_line &lt; 1 (must use 1-based indexing)</p> <code>ValueError</code> <p>If min_segment_size &gt;= segment_size</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __init__(\n    self,\n    total_lines: int,\n    segment_size: int,\n    start_line: int = 1,\n    min_segment_size: Optional[int] = None,\n):\n    \"\"\"\n    Initialize the segment iterator.\n\n    Args:\n        total_lines: Total number of lines to iterate over\n        segment_size: Desired size of each segment\n        start_line: First line number (default: 1)\n        min_segment_size: Minimum size for final segment (default: None)\n            If specified, the last segment will be merged with the previous one\n            if it would be smaller than this size.\n\n    Raises:\n        ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n        ValueError: If start_line &lt; 1 (must use 1-based indexing)\n        ValueError: If min_segment_size &gt;= segment_size\n    \"\"\"\n    if segment_size &lt; 1:\n        raise ValueError(\"Segment size must be at least 1\")\n    if total_lines &lt; 1:\n        raise ValueError(\"Total lines must be at least 1\")\n    if start_line &lt; 1:\n        raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n    if min_segment_size is not None and min_segment_size &gt;= segment_size:\n        raise ValueError(\"Minimum segment size must be less than segment size\")\n\n    self.total_lines = total_lines\n    self.segment_size = segment_size\n    self.start_line = start_line\n    self.min_segment_size = min_segment_size\n\n    # Calculate number of segments\n    remaining_lines = total_lines - start_line + 1\n    self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over line segments.</p> <p>Yields:</p> Type Description <code>LineSegment</code> <p>LineSegment containing start (inclusive) and end (exclusive) indices</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n    \"\"\"\n    Iterate over line segments.\n\n    Yields:\n        LineSegment containing start (inclusive) and end (exclusive) indices\n    \"\"\"\n    current = self.start_line\n\n    for i in range(self.num_segments):\n        is_last_segment = i == self.num_segments - 1\n        segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n        # Handle minimum segment size for last segment\n        if (\n            is_last_segment\n            and self.min_segment_size is not None\n            and segment_end - current &lt; self.min_segment_size\n            and i &gt; 0\n        ):\n            # Merge with previous segment by not yielding\n            break\n\n        yield NumberedText.LineSegment(current, segment_end)\n        current = segment_end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get line content by line number (1-based indexing).</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get line content by line number (1-based indexing).\"\"\"\n    return self.lines[self._to_internal_index(index)]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.__init__","title":"<code>__init__(content=None, start=1, separator=':')</code>","text":"<p>Initialize a numbered text document, detecting and preserving existing numbering.</p> <p>Valid numbered text must have: - Sequential line numbers - Consistent separator character(s) - Every non-empty line must follow the numbering pattern</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Optional[str]</code> <p>Initial text content, if any</p> <code>None</code> <code>start</code> <code>int</code> <p>Starting line number (used only if content isn't already numbered)</p> <code>1</code> <code>separator</code> <code>str</code> <p>Separator between line numbers and content (only if content isn't numbered)</p> <code>':'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Custom separators\n&gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\n2\u2192Second line\")\n&gt;&gt;&gt; doc.separator == \"\u2192\"\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Preserves starting number\n&gt;&gt;&gt; doc = NumberedText(\"5#First\\n6#Second\")\n&gt;&gt;&gt; doc.start == 5\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n&gt;&gt;&gt; doc = NumberedText(\"1. First item\\n2. Second item\")\n&gt;&gt;&gt; doc.numbered_lines\n['1: 1. First item', '2: 2. Second item']\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __init__(\n    self, content: Optional[str] = None, start: int = 1, separator: str = \":\"\n) -&gt; None:\n    \"\"\"\n    Initialize a numbered text document, detecting and preserving existing numbering.\n\n    Valid numbered text must have:\n    - Sequential line numbers\n    - Consistent separator character(s)\n    - Every non-empty line must follow the numbering pattern\n\n    Args:\n        content: Initial text content, if any\n        start: Starting line number (used only if content isn't already numbered)\n        separator: Separator between line numbers and content (only if content isn't numbered)\n\n    Examples:\n        &gt;&gt;&gt; # Custom separators\n        &gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\\\n2\u2192Second line\")\n        &gt;&gt;&gt; doc.separator == \"\u2192\"\n        True\n\n        &gt;&gt;&gt; # Preserves starting number\n        &gt;&gt;&gt; doc = NumberedText(\"5#First\\\\n6#Second\")\n        &gt;&gt;&gt; doc.start == 5\n        True\n\n        &gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n        &gt;&gt;&gt; doc = NumberedText(\"1. First item\\\\n2. Second item\")\n        &gt;&gt;&gt; doc.numbered_lines\n        ['1: 1. First item', '2: 2. Second item']\n    \"\"\"\n\n    self.lines: List[str] = []  # Declare lines here\n    self.start: int = start  # Declare start with its type\n    self.separator: str = separator  # and separator\n\n    if not isinstance(content, str):\n        raise ValueError(\"NumberedText requires string input.\")\n\n    if start &lt; 1:  # enforce 1 based indexing.\n        raise IndexError(\n            \"NumberedText: Numbered lines must begin on an integer great or equal to 1.\"\n        )\n\n    if not content:\n        return\n\n    # Analyze the text format\n    is_numbered, detected_sep, start_num = get_numbered_format(content)\n\n    format_info = get_numbered_format(content)\n\n    if format_info.is_numbered:\n        self.start = format_info.start_num  # type: ignore\n        self.separator = format_info.separator  # type: ignore\n\n        # Extract content by removing number and separator\n        pattern = re.compile(rf\"^\\d+{re.escape(detected_sep)}\")\n        self.lines = []\n\n        for line in content.splitlines():\n            if line.strip():\n                self.lines.append(pattern.sub(\"\", line))\n            else:\n                self.lines.append(line)\n    else:\n        self.lines = content.splitlines()\n        self.start = start\n        self.separator = separator\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over (line_number, line_content) pairs.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self) -&gt; Iterator[tuple[int, str]]:\n    \"\"\"Iterate over (line_number, line_content) pairs.\"\"\"\n    return iter((i, line) for i, line in enumerate(self.lines, self.start))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of lines.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of lines.\"\"\"\n    return len(self.lines)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.__str__","title":"<code>__str__()</code>","text":"<p>Return the numbered text representation.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the numbered text representation.\"\"\"\n    return \"\\n\".join(\n        self._format_line(i, line) for i, line in enumerate(self.lines, self.start)\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.append","title":"<code>append(text)</code>","text":"<p>Append text, splitting into lines if needed.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def append(self, text: str) -&gt; None:\n    \"\"\"Append text, splitting into lines if needed.\"\"\"\n    self.lines.extend(text.splitlines())\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.from_file","title":"<code>from_file(path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a NumberedText instance from a file.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>@classmethod\ndef from_file(cls, path: Path, **kwargs) -&gt; \"NumberedText\":\n    \"\"\"Create a NumberedText instance from a file.\"\"\"\n    return cls(Path(path).read_text(), **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_line","title":"<code>get_line(line_num)</code>","text":"<p>Get content of specified line number.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_line(self, line_num: int) -&gt; str:\n    \"\"\"Get content of specified line number.\"\"\"\n    return self[line_num]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_lines","title":"<code>get_lines(start, end)</code>","text":"<p>Get content of line range, not inclusive of end line.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_lines(self, start: int, end: int) -&gt; List[str]:\n    \"\"\"Get content of line range, not inclusive of end line.\"\"\"\n    return self.lines[self._to_internal_index(start) : self._to_internal_index(end)]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_numbered_line","title":"<code>get_numbered_line(line_num)</code>","text":"<p>Get specified line with line number.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_line(self, line_num: int) -&gt; str:\n    \"\"\"Get specified line with line number.\"\"\"\n    idx = self._to_line_index(line_num)\n    return self._format_line(idx, self[idx])\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_numbered_lines","title":"<code>get_numbered_lines(start, end)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_lines(self, start: int, end: int) -&gt; List[str]:\n    return [\n        self._format_line(i + self._to_internal_index(start) + 1, line)\n        for i, line in enumerate(self.get_lines(start, end))\n    ]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_numbered_segment","title":"<code>get_numbered_segment(start, end)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_segment(self, start: int, end: int) -&gt; str:\n    return \"\\n\".join(self.get_numbered_lines(start, end))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_segment","title":"<code>get_segment(start, end)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_segment(self, start: int, end: int) -&gt; str:\n    if start &lt; self.start:\n        raise IndexError(f\"Start index {start} is before first line {self.start}\")\n    if end &gt; len(self) + 1:\n        raise IndexError(f\"End index {end} is past last line {len(self)}\")\n    if start &gt;= end:\n        raise IndexError(f\"Start index {start} must be less than end index {end}\")\n    return \"\\n\".join(self.get_lines(start, end))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.insert","title":"<code>insert(line_num, text)</code>","text":"<p>Insert text at specified line number. Assumes text is not empty.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def insert(self, line_num: int, text: str) -&gt; None:\n    \"\"\"Insert text at specified line number. Assumes text is not empty.\"\"\"\n    new_lines = text.splitlines()\n    internal_idx = self._to_internal_index(line_num)\n    self.lines[internal_idx:internal_idx] = new_lines\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.iter_segments","title":"<code>iter_segments(segment_size, min_segment_size=None)</code>","text":"<p>Iterate over segments of the text with specified size.</p> <p>Parameters:</p> Name Type Description Default <code>segment_size</code> <code>int</code> <p>Number of lines per segment</p> required <code>min_segment_size</code> <code>Optional[int]</code> <p>Optional minimum size for final segment. If specified, last segment will be merged with previous one if it would be smaller than this size.</p> <code>None</code> <p>Yields:</p> Type Description <code>LineSegment</code> <p>LineSegment objects containing start and end line numbers</p> Example <p>text = NumberedText(\"line1\\nline2\\nline3\\nline4\\nline5\") for segment in text.iter_segments(2): ...     print(f\"Lines {segment.start}-{segment.end}\") Lines 1-3 Lines 3-5 Lines 5-6</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def iter_segments(\n    self, segment_size: int, min_segment_size: Optional[int] = None\n) -&gt; Iterator[LineSegment]:\n    \"\"\"\n    Iterate over segments of the text with specified size.\n\n    Args:\n        segment_size: Number of lines per segment\n        min_segment_size: Optional minimum size for final segment.\n            If specified, last segment will be merged with previous one\n            if it would be smaller than this size.\n\n    Yields:\n        LineSegment objects containing start and end line numbers\n\n    Example:\n        &gt;&gt;&gt; text = NumberedText(\"line1\\\\nline2\\\\nline3\\\\nline4\\\\nline5\")\n        &gt;&gt;&gt; for segment in text.iter_segments(2):\n        ...     print(f\"Lines {segment.start}-{segment.end}\")\n        Lines 1-3\n        Lines 3-5\n        Lines 5-6\n    \"\"\"\n    iterator = self.SegmentIterator(\n        len(self), segment_size, self.start, min_segment_size\n    )\n    return iter(iterator)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.save","title":"<code>save(path, numbered=True)</code>","text":"<p>Save document to file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output file path</p> required <code>numbered</code> <code>bool</code> <p>Whether to save with line numbers (default: True)</p> <code>True</code> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def save(self, path: Path, numbered: bool = True) -&gt; None:\n    \"\"\"\n    Save document to file.\n\n    Args:\n        path: Output file path\n        numbered: Whether to save with line numbers (default: True)\n    \"\"\"\n    content = str(self) if numbered else \"\\n\".join(self.lines)\n    Path(path).write_text(content)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket_lines","title":"<code>bracket_lines(text, number=False)</code>","text":"<pre><code>Encloses each line of the input text with angle brackets.\nIf number is True, adds a line number followed by a colon `:` and then the line.\n\nArgs:\n    text (str): The input string containing lines separated by '\n</code></pre> <p>'.         number (bool): Whether to prepend line numbers to each line.</p> <pre><code>Returns:\n    str: A string where each line is enclosed in angle brackets.\n\nExamples:\n    &gt;&gt;&gt; bracket_lines(\"This is a string with\n</code></pre> <p>two lines.\")         ' &lt;   two lines.&gt;' <pre><code>    &gt;&gt;&gt; bracket_lines(\"This is a string with\n</code></pre> <p>two lines.\", number=True)         '&lt;1:This is a string with&gt; &lt;2:   two lines.&gt;'</p> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def bracket_lines(text: str, number: bool = False) -&gt; str:\n    \"\"\"\n    Encloses each line of the input text with angle brackets.\n    If number is True, adds a line number followed by a colon `:` and then the line.\n\n    Args:\n        text (str): The input string containing lines separated by '\\n'.\n        number (bool): Whether to prepend line numbers to each line.\n\n    Returns:\n        str: A string where each line is enclosed in angle brackets.\n\n    Examples:\n        &gt;&gt;&gt; bracket_lines(\"This is a string with\\n   two lines.\")\n        '&lt;This is a string with&gt;\\n&lt;   two lines.&gt;'\n\n        &gt;&gt;&gt; bracket_lines(\"This is a string with\\n   two lines.\", number=True)\n        '&lt;1:This is a string with&gt;\\n&lt;2:   two lines.&gt;'\n    \"\"\"\n    return \"\\n\".join(\n        f\"&lt;{f'{i+1}:{line}' if number else line}&gt;\"\n        for i, line in enumerate(text.split(\"\\n\"))\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.clean_text","title":"<code>clean_text(text, newline=False)</code>","text":"<p>Cleans a given text by replacing specific unwanted characters such as tab, and non-breaking spaces with regular spaces.</p> <p>This function takes a string as input and applies replacements based on a predefined mapping of characters to replace.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be cleaned.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The cleaned text with unwanted characters replaced by spaces.</p> Example <p>text = \"This is\\n an example\\ttext with\\xa0extra spaces.\" clean_text(text) 'This is an example text with extra spaces.'</p> Source code in <code>src/tnh_scholar/text_processing/text_processing.py</code> <pre><code>def clean_text(text, newline=False):\n    \"\"\"\n    Cleans a given text by replacing specific unwanted characters such as\n    tab, and non-breaking spaces with regular spaces.\n\n    This function takes a string as input and applies replacements\n    based on a predefined mapping of characters to replace.\n\n    Args:\n        text (str): The text to be cleaned.\n\n    Returns:\n        str: The cleaned text with unwanted characters replaced by spaces.\n\n    Example:\n        &gt;&gt;&gt; text = \"This is\\\\n an example\\\\ttext with\\\\xa0extra spaces.\"\n        &gt;&gt;&gt; clean_text(text)\n        'This is an example text with extra spaces.'\n\n    \"\"\"\n    # Define a mapping of characters to replace\n    replace_map = {\n        \"\\t\": \" \",  # Replace tabs with space\n        \"\\xa0\": \" \",  # Replace non-breaking space with regular space\n        # Add more replacements as needed\n    }\n\n    if newline:\n        replace_map[\"\\n\"] = \"\"  # remove newlines\n\n    # Loop through the replace map and replace each character\n    for old_char, new_char in replace_map.items():\n        text = text.replace(old_char, new_char)\n\n    return text.strip()  # Ensure any leading/trailing spaces are removed\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.lines_from_bracketed_text","title":"<code>lines_from_bracketed_text(text, start, end, keep_brackets=False)</code>","text":"<pre><code>Extracts lines from bracketed text between the start and end indices, inclusive.\nHandles both numbered and non-numbered cases.\n\nArgs:\n    text (str): The input bracketed text containing lines like &lt;...&gt;.\n    start (int): The starting line number (1-based).\n    end (int): The ending line number (1-based).\n\nReturns:\n    list[str]: The lines from start to end inclusive, with angle brackets removed.\n\nRaises:\n    FormattingError: If the text contains improperly formatted lines (missing angle brackets).\n    ValueError: If start or end indices are invalid or out of bounds.\n\nExamples:\n    &gt;&gt;&gt; text = \"&lt;1:Line 1&gt;\n</code></pre> <p>&lt;2:Line 2&gt; &lt;3:Line 3&gt;\"         &gt;&gt;&gt; lines_from_bracketed_text(text, 1, 2)         ['Line 1', 'Line 2']</p> <pre><code>    &gt;&gt;&gt; text = \"&lt;Line 1&gt;\n</code></pre> <p> \"         &gt;&gt;&gt; lines_from_bracketed_text(text, 2, 3)         ['Line 2', 'Line 3'] Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def lines_from_bracketed_text(\n    text: str, start: int, end: int, keep_brackets=False\n) -&gt; list[str]:\n    \"\"\"\n    Extracts lines from bracketed text between the start and end indices, inclusive.\n    Handles both numbered and non-numbered cases.\n\n    Args:\n        text (str): The input bracketed text containing lines like &lt;...&gt;.\n        start (int): The starting line number (1-based).\n        end (int): The ending line number (1-based).\n\n    Returns:\n        list[str]: The lines from start to end inclusive, with angle brackets removed.\n\n    Raises:\n        FormattingError: If the text contains improperly formatted lines (missing angle brackets).\n        ValueError: If start or end indices are invalid or out of bounds.\n\n    Examples:\n        &gt;&gt;&gt; text = \"&lt;1:Line 1&gt;\\n&lt;2:Line 2&gt;\\n&lt;3:Line 3&gt;\"\n        &gt;&gt;&gt; lines_from_bracketed_text(text, 1, 2)\n        ['Line 1', 'Line 2']\n\n        &gt;&gt;&gt; text = \"&lt;Line 1&gt;\\n&lt;Line 2&gt;\\n&lt;Line 3&gt;\"\n        &gt;&gt;&gt; lines_from_bracketed_text(text, 2, 3)\n        ['Line 2', 'Line 3']\n    \"\"\"\n    # Split the text into lines\n    lines = text.splitlines()\n\n    # Validate indices\n    if start &lt; 1 or end &lt; 1 or start &gt; end or end &gt; len(lines):\n        raise ValueError(\n            \"Invalid start or end indices for the given text: start:{start}, end: {end}\"\n        )\n\n    # Extract lines and validate formatting\n    result = []\n    for i, line in enumerate(lines, start=1):\n        if start &lt;= i &lt;= end:\n            # Check for proper bracketing and extract the content\n            match = re.match(r\"&lt;(\\d+:)?(.*?)&gt;\", line)\n            if not match:\n                raise FormattingError(f\"Invalid format for line {i}: '{line}'\")\n            # Add the extracted content (group 2) to the result\n            if keep_brackets:\n                result.append(line)\n            else:\n                result.append(match[2].strip())\n\n    return \"\\n\".join(result)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.normalize_newlines","title":"<code>normalize_newlines(text, spacing=2)</code>","text":"<pre><code>Normalize newline blocks in the input text by reducing consecutive newlines\nto the specified number of newlines for consistent readability and formatting.\n\nParameters:\n----------\ntext : str\n    The input text containing inconsistent newline spacing.\nspacing : int, optional\n    The number of newlines to insert between lines. Defaults to 2.\n\nReturns:\n-------\nstr\n    The text with consecutive newlines reduced to the specified number of newlines.\n\nExample:\n--------\n&gt;&gt;&gt; raw_text = \"Heading\n</code></pre> <p>Paragraph text 1 Paragraph text 2</p> <p>\"     &gt;&gt;&gt; normalize_newlines(raw_text, spacing=2)     'Heading</p> <p>Paragraph text 1</p> <p>Paragraph text 2</p> <p>'</p> Source code in <code>src/tnh_scholar/text_processing/text_processing.py</code> <pre><code>def normalize_newlines(text: str, spacing: int = 2) -&gt; str:\n    \"\"\"\n    Normalize newline blocks in the input text by reducing consecutive newlines\n    to the specified number of newlines for consistent readability and formatting.\n\n    Parameters:\n    ----------\n    text : str\n        The input text containing inconsistent newline spacing.\n    spacing : int, optional\n        The number of newlines to insert between lines. Defaults to 2.\n\n    Returns:\n    -------\n    str\n        The text with consecutive newlines reduced to the specified number of newlines.\n\n    Example:\n    --------\n    &gt;&gt;&gt; raw_text = \"Heading\\n\\n\\nParagraph text 1\\nParagraph text 2\\n\\n\\n\"\n    &gt;&gt;&gt; normalize_newlines(raw_text, spacing=2)\n    'Heading\\n\\nParagraph text 1\\n\\nParagraph text 2\\n\\n'\n    \"\"\"\n    # Replace one or more newlines with the desired number of newlines\n    newlines = \"\\n\" * spacing\n    return re.sub(r\"\\n{1,}\", newlines, text)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.unbracket_lines","title":"<code>unbracket_lines(text, number=False)</code>","text":"<pre><code>Removes angle brackets (&lt; &gt;) from encapsulated lines and optionally removes line numbers.\n\nArgs:\n    text (str): The input string with encapsulated lines.\n    number (bool): If True, removes line numbers in the format 'digit:'.\n                   Raises a ValueError if `number=True` and a line does not start with a digit followed by a colon.\n\nReturns:\n    str: A newline-separated string with the encapsulation removed, and line numbers stripped if specified.\n\nExamples:\n    &gt;&gt;&gt; unbracket_lines(\"&lt;1:Line 1&gt;\n</code></pre> <p>&lt;2:Line 2&gt;\", number=True)         'Line 1 Line 2'</p> <pre><code>    &gt;&gt;&gt; unbracket_lines(\"&lt;Line 1&gt;\n</code></pre> <p>\")         'Line 1 Line 2' <pre><code>    &gt;&gt;&gt; unbracket_lines(\"&lt;1Line 1&gt;\", number=True)\n    ValueError: Line does not start with a valid number: '1Line 1'\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def unbracket_lines(text: str, number: bool = False) -&gt; str:\n    \"\"\"\n    Removes angle brackets (&lt; &gt;) from encapsulated lines and optionally removes line numbers.\n\n    Args:\n        text (str): The input string with encapsulated lines.\n        number (bool): If True, removes line numbers in the format 'digit:'.\n                       Raises a ValueError if `number=True` and a line does not start with a digit followed by a colon.\n\n    Returns:\n        str: A newline-separated string with the encapsulation removed, and line numbers stripped if specified.\n\n    Examples:\n        &gt;&gt;&gt; unbracket_lines(\"&lt;1:Line 1&gt;\\n&lt;2:Line 2&gt;\", number=True)\n        'Line 1\\nLine 2'\n\n        &gt;&gt;&gt; unbracket_lines(\"&lt;Line 1&gt;\\n&lt;Line 2&gt;\")\n        'Line 1\\nLine 2'\n\n        &gt;&gt;&gt; unbracket_lines(\"&lt;1Line 1&gt;\", number=True)\n        ValueError: Line does not start with a valid number: '1Line 1'\n    \"\"\"\n    unbracketed_lines = []\n\n    for line in text.splitlines():\n        match = (\n            re.match(r\"&lt;(\\d+):(.*?)&gt;\", line) if number else re.match(r\"&lt;(.*?)&gt;\", line)\n        )\n        if match:\n            content = match[2].strip() if number else match[1].strip()\n            unbracketed_lines.append(content)\n        elif number:\n            raise FormattingError(f\"Line does not start with a valid number: '{line}'\")\n        else:\n            raise FormattingError(f\"Line does not follow the expected format: '{line}'\")\n\n    return \"\\n\".join(unbracketed_lines)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket","title":"<code>bracket</code>","text":""},{"location":"api/#tnh_scholar.text_processing.bracket.FormattingError","title":"<code>FormattingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception raised for formatting-related errors.</p> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>class FormattingError(Exception):\n    \"\"\"\n    Custom exception raised for formatting-related errors.\n    \"\"\"\n\n    def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n        super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.FormattingError.__init__","title":"<code>__init__(message='An error occurred due to invalid formatting.')</code>","text":"Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n    super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.bracket_all_lines","title":"<code>bracket_all_lines(pages)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def bracket_all_lines(pages):\n    return [bracket_lines(page) for page in pages]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.bracket_lines","title":"<code>bracket_lines(text, number=False)</code>","text":"<pre><code>Encloses each line of the input text with angle brackets.\nIf number is True, adds a line number followed by a colon `:` and then the line.\n\nArgs:\n    text (str): The input string containing lines separated by '\n</code></pre> <p>'.         number (bool): Whether to prepend line numbers to each line.</p> <pre><code>Returns:\n    str: A string where each line is enclosed in angle brackets.\n\nExamples:\n    &gt;&gt;&gt; bracket_lines(\"This is a string with\n</code></pre> <p>two lines.\")         ' &lt;   two lines.&gt;' <pre><code>    &gt;&gt;&gt; bracket_lines(\"This is a string with\n</code></pre> <p>two lines.\", number=True)         '&lt;1:This is a string with&gt; &lt;2:   two lines.&gt;'</p> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def bracket_lines(text: str, number: bool = False) -&gt; str:\n    \"\"\"\n    Encloses each line of the input text with angle brackets.\n    If number is True, adds a line number followed by a colon `:` and then the line.\n\n    Args:\n        text (str): The input string containing lines separated by '\\n'.\n        number (bool): Whether to prepend line numbers to each line.\n\n    Returns:\n        str: A string where each line is enclosed in angle brackets.\n\n    Examples:\n        &gt;&gt;&gt; bracket_lines(\"This is a string with\\n   two lines.\")\n        '&lt;This is a string with&gt;\\n&lt;   two lines.&gt;'\n\n        &gt;&gt;&gt; bracket_lines(\"This is a string with\\n   two lines.\", number=True)\n        '&lt;1:This is a string with&gt;\\n&lt;2:   two lines.&gt;'\n    \"\"\"\n    return \"\\n\".join(\n        f\"&lt;{f'{i+1}:{line}' if number else line}&gt;\"\n        for i, line in enumerate(text.split(\"\\n\"))\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.lines_from_bracketed_text","title":"<code>lines_from_bracketed_text(text, start, end, keep_brackets=False)</code>","text":"<pre><code>Extracts lines from bracketed text between the start and end indices, inclusive.\nHandles both numbered and non-numbered cases.\n\nArgs:\n    text (str): The input bracketed text containing lines like &lt;...&gt;.\n    start (int): The starting line number (1-based).\n    end (int): The ending line number (1-based).\n\nReturns:\n    list[str]: The lines from start to end inclusive, with angle brackets removed.\n\nRaises:\n    FormattingError: If the text contains improperly formatted lines (missing angle brackets).\n    ValueError: If start or end indices are invalid or out of bounds.\n\nExamples:\n    &gt;&gt;&gt; text = \"&lt;1:Line 1&gt;\n</code></pre> <p>&lt;2:Line 2&gt; &lt;3:Line 3&gt;\"         &gt;&gt;&gt; lines_from_bracketed_text(text, 1, 2)         ['Line 1', 'Line 2']</p> <pre><code>    &gt;&gt;&gt; text = \"&lt;Line 1&gt;\n</code></pre> <p> \"         &gt;&gt;&gt; lines_from_bracketed_text(text, 2, 3)         ['Line 2', 'Line 3'] Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def lines_from_bracketed_text(\n    text: str, start: int, end: int, keep_brackets=False\n) -&gt; list[str]:\n    \"\"\"\n    Extracts lines from bracketed text between the start and end indices, inclusive.\n    Handles both numbered and non-numbered cases.\n\n    Args:\n        text (str): The input bracketed text containing lines like &lt;...&gt;.\n        start (int): The starting line number (1-based).\n        end (int): The ending line number (1-based).\n\n    Returns:\n        list[str]: The lines from start to end inclusive, with angle brackets removed.\n\n    Raises:\n        FormattingError: If the text contains improperly formatted lines (missing angle brackets).\n        ValueError: If start or end indices are invalid or out of bounds.\n\n    Examples:\n        &gt;&gt;&gt; text = \"&lt;1:Line 1&gt;\\n&lt;2:Line 2&gt;\\n&lt;3:Line 3&gt;\"\n        &gt;&gt;&gt; lines_from_bracketed_text(text, 1, 2)\n        ['Line 1', 'Line 2']\n\n        &gt;&gt;&gt; text = \"&lt;Line 1&gt;\\n&lt;Line 2&gt;\\n&lt;Line 3&gt;\"\n        &gt;&gt;&gt; lines_from_bracketed_text(text, 2, 3)\n        ['Line 2', 'Line 3']\n    \"\"\"\n    # Split the text into lines\n    lines = text.splitlines()\n\n    # Validate indices\n    if start &lt; 1 or end &lt; 1 or start &gt; end or end &gt; len(lines):\n        raise ValueError(\n            \"Invalid start or end indices for the given text: start:{start}, end: {end}\"\n        )\n\n    # Extract lines and validate formatting\n    result = []\n    for i, line in enumerate(lines, start=1):\n        if start &lt;= i &lt;= end:\n            # Check for proper bracketing and extract the content\n            match = re.match(r\"&lt;(\\d+:)?(.*?)&gt;\", line)\n            if not match:\n                raise FormattingError(f\"Invalid format for line {i}: '{line}'\")\n            # Add the extracted content (group 2) to the result\n            if keep_brackets:\n                result.append(line)\n            else:\n                result.append(match[2].strip())\n\n    return \"\\n\".join(result)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.number_lines","title":"<code>number_lines(text, start=1, separator=': ')</code>","text":"<p>Numbers each line of text with a readable format, including empty lines.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to be numbered. Can be multi-line.</p> required <code>start</code> <code>int</code> <p>Starting line number. Defaults to 1.</p> <code>1</code> <code>separator</code> <code>str</code> <p>Separator between line number and content. Defaults to \": \".</p> <code>': '</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Numbered text where each line starts with \"{number}: \".</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text = \"First line\\nSecond line\\n\\nFourth line\"\n&gt;&gt;&gt; print(number_lines(text))\n1: First line\n2: Second line\n3:\n4: Fourth line\n</code></pre> <pre><code>&gt;&gt;&gt; print(number_lines(text, start=5, separator=\" | \"))\n5 | First line\n6 | Second line\n7 |\n8 | Fourth line\n</code></pre> Notes <ul> <li>All lines are numbered, including empty lines, to maintain text structure</li> <li>Line numbers are aligned through natural string formatting</li> <li>Customizable separator allows for different formatting needs</li> <li>Can start from any line number for flexibility in text processing</li> </ul> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def number_lines(text: str, start: int = 1, separator: str = \": \") -&gt; str:\n    \"\"\"\n    Numbers each line of text with a readable format, including empty lines.\n\n    Args:\n        text (str): Input text to be numbered. Can be multi-line.\n        start (int, optional): Starting line number. Defaults to 1.\n        separator (str, optional): Separator between line number and content.\n            Defaults to \": \".\n\n    Returns:\n        str: Numbered text where each line starts with \"{number}: \".\n\n    Examples:\n        &gt;&gt;&gt; text = \"First line\\\\nSecond line\\\\n\\\\nFourth line\"\n        &gt;&gt;&gt; print(number_lines(text))\n        1: First line\n        2: Second line\n        3:\n        4: Fourth line\n\n        &gt;&gt;&gt; print(number_lines(text, start=5, separator=\" | \"))\n        5 | First line\n        6 | Second line\n        7 |\n        8 | Fourth line\n\n    Notes:\n        - All lines are numbered, including empty lines, to maintain text structure\n        - Line numbers are aligned through natural string formatting\n        - Customizable separator allows for different formatting needs\n        - Can start from any line number for flexibility in text processing\n    \"\"\"\n    lines = text.splitlines()\n    return \"\\n\".join(f\"{i}{separator}{line}\" for i, line in enumerate(lines, start))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.unbracket_all_lines","title":"<code>unbracket_all_lines(pages)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def unbracket_all_lines(pages):\n    result = []\n    for page in pages:\n        if page == \"blank page\":\n            result.append(page)\n        else:\n            result.append(unbracket_lines(page))\n    return result\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.unbracket_lines","title":"<code>unbracket_lines(text, number=False)</code>","text":"<pre><code>Removes angle brackets (&lt; &gt;) from encapsulated lines and optionally removes line numbers.\n\nArgs:\n    text (str): The input string with encapsulated lines.\n    number (bool): If True, removes line numbers in the format 'digit:'.\n                   Raises a ValueError if `number=True` and a line does not start with a digit followed by a colon.\n\nReturns:\n    str: A newline-separated string with the encapsulation removed, and line numbers stripped if specified.\n\nExamples:\n    &gt;&gt;&gt; unbracket_lines(\"&lt;1:Line 1&gt;\n</code></pre> <p>&lt;2:Line 2&gt;\", number=True)         'Line 1 Line 2'</p> <pre><code>    &gt;&gt;&gt; unbracket_lines(\"&lt;Line 1&gt;\n</code></pre> <p>\")         'Line 1 Line 2' <pre><code>    &gt;&gt;&gt; unbracket_lines(\"&lt;1Line 1&gt;\", number=True)\n    ValueError: Line does not start with a valid number: '1Line 1'\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def unbracket_lines(text: str, number: bool = False) -&gt; str:\n    \"\"\"\n    Removes angle brackets (&lt; &gt;) from encapsulated lines and optionally removes line numbers.\n\n    Args:\n        text (str): The input string with encapsulated lines.\n        number (bool): If True, removes line numbers in the format 'digit:'.\n                       Raises a ValueError if `number=True` and a line does not start with a digit followed by a colon.\n\n    Returns:\n        str: A newline-separated string with the encapsulation removed, and line numbers stripped if specified.\n\n    Examples:\n        &gt;&gt;&gt; unbracket_lines(\"&lt;1:Line 1&gt;\\n&lt;2:Line 2&gt;\", number=True)\n        'Line 1\\nLine 2'\n\n        &gt;&gt;&gt; unbracket_lines(\"&lt;Line 1&gt;\\n&lt;Line 2&gt;\")\n        'Line 1\\nLine 2'\n\n        &gt;&gt;&gt; unbracket_lines(\"&lt;1Line 1&gt;\", number=True)\n        ValueError: Line does not start with a valid number: '1Line 1'\n    \"\"\"\n    unbracketed_lines = []\n\n    for line in text.splitlines():\n        match = (\n            re.match(r\"&lt;(\\d+):(.*?)&gt;\", line) if number else re.match(r\"&lt;(.*?)&gt;\", line)\n        )\n        if match:\n            content = match[2].strip() if number else match[1].strip()\n            unbracketed_lines.append(content)\n        elif number:\n            raise FormattingError(f\"Line does not start with a valid number: '{line}'\")\n        else:\n            raise FormattingError(f\"Line does not follow the expected format: '{line}'\")\n\n    return \"\\n\".join(unbracketed_lines)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text","title":"<code>numbered_text</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedFormat","title":"<code>NumberedFormat</code>","text":"<p>               Bases: <code>NamedTuple</code></p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>class NumberedFormat(NamedTuple):\n    is_numbered: bool\n    separator: Optional[str] = None\n    start_num: Optional[int] = None\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedFormat.is_numbered","title":"<code>is_numbered</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedFormat.separator","title":"<code>separator = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedFormat.start_num","title":"<code>start_num = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText","title":"<code>NumberedText</code>","text":"<p>Represents a text document with numbered lines for easy reference and manipulation.</p> <p>Provides utilities for working with line-numbered text including reading, writing, accessing lines by number, and iterating over numbered lines.</p> <p>Attributes:</p> Name Type Description <code>lines</code> <code>List[str]</code> <p>List of text lines</p> <code>start</code> <code>int</code> <p>Starting line number (default: 1)</p> <code>separator</code> <code>str</code> <p>Separator between line number and content (default: \": \")</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text = \"First line\\nSecond line\\n\\nFourth line\"\n&gt;&gt;&gt; doc = NumberedText(text)\n&gt;&gt;&gt; print(doc)\n1: First line\n2: Second line\n3:\n4: Fourth line\n</code></pre> <pre><code>&gt;&gt;&gt; print(doc.get_line(2))\nSecond line\n</code></pre> <pre><code>&gt;&gt;&gt; for num, line in doc:\n...     print(f\"Line {num}: {len(line)} chars\")\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>class NumberedText:\n    \"\"\"\n    Represents a text document with numbered lines for easy reference and manipulation.\n\n    Provides utilities for working with line-numbered text including reading,\n    writing, accessing lines by number, and iterating over numbered lines.\n\n    Attributes:\n        lines (List[str]): List of text lines\n        start (int): Starting line number (default: 1)\n        separator (str): Separator between line number and content (default: \": \")\n\n    Examples:\n        &gt;&gt;&gt; text = \"First line\\\\nSecond line\\\\n\\\\nFourth line\"\n        &gt;&gt;&gt; doc = NumberedText(text)\n        &gt;&gt;&gt; print(doc)\n        1: First line\n        2: Second line\n        3:\n        4: Fourth line\n\n        &gt;&gt;&gt; print(doc.get_line(2))\n        Second line\n\n        &gt;&gt;&gt; for num, line in doc:\n        ...     print(f\"Line {num}: {len(line)} chars\")\n    \"\"\"\n\n    @dataclass\n    class LineSegment:\n        \"\"\"\n        Represents a segment of lines with start and end indices in 1-based indexing.\n\n        The segment follows Python range conventions where start is inclusive and\n        end is exclusive. However, indexing is 1-based to match NumberedText.\n\n        Attributes:\n            start: Starting line number (inclusive, 1-based)\n            end: Ending line number (exclusive, 1-based)\n        \"\"\"\n\n        start: int\n        end: int\n\n        def __iter__(self):\n            \"\"\"Allow unpacking into start, end pairs.\"\"\"\n            yield self.start\n            yield self.end\n\n    class SegmentIterator:\n        \"\"\"\n        Iterator for generating line segments of specified size.\n\n        Produces segments of lines with start/end indices following 1-based indexing.\n        The final segment may be smaller than the specified segment size.\n\n        Attributes:\n            total_lines: Total number of lines in text\n            segment_size: Number of lines per segment\n            start_line: Starting line number (1-based)\n            min_segment_size: Minimum size for the final segment\n        \"\"\"\n\n        def __init__(\n            self,\n            total_lines: int,\n            segment_size: int,\n            start_line: int = 1,\n            min_segment_size: Optional[int] = None,\n        ):\n            \"\"\"\n            Initialize the segment iterator.\n\n            Args:\n                total_lines: Total number of lines to iterate over\n                segment_size: Desired size of each segment\n                start_line: First line number (default: 1)\n                min_segment_size: Minimum size for final segment (default: None)\n                    If specified, the last segment will be merged with the previous one\n                    if it would be smaller than this size.\n\n            Raises:\n                ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n                ValueError: If start_line &lt; 1 (must use 1-based indexing)\n                ValueError: If min_segment_size &gt;= segment_size\n            \"\"\"\n            if segment_size &lt; 1:\n                raise ValueError(\"Segment size must be at least 1\")\n            if total_lines &lt; 1:\n                raise ValueError(\"Total lines must be at least 1\")\n            if start_line &lt; 1:\n                raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n            if min_segment_size is not None and min_segment_size &gt;= segment_size:\n                raise ValueError(\"Minimum segment size must be less than segment size\")\n\n            self.total_lines = total_lines\n            self.segment_size = segment_size\n            self.start_line = start_line\n            self.min_segment_size = min_segment_size\n\n            # Calculate number of segments\n            remaining_lines = total_lines - start_line + 1\n            self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n\n        def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n            \"\"\"\n            Iterate over line segments.\n\n            Yields:\n                LineSegment containing start (inclusive) and end (exclusive) indices\n            \"\"\"\n            current = self.start_line\n\n            for i in range(self.num_segments):\n                is_last_segment = i == self.num_segments - 1\n                segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n                # Handle minimum segment size for last segment\n                if (\n                    is_last_segment\n                    and self.min_segment_size is not None\n                    and segment_end - current &lt; self.min_segment_size\n                    and i &gt; 0\n                ):\n                    # Merge with previous segment by not yielding\n                    break\n\n                yield NumberedText.LineSegment(current, segment_end)\n                current = segment_end\n\n    def __init__(\n        self, content: Optional[str] = None, start: int = 1, separator: str = \":\"\n    ) -&gt; None:\n        \"\"\"\n        Initialize a numbered text document, detecting and preserving existing numbering.\n\n        Valid numbered text must have:\n        - Sequential line numbers\n        - Consistent separator character(s)\n        - Every non-empty line must follow the numbering pattern\n\n        Args:\n            content: Initial text content, if any\n            start: Starting line number (used only if content isn't already numbered)\n            separator: Separator between line numbers and content (only if content isn't numbered)\n\n        Examples:\n            &gt;&gt;&gt; # Custom separators\n            &gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\\\n2\u2192Second line\")\n            &gt;&gt;&gt; doc.separator == \"\u2192\"\n            True\n\n            &gt;&gt;&gt; # Preserves starting number\n            &gt;&gt;&gt; doc = NumberedText(\"5#First\\\\n6#Second\")\n            &gt;&gt;&gt; doc.start == 5\n            True\n\n            &gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n            &gt;&gt;&gt; doc = NumberedText(\"1. First item\\\\n2. Second item\")\n            &gt;&gt;&gt; doc.numbered_lines\n            ['1: 1. First item', '2: 2. Second item']\n        \"\"\"\n\n        self.lines: List[str] = []  # Declare lines here\n        self.start: int = start  # Declare start with its type\n        self.separator: str = separator  # and separator\n\n        if not isinstance(content, str):\n            raise ValueError(\"NumberedText requires string input.\")\n\n        if start &lt; 1:  # enforce 1 based indexing.\n            raise IndexError(\n                \"NumberedText: Numbered lines must begin on an integer great or equal to 1.\"\n            )\n\n        if not content:\n            return\n\n        # Analyze the text format\n        is_numbered, detected_sep, start_num = get_numbered_format(content)\n\n        format_info = get_numbered_format(content)\n\n        if format_info.is_numbered:\n            self.start = format_info.start_num  # type: ignore\n            self.separator = format_info.separator  # type: ignore\n\n            # Extract content by removing number and separator\n            pattern = re.compile(rf\"^\\d+{re.escape(detected_sep)}\")\n            self.lines = []\n\n            for line in content.splitlines():\n                if line.strip():\n                    self.lines.append(pattern.sub(\"\", line))\n                else:\n                    self.lines.append(line)\n        else:\n            self.lines = content.splitlines()\n            self.start = start\n            self.separator = separator\n\n    @classmethod\n    def from_file(cls, path: Path, **kwargs) -&gt; \"NumberedText\":\n        \"\"\"Create a NumberedText instance from a file.\"\"\"\n        return cls(Path(path).read_text(), **kwargs)\n\n    def _format_line(self, line_num: int, line: str) -&gt; str:\n        return f\"{line_num}{self.separator}{line}\"\n\n    def _to_internal_index(self, idx: int) -&gt; int:\n        \"\"\"return the index into the lines object in Python 0-based indexing.\"\"\"\n        if idx &gt; 0:\n            return idx - self.start\n        elif idx &lt; 0:  # allow negative indexing to index from end\n            if abs(idx) &gt; self.size:\n                raise IndexError(f\"NumberedText: negative index out of range: {idx}\")\n            return self.end + idx  # convert to logical positive location for reference.\n        else:\n            raise IndexError(\"NumberedText: Index cannot be zero in 1-based indexing.\")\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the numbered text representation.\"\"\"\n        return \"\\n\".join(\n            self._format_line(i, line) for i, line in enumerate(self.lines, self.start)\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of lines.\"\"\"\n        return len(self.lines)\n\n    def __iter__(self) -&gt; Iterator[tuple[int, str]]:\n        \"\"\"Iterate over (line_number, line_content) pairs.\"\"\"\n        return iter((i, line) for i, line in enumerate(self.lines, self.start))\n\n    def __getitem__(self, index: int) -&gt; str:\n        \"\"\"Get line content by line number (1-based indexing).\"\"\"\n        return self.lines[self._to_internal_index(index)]\n\n    def get_line(self, line_num: int) -&gt; str:\n        \"\"\"Get content of specified line number.\"\"\"\n        return self[line_num]\n\n    def _to_line_index(self, internal_index: int) -&gt; int:\n        return self.start + self._to_internal_index(internal_index)\n\n    def get_numbered_line(self, line_num: int) -&gt; str:\n        \"\"\"Get specified line with line number.\"\"\"\n        idx = self._to_line_index(line_num)\n        return self._format_line(idx, self[idx])\n\n    def get_lines(self, start: int, end: int) -&gt; List[str]:\n        \"\"\"Get content of line range, not inclusive of end line.\"\"\"\n        return self.lines[self._to_internal_index(start) : self._to_internal_index(end)]\n\n    def get_numbered_lines(self, start: int, end: int) -&gt; List[str]:\n        return [\n            self._format_line(i + self._to_internal_index(start) + 1, line)\n            for i, line in enumerate(self.get_lines(start, end))\n        ]\n\n    def get_segment(self, start: int, end: int) -&gt; str:\n        if start &lt; self.start:\n            raise IndexError(f\"Start index {start} is before first line {self.start}\")\n        if end &gt; len(self) + 1:\n            raise IndexError(f\"End index {end} is past last line {len(self)}\")\n        if start &gt;= end:\n            raise IndexError(f\"Start index {start} must be less than end index {end}\")\n        return \"\\n\".join(self.get_lines(start, end))\n\n    def iter_segments(\n        self, segment_size: int, min_segment_size: Optional[int] = None\n    ) -&gt; Iterator[LineSegment]:\n        \"\"\"\n        Iterate over segments of the text with specified size.\n\n        Args:\n            segment_size: Number of lines per segment\n            min_segment_size: Optional minimum size for final segment.\n                If specified, last segment will be merged with previous one\n                if it would be smaller than this size.\n\n        Yields:\n            LineSegment objects containing start and end line numbers\n\n        Example:\n            &gt;&gt;&gt; text = NumberedText(\"line1\\\\nline2\\\\nline3\\\\nline4\\\\nline5\")\n            &gt;&gt;&gt; for segment in text.iter_segments(2):\n            ...     print(f\"Lines {segment.start}-{segment.end}\")\n            Lines 1-3\n            Lines 3-5\n            Lines 5-6\n        \"\"\"\n        iterator = self.SegmentIterator(\n            len(self), segment_size, self.start, min_segment_size\n        )\n        return iter(iterator)\n\n    def get_numbered_segment(self, start: int, end: int) -&gt; str:\n        return \"\\n\".join(self.get_numbered_lines(start, end))\n\n    def save(self, path: Path, numbered: bool = True) -&gt; None:\n        \"\"\"\n        Save document to file.\n\n        Args:\n            path: Output file path\n            numbered: Whether to save with line numbers (default: True)\n        \"\"\"\n        content = str(self) if numbered else \"\\n\".join(self.lines)\n        Path(path).write_text(content)\n\n    def append(self, text: str) -&gt; None:\n        \"\"\"Append text, splitting into lines if needed.\"\"\"\n        self.lines.extend(text.splitlines())\n\n    def insert(self, line_num: int, text: str) -&gt; None:\n        \"\"\"Insert text at specified line number. Assumes text is not empty.\"\"\"\n        new_lines = text.splitlines()\n        internal_idx = self._to_internal_index(line_num)\n        self.lines[internal_idx:internal_idx] = new_lines\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Get original text without line numbers.\"\"\"\n        return \"\\n\".join(self.lines)\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Get the number of lines.\"\"\"\n        return len(self.lines)\n\n    @property\n    def numbered_lines(self) -&gt; List[str]:\n        \"\"\"\n        Get list of lines with line numbers included.\n\n        Returns:\n            List[str]: Lines with numbers and separator prefixed\n\n        Examples:\n            &gt;&gt;&gt; doc = NumberedText(\"First line\\\\nSecond line\")\n            &gt;&gt;&gt; doc.numbered_lines\n            ['1: First line', '2: Second line']\n\n        Note:\n            - Unlike str(self), this returns a list rather than joined string\n            - Maintains consistent formatting with separator\n            - Useful for processing or displaying individual numbered lines\n        \"\"\"\n        return [\n            f\"{i}{self.separator}{line}\"\n            for i, line in enumerate(self.lines, self.start)\n        ]\n\n    @property\n    def end(self) -&gt; int:\n        return self.start + len(self.lines) - 1\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.content","title":"<code>content</code>  <code>property</code>","text":"<p>Get original text without line numbers.</p>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.end","title":"<code>end</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.lines","title":"<code>lines = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.numbered_lines","title":"<code>numbered_lines</code>  <code>property</code>","text":"<p>Get list of lines with line numbers included.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Lines with numbers and separator prefixed</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; doc = NumberedText(\"First line\\nSecond line\")\n&gt;&gt;&gt; doc.numbered_lines\n['1: First line', '2: Second line']\n</code></pre> Note <ul> <li>Unlike str(self), this returns a list rather than joined string</li> <li>Maintains consistent formatting with separator</li> <li>Useful for processing or displaying individual numbered lines</li> </ul>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.separator","title":"<code>separator = separator</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.size","title":"<code>size</code>  <code>property</code>","text":"<p>Get the number of lines.</p>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.start","title":"<code>start = start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.LineSegment","title":"<code>LineSegment</code>  <code>dataclass</code>","text":"<p>Represents a segment of lines with start and end indices in 1-based indexing.</p> <p>The segment follows Python range conventions where start is inclusive and end is exclusive. However, indexing is 1-based to match NumberedText.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>int</code> <p>Starting line number (inclusive, 1-based)</p> <code>end</code> <code>int</code> <p>Ending line number (exclusive, 1-based)</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>@dataclass\nclass LineSegment:\n    \"\"\"\n    Represents a segment of lines with start and end indices in 1-based indexing.\n\n    The segment follows Python range conventions where start is inclusive and\n    end is exclusive. However, indexing is 1-based to match NumberedText.\n\n    Attributes:\n        start: Starting line number (inclusive, 1-based)\n        end: Ending line number (exclusive, 1-based)\n    \"\"\"\n\n    start: int\n    end: int\n\n    def __iter__(self):\n        \"\"\"Allow unpacking into start, end pairs.\"\"\"\n        yield self.start\n        yield self.end\n</code></pre> <code>end</code> <code>instance-attribute</code> <code>start</code> <code>instance-attribute</code> <code>__init__(start, end)</code> <code>__iter__()</code> <p>Allow unpacking into start, end pairs.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self):\n    \"\"\"Allow unpacking into start, end pairs.\"\"\"\n    yield self.start\n    yield self.end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.SegmentIterator","title":"<code>SegmentIterator</code>","text":"<p>Iterator for generating line segments of specified size.</p> <p>Produces segments of lines with start/end indices following 1-based indexing. The final segment may be smaller than the specified segment size.</p> <p>Attributes:</p> Name Type Description <code>total_lines</code> <p>Total number of lines in text</p> <code>segment_size</code> <p>Number of lines per segment</p> <code>start_line</code> <p>Starting line number (1-based)</p> <code>min_segment_size</code> <p>Minimum size for the final segment</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>class SegmentIterator:\n    \"\"\"\n    Iterator for generating line segments of specified size.\n\n    Produces segments of lines with start/end indices following 1-based indexing.\n    The final segment may be smaller than the specified segment size.\n\n    Attributes:\n        total_lines: Total number of lines in text\n        segment_size: Number of lines per segment\n        start_line: Starting line number (1-based)\n        min_segment_size: Minimum size for the final segment\n    \"\"\"\n\n    def __init__(\n        self,\n        total_lines: int,\n        segment_size: int,\n        start_line: int = 1,\n        min_segment_size: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize the segment iterator.\n\n        Args:\n            total_lines: Total number of lines to iterate over\n            segment_size: Desired size of each segment\n            start_line: First line number (default: 1)\n            min_segment_size: Minimum size for final segment (default: None)\n                If specified, the last segment will be merged with the previous one\n                if it would be smaller than this size.\n\n        Raises:\n            ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n            ValueError: If start_line &lt; 1 (must use 1-based indexing)\n            ValueError: If min_segment_size &gt;= segment_size\n        \"\"\"\n        if segment_size &lt; 1:\n            raise ValueError(\"Segment size must be at least 1\")\n        if total_lines &lt; 1:\n            raise ValueError(\"Total lines must be at least 1\")\n        if start_line &lt; 1:\n            raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n        if min_segment_size is not None and min_segment_size &gt;= segment_size:\n            raise ValueError(\"Minimum segment size must be less than segment size\")\n\n        self.total_lines = total_lines\n        self.segment_size = segment_size\n        self.start_line = start_line\n        self.min_segment_size = min_segment_size\n\n        # Calculate number of segments\n        remaining_lines = total_lines - start_line + 1\n        self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n\n    def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n        \"\"\"\n        Iterate over line segments.\n\n        Yields:\n            LineSegment containing start (inclusive) and end (exclusive) indices\n        \"\"\"\n        current = self.start_line\n\n        for i in range(self.num_segments):\n            is_last_segment = i == self.num_segments - 1\n            segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n            # Handle minimum segment size for last segment\n            if (\n                is_last_segment\n                and self.min_segment_size is not None\n                and segment_end - current &lt; self.min_segment_size\n                and i &gt; 0\n            ):\n                # Merge with previous segment by not yielding\n                break\n\n            yield NumberedText.LineSegment(current, segment_end)\n            current = segment_end\n</code></pre> <code>min_segment_size = min_segment_size</code> <code>instance-attribute</code> <code>num_segments = remaining_lines + segment_size - 1 // segment_size</code> <code>instance-attribute</code> <code>segment_size = segment_size</code> <code>instance-attribute</code> <code>start_line = start_line</code> <code>instance-attribute</code> <code>total_lines = total_lines</code> <code>instance-attribute</code> <code>__init__(total_lines, segment_size, start_line=1, min_segment_size=None)</code> <p>Initialize the segment iterator.</p> <p>Parameters:</p> Name Type Description Default <code>total_lines</code> <code>int</code> <p>Total number of lines to iterate over</p> required <code>segment_size</code> <code>int</code> <p>Desired size of each segment</p> required <code>start_line</code> <code>int</code> <p>First line number (default: 1)</p> <code>1</code> <code>min_segment_size</code> <code>Optional[int]</code> <p>Minimum size for final segment (default: None) If specified, the last segment will be merged with the previous one if it would be smaller than this size.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If segment_size &lt; 1 or total_lines &lt; 1</p> <code>ValueError</code> <p>If start_line &lt; 1 (must use 1-based indexing)</p> <code>ValueError</code> <p>If min_segment_size &gt;= segment_size</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __init__(\n    self,\n    total_lines: int,\n    segment_size: int,\n    start_line: int = 1,\n    min_segment_size: Optional[int] = None,\n):\n    \"\"\"\n    Initialize the segment iterator.\n\n    Args:\n        total_lines: Total number of lines to iterate over\n        segment_size: Desired size of each segment\n        start_line: First line number (default: 1)\n        min_segment_size: Minimum size for final segment (default: None)\n            If specified, the last segment will be merged with the previous one\n            if it would be smaller than this size.\n\n    Raises:\n        ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n        ValueError: If start_line &lt; 1 (must use 1-based indexing)\n        ValueError: If min_segment_size &gt;= segment_size\n    \"\"\"\n    if segment_size &lt; 1:\n        raise ValueError(\"Segment size must be at least 1\")\n    if total_lines &lt; 1:\n        raise ValueError(\"Total lines must be at least 1\")\n    if start_line &lt; 1:\n        raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n    if min_segment_size is not None and min_segment_size &gt;= segment_size:\n        raise ValueError(\"Minimum segment size must be less than segment size\")\n\n    self.total_lines = total_lines\n    self.segment_size = segment_size\n    self.start_line = start_line\n    self.min_segment_size = min_segment_size\n\n    # Calculate number of segments\n    remaining_lines = total_lines - start_line + 1\n    self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n</code></pre> <code>__iter__()</code> <p>Iterate over line segments.</p> <p>Yields:</p> Type Description <code>LineSegment</code> <p>LineSegment containing start (inclusive) and end (exclusive) indices</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n    \"\"\"\n    Iterate over line segments.\n\n    Yields:\n        LineSegment containing start (inclusive) and end (exclusive) indices\n    \"\"\"\n    current = self.start_line\n\n    for i in range(self.num_segments):\n        is_last_segment = i == self.num_segments - 1\n        segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n        # Handle minimum segment size for last segment\n        if (\n            is_last_segment\n            and self.min_segment_size is not None\n            and segment_end - current &lt; self.min_segment_size\n            and i &gt; 0\n        ):\n            # Merge with previous segment by not yielding\n            break\n\n        yield NumberedText.LineSegment(current, segment_end)\n        current = segment_end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get line content by line number (1-based indexing).</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get line content by line number (1-based indexing).\"\"\"\n    return self.lines[self._to_internal_index(index)]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.__init__","title":"<code>__init__(content=None, start=1, separator=':')</code>","text":"<p>Initialize a numbered text document, detecting and preserving existing numbering.</p> <p>Valid numbered text must have: - Sequential line numbers - Consistent separator character(s) - Every non-empty line must follow the numbering pattern</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Optional[str]</code> <p>Initial text content, if any</p> <code>None</code> <code>start</code> <code>int</code> <p>Starting line number (used only if content isn't already numbered)</p> <code>1</code> <code>separator</code> <code>str</code> <p>Separator between line numbers and content (only if content isn't numbered)</p> <code>':'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Custom separators\n&gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\n2\u2192Second line\")\n&gt;&gt;&gt; doc.separator == \"\u2192\"\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Preserves starting number\n&gt;&gt;&gt; doc = NumberedText(\"5#First\\n6#Second\")\n&gt;&gt;&gt; doc.start == 5\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n&gt;&gt;&gt; doc = NumberedText(\"1. First item\\n2. Second item\")\n&gt;&gt;&gt; doc.numbered_lines\n['1: 1. First item', '2: 2. Second item']\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __init__(\n    self, content: Optional[str] = None, start: int = 1, separator: str = \":\"\n) -&gt; None:\n    \"\"\"\n    Initialize a numbered text document, detecting and preserving existing numbering.\n\n    Valid numbered text must have:\n    - Sequential line numbers\n    - Consistent separator character(s)\n    - Every non-empty line must follow the numbering pattern\n\n    Args:\n        content: Initial text content, if any\n        start: Starting line number (used only if content isn't already numbered)\n        separator: Separator between line numbers and content (only if content isn't numbered)\n\n    Examples:\n        &gt;&gt;&gt; # Custom separators\n        &gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\\\n2\u2192Second line\")\n        &gt;&gt;&gt; doc.separator == \"\u2192\"\n        True\n\n        &gt;&gt;&gt; # Preserves starting number\n        &gt;&gt;&gt; doc = NumberedText(\"5#First\\\\n6#Second\")\n        &gt;&gt;&gt; doc.start == 5\n        True\n\n        &gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n        &gt;&gt;&gt; doc = NumberedText(\"1. First item\\\\n2. Second item\")\n        &gt;&gt;&gt; doc.numbered_lines\n        ['1: 1. First item', '2: 2. Second item']\n    \"\"\"\n\n    self.lines: List[str] = []  # Declare lines here\n    self.start: int = start  # Declare start with its type\n    self.separator: str = separator  # and separator\n\n    if not isinstance(content, str):\n        raise ValueError(\"NumberedText requires string input.\")\n\n    if start &lt; 1:  # enforce 1 based indexing.\n        raise IndexError(\n            \"NumberedText: Numbered lines must begin on an integer great or equal to 1.\"\n        )\n\n    if not content:\n        return\n\n    # Analyze the text format\n    is_numbered, detected_sep, start_num = get_numbered_format(content)\n\n    format_info = get_numbered_format(content)\n\n    if format_info.is_numbered:\n        self.start = format_info.start_num  # type: ignore\n        self.separator = format_info.separator  # type: ignore\n\n        # Extract content by removing number and separator\n        pattern = re.compile(rf\"^\\d+{re.escape(detected_sep)}\")\n        self.lines = []\n\n        for line in content.splitlines():\n            if line.strip():\n                self.lines.append(pattern.sub(\"\", line))\n            else:\n                self.lines.append(line)\n    else:\n        self.lines = content.splitlines()\n        self.start = start\n        self.separator = separator\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over (line_number, line_content) pairs.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self) -&gt; Iterator[tuple[int, str]]:\n    \"\"\"Iterate over (line_number, line_content) pairs.\"\"\"\n    return iter((i, line) for i, line in enumerate(self.lines, self.start))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of lines.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of lines.\"\"\"\n    return len(self.lines)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.__str__","title":"<code>__str__()</code>","text":"<p>Return the numbered text representation.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the numbered text representation.\"\"\"\n    return \"\\n\".join(\n        self._format_line(i, line) for i, line in enumerate(self.lines, self.start)\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.append","title":"<code>append(text)</code>","text":"<p>Append text, splitting into lines if needed.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def append(self, text: str) -&gt; None:\n    \"\"\"Append text, splitting into lines if needed.\"\"\"\n    self.lines.extend(text.splitlines())\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.from_file","title":"<code>from_file(path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a NumberedText instance from a file.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>@classmethod\ndef from_file(cls, path: Path, **kwargs) -&gt; \"NumberedText\":\n    \"\"\"Create a NumberedText instance from a file.\"\"\"\n    return cls(Path(path).read_text(), **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_line","title":"<code>get_line(line_num)</code>","text":"<p>Get content of specified line number.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_line(self, line_num: int) -&gt; str:\n    \"\"\"Get content of specified line number.\"\"\"\n    return self[line_num]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_lines","title":"<code>get_lines(start, end)</code>","text":"<p>Get content of line range, not inclusive of end line.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_lines(self, start: int, end: int) -&gt; List[str]:\n    \"\"\"Get content of line range, not inclusive of end line.\"\"\"\n    return self.lines[self._to_internal_index(start) : self._to_internal_index(end)]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_numbered_line","title":"<code>get_numbered_line(line_num)</code>","text":"<p>Get specified line with line number.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_line(self, line_num: int) -&gt; str:\n    \"\"\"Get specified line with line number.\"\"\"\n    idx = self._to_line_index(line_num)\n    return self._format_line(idx, self[idx])\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_numbered_lines","title":"<code>get_numbered_lines(start, end)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_lines(self, start: int, end: int) -&gt; List[str]:\n    return [\n        self._format_line(i + self._to_internal_index(start) + 1, line)\n        for i, line in enumerate(self.get_lines(start, end))\n    ]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_numbered_segment","title":"<code>get_numbered_segment(start, end)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_segment(self, start: int, end: int) -&gt; str:\n    return \"\\n\".join(self.get_numbered_lines(start, end))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_segment","title":"<code>get_segment(start, end)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_segment(self, start: int, end: int) -&gt; str:\n    if start &lt; self.start:\n        raise IndexError(f\"Start index {start} is before first line {self.start}\")\n    if end &gt; len(self) + 1:\n        raise IndexError(f\"End index {end} is past last line {len(self)}\")\n    if start &gt;= end:\n        raise IndexError(f\"Start index {start} must be less than end index {end}\")\n    return \"\\n\".join(self.get_lines(start, end))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.insert","title":"<code>insert(line_num, text)</code>","text":"<p>Insert text at specified line number. Assumes text is not empty.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def insert(self, line_num: int, text: str) -&gt; None:\n    \"\"\"Insert text at specified line number. Assumes text is not empty.\"\"\"\n    new_lines = text.splitlines()\n    internal_idx = self._to_internal_index(line_num)\n    self.lines[internal_idx:internal_idx] = new_lines\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.iter_segments","title":"<code>iter_segments(segment_size, min_segment_size=None)</code>","text":"<p>Iterate over segments of the text with specified size.</p> <p>Parameters:</p> Name Type Description Default <code>segment_size</code> <code>int</code> <p>Number of lines per segment</p> required <code>min_segment_size</code> <code>Optional[int]</code> <p>Optional minimum size for final segment. If specified, last segment will be merged with previous one if it would be smaller than this size.</p> <code>None</code> <p>Yields:</p> Type Description <code>LineSegment</code> <p>LineSegment objects containing start and end line numbers</p> Example <p>text = NumberedText(\"line1\\nline2\\nline3\\nline4\\nline5\") for segment in text.iter_segments(2): ...     print(f\"Lines {segment.start}-{segment.end}\") Lines 1-3 Lines 3-5 Lines 5-6</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def iter_segments(\n    self, segment_size: int, min_segment_size: Optional[int] = None\n) -&gt; Iterator[LineSegment]:\n    \"\"\"\n    Iterate over segments of the text with specified size.\n\n    Args:\n        segment_size: Number of lines per segment\n        min_segment_size: Optional minimum size for final segment.\n            If specified, last segment will be merged with previous one\n            if it would be smaller than this size.\n\n    Yields:\n        LineSegment objects containing start and end line numbers\n\n    Example:\n        &gt;&gt;&gt; text = NumberedText(\"line1\\\\nline2\\\\nline3\\\\nline4\\\\nline5\")\n        &gt;&gt;&gt; for segment in text.iter_segments(2):\n        ...     print(f\"Lines {segment.start}-{segment.end}\")\n        Lines 1-3\n        Lines 3-5\n        Lines 5-6\n    \"\"\"\n    iterator = self.SegmentIterator(\n        len(self), segment_size, self.start, min_segment_size\n    )\n    return iter(iterator)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.save","title":"<code>save(path, numbered=True)</code>","text":"<p>Save document to file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output file path</p> required <code>numbered</code> <code>bool</code> <p>Whether to save with line numbers (default: True)</p> <code>True</code> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def save(self, path: Path, numbered: bool = True) -&gt; None:\n    \"\"\"\n    Save document to file.\n\n    Args:\n        path: Output file path\n        numbered: Whether to save with line numbers (default: True)\n    \"\"\"\n    content = str(self) if numbered else \"\\n\".join(self.lines)\n    Path(path).write_text(content)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.get_numbered_format","title":"<code>get_numbered_format(text)</code>","text":"<p>Analyze text to determine if it follows a consistent line numbering format.</p> <p>Valid formats have: - Sequential numbers starting from some value - Consistent separator character(s) - Every line must follow the format</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to analyze</p> required <p>Returns:</p> Type Description <code>NumberedFormat</code> <p>Tuple of (is_numbered, separator, start_number)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; _analyze_numbered_format(\"1\u2192First\\n2\u2192Second\")\n(True, \"\u2192\", 1)\n&gt;&gt;&gt; _analyze_numbered_format(\"1. First\")  # Numbered list format\n(False, None, None)\n&gt;&gt;&gt; _analyze_numbered_format(\"5#Line\\n6#Other\")\n(True, \"#\", 5)\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_format(text: str) -&gt; NumberedFormat:\n    \"\"\"\n    Analyze text to determine if it follows a consistent line numbering format.\n\n    Valid formats have:\n    - Sequential numbers starting from some value\n    - Consistent separator character(s)\n    - Every line must follow the format\n\n    Args:\n        text: Text to analyze\n\n    Returns:\n        Tuple of (is_numbered, separator, start_number)\n\n    Examples:\n        &gt;&gt;&gt; _analyze_numbered_format(\"1\u2192First\\\\n2\u2192Second\")\n        (True, \"\u2192\", 1)\n        &gt;&gt;&gt; _analyze_numbered_format(\"1. First\")  # Numbered list format\n        (False, None, None)\n        &gt;&gt;&gt; _analyze_numbered_format(\"5#Line\\\\n6#Other\")\n        (True, \"#\", 5)\n    \"\"\"\n    if not text.strip():\n        return NumberedFormat(False)\n\n    lines = [line for line in text.splitlines() if line.strip()]\n    if not lines:\n        return NumberedFormat(False)\n\n    # Try to detect pattern from first line\n    SEPARATOR_PATTERN = r\"[^\\w\\s.]\"  # not (word char or whitespace or period)\n    first_match = re.match(rf\"^(\\d+)({SEPARATOR_PATTERN})(.*?)$\", lines[0])\n\n    try:\n        return _check_line_structure(first_match, lines)\n    except (ValueError, AttributeError):\n        return NumberedFormat(False)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.simple_section","title":"<code>simple_section</code>","text":""},{"location":"api/#tnh_scholar.text_processing.simple_section.MatchObject","title":"<code>MatchObject</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Basic Match Object definition.</p> Source code in <code>src/tnh_scholar/text_processing/simple_section.py</code> <pre><code>class MatchObject(BaseModel):\n    \"\"\"Basic Match Object definition.\"\"\"\n    type: str\n    level: Optional[int] = None\n    words: Optional[List[str]] = None\n    case_sensitive: Optional[bool] = False\n    decorator: Optional[str] = None\n    pattern: Optional[str] = None\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.simple_section.MatchObject.case_sensitive","title":"<code>case_sensitive = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.simple_section.MatchObject.decorator","title":"<code>decorator = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.simple_section.MatchObject.level","title":"<code>level = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.simple_section.MatchObject.pattern","title":"<code>pattern = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.simple_section.MatchObject.type","title":"<code>type</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.simple_section.MatchObject.words","title":"<code>words = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.simple_section.SectionConfig","title":"<code>SectionConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for section detection.</p> Source code in <code>src/tnh_scholar/text_processing/simple_section.py</code> <pre><code>class SectionConfig(BaseModel):\n    \"\"\"Configuration for section detection.\"\"\"\n    name: str\n    description: Optional[str] = None\n    patterns: List[MatchObject]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.simple_section.SectionConfig.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.simple_section.SectionConfig.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.simple_section.SectionConfig.patterns","title":"<code>patterns</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.simple_section.create_text_object","title":"<code>create_text_object(text, boundaries)</code>","text":"<p>Create TextObject from text and section boundaries.</p> Source code in <code>src/tnh_scholar/text_processing/simple_section.py</code> <pre><code>def create_text_object(text: str, boundaries: List[int]) -&gt; TextObject:\n    \"\"\"Create TextObject from text and section boundaries.\"\"\"\n    lines = text.splitlines()\n    sections = []\n\n    # Handle first section starting after line 1\n    if not boundaries or boundaries[0] != 1:\n        boundaries.insert(0, 1)\n\n    # Create sections from boundaries\n    for i in range(len(boundaries)):\n        start = boundaries[i]\n        end = boundaries[i + 1] - 1 if i + 1 &lt; len(boundaries) else len(lines)\n\n        # Get section title from first line\n        title = lines[start - 1].strip()\n\n        section = LogicalSection(\n            title=title,\n            start_line=start,\n            end_line=end\n        )\n        sections.append(section)\n\n    return TextObject(\n        language=\"en\",  # Default to English for PoC\n        sections=sections\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.simple_section.find_keyword","title":"<code>find_keyword(line, words, case_sensitive, decorator)</code>","text":"<p>Check if line matches keyword pattern.</p> Source code in <code>src/tnh_scholar/text_processing/simple_section.py</code> <pre><code>def find_keyword(line: str, words: List[str], case_sensitive: bool, decorator: Optional[str]) -&gt; bool:\n    \"\"\"Check if line matches keyword pattern.\"\"\"\n    if not case_sensitive:\n        line = line.lower()\n        words = [w.lower() for w in words]\n\n    # Check if line starts with any keyword\n    if not any(line.lstrip().startswith(word) for word in words):\n        return False\n\n    # If decorator specified, check if it appears in line\n    return not decorator or decorator in line\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.simple_section.find_markdown_header","title":"<code>find_markdown_header(line, level)</code>","text":"<p>Check if line matches markdown header pattern.</p> Source code in <code>src/tnh_scholar/text_processing/simple_section.py</code> <pre><code>def find_markdown_header(line: str, level: int) -&gt; bool:\n    \"\"\"Check if line matches markdown header pattern.\"\"\"\n    stripped = line.lstrip()\n    return stripped.startswith('#' * level + ' ')\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.simple_section.find_regex","title":"<code>find_regex(line, pattern)</code>","text":"<p>Check if line matches regex pattern.</p> Source code in <code>src/tnh_scholar/text_processing/simple_section.py</code> <pre><code>def find_regex(line: str, pattern: str) -&gt; bool:\n    \"\"\"Check if line matches regex pattern.\"\"\"\n    return bool(re.match(pattern, line))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.simple_section.find_section_boundaries","title":"<code>find_section_boundaries(text, config)</code>","text":"<p>Find all section boundary line numbers.</p> Source code in <code>src/tnh_scholar/text_processing/simple_section.py</code> <pre><code>def find_section_boundaries(text: str, config: SectionConfig) -&gt; List[int]:\n    \"\"\"Find all section boundary line numbers.\"\"\"\n    boundaries = []\n\n    for i, line in enumerate(text.splitlines(), 1):\n        for pattern in config.patterns:\n            matched = False\n\n            if pattern.type == \"markdown_header\" and pattern.level:\n                matched = find_markdown_header(line, pattern.level)\n\n            elif pattern.type == \"keyword\" and pattern.words:\n                matched = find_keyword(\n                    line, \n                    pattern.words,\n                    pattern.case_sensitive or False,\n                    pattern.decorator\n                )\n\n            elif pattern.type == \"regex\" and pattern.pattern:\n                matched = find_regex(line, pattern.pattern)\n\n            if matched:\n                boundaries.append(i)\n                break  # Stop checking patterns if we found a match\n\n    return boundaries\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object","title":"<code>text_object</code>","text":"<p>Text object system for managing sectioned content with metadata.</p> <p>This module provides the core TextObject implementation, handling both internal representation and API interactions. It uses Dublin Core for metadata standards and provides a simplified format for AI service integration.</p>"},{"location":"api/#tnh_scholar.text_processing.text_object.LogicalSection","title":"<code>LogicalSection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a logical division of text content.</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>class LogicalSection(BaseModel):\n    \"\"\"Represents a logical division of text content.\"\"\"\n\n    start_line: int = Field(..., description=\"Starting line number of section (inclusive)\")\n    title: str = Field(..., description=\"Title describing section content\")\n\n    def __lt__(self, other: \"LogicalSection\") -&gt; bool:\n        \"\"\"Enable sorting by start line.\"\"\"\n        return self.start_line &lt; other.start_line\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.LogicalSection.start_line","title":"<code>start_line = Field(..., description='Starting line number of section (inclusive)')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.LogicalSection.title","title":"<code>title = Field(..., description='Title describing section content')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.LogicalSection.__lt__","title":"<code>__lt__(other)</code>","text":"<p>Enable sorting by start line.</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>def __lt__(self, other: \"LogicalSection\") -&gt; bool:\n    \"\"\"Enable sorting by start line.\"\"\"\n    return self.start_line &lt; other.start_line\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata","title":"<code>TextMetadata</code>  <code>dataclass</code>","text":"<p>Rich metadata container following Dublin Core standards.</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>@dataclass\nclass TextMetadata:\n    \"\"\"Rich metadata container following Dublin Core standards.\"\"\"\n\n    # Core Dublin Core elements\n    title: str\n    creator: List[str]\n    subject: List[str]\n    description: str\n    publisher: Optional[str] = None\n    contributor: List[str] = field(default_factory=list)\n    date: Optional[str] = None\n    type: str = \"Text\"\n    format: str = \"text/plain\"\n    identifier: Optional[str] = None\n    source: Optional[str] = None\n    language: str = \"en\"\n\n    # Additional contextual information\n    context: str = field(default=\"\")\n    additional_info: Dict[str, Any] = field(default_factory=dict)\n\n    def to_string(self) -&gt; str:\n        \"\"\"Convert metadata to human-readable string format.\"\"\"\n        parts = []\n\n        # Add core elements if present\n        if self.title:\n            parts.append(f\"Title: {self.title}\")\n        if self.creator:\n            parts.append(f\"Creator(s): {', '.join(self.creator)}\")\n        if self.subject:\n            parts.append(f\"Subject(s): {', '.join(self.subject)}\")\n        if self.description:\n            parts.append(f\"Description: {self.description}\")\n        if self.publisher:\n            parts.append(f\"Publisher: {self.publisher}\")\n        if self.contributor:\n            parts.append(f\"Contributor(s): {', '.join(self.contributor)}\")\n        if self.date:\n            parts.append(f\"Date: {self.date}\")\n\n        parts.extend((f\"Type: {self.type}\", f\"Format: {self.format}\"))\n\n        if self.identifier:\n            parts.append(f\"Identifier: {self.identifier}\")\n        if self.source:\n            parts.append(f\"Source: {self.source}\")\n\n        parts.append(f\"Language: {self.language}\")\n\n        return \"\\n\".join(parts)\n\n    @classmethod\n    def from_string(cls, metadata_str: str, context: str = \"\") -&gt; \"TextMetadata\":\n        \"\"\"Parse metadata from string representation.\"\"\"\n        fields: Dict[str, Any] = {\n            \"creator\": [],\n            \"subject\": [],\n            \"contributor\": [],\n            \"context\": context\n        }\n\n        for line in metadata_str.splitlines():\n            if \":\" not in line:\n                continue\n\n            key, value = line.split(\":\", 1)\n            key = key.strip().lower()\n            value = value.strip()\n\n            if key in [\"creator(s)\", \"creator\"]:\n                fields[\"creator\"] = [c.strip() for c in value.split(\",\")]\n            elif key == \"subject(s)\" or key == \"subject\":\n                fields[\"subject\"] = [s.strip() for s in value.split(\",\")]\n            elif key == \"contributor(s)\" or key == \"contributor\":\n                fields[\"contributor\"] = [c.strip() for c in value.split(\",\")]\n            else:\n                # Convert key to match dataclass field names\n                key = key.replace(\"(s)\", \"\")\n                if key in cls.__dataclass_fields__:\n                    fields[key] = value\n\n        return cls(**fields)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.additional_info","title":"<code>additional_info = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.context","title":"<code>context = field(default='')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.contributor","title":"<code>contributor = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.creator","title":"<code>creator</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.date","title":"<code>date = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.description","title":"<code>description</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.format","title":"<code>format = 'text/plain'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.identifier","title":"<code>identifier = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.language","title":"<code>language = 'en'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.publisher","title":"<code>publisher = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.source","title":"<code>source = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.subject","title":"<code>subject</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.title","title":"<code>title</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.type","title":"<code>type = 'Text'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.__init__","title":"<code>__init__(title, creator, subject, description, publisher=None, contributor=list(), date=None, type='Text', format='text/plain', identifier=None, source=None, language='en', context='', additional_info=dict())</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.from_string","title":"<code>from_string(metadata_str, context='')</code>  <code>classmethod</code>","text":"<p>Parse metadata from string representation.</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_string(cls, metadata_str: str, context: str = \"\") -&gt; \"TextMetadata\":\n    \"\"\"Parse metadata from string representation.\"\"\"\n    fields: Dict[str, Any] = {\n        \"creator\": [],\n        \"subject\": [],\n        \"contributor\": [],\n        \"context\": context\n    }\n\n    for line in metadata_str.splitlines():\n        if \":\" not in line:\n            continue\n\n        key, value = line.split(\":\", 1)\n        key = key.strip().lower()\n        value = value.strip()\n\n        if key in [\"creator(s)\", \"creator\"]:\n            fields[\"creator\"] = [c.strip() for c in value.split(\",\")]\n        elif key == \"subject(s)\" or key == \"subject\":\n            fields[\"subject\"] = [s.strip() for s in value.split(\",\")]\n        elif key == \"contributor(s)\" or key == \"contributor\":\n            fields[\"contributor\"] = [c.strip() for c in value.split(\",\")]\n        else:\n            # Convert key to match dataclass field names\n            key = key.replace(\"(s)\", \"\")\n            if key in cls.__dataclass_fields__:\n                fields[key] = value\n\n    return cls(**fields)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadata.to_string","title":"<code>to_string()</code>","text":"<p>Convert metadata to human-readable string format.</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"Convert metadata to human-readable string format.\"\"\"\n    parts = []\n\n    # Add core elements if present\n    if self.title:\n        parts.append(f\"Title: {self.title}\")\n    if self.creator:\n        parts.append(f\"Creator(s): {', '.join(self.creator)}\")\n    if self.subject:\n        parts.append(f\"Subject(s): {', '.join(self.subject)}\")\n    if self.description:\n        parts.append(f\"Description: {self.description}\")\n    if self.publisher:\n        parts.append(f\"Publisher: {self.publisher}\")\n    if self.contributor:\n        parts.append(f\"Contributor(s): {', '.join(self.contributor)}\")\n    if self.date:\n        parts.append(f\"Date: {self.date}\")\n\n    parts.extend((f\"Type: {self.type}\", f\"Format: {self.format}\"))\n\n    if self.identifier:\n        parts.append(f\"Identifier: {self.identifier}\")\n    if self.source:\n        parts.append(f\"Source: {self.source}\")\n\n    parts.append(f\"Language: {self.language}\")\n\n    return \"\\n\".join(parts)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadataFormat","title":"<code>TextMetadataFormat</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Simplified metadata format optimized for AI processing.</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>class TextMetadataFormat(BaseModel):\n    \"\"\"Simplified metadata format optimized for AI processing.\"\"\"\n\n    metadata_summary: str = Field(\n        ..., \n        description=\"Available metadata in human-readable format\"\n    )\n    context: str = Field(\n        ...,\n        description=\"Rich contextual information for AI understanding\"\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadataFormat.context","title":"<code>context = Field(..., description='Rich contextual information for AI understanding')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextMetadataFormat.metadata_summary","title":"<code>metadata_summary = Field(..., description='Available metadata in human-readable format')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject","title":"<code>TextObject</code>","text":"<p>Main class for managing sectioned text content with metadata.</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>class TextObject:\n    \"\"\"Main class for managing sectioned text content with metadata.\"\"\"\n\n    def __init__(\n        self,\n        numbered_text: NumberedText,\n        language: str,\n        sections: List[LogicalSection],\n        metadata: TextMetadata\n    ) -&gt; None:\n        \"\"\"\n        Initialize TextObject with content and metadata.\n\n        Args:\n            numbered_text: Text content with line numbering\n            language: ISO 639-1 language code\n            sections: List of logical sections\n            metadata: Dublin Core metadata\n\n        Raises:\n            ValueError: If sections are invalid or text is empty\n        \"\"\"\n        self.content = numbered_text\n        self.language = get_language_code(language)\n        self.sections = sorted(sections)\n        self.metadata = metadata\n        self.total_lines = numbered_text.size\n        self._validate()\n\n    def _validate(self) -&gt; None:\n        \"\"\"\n        Validate section integrity.\n\n        Raises:\n            ValueError: If sections are invalid\n        \"\"\"\n        if not self.sections:\n            raise ValueError(\"TextObject must have at least one section\")\n\n        # Validate section ordering\n        for i, section in enumerate(self.sections):\n            if section.start_line &lt; 1:\n                raise ValueError(f\"Section {i}: start line must be &gt;= 1\")\n            if section.start_line &gt; self.total_lines:\n                raise ValueError(f\"Section {i}: start line exceeds text length\")\n            if i &gt; 0 and section.start_line &lt;= self.sections[i-1].start_line:\n                raise ValueError(f\"Section {i}: non-sequential start line\")\n\n    def get_section_content(self, index: int) -&gt; str:\n        \"\"\"\n        Retrieve content for specific section.\n\n        Args:\n            index: Section index\n\n        Returns:\n            Text content for the section\n\n        Raises:\n            IndexError: If index is out of range\n        \"\"\"\n        if not 0 &lt;= index &lt; len(self.sections):\n            raise IndexError(f\"Section index {index} out of range\")\n\n        start = self.sections[index].start_line\n        end = (self.sections[index + 1].start_line \n               if index &lt; len(self.sections) - 1 \n               else self.total_lines + 1)\n\n        return self.content.get_segment(start, end)\n\n    def to_response_format(self) -&gt; TextObjectFormat:\n        \"\"\"\n        Convert to API format.\n\n        Returns:\n            TextObjectFormat for API interaction\n        \"\"\"\n        metadata_summary = self.metadata.to_string()\n\n        return TextObjectFormat(\n            metadata=TextMetadataFormat(\n                metadata_summary=metadata_summary,\n                context=self.metadata.context\n            ),\n            language=self.language,\n            sections=self.sections\n        )\n\n    @classmethod\n    def from_response_format(\n        cls, \n        text: NumberedText,\n        response: TextObjectFormat\n    ) -&gt; \"TextObject\":\n        \"\"\"\n        Create from API response.\n\n        Args:\n            text: Text content\n            response: API response format\n\n        Returns:\n            New TextObject instance\n        \"\"\"\n        metadata = TextMetadata.from_string(\n            response.metadata.metadata_summary,\n            response.metadata.context\n        )\n\n        return cls(\n            numbered_text=text,\n            language=response.language,\n            sections=response.sections,\n            metadata=metadata\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of sections.\"\"\"\n        return len(self.sections)\n\n    def __iter__(self):\n        \"\"\"Iterate over sections.\"\"\"\n        return iter(self.sections)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.content","title":"<code>content = numbered_text</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.language","title":"<code>language = get_language_code(language)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.metadata","title":"<code>metadata = metadata</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.sections","title":"<code>sections = sorted(sections)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.total_lines","title":"<code>total_lines = numbered_text.size</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.__init__","title":"<code>__init__(numbered_text, language, sections, metadata)</code>","text":"<p>Initialize TextObject with content and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>numbered_text</code> <code>NumberedText</code> <p>Text content with line numbering</p> required <code>language</code> <code>str</code> <p>ISO 639-1 language code</p> required <code>sections</code> <code>List[LogicalSection]</code> <p>List of logical sections</p> required <code>metadata</code> <code>TextMetadata</code> <p>Dublin Core metadata</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If sections are invalid or text is empty</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>def __init__(\n    self,\n    numbered_text: NumberedText,\n    language: str,\n    sections: List[LogicalSection],\n    metadata: TextMetadata\n) -&gt; None:\n    \"\"\"\n    Initialize TextObject with content and metadata.\n\n    Args:\n        numbered_text: Text content with line numbering\n        language: ISO 639-1 language code\n        sections: List of logical sections\n        metadata: Dublin Core metadata\n\n    Raises:\n        ValueError: If sections are invalid or text is empty\n    \"\"\"\n    self.content = numbered_text\n    self.language = get_language_code(language)\n    self.sections = sorted(sections)\n    self.metadata = metadata\n    self.total_lines = numbered_text.size\n    self._validate()\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over sections.</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over sections.\"\"\"\n    return iter(self.sections)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.__len__","title":"<code>__len__()</code>","text":"<p>Return number of sections.</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of sections.\"\"\"\n    return len(self.sections)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.from_response_format","title":"<code>from_response_format(text, response)</code>  <code>classmethod</code>","text":"<p>Create from API response.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>NumberedText</code> <p>Text content</p> required <code>response</code> <code>TextObjectFormat</code> <p>API response format</p> required <p>Returns:</p> Type Description <code>TextObject</code> <p>New TextObject instance</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_response_format(\n    cls, \n    text: NumberedText,\n    response: TextObjectFormat\n) -&gt; \"TextObject\":\n    \"\"\"\n    Create from API response.\n\n    Args:\n        text: Text content\n        response: API response format\n\n    Returns:\n        New TextObject instance\n    \"\"\"\n    metadata = TextMetadata.from_string(\n        response.metadata.metadata_summary,\n        response.metadata.context\n    )\n\n    return cls(\n        numbered_text=text,\n        language=response.language,\n        sections=response.sections,\n        metadata=metadata\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.get_section_content","title":"<code>get_section_content(index)</code>","text":"<p>Retrieve content for specific section.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Section index</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text content for the section</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index is out of range</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>def get_section_content(self, index: int) -&gt; str:\n    \"\"\"\n    Retrieve content for specific section.\n\n    Args:\n        index: Section index\n\n    Returns:\n        Text content for the section\n\n    Raises:\n        IndexError: If index is out of range\n    \"\"\"\n    if not 0 &lt;= index &lt; len(self.sections):\n        raise IndexError(f\"Section index {index} out of range\")\n\n    start = self.sections[index].start_line\n    end = (self.sections[index + 1].start_line \n           if index &lt; len(self.sections) - 1 \n           else self.total_lines + 1)\n\n    return self.content.get_segment(start, end)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextObject.to_response_format","title":"<code>to_response_format()</code>","text":"<p>Convert to API format.</p> <p>Returns:</p> Type Description <code>TextObjectFormat</code> <p>TextObjectFormat for API interaction</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>def to_response_format(self) -&gt; TextObjectFormat:\n    \"\"\"\n    Convert to API format.\n\n    Returns:\n        TextObjectFormat for API interaction\n    \"\"\"\n    metadata_summary = self.metadata.to_string()\n\n    return TextObjectFormat(\n        metadata=TextMetadataFormat(\n            metadata_summary=metadata_summary,\n            context=self.metadata.context\n        ),\n        language=self.language,\n        sections=self.sections\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextObjectFormat","title":"<code>TextObjectFormat</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete format for API interactions.</p> Source code in <code>src/tnh_scholar/text_processing/text_object.py</code> <pre><code>class TextObjectFormat(BaseModel):\n    \"\"\"Complete format for API interactions.\"\"\"\n\n    metadata: TextMetadataFormat\n    language: str = Field(..., description=\"ISO 639-1 language code\")\n    sections: List[LogicalSection]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_object.TextObjectFormat.language","title":"<code>language = Field(..., description='ISO 639-1 language code')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextObjectFormat.metadata","title":"<code>metadata</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_object.TextObjectFormat.sections","title":"<code>sections</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_processing","title":"<code>text_processing</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_processing.clean_text","title":"<code>clean_text(text, newline=False)</code>","text":"<p>Cleans a given text by replacing specific unwanted characters such as tab, and non-breaking spaces with regular spaces.</p> <p>This function takes a string as input and applies replacements based on a predefined mapping of characters to replace.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be cleaned.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The cleaned text with unwanted characters replaced by spaces.</p> Example <p>text = \"This is\\n an example\\ttext with\\xa0extra spaces.\" clean_text(text) 'This is an example text with extra spaces.'</p> Source code in <code>src/tnh_scholar/text_processing/text_processing.py</code> <pre><code>def clean_text(text, newline=False):\n    \"\"\"\n    Cleans a given text by replacing specific unwanted characters such as\n    tab, and non-breaking spaces with regular spaces.\n\n    This function takes a string as input and applies replacements\n    based on a predefined mapping of characters to replace.\n\n    Args:\n        text (str): The text to be cleaned.\n\n    Returns:\n        str: The cleaned text with unwanted characters replaced by spaces.\n\n    Example:\n        &gt;&gt;&gt; text = \"This is\\\\n an example\\\\ttext with\\\\xa0extra spaces.\"\n        &gt;&gt;&gt; clean_text(text)\n        'This is an example text with extra spaces.'\n\n    \"\"\"\n    # Define a mapping of characters to replace\n    replace_map = {\n        \"\\t\": \" \",  # Replace tabs with space\n        \"\\xa0\": \" \",  # Replace non-breaking space with regular space\n        # Add more replacements as needed\n    }\n\n    if newline:\n        replace_map[\"\\n\"] = \"\"  # remove newlines\n\n    # Loop through the replace map and replace each character\n    for old_char, new_char in replace_map.items():\n        text = text.replace(old_char, new_char)\n\n    return text.strip()  # Ensure any leading/trailing spaces are removed\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_processing.normalize_newlines","title":"<code>normalize_newlines(text, spacing=2)</code>","text":"<pre><code>Normalize newline blocks in the input text by reducing consecutive newlines\nto the specified number of newlines for consistent readability and formatting.\n\nParameters:\n----------\ntext : str\n    The input text containing inconsistent newline spacing.\nspacing : int, optional\n    The number of newlines to insert between lines. Defaults to 2.\n\nReturns:\n-------\nstr\n    The text with consecutive newlines reduced to the specified number of newlines.\n\nExample:\n--------\n&gt;&gt;&gt; raw_text = \"Heading\n</code></pre> <p>Paragraph text 1 Paragraph text 2</p> <p>\"     &gt;&gt;&gt; normalize_newlines(raw_text, spacing=2)     'Heading</p> <p>Paragraph text 1</p> <p>Paragraph text 2</p> <p>'</p> Source code in <code>src/tnh_scholar/text_processing/text_processing.py</code> <pre><code>def normalize_newlines(text: str, spacing: int = 2) -&gt; str:\n    \"\"\"\n    Normalize newline blocks in the input text by reducing consecutive newlines\n    to the specified number of newlines for consistent readability and formatting.\n\n    Parameters:\n    ----------\n    text : str\n        The input text containing inconsistent newline spacing.\n    spacing : int, optional\n        The number of newlines to insert between lines. Defaults to 2.\n\n    Returns:\n    -------\n    str\n        The text with consecutive newlines reduced to the specified number of newlines.\n\n    Example:\n    --------\n    &gt;&gt;&gt; raw_text = \"Heading\\n\\n\\nParagraph text 1\\nParagraph text 2\\n\\n\\n\"\n    &gt;&gt;&gt; normalize_newlines(raw_text, spacing=2)\n    'Heading\\n\\nParagraph text 1\\n\\nParagraph text 2\\n\\n'\n    \"\"\"\n    # Replace one or more newlines with the desired number of newlines\n    newlines = \"\\n\" * spacing\n    return re.sub(r\"\\n{1,}\", newlines, text)\n</code></pre>"},{"location":"api/#tnh_scholar.utils","title":"<code>utils</code>","text":""},{"location":"api/#tnh_scholar.utils.file_utils","title":"<code>file_utils</code>","text":""},{"location":"api/#tnh_scholar.utils.file_utils.FileExistsWarning","title":"<code>FileExistsWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>class FileExistsWarning(UserWarning):\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.copy_files_with_regex","title":"<code>copy_files_with_regex(source_dir, destination_dir, regex_patterns, preserve_structure=True)</code>","text":"<p>Copies files from subdirectories one level down in the source directory to the destination directory if they match any regex pattern. Optionally preserves the directory structure.</p> <p>Parameters:</p> Name Type Description Default <code>source_dir</code> <code>Path</code> <p>Path to the source directory to search files in.</p> required <code>destination_dir</code> <code>Path</code> <p>Path to the destination directory where files will be copied.</p> required <code>regex_patterns</code> <code>list[str]</code> <p>List of regex patterns to match file names.</p> required <code>preserve_structure</code> <code>bool</code> <p>Whether to preserve the directory structure. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source directory does not exist or is not a directory.</p> Example <p>copy_files_with_regex( ...     source_dir=Path(\"/path/to/source\"), ...     destination_dir=Path(\"/path/to/destination\"), ...     regex_patterns=[r'..txt$', r'..log$'], ...     preserve_structure=True ... )</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def copy_files_with_regex(\n    source_dir: Path,\n    destination_dir: Path,\n    regex_patterns: list[str],\n    preserve_structure: bool = True,\n) -&gt; None:\n    \"\"\"\n    Copies files from subdirectories one level down in the source directory to the destination directory\n    if they match any regex pattern. Optionally preserves the directory structure.\n\n    Args:\n        source_dir (Path): Path to the source directory to search files in.\n        destination_dir (Path): Path to the destination directory where files will be copied.\n        regex_patterns (list[str]): List of regex patterns to match file names.\n        preserve_structure (bool): Whether to preserve the directory structure. Defaults to True.\n\n    Raises:\n        ValueError: If the source directory does not exist or is not a directory.\n\n    Example:\n        &gt;&gt;&gt; copy_files_with_regex(\n        ...     source_dir=Path(\"/path/to/source\"),\n        ...     destination_dir=Path(\"/path/to/destination\"),\n        ...     regex_patterns=[r'.*\\\\.txt$', r'.*\\\\.log$'],\n        ...     preserve_structure=True\n        ... )\n    \"\"\"\n    if not source_dir.is_dir():\n        raise ValueError(\n            f\"The source directory {source_dir} does not exist or is not a directory.\"\n        )\n\n    if not destination_dir.exists():\n        destination_dir.mkdir(parents=True, exist_ok=True)\n\n    # Compile regex patterns for efficiency\n    compiled_patterns = [re.compile(pattern) for pattern in regex_patterns]\n\n    # Process only one level down\n    for subdir in source_dir.iterdir():\n        if subdir.is_dir():  # Only process subdirectories\n            print(f\"processing {subdir}:\")\n            for file_path in subdir.iterdir():  # Only files in this subdirectory\n                if file_path.is_file():\n                    print(f\"checking file: {file_path.name}\")\n                    # Check if the file matches any of the regex patterns\n                    if any(\n                        pattern.match(file_path.name) for pattern in compiled_patterns\n                    ):\n                        if preserve_structure:\n                            # Construct the target path, preserving relative structure\n                            relative_path = (\n                                subdir.relative_to(source_dir) / file_path.name\n                            )\n                            target_path = destination_dir / relative_path\n                            target_path.parent.mkdir(parents=True, exist_ok=True)\n                        else:\n                            # Place directly in destination without subdirectory structure\n                            target_path = destination_dir / file_path.name\n\n                        shutil.copy2(file_path, target_path)\n                        print(f\"Copied: {file_path} -&gt; {target_path}\")\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.ensure_directory_exists","title":"<code>ensure_directory_exists(dir_path)</code>","text":"<p>Create directory if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Path</code> <p>Directory path to ensure exists.</p> required Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def ensure_directory_exists(dir_path: Path) -&gt; bool:\n    \"\"\"\n    Create directory if it doesn't exist.\n\n    Args:\n        dir_path (Path): Directory path to ensure exists.\n    \"\"\"\n    # Stub Implementation\n    return dir_path.exists()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.get_text_from_file","title":"<code>get_text_from_file(file_path)</code>","text":"<p>Reads the entire content of a text file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the text file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The content of the text file as a single string.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def get_text_from_file(file_path: Path) -&gt; str:\n    \"\"\"Reads the entire content of a text file.\n\n    Args:\n        file_path: The path to the text file.\n\n    Returns:\n        The content of the text file as a single string.\n    \"\"\"\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        return file.read()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.iterate_subdir","title":"<code>iterate_subdir(directory, recursive=False)</code>","text":"<p>Iterates through subdirectories in the given directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>The root directory to start the iteration.</p> required <code>recursive</code> <code>bool</code> <p>If True, iterates recursively through all subdirectories.               If False, iterates only over the immediate subdirectories.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Paths to each subdirectory.</p> Example <p>for subdir in iterate_subdir(Path('/root'), recursive=False): ...     print(subdir)</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def iterate_subdir(\n    directory: Path, recursive: bool = False\n) -&gt; Generator[Path, None, None]:\n    \"\"\"\n    Iterates through subdirectories in the given directory.\n\n    Args:\n        directory (Path): The root directory to start the iteration.\n        recursive (bool): If True, iterates recursively through all subdirectories.\n                          If False, iterates only over the immediate subdirectories.\n\n    Yields:\n        Path: Paths to each subdirectory.\n\n    Example:\n        &gt;&gt;&gt; for subdir in iterate_subdir(Path('/root'), recursive=False):\n        ...     print(subdir)\n    \"\"\"\n    if recursive:\n        for subdirectory in directory.rglob(\"*\"):\n            if subdirectory.is_dir():\n                yield subdirectory\n    else:\n        for subdirectory in directory.iterdir():\n            if subdirectory.is_dir():\n                yield subdirectory\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.write_text_to_file","title":"<code>write_text_to_file(file_path, content, overwrite=False, append=False)</code>","text":"<p>Writes text content to a file, handling overwriting and appending.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the file.</p> required <code>content</code> <code>str</code> <p>The text content to write.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrites the file if it exists.</p> <code>False</code> <code>append</code> <code>bool</code> <p>If True, appends the content to the file if it exists.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileExistsWarning</code> <p>If the file exists and neither overwrite nor append are True.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def write_text_to_file(\n    file_path: Path, content: str, overwrite: bool = False, append: bool = False\n) -&gt; None:\n    \"\"\"Writes text content to a file, handling overwriting and appending.\n\n    Args:\n        file_path: The path to the file.\n        content: The text content to write.\n        overwrite: If True, overwrites the file if it exists.\n        append: If True, appends the content to the file if it exists.\n\n    Raises:\n        FileExistsWarning: If the file exists and neither overwrite nor append are True.\n    \"\"\"\n\n    if file_path.exists():\n        if not overwrite and not append:\n            warnings.warn(\n                f\"File '{file_path}' already exists. Use overwrite or append.\",\n                FileExistsWarning,\n            )\n            return  # Do not write if neither flag is set\n        mode = \"a\" if append else \"w\"  # Choose mode based on flags\n    else:\n        mode = \"w\"  # Default to write mode if file doesn't exist\n\n    with open(file_path, mode, encoding=\"utf-8\") as file:\n        file.write(content)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.json_utils","title":"<code>json_utils</code>","text":""},{"location":"api/#tnh_scholar.utils.json_utils.format_json","title":"<code>format_json(file)</code>","text":"<p>Formats a JSON file with line breaks and indentation for readability.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the JSON file to be formatted.</p> required Example <p>format_json(Path(\"data.json\"))</p> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def format_json(file: Path) -&gt; None:\n    \"\"\"\n    Formats a JSON file with line breaks and indentation for readability.\n\n    Args:\n        file (Path): Path to the JSON file to be formatted.\n\n    Example:\n        format_json(Path(\"data.json\"))\n    \"\"\"\n    with file.open(\"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    with file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=4, ensure_ascii=False)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.json_utils.load_json_into_model","title":"<code>load_json_into_model(file, model)</code>","text":"<p>Loads a JSON file and validates it against a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the JSON file.</p> required <code>model</code> <code>type[BaseModel]</code> <p>The Pydantic model to validate against.</p> required <p>Returns:</p> Name Type Description <code>BaseModel</code> <code>BaseModel</code> <p>An instance of the validated Pydantic model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file content is invalid JSON or does not match the model.</p> <p>Example:     class ExampleModel(BaseModel):     name: str     age: int     city: str</p> <pre><code>if __name__ == \"__main__\":\n    json_file = Path(\"example.json\")\n    try:\n        data = load_json_into_model(json_file, ExampleModel)\n        print(data)\n    except ValueError as e:\n        print(e)\n</code></pre> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def load_json_into_model(file: Path, model: type[BaseModel]) -&gt; BaseModel:\n    \"\"\"\n    Loads a JSON file and validates it against a Pydantic model.\n\n    Args:\n        file (Path): Path to the JSON file.\n        model (type[BaseModel]): The Pydantic model to validate against.\n\n    Returns:\n        BaseModel: An instance of the validated Pydantic model.\n\n    Raises:\n        ValueError: If the file content is invalid JSON or does not match the model.\n    Example:\n        class ExampleModel(BaseModel):\n        name: str\n        age: int\n        city: str\n\n        if __name__ == \"__main__\":\n            json_file = Path(\"example.json\")\n            try:\n                data = load_json_into_model(json_file, ExampleModel)\n                print(data)\n            except ValueError as e:\n                print(e)\n    \"\"\"\n    try:\n        with file.open(\"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return model(**data)\n    except (json.JSONDecodeError, ValidationError) as e:\n        raise ValueError(f\"Error loading or validating JSON file '{file}': {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.utils.json_utils.load_jsonl_to_dict","title":"<code>load_jsonl_to_dict(file_path)</code>","text":"<p>Load a JSONL file into a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the JSONL file.</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: A list of dictionaries, each representing a line in the JSONL file.</p> Example <p>from pathlib import Path file_path = Path(\"data.jsonl\") data = load_jsonl_to_dict(file_path) print(data) [{'key1': 'value1'}, {'key2': 'value2'}]</p> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def load_jsonl_to_dict(file_path: Path) -&gt; List[Dict]:\n    \"\"\"\n    Load a JSONL file into a list of dictionaries.\n\n    Args:\n        file_path (Path): Path to the JSONL file.\n\n    Returns:\n        List[Dict]: A list of dictionaries, each representing a line in the JSONL file.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; file_path = Path(\"data.jsonl\")\n        &gt;&gt;&gt; data = load_jsonl_to_dict(file_path)\n        &gt;&gt;&gt; print(data)\n        [{'key1': 'value1'}, {'key2': 'value2'}]\n    \"\"\"\n    with file_path.open(\"r\", encoding=\"utf-8\") as file:\n        return [json.loads(line.strip()) for line in file if line.strip()]\n</code></pre>"},{"location":"api/#tnh_scholar.utils.json_utils.save_model_to_json","title":"<code>save_model_to_json(file, model, indent=4, ensure_ascii=False)</code>","text":"<p>Saves a Pydantic model to a JSON file, formatted with indentation for readability.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the JSON file where the model will be saved.</p> required <code>model</code> <code>BaseModel</code> <p>The Pydantic model instance to save.</p> required <code>indent</code> <code>int</code> <p>Number of spaces for JSON indentation. Defaults to 4.</p> <code>4</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to escape non-ASCII characters. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model cannot be serialized to JSON.</p> <code>IOError</code> <p>If there is an issue writing to the file.</p> Example <p>class ExampleModel(BaseModel):     name: str     age: int</p> <p>if name == \"main\":     model_instance = ExampleModel(name=\"John\", age=30)     json_file = Path(\"example.json\")     try:         save_model_to_json(json_file, model_instance)         print(f\"Model saved to {json_file}\")     except (ValueError, IOError) as e:         print(e)</p> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def save_model_to_json(\n    file: Path, model: BaseModel, indent: int = 4, ensure_ascii: bool = False\n) -&gt; None:\n    \"\"\"\n    Saves a Pydantic model to a JSON file, formatted with indentation for readability.\n\n    Args:\n        file (Path): Path to the JSON file where the model will be saved.\n        model (BaseModel): The Pydantic model instance to save.\n        indent (int): Number of spaces for JSON indentation. Defaults to 4.\n        ensure_ascii (bool): Whether to escape non-ASCII characters. Defaults to False.\n\n    Raises:\n        ValueError: If the model cannot be serialized to JSON.\n        IOError: If there is an issue writing to the file.\n\n    Example:\n        class ExampleModel(BaseModel):\n            name: str\n            age: int\n\n        if __name__ == \"__main__\":\n            model_instance = ExampleModel(name=\"John\", age=30)\n            json_file = Path(\"example.json\")\n            try:\n                save_model_to_json(json_file, model_instance)\n                print(f\"Model saved to {json_file}\")\n            except (ValueError, IOError) as e:\n                print(e)\n    \"\"\"\n    try:\n        # Serialize model to JSON string\n        model_dict = model.model_dump()\n    except TypeError as e:\n        raise ValueError(f\"Error serializing model to JSON: {e}\")\n\n    # Write the JSON string to the file\n    write_data_to_json_file(file, model_dict, indent=indent, ensure_ascii=ensure_ascii)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.json_utils.write_data_to_json_file","title":"<code>write_data_to_json_file(file, data, indent=4, ensure_ascii=False)</code>","text":"<p>Writes a dictionary or list as a JSON string to a file, ensuring the parent directory exists, and supports formatting with indentation and ASCII control.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the JSON file where the data will be written.</p> required <code>data</code> <code>Union[dict, list]</code> <p>The data to write to the file. Typically a dict or list.</p> required <code>indent</code> <code>int</code> <p>Number of spaces for JSON indentation. Defaults to 4.</p> <code>4</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to escape non-ASCII characters. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data cannot be serialized to JSON.</p> <code>IOError</code> <p>If there is an issue writing to the file.</p> Example <p>from pathlib import Path data = {\"key\": \"value\"} write_json_str_to_file(Path(\"output.json\"), data, indent=2, ensure_ascii=True)</p> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def write_data_to_json_file(\n    file: Path, data: Union[dict, list], indent: int = 4, ensure_ascii: bool = False\n) -&gt; None:\n    \"\"\"\n    Writes a dictionary or list as a JSON string to a file, ensuring the parent directory exists,\n    and supports formatting with indentation and ASCII control.\n\n    Args:\n        file (Path): Path to the JSON file where the data will be written.\n        data (Union[dict, list]): The data to write to the file. Typically a dict or list.\n        indent (int): Number of spaces for JSON indentation. Defaults to 4.\n        ensure_ascii (bool): Whether to escape non-ASCII characters. Defaults to False.\n\n    Raises:\n        ValueError: If the data cannot be serialized to JSON.\n        IOError: If there is an issue writing to the file.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; data = {\"key\": \"value\"}\n        &gt;&gt;&gt; write_json_str_to_file(Path(\"output.json\"), data, indent=2, ensure_ascii=True)\n    \"\"\"\n    try:\n        # Convert the data to a formatted JSON string\n        json_str = json.dumps(data, indent=indent, ensure_ascii=ensure_ascii)\n    except TypeError as e:\n        raise ValueError(f\"Error serializing data to JSON: {e}\")\n\n    try:\n        # Ensure the parent directory exists\n        file.parent.mkdir(parents=True, exist_ok=True)\n\n        # Write the JSON string to the file\n        with file.open(\"w\", encoding=\"utf-8\") as f:\n            f.write(json_str)\n    except IOError as e:\n        raise IOError(f\"Error writing JSON string to file '{file}': {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.utils.lang","title":"<code>lang</code>","text":""},{"location":"api/#tnh_scholar.utils.lang.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.lang.get_language_code","title":"<code>get_language_code(text)</code>","text":"<p>Detect the language of the provided text using langdetect.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to analyze</p> <pre><code>      code or 'name' for full English language name\n</code></pre> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>return result 'code' ISO 639-1 for detected language.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text is empty or invalid</p> Source code in <code>src/tnh_scholar/utils/lang.py</code> <pre><code>def get_language_code(text: str) -&gt; str:\n    \"\"\"\n    Detect the language of the provided text using langdetect.\n\n    Args:\n        text: Text to analyze\n\n                      code or 'name' for full English language name\n\n    Returns:\n        str: return result 'code' ISO 639-1 for detected language.\n\n    Raises:\n        ValueError: If text is empty or invalid\n    \"\"\"\n\n    if not text or text.isspace():\n        raise ValueError(\"Input text cannot be empty\")\n\n    sample = _get_sample_text(text)\n\n    try:\n        return detect(sample)\n    except LangDetectException:\n        logger.warning(\"Language could not be detected in get_language().\")\n        return \"un\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.lang.get_language_from_code","title":"<code>get_language_from_code(code)</code>","text":"Source code in <code>src/tnh_scholar/utils/lang.py</code> <pre><code>def get_language_from_code(code: str):\n    if language := pycountry.languages.get(alpha_2=code):\n        return language.name\n    logger.warning(f\"No language name found for code: {code}\")\n    return \"Unknown\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.lang.get_language_name","title":"<code>get_language_name(text)</code>","text":"Source code in <code>src/tnh_scholar/utils/lang.py</code> <pre><code>def get_language_name(text: str) -&gt; str:\n    return get_language_from_code(get_language_code(text))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils","title":"<code>progress_utils</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.BAR_FORMAT","title":"<code>BAR_FORMAT = '{desc}: {percentage:3.0f}%|{bar}| Total: {total_fmt} sec. [elapsed: {elapsed}]'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM","title":"<code>ExpectedTimeTQDM</code>","text":"<p>A context manager for a time-based tqdm progress bar with optional delay.</p> <ul> <li>'expected_time': number of seconds we anticipate the task might take.</li> <li>'display_interval': how often (seconds) to refresh the bar.</li> <li>'desc': a short description for the bar.</li> <li>'delay_start': how many seconds to wait (sleep) before we even create/start the bar.</li> </ul> <p>If the task finishes before 'delay_start' has elapsed, the bar may never appear.</p> Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>class ExpectedTimeTQDM:\n    \"\"\"\n    A context manager for a time-based tqdm progress bar with optional delay.\n\n    - 'expected_time': number of seconds we anticipate the task might take.\n    - 'display_interval': how often (seconds) to refresh the bar.\n    - 'desc': a short description for the bar.\n    - 'delay_start': how many seconds to wait (sleep) before we even create/start the bar.\n\n    If the task finishes before 'delay_start' has elapsed, the bar may never appear.\n    \"\"\"\n\n    def __init__(\n        self,\n        expected_time: float,\n        display_interval: float = 0.5,\n        desc: str = \"Time-based Progress\",\n        delay_start: float = 1.0,\n    ) -&gt; None:\n        self.expected_time = round(expected_time)  # use nearest second.\n        self.display_interval = display_interval\n        self.desc = desc\n        self.delay_start = delay_start\n\n        self._stop_event = threading.Event()\n        self._pbar = None  # We won't create the bar until after 'delay_start'\n        self._start_time = None\n\n    def __enter__(self):\n        # Record the start time for reference\n        self._start_time = time.time()\n\n        # Spawn the background thread; it will handle waiting and then creating/updating the bar\n        self._thread = threading.Thread(target=self._update_bar, daemon=True)\n        self._thread.start()\n\n        return self\n\n    def _update_bar(self):\n        # 1) Delay so warnings/logs can appear before the bar\n        if self.delay_start &gt; 0:\n            time.sleep(self.delay_start)\n\n        # 2) Create the tqdm bar (only now does it appear)\n        self._pbar = tqdm(\n            total=self.expected_time, desc=self.desc, unit=\"sec\", bar_format=BAR_FORMAT\n        )\n\n        # 3) Update until told to stop\n        while not self._stop_event.is_set():\n            elapsed = time.time() - self._start_time\n            current_value = min(elapsed, self.expected_time)\n            if self._pbar:\n                self._pbar.n = round(current_value)\n                self._pbar.refresh()\n            time.sleep(self.display_interval)\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # Signal the thread to stop\n        self._stop_event.set()\n        self._thread.join()\n\n        # If the bar was actually created (i.e., we didn't finish too quickly),\n        # do a final update and close\n        if self._pbar:\n            elapsed = time.time() - self._start_time\n            self._pbar.n = round(min(elapsed, self.expected_time))\n            self._pbar.refresh()\n            self._pbar.close()\n\n    import time\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.delay_start","title":"<code>delay_start = delay_start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.desc","title":"<code>desc = desc</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.display_interval","title":"<code>display_interval = display_interval</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.expected_time","title":"<code>expected_time = round(expected_time)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.__enter__","title":"<code>__enter__()</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __enter__(self):\n    # Record the start time for reference\n    self._start_time = time.time()\n\n    # Spawn the background thread; it will handle waiting and then creating/updating the bar\n    self._thread = threading.Thread(target=self._update_bar, daemon=True)\n    self._thread.start()\n\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    # Signal the thread to stop\n    self._stop_event.set()\n    self._thread.join()\n\n    # If the bar was actually created (i.e., we didn't finish too quickly),\n    # do a final update and close\n    if self._pbar:\n        elapsed = time.time() - self._start_time\n        self._pbar.n = round(min(elapsed, self.expected_time))\n        self._pbar.refresh()\n        self._pbar.close()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.__init__","title":"<code>__init__(expected_time, display_interval=0.5, desc='Time-based Progress', delay_start=1.0)</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __init__(\n    self,\n    expected_time: float,\n    display_interval: float = 0.5,\n    desc: str = \"Time-based Progress\",\n    delay_start: float = 1.0,\n) -&gt; None:\n    self.expected_time = round(expected_time)  # use nearest second.\n    self.display_interval = display_interval\n    self.desc = desc\n    self.delay_start = delay_start\n\n    self._stop_event = threading.Event()\n    self._pbar = None  # We won't create the bar until after 'delay_start'\n    self._start_time = None\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress","title":"<code>TimeProgress</code>","text":"<p>A context manager for a time-based progress display using dots.</p> <p>The display updates once per second, printing a dot and showing: - Expected time (if provided) - Elapsed time (always displayed)</p> <p>Example:</p> <p>import time with ExpectedTimeProgress(expected_time=60, desc=\"Transcribing...\"): ...     time.sleep(5)  # Simulate work [Expected Time: 1:00, Elapsed Time: 0:05] .....</p> <p>Parameters:</p> Name Type Description Default <code>expected_time</code> <code>Optional[float]</code> <p>Expected time in seconds. Optional.</p> <code>None</code> <code>display_interval</code> <code>float</code> <p>How often to print a dot (seconds).</p> <code>1.0</code> <code>desc</code> <code>str</code> <p>Description to display alongside the progress.</p> <code>''</code> Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>class TimeProgress:\n    \"\"\"\n    A context manager for a time-based progress display using dots.\n\n    The display updates once per second, printing a dot and showing:\n    - Expected time (if provided)\n    - Elapsed time (always displayed)\n\n    Example:\n    &gt;&gt;&gt; import time\n    &gt;&gt;&gt; with ExpectedTimeProgress(expected_time=60, desc=\"Transcribing...\"):\n    ...     time.sleep(5)  # Simulate work\n    [Expected Time: 1:00, Elapsed Time: 0:05] .....\n\n    Args:\n        expected_time (Optional[float]): Expected time in seconds. Optional.\n        display_interval (float): How often to print a dot (seconds).\n        desc (str): Description to display alongside the progress.\n    \"\"\"\n\n    def __init__(\n        self,\n        expected_time: Optional[float] = None,\n        display_interval: float = 1.0,\n        desc: str = \"\",\n    ):\n        self.expected_time = expected_time\n        self.display_interval = display_interval\n        self._stop_event = threading.Event()\n        self._start_time = None\n        self._thread = None\n        self.desc = desc\n        self._last_length = 0  # To keep track of the last printed line length\n\n    def __enter__(self):\n        # Record the start time\n        self._start_time = time.time()\n\n        # Spawn the background thread\n        self._thread = threading.Thread(target=self._print_progress, daemon=True)\n        self._thread.start()\n\n        return self\n\n    def _print_progress(self):\n        \"\"\"\n        Continuously prints progress alternating between | and \u2014 along with elapsed/expected time.\n        \"\"\"\n        symbols = [\"|\", \"/\", \"\u2014\", \"\\\\\"]  # Symbols to alternate between\n        symbol_index = 0  # Keep track of the current symbol\n\n        while not self._stop_event.is_set():\n            elapsed = time.time() - self._start_time\n\n            # Format elapsed time as mm:ss\n            elapsed_str = self._format_time(elapsed)\n\n            # Format expected time if provided\n            if self.expected_time is not None:\n                expected_str = self._format_time(self.expected_time)\n                header = f\"{self.desc} [Expected Time: {expected_str}, Elapsed Time: {elapsed_str}]\"\n            else:\n                header = f\"{self.desc} [Elapsed Time: {elapsed_str}]\"\n\n            # Get the current symbol for the spinner\n            spinner = symbols[symbol_index]\n\n            # Construct the line with the spinner\n            line = f\"\\r{header} {spinner}\"\n\n            # Write to stdout\n            sys.stdout.write(line)\n            sys.stdout.flush()\n\n            # Update the symbol index to alternate\n            symbol_index = (symbol_index + 1) % len(symbols)\n\n            # Sleep before next update\n            time.sleep(self.display_interval)\n\n        # Clear the spinner after finishing\n        sys.stdout.write(\"\\r\" + \" \" * len(line) + \"\\r\")\n        sys.stdout.flush()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # Signal the thread to stop\n        self._stop_event.set()\n        self._thread.join()\n\n        # Final elapsed time\n        elapsed = time.time() - self._start_time\n        elapsed_str = self._format_time(elapsed)\n\n        # Construct the final line\n        if self.expected_time is not None:\n            expected_str = self._format_time(self.expected_time)\n            final_header = f\"{self.desc} [Expected Time: {expected_str}, Elapsed Time: {elapsed_str}]\"\n        else:\n            final_header = f\"{self.desc} [Elapsed Time: {elapsed_str}]\"\n\n        # Final dots\n        final_line = f\"\\r{final_header}\"\n\n        # Clear the line and move to the next line\n        padding = \" \" * max(self._last_length - len(final_line), 0)\n        sys.stdout.write(final_line + padding + \"\\n\")\n        sys.stdout.flush()\n\n    @staticmethod\n    def _format_time(seconds: float) -&gt; str:\n        \"\"\"\n        Converts seconds to a formatted string (mm:ss).\n        \"\"\"\n        minutes = int(seconds // 60)\n        seconds = int(seconds % 60)\n        return f\"{minutes}:{seconds:02}\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.desc","title":"<code>desc = desc</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.display_interval","title":"<code>display_interval = display_interval</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.expected_time","title":"<code>expected_time = expected_time</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.__enter__","title":"<code>__enter__()</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __enter__(self):\n    # Record the start time\n    self._start_time = time.time()\n\n    # Spawn the background thread\n    self._thread = threading.Thread(target=self._print_progress, daemon=True)\n    self._thread.start()\n\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    # Signal the thread to stop\n    self._stop_event.set()\n    self._thread.join()\n\n    # Final elapsed time\n    elapsed = time.time() - self._start_time\n    elapsed_str = self._format_time(elapsed)\n\n    # Construct the final line\n    if self.expected_time is not None:\n        expected_str = self._format_time(self.expected_time)\n        final_header = f\"{self.desc} [Expected Time: {expected_str}, Elapsed Time: {elapsed_str}]\"\n    else:\n        final_header = f\"{self.desc} [Elapsed Time: {elapsed_str}]\"\n\n    # Final dots\n    final_line = f\"\\r{final_header}\"\n\n    # Clear the line and move to the next line\n    padding = \" \" * max(self._last_length - len(final_line), 0)\n    sys.stdout.write(final_line + padding + \"\\n\")\n    sys.stdout.flush()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.__init__","title":"<code>__init__(expected_time=None, display_interval=1.0, desc='')</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __init__(\n    self,\n    expected_time: Optional[float] = None,\n    display_interval: float = 1.0,\n    desc: str = \"\",\n):\n    self.expected_time = expected_time\n    self.display_interval = display_interval\n    self._stop_event = threading.Event()\n    self._start_time = None\n    self._thread = None\n    self.desc = desc\n    self._last_length = 0  # To keep track of the last printed line length\n</code></pre>"},{"location":"api/#tnh_scholar.utils.slugify","title":"<code>slugify</code>","text":""},{"location":"api/#tnh_scholar.utils.slugify.slugify","title":"<code>slugify(string)</code>","text":"<p>Slugify a Unicode string.</p> <p>Converts a string to a strict URL-friendly slug format, allowing only lowercase letters, digits, and hyphens.</p> Example <p>slugify(\"H\u00e9ll\u00f8_W\u00f6rld!\") 'hello-world'</p> Source code in <code>src/tnh_scholar/utils/slugify.py</code> <pre><code>def slugify(string: str) -&gt; str:\n    \"\"\"\n    Slugify a Unicode string.\n\n    Converts a string to a strict URL-friendly slug format,\n    allowing only lowercase letters, digits, and hyphens.\n\n    Example:\n        &gt;&gt;&gt; slugify(\"H\u00e9ll\u00f8_W\u00f6rld!\")\n        'hello-world'\n    \"\"\"\n    # Normalize Unicode to remove accents and convert to ASCII\n    string = (\n        unicodedata.normalize(\"NFKD\", string).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    )\n\n    # Replace all non-alphanumeric characters with spaces (only keep a-z and 0-9)\n    string = re.sub(r\"[^a-z0-9\\s]\", \" \", string.lower().strip())\n\n    # Replace any sequence of spaces with a single hyphen\n    return re.sub(r\"\\s+\", \"-\", string)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.user_io_utils","title":"<code>user_io_utils</code>","text":""},{"location":"api/#tnh_scholar.utils.user_io_utils.get_single_char","title":"<code>get_single_char(prompt=None)</code>","text":"<p>Get a single character from input, adapting to the execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[str]</code> <p>Optional prompt to display before getting input</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A single character string from user input</p> Note <ul> <li>In terminal environments, uses raw input mode without requiring Enter</li> <li>In Jupyter/IPython, falls back to regular input with message about Enter</li> </ul> Source code in <code>src/tnh_scholar/utils/user_io_utils.py</code> <pre><code>def get_single_char(prompt: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Get a single character from input, adapting to the execution environment.\n\n    Args:\n        prompt: Optional prompt to display before getting input\n\n    Returns:\n        A single character string from user input\n\n    Note:\n        - In terminal environments, uses raw input mode without requiring Enter\n        - In Jupyter/IPython, falls back to regular input with message about Enter\n    \"\"\"\n    # Check if we're in IPython/Jupyter\n    is_notebook = hasattr(sys, 'ps1') or bool(sys.flags.interactive)\n\n    if prompt:\n        print(prompt, end='', flush=True)\n\n    if is_notebook:\n        # Jupyter/IPython environment - use regular input\n        entry = input(\"Single character input required \")\n        return entry[0] if entry else \"\\n\" # Use newline if no entry\n\n    # Terminal environment\n    if os.name == \"nt\":  # Windows\n        import msvcrt\n        return msvcrt.getch().decode(\"utf-8\")\n    else:  # Unix-like\n        import termios\n        import tty\n\n        try:\n            fd = sys.stdin.fileno()\n            old_settings = termios.tcgetattr(fd)\n            try:\n                tty.setraw(fd)\n                char = sys.stdin.read(1)\n            finally:\n                termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n            return char\n        except termios.error:\n            # Fallback if terminal handling fails\n            return input(\"Single character input required \")[0]\n</code></pre>"},{"location":"api/#tnh_scholar.utils.user_io_utils.get_user_confirmation","title":"<code>get_user_confirmation(prompt, default=True)</code>","text":"<p>Prompt the user for a yes/no confirmation with single-character input. Cross-platform implementation. Returns True if 'y' is entered, and False if 'n' Allows for default value if return is entered.</p> <p>Example usage     if get_user_confirmation(\"Do you want to continue\"):         print(\"Continuing...\")     else:         print(\"Exiting...\")</p> Source code in <code>src/tnh_scholar/utils/user_io_utils.py</code> <pre><code>def get_user_confirmation(prompt: str, default: bool = True) -&gt; bool:\n    \"\"\"\n    Prompt the user for a yes/no confirmation with single-character input.\n    Cross-platform implementation. Returns True if 'y' is entered, and False if 'n'\n    Allows for default value if return is entered.\n\n    Example usage\n        if get_user_confirmation(\"Do you want to continue\"):\n            print(\"Continuing...\")\n        else:\n            print(\"Exiting...\")\n    \"\"\"\n    print(f\"{prompt} \", end=\"\", flush=True)\n\n    while True:\n        char = get_single_char().lower()\n        if char == \"y\":\n            print(char)  # Echo the choice\n            return True\n        elif char == \"n\":\n            print(char)\n            return False\n        elif char in (\"\\r\", \"\\n\"):  # Enter key (use default)\n            print()  # Add a newline\n            return default\n        else:\n            print(\n                f\"\\nInvalid input: {char}. Please type 'y' or 'n': \", end=\"\", flush=True\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.utils.validate","title":"<code>validate</code>","text":""},{"location":"api/#tnh_scholar.utils.validate.OCR_ENV_VARS","title":"<code>OCR_ENV_VARS = {'GOOGLE_APPLICATION_CREDENTIALS'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.validate.OPENAI_ENV_VARS","title":"<code>OPENAI_ENV_VARS = {'OPENAI_API_KEY'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.validate.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.validate.check_env","title":"<code>check_env(required_vars, feature='this feature', output=True)</code>","text":"<p>Check environment variables and provide user-friendly error messages.</p> <p>Parameters:</p> Name Type Description Default <code>required_vars</code> <code>Set[str]</code> <p>Set of environment variable names to check</p> required <code>feature</code> <code>str</code> <p>Description of feature requiring these variables</p> <code>'this feature'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all required variables are set</p> Source code in <code>src/tnh_scholar/utils/validate.py</code> <pre><code>def check_env(required_vars: Set[str], feature: str = \"this feature\", output: bool = True) -&gt; bool:\n    \"\"\"\n    Check environment variables and provide user-friendly error messages.\n\n    Args:\n        required_vars: Set of environment variable names to check\n        feature: Description of feature requiring these variables\n\n    Returns:\n        bool: True if all required variables are set\n    \"\"\"\n    if missing := [var for var in required_vars if not os.getenv(var)]:\n        if output:\n            message = get_env_message(missing, feature)\n            logger.error(f\"Missing environment variables: {', '.join(missing)}\")\n            print(message, file=sys.stderr)\n        return False\n    return True\n</code></pre>"},{"location":"api/#tnh_scholar.utils.validate.check_ocr_env","title":"<code>check_ocr_env(output=True)</code>","text":"<p>Check OCR processing requirements.</p> Source code in <code>src/tnh_scholar/utils/validate.py</code> <pre><code>def check_ocr_env(output: bool = True) -&gt; bool:\n    \"\"\"Check OCR processing requirements.\"\"\"\n    return check_env(OCR_ENV_VARS, \"OCR processing\", output=output)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.validate.check_openai_env","title":"<code>check_openai_env(output=True)</code>","text":"<p>Check OpenAI API requirements.</p> Source code in <code>src/tnh_scholar/utils/validate.py</code> <pre><code>def check_openai_env(output: bool = True) -&gt; bool:\n    \"\"\"Check OpenAI API requirements.\"\"\"\n    return check_env(OPENAI_ENV_VARS, \"OpenAI API access\", output=output)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.validate.get_env_message","title":"<code>get_env_message(missing_vars, feature='this feature')</code>","text":"<p>Generate user-friendly environment setup message.</p> <p>Parameters:</p> Name Type Description Default <code>missing_vars</code> <code>List[str]</code> <p>List of missing environment variable names</p> required <code>feature</code> <code>str</code> <p>Name of feature requiring the variables</p> <code>'this feature'</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted error message with setup instructions</p> Source code in <code>src/tnh_scholar/utils/validate.py</code> <pre><code>def get_env_message(missing_vars: List[str], feature: str = \"this feature\") -&gt; str:\n    \"\"\"Generate user-friendly environment setup message.\n\n    Args:\n        missing_vars: List of missing environment variable names\n        feature: Name of feature requiring the variables\n\n    Returns:\n        Formatted error message with setup instructions\n    \"\"\"\n    export_cmds = \" \".join(f\"{var}=your_{var.lower()}_here\" for var in missing_vars)\n\n    return \"\\n\".join([\n        f\"\\nEnvironment Error: Missing required variables for {feature}:\",\n        \", \".join(missing_vars),\n        \"\\nSet variables in your shell:\",\n        f\"export {export_cmds}\",\n        \"\\nSee documentation for details.\",\n        \"\\nFor development: Add to .env file in project root.\\n\"\n    ])\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing","title":"<code>video_processing</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing","title":"<code>video_processing</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.DEFAULT_TRANSCRIPT_DIR","title":"<code>DEFAULT_TRANSCRIPT_DIR = Path.home() / '.yt_dlp_transcripts'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.DEFAULT_TRANSCRIPT_OPTIONS","title":"<code>DEFAULT_TRANSCRIPT_OPTIONS = {'skip_download': True, 'quiet': True, 'no_warnings': True, 'extract_flat': True, 'socket_timeout': 30, 'retries': 3, 'ignoreerrors': True, 'logger': logger}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.SubtitleTrack","title":"<code>SubtitleTrack</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type definition for a subtitle track entry.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class SubtitleTrack(TypedDict):\n    \"\"\"Type definition for a subtitle track entry.\"\"\"\n\n    url: str\n    ext: str\n    name: str\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.SubtitleTrack.ext","title":"<code>ext</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.SubtitleTrack.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.SubtitleTrack.url","title":"<code>url</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.TranscriptNotFoundError","title":"<code>TranscriptNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when no transcript is available for the requested language.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class TranscriptNotFoundError(Exception):\n    \"\"\"Raised when no transcript is available for the requested language.\"\"\"\n\n    def __init__(\n        self,\n        video_url: str,\n        language: str,\n    ) -&gt; None:\n        \"\"\"\n        Initialize TranscriptNotFoundError.\n\n        Args:\n            video_url: URL of the video where transcript was not found\n            language: Language code that was requested\n            available_manual: List of available manual transcript languages\n            available_auto: List of available auto-generated transcript languages\n        \"\"\"\n        self.video_url = video_url\n        self.language = language\n\n        message = (\n            f\"No transcript found for {self.video_url} in language {self.language}. \"\n        )\n        super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.TranscriptNotFoundError.language","title":"<code>language = language</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.TranscriptNotFoundError.video_url","title":"<code>video_url = video_url</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.TranscriptNotFoundError.__init__","title":"<code>__init__(video_url, language)</code>","text":"<p>Initialize TranscriptNotFoundError.</p> <p>Parameters:</p> Name Type Description Default <code>video_url</code> <code>str</code> <p>URL of the video where transcript was not found</p> required <code>language</code> <code>str</code> <p>Language code that was requested</p> required <code>available_manual</code> <p>List of available manual transcript languages</p> required <code>available_auto</code> <p>List of available auto-generated transcript languages</p> required Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def __init__(\n    self,\n    video_url: str,\n    language: str,\n) -&gt; None:\n    \"\"\"\n    Initialize TranscriptNotFoundError.\n\n    Args:\n        video_url: URL of the video where transcript was not found\n        language: Language code that was requested\n        available_manual: List of available manual transcript languages\n        available_auto: List of available auto-generated transcript languages\n    \"\"\"\n    self.video_url = video_url\n    self.language = language\n\n    message = (\n        f\"No transcript found for {self.video_url} in language {self.language}. \"\n    )\n    super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoInfo","title":"<code>VideoInfo</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type definition for relevant video info fields.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class VideoInfo(TypedDict):\n    \"\"\"Type definition for relevant video info fields.\"\"\"\n\n    subtitles: Dict[str, List[SubtitleTrack]]\n    automatic_captions: Dict[str, List[SubtitleTrack]]\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoInfo.automatic_captions","title":"<code>automatic_captions</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoInfo.subtitles","title":"<code>subtitles</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.download_audio_yt","title":"<code>download_audio_yt(url, output_dir, start_time=None, prompt_overwrite=True)</code>","text":"<p>Downloads audio from a YouTube video using yt_dlp.YoutubeDL, with an optional start time.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the YouTube video.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the downloaded audio file.</p> required <code>start_time</code> <code>str</code> <p>Optional start time (e.g., '00:01:30' for 1 minute 30 seconds).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the downloaded audio file.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def download_audio_yt(\n    url: str, output_dir: Path, start_time: str = None, prompt_overwrite=True\n) -&gt; Path:\n    \"\"\"\n    Downloads audio from a YouTube video using yt_dlp.YoutubeDL, with an optional start time.\n\n    Args:\n        url (str): URL of the YouTube video.\n        output_dir (Path): Directory to save the downloaded audio file.\n        start_time (str): Optional start time (e.g., '00:01:30' for 1 minute 30 seconds).\n\n    Returns:\n        Path: Path to the downloaded audio file.\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n    ydl_opts = {\n        \"format\": \"bestaudio/best\",\n        \"postprocessors\": [\n            {\n                \"key\": \"FFmpegExtractAudio\",\n                \"preferredcodec\": \"mp3\",\n                \"preferredquality\": \"192\",\n            }\n        ],\n        \"postprocessor_args\": [],\n        \"noplaylist\": True,\n        \"outtmpl\": str(output_dir / \"%(title)s.%(ext)s\"),\n    }\n\n    # Add start time to the FFmpeg postprocessor if provided\n    if start_time:\n        ydl_opts[\"postprocessor_args\"].extend([\"-ss\", start_time])\n        logger.info(f\"Postprocessor start time set to: {start_time}\")\n\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        info = ydl.extract_info(url, download=True)  # Extract metadata and download\n        filename = ydl.prepare_filename(info)\n        return Path(filename).with_suffix(\".mp3\")\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.get_transcript","title":"<code>get_transcript(url, lang='en', download_dir=DEFAULT_TRANSCRIPT_DIR, keep_transcript_file=False)</code>","text":"<p>Downloads and extracts the transcript for a given YouTube video URL.</p> <p>Retrieves the transcript file, extracts the text content, and returns the raw text.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the YouTube video.</p> required <code>lang</code> <code>str</code> <p>The language code for the transcript (default: 'en').</p> <code>'en'</code> <code>download_dir</code> <code>Path</code> <p>The directory to download the transcript to.</p> <code>DEFAULT_TRANSCRIPT_DIR</code> <code>keep_transcript_file</code> <code>bool</code> <p>Whether to keep the downloaded transcript file (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The extracted transcript text.</p> <p>Raises:</p> Type Description <code>TranscriptNotFoundError</code> <p>If no transcript is available in the specified language.</p> <code>DownloadError</code> <p>If video info extraction or download fails.</p> <code>ValueError</code> <p>If the downloaded transcript file is invalid or empty.</p> <code>ParseError</code> <p>If XML parsing of the transcript fails.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_transcript(\n    url: str,\n    lang: str = \"en\",\n    download_dir: Path = DEFAULT_TRANSCRIPT_DIR,\n    keep_transcript_file: bool = False,\n) -&gt; str:\n    \"\"\"Downloads and extracts the transcript for a given YouTube video URL.\n\n    Retrieves the transcript file, extracts the text content, and returns the raw text.\n\n    Args:\n        url: The URL of the YouTube video.\n        lang: The language code for the transcript (default: 'en').\n        download_dir: The directory to download the transcript to.\n        keep_transcript_file: Whether to keep the downloaded transcript file (default: False).\n\n    Returns:\n        The extracted transcript text.\n\n    Raises:\n        TranscriptNotFoundError: If no transcript is available in the specified language.\n        yt_dlp.utils.DownloadError: If video info extraction or download fails.\n        ValueError: If the downloaded transcript file is invalid or empty.\n        ParseError: If XML parsing of the transcript fails.\n    \"\"\"\n\n    transcript_file = _download_yt_ttml(download_dir, url=url, lang=lang)\n\n    text = get_text_from_file(transcript_file)\n\n    if not keep_transcript_file:\n        try:\n            os.remove(transcript_file)\n            logger.debug(f\"Removed temporary transcript file: {transcript_file}\")\n        except OSError as e:\n            logger.warning(\n                f\"Failed to remove temporary transcript file {transcript_file}: {e}\"\n            )\n\n    return _extract_ttml_text(text)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.get_transcript_info","title":"<code>get_transcript_info(video_url, lang='en')</code>","text":"<p>Retrieves the transcript URL for a video in the specified language.</p> <p>Parameters:</p> Name Type Description Default <code>video_url</code> <code>str</code> <p>The URL of the video</p> required <code>lang</code> <code>str</code> <p>The desired language code</p> <code>'en'</code> <p>Returns:</p> Type Description <p>URL of the transcript</p> <p>Raises:</p> Type Description <code>TranscriptNotFoundError</code> <p>If no transcript is available in the specified language</p> <code>DownloadError</code> <p>If video info extraction fails</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_transcript_info(video_url: str, lang: str = \"en\"):\n    \"\"\"\n    Retrieves the transcript URL for a video in the specified language.\n\n    Args:\n        video_url: The URL of the video\n        lang: The desired language code\n\n    Returns:\n        URL of the transcript\n\n    Raises:\n        TranscriptNotFoundError: If no transcript is available in the specified language\n        yt_dlp.utils.DownloadError: If video info extraction fails\n    \"\"\"\n    options = {\n        \"writesubtitles\": True,\n        \"writeautomaticsub\": True,\n        \"subtitleslangs\": [lang],\n        \"skip_download\": True,\n        #    'verbose': True\n    }\n\n    with yt_dlp.YoutubeDL(options) as ydl:\n        # This may raise yt_dlp.utils.DownloadError which we let propagate\n        info: VideoInfo = ydl.extract_info(video_url, download=False)  # type: ignore\n\n        subtitles = info.get(\"subtitles\", {})\n        auto_subtitles = info.get(\"automatic_captions\", {})\n\n        # Log available subtitle information\n        logger.debug(\"Available subtitles:\")\n        logger.debug(f\"Manual subtitles: {list(subtitles.keys())}\")\n        logger.debug(f\"Auto captions: {list(auto_subtitles.keys())}\")\n\n        if lang in subtitles:\n            return subtitles[lang][0][\"url\"]\n        elif lang in auto_subtitles:\n            return auto_subtitles[lang][0][\"url\"]\n\n        raise TranscriptNotFoundError(video_url=video_url, language=lang)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.get_video_download_path_yt","title":"<code>get_video_download_path_yt(output_dir, url)</code>","text":"<p>Extracts the video title using yt-dlp.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The YouTube URL.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Path</code> <p>The title of the video.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_video_download_path_yt(output_dir: Path, url: str) -&gt; Path:\n    \"\"\"\n    Extracts the video title using yt-dlp.\n\n    Args:\n        url (str): The YouTube URL.\n\n    Returns:\n        str: The title of the video.\n    \"\"\"\n    ydl_opts = {\n        \"quiet\": True,  # Suppress output\n        \"skip_download\": True,  # Don't download, just fetch metadata\n        \"outtmpl\": str(output_dir / \"%(title)s.%(ext)s\"),\n    }\n\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        info = ydl.extract_info(\n            url, download=False\n        )  # Extract metadata without downloading\n        filepath = ydl.prepare_filename(info)\n\n    return Path(filepath).with_suffix(\".mp3\")\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.get_youtube_urls_from_csv","title":"<code>get_youtube_urls_from_csv(file_path)</code>","text":"<p>Reads a CSV file containing YouTube URLs and titles, logs the titles, and returns a list of URLs.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the CSV file containing YouTube URLs and titles.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of YouTube URLs.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> <code>ValueError</code> <p>If the CSV file is improperly formatted.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_youtube_urls_from_csv(file_path: Path) -&gt; List[str]:\n    \"\"\"\n    Reads a CSV file containing YouTube URLs and titles, logs the titles,\n    and returns a list of URLs.\n\n    Args:\n        file_path (Path): Path to the CSV file containing YouTube URLs and titles.\n\n    Returns:\n        List[str]: List of YouTube URLs.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the CSV file is improperly formatted.\n    \"\"\"\n    if not file_path.exists():\n        logger.error(f\"File not found: {file_path}\")\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    urls = []\n\n    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n\n        if \"url\" not in reader.fieldnames or \"title\" not in reader.fieldnames:\n            logger.error(\"CSV file must contain 'url' and 'title' columns.\")\n            raise ValueError(\"CSV file must contain 'url' and 'title' columns.\")\n\n        for row in reader:\n            url = row[\"url\"]\n            title = row[\"title\"]\n            urls.append(url)\n            logger.info(f\"Found video title: {title}\")\n\n    return urls\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.yt_transcribe","title":"<code>yt_transcribe</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.DEFAULT_CHUNK_DURATION_MS","title":"<code>DEFAULT_CHUNK_DURATION_MS = 10 * 60 * 1000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.DEFAULT_CHUNK_DURATION_S","title":"<code>DEFAULT_CHUNK_DURATION_S = 10 * 60</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.DEFAULT_OUTPUT_DIR","title":"<code>DEFAULT_OUTPUT_DIR = './video_transcriptions'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.DEFAULT_PROMPT","title":"<code>DEFAULT_PROMPT = 'Dharma, Deer Park, Thay, Thich Nhat Hanh, Bodhicitta, Bodhisattva, Mahayana'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.EXPECTED_ENV","title":"<code>EXPECTED_ENV = 'tnh-scholar'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.group","title":"<code>group = parser.add_mutually_exclusive_group(required=True)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.logger","title":"<code>logger = get_child_logger('yt_transcribe')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.output_directory","title":"<code>output_directory = Path(args.output_dir)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.parser","title":"<code>parser = argparse.ArgumentParser(description='Transcribe YouTube videos from a URL or a file containing URLs.')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.url_file","title":"<code>url_file = Path(args.file)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.video_urls","title":"<code>video_urls = []</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.check_conda_env","title":"<code>check_conda_env()</code>","text":"Source code in <code>src/tnh_scholar/video_processing/yt_transcribe.py</code> <pre><code>def check_conda_env():\n    active_env = os.environ.get(\"CONDA_DEFAULT_ENV\")\n    if active_env != EXPECTED_ENV:\n        logger.warning(\n            f\"WARNING: The active conda environment is '{active_env}', but '{EXPECTED_ENV}' is required. \"\n            \"Please activate the correct environment.\"\n        )\n        # Optionally exit the script\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.transcribe_youtube_videos","title":"<code>transcribe_youtube_videos(urls, output_base_dir, max_chunk_duration=DEFAULT_CHUNK_DURATION_S, start=None, translate=False)</code>","text":"<p>Full pipeline for transcribing a list of YouTube videos.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>List of YouTube video URLs.</p> required <code>output_base_dir</code> <code>Path</code> <p>Base directory for storing output.</p> required <code>max_chunk_duration</code> <code>int</code> <p>Maximum duration for audio chunks in seconds (default is 10 minutes).</p> <code>DEFAULT_CHUNK_DURATION_S</code> Source code in <code>src/tnh_scholar/video_processing/yt_transcribe.py</code> <pre><code>def transcribe_youtube_videos(\n    urls: list[str],\n    output_base_dir: Path,\n    max_chunk_duration: int = DEFAULT_CHUNK_DURATION_S,\n    start: str = None,\n    translate=False,\n):\n    \"\"\"\n    Full pipeline for transcribing a list of YouTube videos.\n\n    Args:\n        urls (list[str]): List of YouTube video URLs.\n        output_base_dir (Path): Base directory for storing output.\n        max_chunk_duration (int): Maximum duration for audio chunks in seconds (default is 10 minutes).\n    \"\"\"\n    output_base_dir.mkdir(parents=True, exist_ok=True)\n\n    for url in urls:\n        try:\n            logger.info(f\"Processing video: {url}\")\n\n            # Step 1: Download audio\n            logger.info(\"Downloading audio...\")\n            tmp_audio_file = download_audio_yt(url, output_base_dir, start_time=start)\n            logger.info(f\"Downloaded audio file: {tmp_audio_file}\")\n\n            # Prepare directories for chunks and outputs\n            video_name = (\n                tmp_audio_file.stem\n            )  # Use the stem of the audio file (title without extension)\n            video_output_dir = output_base_dir / video_name\n            chunks_dir = video_output_dir / \"chunks\"\n            chunks_dir.mkdir(parents=True, exist_ok=True)\n\n            # Create the video directory and move the audio file into it\n            video_output_dir.mkdir(parents=True, exist_ok=True)\n            audio_file = video_output_dir / tmp_audio_file.name\n\n            try:\n                tmp_audio_file.rename(\n                    audio_file\n                )  # Move the audio file to the video directory\n                logger.info(f\"Moved audio file to: {audio_file}\")\n            except Exception as e:\n                logger.error(f\"Failed to move audio file to {video_output_dir}: {e}\")\n                # Ensure the code gracefully handles issues here, reassigning to the original tmp path.\n                audio_file = tmp_audio_file\n\n            # Step 2: Detect boundaries\n            logger.info(\"Detecting boundaries...\")\n            boundaries = detect_boundaries(audio_file)\n            logger.info(\"Boundaries generated.\")\n\n            # Step 3: Split audio into chunks\n            logger.info(\"Splitting audio into chunks...\")\n            split_audio_at_boundaries(\n                audio_file=audio_file,\n                boundaries=boundaries,\n                output_dir=chunks_dir,\n                max_duration=max_chunk_duration,\n            )\n            logger.info(f\"Audio chunks saved to: {chunks_dir}\")\n\n            # Step 4: Transcribe audio chunks\n            logger.info(\"Transcribing audio chunks...\")\n            transcript_file = video_output_dir / f\"{video_name}.txt\"\n            jsonl_file = video_output_dir / f\"{video_name}.jsonl\"\n            process_audio_chunks(\n                directory=chunks_dir,\n                output_file=transcript_file,\n                jsonl_file=jsonl_file,\n                prompt=DEFAULT_PROMPT,\n                translate=translate,\n            )\n            logger.info(f\"Transcription completed for {url}\")\n            logger.info(f\"Transcript saved to: {transcript_file}\")\n            logger.info(f\"Raw transcription data saved to: {jsonl_file}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to process video {url}: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing","title":"<code>xml_processing</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.FormattingError","title":"<code>FormattingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception raised for formatting-related errors.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>class FormattingError(Exception):\n    \"\"\"\n    Custom exception raised for formatting-related errors.\n    \"\"\"\n\n    def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n        super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.FormattingError.__init__","title":"<code>__init__(message='An error occurred due to invalid formatting.')</code>","text":"Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n    super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.join_xml_data_to_doc","title":"<code>join_xml_data_to_doc(file_path, data, overwrite=False)</code>","text":"<p>Joins a list of XML-tagged data with newlines, wraps it with  tags, and writes it to the specified file. Raises an exception if the file exists and overwrite is not set. <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the output file.</p> required <code>data</code> <code>List[str]</code> <p>List of XML-tagged data strings.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it exists.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file exists and overwrite is False.</p> <code>ValueError</code> <p>If the data list is empty.</p> Example <p>join_xml_data_to_doc(Path(\"output.xml\"), [\"Data\"], overwrite=True)</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def join_xml_data_to_doc(\n    file_path: Path, data: List[str], overwrite: bool = False\n) -&gt; None:\n    \"\"\"\n    Joins a list of XML-tagged data with newlines, wraps it with &lt;document&gt; tags,\n    and writes it to the specified file. Raises an exception if the file exists\n    and overwrite is not set.\n\n    Args:\n        file_path (Path): Path to the output file.\n        data (List[str]): List of XML-tagged data strings.\n        overwrite (bool): Whether to overwrite the file if it exists.\n\n    Raises:\n        FileExistsError: If the file exists and overwrite is False.\n        ValueError: If the data list is empty.\n\n    Example:\n        &gt;&gt;&gt; join_xml_data_to_doc(Path(\"output.xml\"), [\"&lt;tag&gt;Data&lt;/tag&gt;\"], overwrite=True)\n    \"\"\"\n    if file_path.exists() and not overwrite:\n        raise FileExistsError(\n            f\"The file {file_path} already exists and overwrite is not set.\"\n        )\n\n    if not data:\n        raise ValueError(\"The data list cannot be empty.\")\n\n    # Create the XML content\n    joined_data = \"\\n\".join(data)  # Joining data with newline\n    xml_content = f\"&lt;document&gt;\\n{joined_data}\\n&lt;/document&gt;\"\n\n    # Write to file\n    file_path.write_text(xml_content, encoding=\"utf-8\")\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.remove_page_tags","title":"<code>remove_page_tags(text)</code>","text":"<p>Removes  and  tags from a text string.</p> <p>Parameters: - text (str): The input text containing  tags. <p>Returns: - str: The text with  tags removed. Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def remove_page_tags(text):\n    \"\"\"\n    Removes &lt;page ...&gt; and &lt;/page&gt; tags from a text string.\n\n    Parameters:\n    - text (str): The input text containing &lt;page&gt; tags.\n\n    Returns:\n    - str: The text with &lt;page&gt; tags removed.\n    \"\"\"\n    # Remove opening &lt;page ...&gt; tags\n    text = re.sub(r\"&lt;page[^&gt;]*&gt;\", \"\", text)\n    # Remove closing &lt;/page&gt; tags\n    text = re.sub(r\"&lt;/page&gt;\", \"\", text)\n    return text\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.save_pages_to_xml","title":"<code>save_pages_to_xml(output_xml_path, text_pages, overwrite=False)</code>","text":"<p>Generates and saves an XML file containing text pages, with a  tag indicating the page ends. <p>Parameters:</p> Name Type Description Default <code>output_xml_path</code> <code>Path</code> <p>The Path object for the file where the XML file will be saved.</p> required <code>text_pages</code> <code>List[str]</code> <p>A list of strings, each representing the text content of a page.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrites the file if it exists. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input list of text_pages is empty or contains invalid types.</p> <code>FileExistsError</code> <p>If the file already exists and overwrite is False.</p> <code>PermissionError</code> <p>If the file cannot be created due to insufficient permissions.</p> <code>OSError</code> <p>For other file I/O-related errors.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def save_pages_to_xml(\n    output_xml_path: Path,\n    text_pages: List[str],\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Generates and saves an XML file containing text pages, with a &lt;pagebreak&gt; tag indicating the page ends.\n\n    Parameters:\n        output_xml_path (Path): The Path object for the file where the XML file will be saved.\n        text_pages (List[str]): A list of strings, each representing the text content of a page.\n        overwrite (bool): If True, overwrites the file if it exists. Default is False.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the input list of text_pages is empty or contains invalid types.\n        FileExistsError: If the file already exists and overwrite is False.\n        PermissionError: If the file cannot be created due to insufficient permissions.\n        OSError: For other file I/O-related errors.\n    \"\"\"\n    if not text_pages:\n        raise ValueError(\"The text_pages list is empty. Cannot generate XML.\")\n\n    # Check if the file exists and handle overwrite behavior\n    if output_xml_path.exists() and not overwrite:\n        raise FileExistsError(\n            f\"The file '{output_xml_path}' already exists. Set overwrite=True to overwrite.\"\n        )\n\n    try:\n        # Ensure the output directory exists\n        output_xml_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Write the XML file\n        with output_xml_path.open(\"w\", encoding=\"utf-8\") as xml_file:\n            # Write XML declaration and root element\n            xml_file.write(\"&lt;?xml version='1.0' encoding='UTF-8'?&gt;\\n\")\n            xml_file.write(\"&lt;document&gt;\\n\")\n\n            # Add each page with its content and &lt;pagebreak&gt; tag\n            for page_number, text in enumerate(text_pages, start=1):\n                if not isinstance(text, str):\n                    raise ValueError(\n                        f\"Invalid page content at index {page_number - 1}: expected a string.\"\n                    )\n\n                content = text.strip()\n                escaped_text = escape(content)\n                xml_file.write(f\"    {escaped_text}\\n\")\n                xml_file.write(f\"    &lt;pagebreak page='{page_number}' /&gt;\\n\")\n\n            # Close the root element\n            xml_file.write(\"&lt;/document&gt;\\n\")\n\n        print(f\"XML file successfully saved at {output_xml_path}\")\n\n    except PermissionError as e:\n        raise PermissionError(\n            f\"Permission denied while writing to {output_xml_path}: {e}\"\n        ) from e\n\n    except OSError as e:\n        raise OSError(\n            f\"An OS-related error occurred while saving XML file at {output_xml_path}: {e}\"\n        ) from e\n\n    except Exception as e:\n        raise RuntimeError(f\"An unexpected error occurred: {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.split_xml_on_pagebreaks","title":"<code>split_xml_on_pagebreaks(text, page_groups=None, keep_pagebreaks=True)</code>","text":"<p>Splits an XML document into individual pages based on  tags. Optionally groups pages together based on page_groups and retains  tags if keep_pagebreaks is True. <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The XML document as a string.</p> required <code>page_groups</code> <code>Optional[List[Tuple[int, int]]]</code> <p>A list of tuples defining page ranges to group together.                                           Each tuple is of the form (start_page, end_page), inclusive.</p> <code>None</code> <code>keep_pagebreaks</code> <code>bool</code> <p>Whether to retain the  tags in the returned data. Default is False. <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of page contents as strings, either split by pages or grouped by page_groups.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the expected preamble or  tags are missing. Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def split_xml_on_pagebreaks(\n    text: str,\n    page_groups: Optional[List[Tuple[int, int]]] = None,\n    keep_pagebreaks: bool = True,\n) -&gt; List[str]:\n    \"\"\"\n    Splits an XML document into individual pages based on &lt;pagebreak&gt; tags.\n    Optionally groups pages together based on page_groups and retains &lt;pagebreak&gt; tags if keep_pagebreaks is True.\n\n    Parameters:\n        text (str): The XML document as a string.\n        page_groups (Optional[List[Tuple[int, int]]]): A list of tuples defining page ranges to group together.\n                                                      Each tuple is of the form (start_page, end_page), inclusive.\n        keep_pagebreaks (bool): Whether to retain the &lt;pagebreak&gt; tags in the returned data. Default is False.\n\n    Returns:\n        List[str]: A list of page contents as strings, either split by pages or grouped by page_groups.\n\n    Raises:\n        ValueError: If the expected preamble or &lt;document&gt; tags are missing.\n    \"\"\"\n    # Split text into lines\n    lines = text.splitlines()\n\n    # Preprocess: Remove `&lt;?xml ... ?&gt;` preamble and &lt;document&gt; tags\n    if lines[0].startswith(\"&lt;?xml\"):\n        lines.pop(0)\n    else:\n        raise ValueError(\"Missing `&lt;?xml ... ?&gt;` preamble on the first line.\")\n    if lines[0].strip() == \"&lt;document&gt;\":\n        lines.pop(0)\n    else:\n        raise ValueError(\"Missing `&lt;document&gt;` opening tag on the second line.\")\n    if lines[-1].strip() == \"&lt;/document&gt;\":\n        lines.pop(-1)\n    else:\n        raise ValueError(\"Missing `&lt;/document&gt;` closing tag on the last line.\")\n\n    # Process content to split pages based on &lt;pagebreak&gt; tags\n    pages = []\n    current_page = []\n\n    for line in lines:\n        if \"&lt;pagebreak\" in line:  # Page boundary detected\n            if current_page:\n                page_content = \"\\n\".join(current_page).strip()\n                if keep_pagebreaks:\n                    page_content += f\"\\n{line.strip()}\"  # Retain the &lt;pagebreak&gt; tag\n                pages.append(page_content)\n                current_page = []\n        else:\n            current_page.append(line)\n\n    # Append the last page if it exists\n    if current_page:\n        pages.append(\"\\n\".join(current_page).strip())\n\n    # Validate that pages are extracted\n    if not pages:\n        raise ValueError(\"No pages found in the XML content.\")\n\n    # Group pages if page_groups is provided\n    if page_groups:\n        grouped_pages = []\n        for start, end in page_groups:\n            if group_content := [\n                pages[i] for i in range(start - 1, end) if 0 &lt;= i &lt; len(pages)\n            ]:\n                grouped_pages.append(\"\\n\".join(group_content).strip())\n        return grouped_pages\n\n    return pages\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.extract_tags","title":"<code>extract_tags</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.extract_tags.extract_unique_tags","title":"<code>extract_unique_tags(xml_file)</code>","text":"<p>Extract all unique tags from an XML file using lxml.</p> <p>Parameters:</p> Name Type Description Default <code>xml_file</code> <code>str</code> <p>Path to the XML file.</p> required <p>Returns:</p> Name Type Description <code>set</code> <p>A set of unique tags in the XML document.</p> Source code in <code>src/tnh_scholar/xml_processing/extract_tags.py</code> <pre><code>def extract_unique_tags(xml_file):\n    \"\"\"\n    Extract all unique tags from an XML file using lxml.\n\n    Parameters:\n        xml_file (str): Path to the XML file.\n\n    Returns:\n        set: A set of unique tags in the XML document.\n    \"\"\"\n    # Parse the XML file\n    tree = etree.parse(xml_file)\n\n    # Find all unique tags and return\n    return {element.tag for element in tree.iter()}\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.extract_tags.main","title":"<code>main()</code>","text":"Source code in <code>src/tnh_scholar/xml_processing/extract_tags.py</code> <pre><code>def main():\n    # Create argument parser\n    parser = argparse.ArgumentParser(\n        description=\"Extract all unique tags from an XML file.\"\n    )\n    parser.add_argument(\"xml_file\", type=str, help=\"Path to the XML file.\")\n\n    # Parse command-line arguments\n    args = parser.parse_args()\n\n    # Extract tags\n    tags = extract_unique_tags(args.xml_file)\n\n    # Print results\n    print(\"Unique Tags Found:\")\n    for tag in sorted(tags):\n        print(tag)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing","title":"<code>xml_processing</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.xml_processing.FormattingError","title":"<code>FormattingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception raised for formatting-related errors.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>class FormattingError(Exception):\n    \"\"\"\n    Custom exception raised for formatting-related errors.\n    \"\"\"\n\n    def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n        super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.FormattingError.__init__","title":"<code>__init__(message='An error occurred due to invalid formatting.')</code>","text":"Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n    super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.join_xml_data_to_doc","title":"<code>join_xml_data_to_doc(file_path, data, overwrite=False)</code>","text":"<p>Joins a list of XML-tagged data with newlines, wraps it with  tags, and writes it to the specified file. Raises an exception if the file exists and overwrite is not set. <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the output file.</p> required <code>data</code> <code>List[str]</code> <p>List of XML-tagged data strings.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it exists.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file exists and overwrite is False.</p> <code>ValueError</code> <p>If the data list is empty.</p> Example <p>join_xml_data_to_doc(Path(\"output.xml\"), [\"Data\"], overwrite=True)</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def join_xml_data_to_doc(\n    file_path: Path, data: List[str], overwrite: bool = False\n) -&gt; None:\n    \"\"\"\n    Joins a list of XML-tagged data with newlines, wraps it with &lt;document&gt; tags,\n    and writes it to the specified file. Raises an exception if the file exists\n    and overwrite is not set.\n\n    Args:\n        file_path (Path): Path to the output file.\n        data (List[str]): List of XML-tagged data strings.\n        overwrite (bool): Whether to overwrite the file if it exists.\n\n    Raises:\n        FileExistsError: If the file exists and overwrite is False.\n        ValueError: If the data list is empty.\n\n    Example:\n        &gt;&gt;&gt; join_xml_data_to_doc(Path(\"output.xml\"), [\"&lt;tag&gt;Data&lt;/tag&gt;\"], overwrite=True)\n    \"\"\"\n    if file_path.exists() and not overwrite:\n        raise FileExistsError(\n            f\"The file {file_path} already exists and overwrite is not set.\"\n        )\n\n    if not data:\n        raise ValueError(\"The data list cannot be empty.\")\n\n    # Create the XML content\n    joined_data = \"\\n\".join(data)  # Joining data with newline\n    xml_content = f\"&lt;document&gt;\\n{joined_data}\\n&lt;/document&gt;\"\n\n    # Write to file\n    file_path.write_text(xml_content, encoding=\"utf-8\")\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.remove_page_tags","title":"<code>remove_page_tags(text)</code>","text":"<p>Removes  and  tags from a text string.</p> <p>Parameters: - text (str): The input text containing  tags. <p>Returns: - str: The text with  tags removed. Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def remove_page_tags(text):\n    \"\"\"\n    Removes &lt;page ...&gt; and &lt;/page&gt; tags from a text string.\n\n    Parameters:\n    - text (str): The input text containing &lt;page&gt; tags.\n\n    Returns:\n    - str: The text with &lt;page&gt; tags removed.\n    \"\"\"\n    # Remove opening &lt;page ...&gt; tags\n    text = re.sub(r\"&lt;page[^&gt;]*&gt;\", \"\", text)\n    # Remove closing &lt;/page&gt; tags\n    text = re.sub(r\"&lt;/page&gt;\", \"\", text)\n    return text\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.save_pages_to_xml","title":"<code>save_pages_to_xml(output_xml_path, text_pages, overwrite=False)</code>","text":"<p>Generates and saves an XML file containing text pages, with a  tag indicating the page ends. <p>Parameters:</p> Name Type Description Default <code>output_xml_path</code> <code>Path</code> <p>The Path object for the file where the XML file will be saved.</p> required <code>text_pages</code> <code>List[str]</code> <p>A list of strings, each representing the text content of a page.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrites the file if it exists. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input list of text_pages is empty or contains invalid types.</p> <code>FileExistsError</code> <p>If the file already exists and overwrite is False.</p> <code>PermissionError</code> <p>If the file cannot be created due to insufficient permissions.</p> <code>OSError</code> <p>For other file I/O-related errors.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def save_pages_to_xml(\n    output_xml_path: Path,\n    text_pages: List[str],\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Generates and saves an XML file containing text pages, with a &lt;pagebreak&gt; tag indicating the page ends.\n\n    Parameters:\n        output_xml_path (Path): The Path object for the file where the XML file will be saved.\n        text_pages (List[str]): A list of strings, each representing the text content of a page.\n        overwrite (bool): If True, overwrites the file if it exists. Default is False.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the input list of text_pages is empty or contains invalid types.\n        FileExistsError: If the file already exists and overwrite is False.\n        PermissionError: If the file cannot be created due to insufficient permissions.\n        OSError: For other file I/O-related errors.\n    \"\"\"\n    if not text_pages:\n        raise ValueError(\"The text_pages list is empty. Cannot generate XML.\")\n\n    # Check if the file exists and handle overwrite behavior\n    if output_xml_path.exists() and not overwrite:\n        raise FileExistsError(\n            f\"The file '{output_xml_path}' already exists. Set overwrite=True to overwrite.\"\n        )\n\n    try:\n        # Ensure the output directory exists\n        output_xml_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Write the XML file\n        with output_xml_path.open(\"w\", encoding=\"utf-8\") as xml_file:\n            # Write XML declaration and root element\n            xml_file.write(\"&lt;?xml version='1.0' encoding='UTF-8'?&gt;\\n\")\n            xml_file.write(\"&lt;document&gt;\\n\")\n\n            # Add each page with its content and &lt;pagebreak&gt; tag\n            for page_number, text in enumerate(text_pages, start=1):\n                if not isinstance(text, str):\n                    raise ValueError(\n                        f\"Invalid page content at index {page_number - 1}: expected a string.\"\n                    )\n\n                content = text.strip()\n                escaped_text = escape(content)\n                xml_file.write(f\"    {escaped_text}\\n\")\n                xml_file.write(f\"    &lt;pagebreak page='{page_number}' /&gt;\\n\")\n\n            # Close the root element\n            xml_file.write(\"&lt;/document&gt;\\n\")\n\n        print(f\"XML file successfully saved at {output_xml_path}\")\n\n    except PermissionError as e:\n        raise PermissionError(\n            f\"Permission denied while writing to {output_xml_path}: {e}\"\n        ) from e\n\n    except OSError as e:\n        raise OSError(\n            f\"An OS-related error occurred while saving XML file at {output_xml_path}: {e}\"\n        ) from e\n\n    except Exception as e:\n        raise RuntimeError(f\"An unexpected error occurred: {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.split_xml_on_pagebreaks","title":"<code>split_xml_on_pagebreaks(text, page_groups=None, keep_pagebreaks=True)</code>","text":"<p>Splits an XML document into individual pages based on  tags. Optionally groups pages together based on page_groups and retains  tags if keep_pagebreaks is True. <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The XML document as a string.</p> required <code>page_groups</code> <code>Optional[List[Tuple[int, int]]]</code> <p>A list of tuples defining page ranges to group together.                                           Each tuple is of the form (start_page, end_page), inclusive.</p> <code>None</code> <code>keep_pagebreaks</code> <code>bool</code> <p>Whether to retain the  tags in the returned data. Default is False. <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of page contents as strings, either split by pages or grouped by page_groups.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the expected preamble or  tags are missing. Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def split_xml_on_pagebreaks(\n    text: str,\n    page_groups: Optional[List[Tuple[int, int]]] = None,\n    keep_pagebreaks: bool = True,\n) -&gt; List[str]:\n    \"\"\"\n    Splits an XML document into individual pages based on &lt;pagebreak&gt; tags.\n    Optionally groups pages together based on page_groups and retains &lt;pagebreak&gt; tags if keep_pagebreaks is True.\n\n    Parameters:\n        text (str): The XML document as a string.\n        page_groups (Optional[List[Tuple[int, int]]]): A list of tuples defining page ranges to group together.\n                                                      Each tuple is of the form (start_page, end_page), inclusive.\n        keep_pagebreaks (bool): Whether to retain the &lt;pagebreak&gt; tags in the returned data. Default is False.\n\n    Returns:\n        List[str]: A list of page contents as strings, either split by pages or grouped by page_groups.\n\n    Raises:\n        ValueError: If the expected preamble or &lt;document&gt; tags are missing.\n    \"\"\"\n    # Split text into lines\n    lines = text.splitlines()\n\n    # Preprocess: Remove `&lt;?xml ... ?&gt;` preamble and &lt;document&gt; tags\n    if lines[0].startswith(\"&lt;?xml\"):\n        lines.pop(0)\n    else:\n        raise ValueError(\"Missing `&lt;?xml ... ?&gt;` preamble on the first line.\")\n    if lines[0].strip() == \"&lt;document&gt;\":\n        lines.pop(0)\n    else:\n        raise ValueError(\"Missing `&lt;document&gt;` opening tag on the second line.\")\n    if lines[-1].strip() == \"&lt;/document&gt;\":\n        lines.pop(-1)\n    else:\n        raise ValueError(\"Missing `&lt;/document&gt;` closing tag on the last line.\")\n\n    # Process content to split pages based on &lt;pagebreak&gt; tags\n    pages = []\n    current_page = []\n\n    for line in lines:\n        if \"&lt;pagebreak\" in line:  # Page boundary detected\n            if current_page:\n                page_content = \"\\n\".join(current_page).strip()\n                if keep_pagebreaks:\n                    page_content += f\"\\n{line.strip()}\"  # Retain the &lt;pagebreak&gt; tag\n                pages.append(page_content)\n                current_page = []\n        else:\n            current_page.append(line)\n\n    # Append the last page if it exists\n    if current_page:\n        pages.append(\"\\n\".join(current_page).strip())\n\n    # Validate that pages are extracted\n    if not pages:\n        raise ValueError(\"No pages found in the XML content.\")\n\n    # Group pages if page_groups is provided\n    if page_groups:\n        grouped_pages = []\n        for start, end in page_groups:\n            if group_content := [\n                pages[i] for i in range(start - 1, end) if 0 &lt;= i &lt; len(pages)\n            ]:\n                grouped_pages.append(\"\\n\".join(group_content).strip())\n        return grouped_pages\n\n    return pages\n</code></pre>"},{"location":"cli/audio-transcribe/","title":"audio-transcribe","text":"<p>Command-line tool for audio transcription tasks.</p>"},{"location":"cli/audio-transcribe/#usage","title":"Usage","text":"<pre><code>audio-transcribe [OPTIONS] [INPUT_FILE]\n</code></pre>"},{"location":"cli/audio-transcribe/#options","title":"Options","text":"<pre><code>-s, --split              Split audio into chunks\n-t, --transcribe         Transcribe the audio chunks\n-y, --yt_url TEXT        Single YouTube URL to process\n-v, --yt_url_csv PATH    CSV file containing multiple YouTube URLs\n...\n</code></pre>"},{"location":"cli/audio-transcribe/#examples","title":"Examples","text":""},{"location":"cli/audio-transcribe/#download-and-transcribe-from-youtube","title":"Download and Transcribe from YouTube","text":"<pre><code>audio-transcribe --yt_url \"https://youtube.com/watch?v=example\" --split --transcribe\n</code></pre>"},{"location":"cli/audio-transcribe/#process-local-audio-file","title":"Process Local Audio File","text":"<pre><code>audio-transcribe -f my_audio.mp3 --split --transcribe\n</code></pre>"},{"location":"cli/nfmt/","title":"nfmt","text":"<p><code>nfmt</code>, a newline formatting utility, standardizes line endings and spacing in text files.</p>"},{"location":"cli/nfmt/#usage","title":"Usage","text":"<pre><code>nfmt [OPTIONS] [INPUT_FILE]\n</code></pre>"},{"location":"cli/nfmt/#options","title":"Options","text":"<pre><code>-s, --spacing INTEGER  Number of newlines between paragraphs (default: 1)\n-o, --output PATH     Output file (default: stdout)\n</code></pre>"},{"location":"cli/nfmt/#examples","title":"Examples","text":""},{"location":"cli/nfmt/#basic-usage","title":"Basic Usage","text":"<pre><code># Format with single newline spacing\nnfmt input.txt &gt; formatted.txt\n\n# Format with double spacing\nnfmt -s 2 input.txt &gt; formatted.txt\n\n# Process from stdin\ncat input.txt | nfmt &gt; formatted.txt\n</code></pre>"},{"location":"cli/nfmt/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Standardizing line endings before processing</li> <li>Preparing text for pattern-based processing</li> <li>Cleaning up transcribed text</li> </ul>"},{"location":"cli/overview/","title":"Command Line Tools Overview","text":"<p>TNH Scholar provides a suite of command-line tools designed to work together for text processing. Each tool focuses on specific tasks while maintaining consistent interfaces and behavior. This overview introduces the available tools and their primary functions. </p>"},{"location":"cli/overview/#tnh-fab","title":"TNH-FAB","text":"<p>The primary text processing tool, TNH-FAB ('fab' short for 'fabric'), provides core functionality for text manipulation and analysis. This versatile tool includes several subcommands:</p> <p>The punctuate command handles text punctuation and basic formatting. It can work with multiple languages and adapts to various writing styles.</p> <p>The section command analyzes text structure and identifies logical divisions. It helps organize content into meaningful segments for further processing. This is crucial for working with larger text where model context limits may be reached or where processing will be ineffective due to content size.</p> <p>The translate command performs line-by-line translation while maintaining document structure. It provides context-aware translation particularly suited for wisdom and mindfulness content.</p> <p>The process command applies custom pattern-based processing to text. It offers flexible text transformation capabilities through the pattern system.</p>"},{"location":"cli/overview/#audio-transcribe","title":"Audio-Transcribe","text":"<p>The audio transcription tool handles conversion of audio content to text format. It provides several key capabilities:</p> <p>Audio downloading supports direct processing from YouTube URLs. The tool can handle both single videos and batch processing from CSV files.</p> <p>Audio splitting automatically divides long audio files into manageable chunks. It supports both silence-based and AI-assisted splitting methods.</p> <p>Transcription processes audio into text while maintaining timing information. The tool supports multiple languages and can include translations.</p>"},{"location":"cli/overview/#ytt-fetch","title":"YTT-Fetch","text":"<p>The YouTube Transcript Fetch utility specializes in retrieving video transcripts. It offers streamlined functionality:</p> <p>Direct transcript downloading from YouTube videos eliminates the need for full video processing. This approach saves time and resources when only text is needed.</p> <p>Language selection allows retrieval of transcripts in specific languages when available. The tool automatically handles language code conversion.</p> <p>Output formatting provides options for saving or displaying retrieved transcripts. The tool integrates smoothly with other processing commands.</p>"},{"location":"cli/overview/#token-count","title":"Token-Count","text":"<p>The token counting utility helps manage AI processing limits and costs. It provides essential information:</p> <p>Token calculation matches OpenAI's counting method exactly. This accuracy helps predict API usage and costs.</p> <p>Pipeline integration allows token counting at any processing stage. This capability helps optimize processing workflows.</p> <p>Standard input support enables flexible usage in command chains. The tool works seamlessly with other text processing commands.</p>"},{"location":"cli/overview/#tnh-setup","title":"TNH-Setup","text":"<p>The setup tool handles initial configuration and maintenance of the TNH Scholar environment. It manages several aspects:</p> <p>Directory creation establishes the required folder structure for patterns and logs. The tool ensures proper permissions and organization.</p> <p>Pattern management handles downloading and installation of default patterns. It maintains the pattern repository structure.</p> <p>Environment verification checks for necessary API keys and configurations. The tool provides guidance for missing requirements.</p>"},{"location":"cli/overview/#nfmt","title":"NFMT","text":"<p>The newline formatting utility standardizes text file formatting. It provides focused functionality:</p> <p>Line ending standardization ensures consistent text formatting. This standardization is particularly important for pattern processing.</p> <p>Spacing control allows customization of line spacing between paragraphs. The tool helps prepare text for various processing needs.</p> <p>Pipeline compatibility enables integration with other processing tools. This integration supports complex text processing workflows.</p>"},{"location":"cli/overview/#workflow-integration","title":"Workflow Integration","text":"<p>The TNH Scholar command-line tools are designed to work together effectively:</p> <p>Pipeline Support: All tools support both file input/output and standard streams, enabling complex processing pipelines.</p> <p>Consistent Interfaces: Tools maintain consistent option patterns and behavior, making them easy to learn and use together.</p> <p>Error Handling: Tools are intended to provide consistent error reporting and status information, helping with workflow debugging. (In the prototyping phase not all error handling is implemented.)</p> <p>Resource Management: Tools coordinate resource usage and maintain efficient processing patterns.</p>"},{"location":"cli/overview/#common-features","title":"Common Features","text":"<p>Several features are common across all TNH Scholar command-line tools:</p> <p>Standard Input/Output: All tools support both file-based and stream-based processing, enabling flexible usage.</p> <p>Verbose Modes: Tools provide detailed logging options for troubleshooting and monitoring.</p> <p>Documentation: Each tool includes help information and usage examples.</p> <p>Configuration: Tools respect both global and command-specific configuration options.</p>"},{"location":"cli/overview/#getting-started","title":"Getting Started","text":"<p>To begin using the command-line tools:</p> <ol> <li>Install TNH Scholar using pip</li> <li>Run tnh-setup to configure your environment</li> <li>Review individual tool documentation for specific usage details</li> <li>Start with simple commands and gradually build more complex pipelines</li> </ol> <p>For detailed information about each tool, refer to their specific documentation sections.</p>"},{"location":"cli/tnh-fab/","title":"tnh-fab","text":""},{"location":"cli/tnh-fab/#overview","title":"Overview","text":"<p><code>tnh-fab</code> is a specialized command-line tool, part of the Thich Nhat Hanh Scholar Project. <code>tnh-fab</code> can process multilingual texts using AI 'patterns'. It is originally developed to work with processing Dharma-based materials such as talks by Thich Nhat Hanh.</p> <p>It provides functionality for:</p> <ul> <li>Adding/correcting punctuation</li> <li>Identifying logical sections</li> <li>Performing line-based translation</li> <li>Applying custom text processing patterns</li> </ul>"},{"location":"cli/tnh-fab/#installation","title":"Installation","text":"<pre><code>pip install tnh-scholar\n</code></pre>"},{"location":"cli/tnh-fab/#basic-usage","title":"Basic Usage","text":"<pre><code>tnh-fab [COMMAND] [OPTIONS] [INPUT_FILE]\n</code></pre> <p>Input can be provided either as a file or through standard input (stdin).</p> <p>Global options:</p> <pre><code>-v, --verbose        Enable detailed logging\n--debug             Enable debug output\n--quiet             Suppress all non-error output\n</code></pre>"},{"location":"cli/tnh-fab/#commands","title":"Commands","text":""},{"location":"cli/tnh-fab/#1-punctuate","title":"1. Punctuate","text":"<p>Adds or corrects punctuation based on language-specific rules.</p> <p>Basic usage:</p> <pre><code>tnh-fab punctuate [OPTIONS] [INPUT_FILE]\n</code></pre> <p>Options:</p> <pre><code>-l, --language      Source language code (auto-detected if not specified)\n-y, --style         Punctuation style (default: 'APA')\n-c, --review-count  Number of review passes (default: 3)\n-p, --pattern       Pattern name (default: 'default_punctuate')\n</code></pre> <p>Examples:</p> <pre><code># Basic punctuation of a file\ntnh-fab punctuate input.txt\n\n# Punctuate Vietnamese text with specific style\ntnh-fab punctuate -l vi -y \"Modern\" vietnamese_text.txt\n\n# Process from stdin with extra review passes\ncat unpunctuated.txt | tnh-fab punctuate -c 5\n\n# Use custom pattern with specific language\ntnh-fab punctuate -p dharma_punctuation -l en text.txt\n</code></pre>"},{"location":"cli/tnh-fab/#2-section","title":"2. Section","text":"<p>Analyzes text and divides it into logical sections.</p> <p>Basic usage:</p> <pre><code>tnh-fab section [OPTIONS] [INPUT_FILE]\n</code></pre> <p>Options:</p> <pre><code>-l, --language      Source language code (auto-detected if not specified)\n-n, --num-sections  Target number of sections (auto-calculated if not specified)\n-c, --review-count  Number of review passes (default: 3)\n-p, --pattern       Pattern name (default: 'default_section')\n</code></pre> <p>Examples:</p> <pre><code># Auto-detect sections\ntnh-fab section dharma_talk.txt\n\n# Create specific number of sections\ntnh-fab section -n 5 long_text.txt\n\n# Section Vietnamese text with custom pattern\ntnh-fab section -l vi -p vn_section_pattern text.txt\n\n# Process from stdin and save to file\ncat text.txt | tnh-fab section &gt; sections.json\n</code></pre>"},{"location":"cli/tnh-fab/#3-translate","title":"3. Translate","text":"<p>Performs line-by-line translation while maintaining structure.</p> <p>Basic usage:</p> <pre><code>tnh-fab translate [OPTIONS] [INPUT_FILE]\n</code></pre> <p>Options:</p> <pre><code>-l, --language       Source language code (auto-detected if not specified)\n-r, --target         Target language code (default: 'en')\n-y, --style          Translation style\n--context-lines      Number of context lines (default: 3)\n--segment-size       Lines per translation segment (auto-calculated if not specified)\n-p, --pattern        Pattern name (default: 'default_line_translation')\n</code></pre> <p>Examples:</p> <pre><code># Basic Vietnamese to English translation\ntnh-fab translate -l vi vietnamese_text.txt\n\n# Translation with specific style\ntnh-fab translate -l vi -y \"American Dharma Teaching\" text.txt\n\n# French translation with increased context\ntnh-fab translate -l vi -r fr --context-lines 5 text.txt\n\n# Custom segment size and pattern\ntnh-fab translate --segment-size 20 -p custom_translation text.txt\n</code></pre>"},{"location":"cli/tnh-fab/#4-process","title":"4. Process","text":"<p>Applies custom pattern-based processing with flexible structuring.</p> <p>Basic usage:</p> <pre><code>tnh-fab process -p PATTERN [OPTIONS] [INPUT_FILE]\n</code></pre> <p>Options:</p> <pre><code>-p, --pattern     Pattern name (required)\n-s, --section     Process using sections from JSON file\n-g, --paragraph   Process text by paragraphs\n-t, --template    YAML file with template values\n</code></pre> <p>Examples:</p> <pre><code># Basic processing with pattern\ntnh-fab process -p format_xml input.txt\n\n# Process using sections from file\ntnh-fab process -p format_xml -s sections.json input.txt\n\n# Process by paragraphs\ntnh-fab process -p format_xml -g long_text.txt\n\n# Process with template values\ntnh-fab process -p format_xml -t template.yaml input.txt\n</code></pre>"},{"location":"cli/tnh-fab/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"cli/tnh-fab/#pipeline-processing","title":"Pipeline Processing","text":""},{"location":"cli/tnh-fab/#1-punctuate-section-and-process","title":"1. Punctuate, section, and process","text":"<pre><code>cat raw_text.txt | \\\ntnh-fab punctuate -l vi | \\\ntnh-fab section -n 5 | \\\ntnh-fab process -p format_xml &gt; final_output.xml\n</code></pre>"},{"location":"cli/tnh-fab/#2-section-and-translate","title":"2. Section and translate","text":"<pre><code>tnh-fab section input.txt &gt; sections.json\ntnh-fab translate -l vi -s sections.json input.txt &gt; translated.txt\n</code></pre>"},{"location":"cli/tnh-fab/#3-complete-processing-pipeline","title":"3. Complete processing pipeline","text":"<pre><code>cat vietnamese_text.txt | \\\ntnh-fab punctuate -l vi -y \"Modern\" | \\\ntnh-fab section --num-sections 10 | \\\ntnh-fab translate -r en --context-lines 5 | \\\ntnh-fab process -p format_xml &gt; final_output.xml\n</code></pre>"},{"location":"cli/tnh-fab/#pattern-system","title":"Pattern System","text":"<p>Patterns are stored in the pattern directory (default or specified by TNH_PATTERN_DIR environment variable). Each command uses specific default patterns that can be overridden using the -p option.</p> <p>For information about creating custom patterns, please refer to the pattern documentation.</p>"},{"location":"cli/tnh-setup/","title":"tnh-setup","text":"<p>The <code>tnh-setup</code> command configures the TNH Scholar environment, setting up necessary directories and downloading default patterns.</p>"},{"location":"cli/tnh-setup/#usage","title":"Usage","text":"<pre><code>tnh-setup [OPTIONS]\n</code></pre>"},{"location":"cli/tnh-setup/#options","title":"Options","text":"<pre><code>--skip-env       Skip API key setup check\n--skip-patterns  Skip pattern download\n--help          Show this message and exit\n</code></pre>"},{"location":"cli/tnh-setup/#configuration-steps","title":"Configuration Steps","text":"<p>The setup process includes:</p> <ol> <li>Directory Creation</li> <li>Creates ~/.config/tnh_scholar/</li> <li>Creates patterns directory</li> <li> <p>Creates logs directory</p> </li> <li> <p>Pattern Download</p> </li> <li>Offers to download default patterns from GitHub</li> <li>Patterns are saved to ~/.config/tnh_scholar/patterns/</li> <li> <p>Maintains directory structure from repository</p> </li> <li> <p>Environment Check</p> </li> <li>Verifies OpenAI API key configuration</li> <li>Provides guidance if key is missing</li> </ol>"},{"location":"cli/tnh-setup/#examples","title":"Examples","text":""},{"location":"cli/tnh-setup/#complete-setup","title":"Complete Setup","text":"<pre><code># Run full setup\ntnh-setup\n</code></pre>"},{"location":"cli/tnh-setup/#selective-setup","title":"Selective Setup","text":"<pre><code># Skip API key check\ntnh-setup --skip-env\n\n# Skip pattern download\ntnh-setup --skip-patterns\n\n# Skip both\ntnh-setup --skip-env --skip-patterns\n</code></pre>"},{"location":"cli/tnh-setup/#default-patterns","title":"Default Patterns","text":"<p>When downloading patterns, the following are included:</p> <ul> <li>default_punctuate.md</li> <li>default_section.md</li> <li>default_line_translation.md</li> <li>default_xml_format.md</li> <li>default_xml_paragraph_format.md</li> </ul>"},{"location":"cli/tnh-setup/#environment-variables","title":"Environment Variables","text":"<p>The setup process checks for and uses:</p> <ul> <li>OPENAI_API_KEY: Required for AI functionality</li> <li>TNH_PATTERN_DIR: Optional custom pattern directory</li> </ul>"},{"location":"cli/tnh-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/tnh-setup/#missing-api-key","title":"Missing API Key","text":"<p>If the OpenAI API key is not found, the setup tool displays guidance:</p> <pre><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; OpenAI API key not found in environment. &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n\nFor AI processing with TNH-scholar:\n\n1. Get an API key from https://platform.openai.com/api-keys\n2. Set the OPENAI_API_KEY environment variable:\n\n   export OPENAI_API_KEY='your-api-key-here'  # Linux/Mac\n   set OPENAI_API_KEY=your-api-key-here       # Windows\n\nFor OpenAI API access help: https://platform.openai.com/\n</code></pre>"},{"location":"cli/tnh-setup/#pattern-download-issues","title":"Pattern Download Issues","text":"<ul> <li>Check internet connection</li> <li>Verify GitHub access</li> <li>Ensure write permissions in config directory</li> <li>Check disk space</li> </ul>"},{"location":"cli/tnh-setup/#post-setup-verification","title":"Post-Setup Verification","text":"<p>After running setup, verify:</p> <ol> <li>Directory Structure:</li> </ol> <p><code>plaintext    ~/.config/tnh_scholar/    \u251c\u2500\u2500 patterns/    \u2514\u2500\u2500 logs/</code></p> <ol> <li>Pattern Files:</li> <li>Check that pattern files are present</li> <li>Verify file permissions</li> <li> <p>Ensure proper formatting</p> </li> <li> <p>Environment:</p> </li> <li>Confirm API key is set</li> <li>Test basic AI functionality</li> </ol>"},{"location":"cli/token-count/","title":"token-count","text":"<p>The <code>token-count</code> command calculates the OpenAI API token count for text input. This is useful for ensuring that a text is within maximum token limits for the API model and also for estimating API costs.</p>"},{"location":"cli/token-count/#usage","title":"Usage","text":"<pre><code>token-count [INPUT_FILE]\n</code></pre> <p>If no input file is specified, reads from standard input (stdin).</p>"},{"location":"cli/token-count/#examples","title":"Examples","text":""},{"location":"cli/token-count/#count-tokens-in-file","title":"Count Tokens in File","text":"<pre><code>token-count input.txt\n</code></pre>"},{"location":"cli/token-count/#count-tokens-from-stdin","title":"Count Tokens from Stdin","text":"<pre><code>echo \"Sample text\" | token-count\ncat input.txt | token-count\n</code></pre>"},{"location":"cli/token-count/#pipeline-usage","title":"Pipeline Usage","text":"<pre><code># Count tokens after processing\ncat input.txt | tnh-fab punctuate | token-count\n\n# Count tokens at multiple stages\ncat input.txt | tee &gt;(token-count &gt;&amp;2) | \\\n  tnh-fab punctuate | tee &gt;(token-count &gt;&amp;2) | \\\n  tnh-fab process -p format_xml | token-count\n</code></pre>"},{"location":"cli/token-count/#output","title":"Output","text":"<p>Returns a single integer representing the number of tokens in the input text, calculated using the OpenAI tokenizer.</p>"},{"location":"cli/token-count/#notes","title":"Notes","text":"<ul> <li>Uses the same tokenizer as GPT-4</li> <li>Counts are exact matches to OpenAI API usage</li> <li>Useful for:</li> <li>Cost estimation</li> <li>Context window planning</li> <li>Processing pipeline optimization</li> <li>Model input validation</li> </ul>"},{"location":"cli/token-count/#see-also","title":"See Also","text":"<ul> <li>OpenAI tokenizer documentation</li> <li>TNH Scholar API documentation for token counting</li> <li>tnh-fab documentation for text processing</li> </ul>"},{"location":"cli/ytt-fetch/","title":"ytt-fetch","text":"<p>(Y)ou(T)ube (T)ranscript-(Fetch)ing utility.</p>"},{"location":"cli/ytt-fetch/#usage","title":"Usage","text":"<pre><code>ytt-fetch [OPTIONS] URL\n</code></pre>"},{"location":"cli/ytt-fetch/#options","title":"Options","text":"<pre><code>-l, --lang TEXT     Language code for transcript (default: en)\n-o, --output PATH   Save transcript text to file\n</code></pre>"},{"location":"cli/ytt-fetch/#examples","title":"Examples","text":""},{"location":"cli/ytt-fetch/#download-english-transcript","title":"Download English Transcript","text":"<pre><code>ytt-fetch \"https://youtube.com/watch?v=example\" -l en -o transcript.txt\n</code></pre>"},{"location":"cli/ytt-fetch/#print-transcript-to-console","title":"Print Transcript to Console","text":"<pre><code>ytt-fetch \"https://youtube.com/watch?v=example\"\n</code></pre>"},{"location":"design/sectioning/TextObject%20New%20Design/","title":"New TextObject System Design Document","text":""},{"location":"design/sectioning/TextObject%20New%20Design/#1-overview-and-purpose","title":"1. Overview and Purpose","text":"<p>The TextObject system provides a robust way to manage sectioned text content with associated metadata. It serves two key purposes: internal text processing with rich metadata support, and integration with AI services through a simplified format. The design emphasizes simplicity, standardization, and effective AI context handling.</p>"},{"location":"design/sectioning/TextObject%20New%20Design/#2-core-components","title":"2. Core Components","text":""},{"location":"design/sectioning/TextObject%20New%20Design/#21-response-format-api-layer","title":"2.1 Response Format (API Layer)","text":"<p>The response format provides a simplified structure optimized for AI interaction:</p> <pre><code>class LogicalSection(BaseModel):\n    \"\"\"Represents a logical division of text content.\"\"\"\n    start_line: int = Field(..., description=\"Starting line number of section (inclusive)\")\n    title: str = Field(..., description=\"Title describing section content\")\n\nclass TextMetadataFormat(BaseModel):\n    \"\"\"Simplified metadata format for AI processing.\"\"\"\n    metadata_summary: str = Field(\n        ...,\n        description=\"Available metadata in human-readable format\"\n    )\n    context: str = Field(\n        ...,\n        description=\"Rich contextual information for AI understanding\"\n    )\n\nclass TextObjectFormat(BaseModel):\n    \"\"\"Complete format for API interactions.\"\"\"\n    metadata: TextMetadataFormat\n    language: str = Field(..., description=\"ISO 639-1 language code\")\n    sections: List[LogicalSection]\n</code></pre>"},{"location":"design/sectioning/TextObject%20New%20Design/#22-internal-representation","title":"2.2 Internal Representation","text":"<p>The internal system uses a richer structure based on Dublin Core standards:</p> <pre><code>@dataclass\nclass TextMetadata:\n    \"\"\"Rich metadata container following Dublin Core standards.\"\"\"\n    # Core Dublin Core elements\n    title: str\n    creator: List[str]\n    subject: List[str]\n    description: str\n    publisher: Optional[str] = None\n    contributor: List[str] = field(default_factory=list)\n    date: Optional[str] = None\n    type: str\n    format: str\n    identifier: Optional[str] = None\n    source: Optional[str] = None\n    language: str\n\n    # Additional contextual information\n    context: str = field(default=\"\")\n    additional_info: Dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"design/sectioning/TextObject%20New%20Design/#23-primary-textobject-class","title":"2.3 Primary TextObject Class","text":"<pre><code>class TextObject:\n    \"\"\"\n    Main class for managing sectioned text content with metadata.\n    \"\"\"\n    def __init__(\n        self,\n        numbered_text: NumberedText,\n        language: str,\n        sections: List[LogicalSection],\n        metadata: TextMetadata\n    ):\n        self.content = numbered_text\n        self.language = language\n        self.sections = sorted(sections, key=lambda x: x.start_line)\n        self.metadata = metadata\n        self.total_lines = numbered_text.size\n        self._validate()\n</code></pre>"},{"location":"design/sectioning/TextObject%20New%20Design/#3-key-design-decisions","title":"3. Key Design Decisions","text":""},{"location":"design/sectioning/TextObject%20New%20Design/#31-metadata-handling","title":"3.1 Metadata Handling","text":"<p>The system employs a dual approach to metadata:</p> <ol> <li>Internal Storage</li> <li>Uses Dublin Core standards for structured metadata</li> <li>Maintains complete metadata in standardized format</li> <li> <p>Supports optional fields and additional information</p> </li> <li> <p>API Representation</p> </li> <li>Converts metadata to human-readable string format</li> <li>Separates factual metadata from contextual information</li> <li>Optimizes for AI processing and generation</li> </ol>"},{"location":"design/sectioning/TextObject%20New%20Design/#32-section-management","title":"3.2 Section Management","text":"<p>The system uses a simplified section representation:</p> <ul> <li>Sections defined only by start line</li> <li>Implicit end lines (next section's start - 1)</li> <li>Guaranteed contiguous coverage</li> <li>Ordered storage and validation</li> </ul>"},{"location":"design/sectioning/TextObject%20New%20Design/#33-content-integration","title":"3.3 Content Integration","text":"<p>Content management is delegated to NumberedText class:</p> <ul> <li>Clean separation of concerns</li> <li>Efficient text storage and access</li> <li>Section-aware interface</li> </ul>"},{"location":"design/sectioning/TextObject%20New%20Design/#4-implementation-details","title":"4. Implementation Details","text":""},{"location":"design/sectioning/TextObject%20New%20Design/#41-format-conversion","title":"4.1 Format Conversion","text":"<pre><code>class TextObject:\n    def to_response_format(self) -&gt; TextObjectFormat:\n        \"\"\"Convert to API format.\"\"\"\n        metadata_summary = self._format_metadata_string()\n\n        return TextObjectFormat(\n            metadata=TextMetadataFormat(\n                metadata_summary=metadata_summary,\n                context=self.metadata.context\n            ),\n            language=self.language,\n            sections=self.sections\n        )\n\n    @classmethod\n    def from_response_format(cls, \n                           text: NumberedText,\n                           response: TextObjectFormat) -&gt; \"TextObject\":\n        \"\"\"Create from API response.\"\"\"\n        metadata = cls._parse_metadata_string(\n            response.metadata.metadata_summary,\n            response.metadata.context\n        )\n        return cls(\n            numbered_text=text,\n            language=response.language,\n            sections=response.sections,\n            metadata=metadata\n        )\n</code></pre>"},{"location":"design/sectioning/TextObject%20New%20Design/#42-validation","title":"4.2 Validation","text":"<pre><code>def _validate(self) -&gt; None:\n    \"\"\"Validate section integrity.\"\"\"\n    if not self.sections:\n        raise ValueError(\"TextObject must have at least one section\")\n\n    # Validate section ordering\n    for i, section in enumerate(self.sections):\n        if section.start_line &lt; 1:\n            raise ValueError(f\"Section {i}: start line must be &gt;= 1\")\n        if section.start_line &gt; self.total_lines:\n            raise ValueError(f\"Section {i}: start line exceeds text length\")\n        if i &gt; 0 and section.start_line &lt;= self.sections[i-1].start_line:\n            raise ValueError(f\"Section {i}: non-sequential start line\")\n</code></pre>"},{"location":"design/sectioning/TextObject%20New%20Design/#5-usage-patterns","title":"5. Usage Patterns","text":""},{"location":"design/sectioning/TextObject%20New%20Design/#51-section-access","title":"5.1 Section Access","text":"<pre><code>def get_section_content(self, index: int) -&gt; str:\n    \"\"\"Retrieve content for specific section.\"\"\"\n    start = self.sections[index].start_line\n    end = (self.sections[index + 1].start_line \n           if index &lt; len(self.sections) - 1 \n           else self.total_lines + 1)\n    return self.content.get_segment(start, end)\n</code></pre>"},{"location":"design/sectioning/TextObject%20New%20Design/#52-metadata-processing","title":"5.2 Metadata Processing","text":"<pre><code>def _format_metadata_string(self) -&gt; str:\n    \"\"\"Convert structured metadata to formatted string.\"\"\"\n    parts = []\n    if self.metadata.title:\n        parts.append(f\"Title: {self.metadata.title}\")\n    if self.metadata.creator:\n        parts.append(f\"Creator(s): {', '.join(self.metadata.creator)}\")\n    # Add other metadata fields...\n    return \"\\n\".join(parts)\n</code></pre>"},{"location":"design/sectioning/TextObject%20New%20Design/#6-future-considerations","title":"6. Future Considerations","text":"<ol> <li>Performance Optimization</li> <li>Index sections for faster access</li> <li>Cache frequently accessed content</li> <li> <p>Optimize metadata string parsing</p> </li> <li> <p>Extended Functionality</p> </li> <li>Section manipulation (merge/split)</li> <li>Advanced metadata querying</li> <li> <p>Enhanced validation rules</p> </li> <li> <p>Integration Enhancements</p> </li> <li>Support for additional metadata standards</li> <li>Expanded AI context generation</li> <li>Bulk processing capabilities</li> </ol>"},{"location":"design/sectioning/TextObject%20Original%20Design/","title":"TextObject Current Design","text":""},{"location":"design/sectioning/TextObject%20Original%20Design/#overview","title":"Overview","text":"<p>TextObject provides structured text handling with explicit section boundaries and metadata support, built on Pydantic models for validation.</p>"},{"location":"design/sectioning/TextObject%20Original%20Design/#core-components","title":"Core Components","text":""},{"location":"design/sectioning/TextObject%20Original%20Design/#logicalsection","title":"LogicalSection","text":"<pre><code>class LogicalSection(BaseModel):\n    title: str\n    start_line: int\n    end_line: int\n</code></pre>"},{"location":"design/sectioning/TextObject%20Original%20Design/#textobject","title":"TextObject","text":"<p>Primary container managing:</p> <ul> <li>Section boundaries</li> <li>Language information</li> <li>Dublin Core metadata</li> <li>NumberedText content</li> </ul>"},{"location":"design/sectioning/TextObject%20Original%20Design/#key-design-points","title":"Key Design Points","text":""},{"location":"design/sectioning/TextObject%20Original%20Design/#section-management","title":"Section Management","text":"<ul> <li>Explicit start and end lines for sections</li> <li>Validation ensures no overlaps or gaps</li> <li>Ordered storage by line number</li> </ul>"},{"location":"design/sectioning/TextObject%20Original%20Design/#content-integration","title":"Content Integration","text":"<ul> <li>Immutable NumberedText reference</li> <li>Line number integrity maintained</li> <li>Section boundaries must match content</li> </ul>"},{"location":"design/sectioning/TextObject%20Original%20Design/#metadata","title":"Metadata","text":"<ul> <li>Dublin Core based structure</li> <li>Optional/required field handling</li> <li>Flexible additional metadata support</li> </ul>"},{"location":"design/sectioning/TextObject%20Original%20Design/#implementation-notes","title":"Implementation Notes","text":""},{"location":"design/sectioning/TextObject%20Original%20Design/#validation-requirements","title":"Validation Requirements","text":"<ul> <li>Sections must not overlap</li> <li>No gaps between sections allowed</li> <li>Line numbers must exist in content</li> <li>Language codes accepted but not validated</li> </ul>"},{"location":"design/sectioning/TextObject%20Original%20Design/#api-considerations","title":"API Considerations","text":"<ul> <li>Pydantic models ensure clean serialization</li> <li>Clear validation errors</li> <li>Type safety throughout</li> </ul>"},{"location":"design/setup-tnh/setup-design/","title":"minimal but extensible setup tool for the prototyping phase","text":"<p>Core Requirements:</p> <ol> <li>Configuration directory setup (~/.config/tnh_scholar)</li> <li>Basic .env file for API keys</li> <li>Pattern directory preparation</li> <li>Simple validation checks</li> </ol>"},{"location":"design/setup-tnh/setup-design/#high-level-design-structure-iteration-1","title":"high-level design structure (iteration 1)","text":"<pre><code>tnh-setup/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 setup.py           # Main CLI entry point\n\u251c\u2500\u2500 config.py          # Configuration handling\n\u251c\u2500\u2500 patterns.py        # Pattern management \n\u2514\u2500\u2500 validators.py      # Basic validation\n\nKey Functions:\n1. Directory Setup\n   - Create ~/.config/tnh_scholar\n   - Create patterns subdirectory\n   - Create logs subdirectory\n\n2. Environment Setup\n   - Create/update .env file\n   - Basic OpenAI key validation\n   - (Optional) Google Cloud key setup\n\n3. Pattern Setup\n   - Create patterns directory\n   - Minimal set of default patterns\n   - (Future) Pattern repository integration\n\n4. Validation\n   - Check directory permissions\n   - Verify API key format\n   - Test pattern loading\n</code></pre> <p>For prototyping, suggested implementation these core functions:</p> <ol> <li>Directory creation</li> <li>Basic .env file with OpenAI key</li> <li>Pattern directory preparation</li> <li>Simple CLI interface</li> </ol>"},{"location":"design/setup-tnh/setup-design/#high-level-design-structure-iteration-2","title":"high-level design structure (iteration 2)","text":"<pre><code>TNH-SETUP TOOL DESIGN\n\n1. Core Functions\n\n   - Create config directory (~/.config/tnh_scholar)\n\n   - Set up OpenAI API key in .env\n\n   - Optional pattern download\n\n2. User Flow\n\n   A. Config Directory\n\n      - Create ~/.config/tnh_scholar\n\n      - Create logs subdirectory\n\n   B. Environment Setup \n\n      - Prompt for OpenAI API key\n\n      - Create/update .env file\n\n   C. Pattern Setup (Optional)\n\n      - Show confirmation message:\n\n        \"Public pattern files (markdown templates) will be downloaded from:\n\n         https://github.com/aaronksolomon/patterns \n\n         and installed to: ~/.config/tnh_scholar/patterns\"\n\n      - If confirmed, download and extract patterns\n\n3. Command Line Interface\n\n   tnh-setup [OPTIONS]\n\n   Options:\n\n   --skip-env        Skip API key setup\n\n   --skip-patterns   Skip pattern download\n\n4. Files Structure\n\n   setup/\n\n   \u251c\u2500\u2500 init.py\n\n   \u251c\u2500\u2500 setup.py      # Main CLI tool\n\n   \u2514\u2500\u2500 download.py   # Pattern download function\n\n5. Dependencies\n\n   - click\n\n   - requests\n\n   - python-dotenv\n\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/","title":"TNH FAB Design Document","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#overview","title":"Overview","text":"<p><code>tnh-fab</code> is a command-line text processing tool that provides standalone but pipeable text processing operations, with a focus on simplicity and flexibility. It is part of the <code>tnh-scholar</code> suite of tools.</p>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#core-functionality","title":"Core Functionality","text":"<ul> <li>Text punctuation</li> <li>Section creation and management </li> <li>Translation (line-by-line and block)</li> <li>General text processing with patterns</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#command-structure","title":"Command Structure","text":"<pre><code>tnh-fab &lt;command&gt; [options] [input_file]\n</code></pre> <p>Commands: - <code>punctuate</code>: Add punctuation and structure to text - <code>section</code>: Create text sections - <code>translate</code>: Perform line-by-line translation - <code>process</code>: Execute pattern-based text processing (typically outputting XML format)</p>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#global-options","title":"Global Options","text":"<pre><code>-d, --output-dir DIR     Output directory (default: current)\n-v, --verbose            Detailed logging\n-l, --language LANG     Source language code of input text (auto-detect if not specified)\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#command-specific-options","title":"Command-Specific Options","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#punctuate","title":"Punctuate","text":"<pre><code>-p, --pattern NAME       Pattern name for punctuation (uses default if not specified)\n-y, --style STYLE       Punctuation style (default: APA)\n-o, --output FILE       Output file (default: stdout or FILE_punct.txt)\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#section","title":"Section","text":"<pre><code>-p, --pattern NAME      Pattern name for sectioning\n-n, --num NUM          Target number of sections\n-o, --output FILE      Output JSON file (default: stdout or FILE_sections.json)\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#translate","title":"Translate","text":"<pre><code>-p, --pattern NAME      Pattern for translation (uses default if not specified)\n-t, --template FILE     Template values file\n-o, --output FILE      Output file (default: stdout)\n-r, --target LANG       The target output language code (default is 'en,' English)\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#process","title":"Process","text":"<pre><code>-p, --pattern NAME      Pattern for processing (REQUIRED)\n-g, --paragraph         Use line separated paragraphs as sections.\n-s, --sections FILE     JSON file containing section data\n-t, --template FILE     Template values file\n-f, --format FORMAT     Output format: txt/json/yaml (default: txt)\n-o, --output FILE      Output file (default: stdout)\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#usage-examples","title":"Usage Examples","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#basic-usage","title":"Basic Usage","text":"<pre><code># Punctuate text\ntnh-fab punctuate input.txt\ntnh-fab punctuate -l vi input.txt &gt; punctuated.txt\n\n# Create sections\ntnh-fab section input.txt -n 5 -o sections.json\ncat input.txt | tnh-fab section &gt; sections.json\n\n# Translate text\ntnh-fab translate input.txt -p vi_en\ncat input.txt | tnh-fab translate -p vi_en &gt; translated.txt\n\n# Process with pattern\ntnh-fab process -p format_xml -s sections.json input.txt\ncat sections.json | tnh-fab process -p format_xml input.txt\n\n# Process by paragraphs\ntnh-fab process -g -p format_xml input.txt\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#pipeline-examples","title":"Pipeline Examples","text":"<pre><code># Punctuate and section\ncat input.txt | tnh-fab punctuate | tnh-fab section &gt; sections.json\n\n# Section and process\ntnh-fab section input.txt | tnh-fab process -p format_xml &gt; output.xml\n\n# Translate and process\ncat input.txt | tnh-fab translate | tnh-fab process -p format_md\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#configuration","title":"Configuration","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#directory-structure","title":"Directory Structure","text":"<pre><code>~/.config/tnh_scholar/\n\u251c\u2500\u2500 patterns/           # Pattern files\n\u2502   \u251c\u2500\u2500 punctuate/\n\u2502   \u251c\u2500\u2500 section/\n\u2502   \u251c\u2500\u2500 translate/\n\u2502   \u2514\u2500\u2500 process/\n\u2514\u2500\u2500 tnh-fab/\n    \u2514\u2500\u2500 settings.yaml   # Default configurations\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#default-configuration-settingsyaml","title":"Default Configuration (settings.yaml)","text":"<pre><code>defaults:\n  punctuate:\n    pattern: default_punctuate\n    style: APA\n    language: auto\n  section:\n    pattern: default_section\n    num_sections: auto\n    review_count: 3\n  translate:\n    pattern: default_translate\n  process:\n    format: txt\n\npattern_path: ~/.config/tnh_scholar/patterns\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#inputoutput-handling","title":"Input/Output Handling","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#input-sources","title":"Input Sources","text":"<ul> <li>File specified as argument</li> <li>STDIN (piped input)</li> <li>Section data (JSON format)</li> <li>Template files (YAML format)</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#output-handling","title":"Output Handling","text":"<ul> <li>STDOUT (default for piping)</li> <li>Specified output file (-o)</li> <li>Default file naming (if no -o): input_stage.ext</li> <li>JSON output for sections</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#pattern-management","title":"Pattern Management","text":"<ul> <li>Uses existing PatternManager class for pattern resolution</li> <li>Uses configured Pattern path</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#special-notes","title":"Special Notes","text":"<ol> <li>Translation is implemented as a standalone command (<code>translate</code>) for line-by-line processing, however can also be accomplished as a process pattern option for section translation</li> <li>Each command is standalone but designed for pipeline compatibility</li> <li>All commands default to STDIN/STDOUT unless specific files are provided</li> <li>Section data is always in JSON format for compatibility</li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#-","title":"---","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#tnh-fab-process-detailed-specification","title":"TNH-FAB PROCESS: detailed Specification","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#overview_1","title":"Overview","text":"<p>The <code>process</code> command applies pattern-based text processing using optional section data. It can receive input from files and/or STDIN, with flexible output options. Typical usage is XML output.</p>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#command-format","title":"Command Format","text":"<pre><code>tnh-fab process [options] [input_file]\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#options","title":"Options","text":"<pre><code>-p, --pattern NAME      Pattern name for processing (REQUIRED)\n-s, --sections FILE     JSON file containing section data\n-g, --paragraph         Process text by newliine separated paragraphs\n-t, --template FILE     Template values file (YAML format)\n-k, --key-values PAIRS  Space-separated key:value pairs (e.g., speaker:\"Name\" title:\"Title\")\n-f, --format FORMAT     Output format: XML/txt (default: XML)\n-o, --output FILE      Output file (default: stdout)\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#input-handling","title":"Input Handling","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#input-sources_1","title":"Input Sources","text":"<ul> <li>Text content can come from:</li> <li>File specified as argument</li> <li>STDIN</li> <li>Section data can come from:</li> <li>JSON file specified with -s</li> <li>STDIN when paired with input file</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#input-scenarios","title":"Input Scenarios","text":"<ol> <li>Single File Input <code>bash    tnh-fab process -p format_xml input.txt</code></li> <li> <p>Processes input.txt directly</p> </li> <li> <p>STDIN Only <code>bash    cat input.txt | tnh-fab process -p format_xml    cat input.txt | tnh-fab process -g -p format_xml  # process by paragraphs</code></p> </li> <li> <p>Processes text from STDIN</p> </li> <li> <p>File + Sections File <code>bash    tnh-fab process -p format_xml -s sections.json input.txt</code></p> </li> <li> <p>Processes input.txt using sections from sections.json</p> </li> <li> <p>STDIN + Sections File <code>bash    cat input.txt | tnh-fab process -p format_xml -s sections.json</code></p> </li> <li> <p>Processes STDIN text using sections from sections.json</p> </li> <li> <p>Section Stream + Input File <code>bash    tnh-fab section input.txt | tnh-fab process -p format_xml input.txt</code></p> </li> <li>Processes input.txt using sections from STDIN</li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#input-validation","title":"Input Validation","text":"<ul> <li>When sections are provided (via -s or STDIN):</li> <li>Validates JSON format matches TextObject schema</li> <li>Checks source_file field in TextObject if present</li> <li>Warns if source_file doesn't match input file name</li> <li>Validates section line ranges against input text</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#template-value-handling","title":"Template Value Handling","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#priority-order-highest-to-lowest","title":"Priority Order (highest to lowest)","text":"<ol> <li>Command line key-values (-k)</li> <li>Template file values (-t)</li> <li>Default values from pattern</li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#key-value-format","title":"Key-Value Format","text":"<ul> <li>Space-separated pairs</li> <li>Key and value joined by colon</li> <li>Values with spaces must be quoted</li> <li>Example: <code>speaker:\"Robert Smith\" title:\"My Journey\"</code></li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#output-handling_1","title":"Output Handling","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#output-destinations","title":"Output Destinations","text":"<ul> <li>STDOUT (default)</li> <li>File specified by -o option</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#format-options","title":"Format Options","text":"<ul> <li>txt (default): Plain text output</li> <li>json: JSON formatted output</li> <li>yaml: YAML formatted output</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#error-handling","title":"Error Handling","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#input-errors","title":"Input Errors","text":"<ul> <li>Missing required pattern</li> <li>Invalid section JSON format</li> <li>Section/input file mismatch</li> <li>Missing input when required</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#template-errors","title":"Template Errors","text":"<ul> <li>Invalid template file format</li> <li>Invalid key-value pair format</li> <li>Missing required template values</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v1/#usage-examples_1","title":"Usage Examples","text":"<pre><code># Basic file processing\ntnh-fab process -p format_pattern input.txt &gt; output.xml\n\n# Process with template values\ntnh-fab process -p format_pattern -k speaker:\"Robert Smith\" input.txt\n\n# Process with sections file\ntnh-fab process -p format_pattern -s sections.json input.txt\n\n# Process STDIN with sections\ncat input.txt | tnh-fab process -p format_pattern -s sections.json\n\n# Pipeline from section command\ntnh-fab section input.txt | tnh-fab process -p format_pattern input.txt\n\n# Complete example with all options\ntnh-fab process -p format_pattern \\\n  -s sections.json \\\n  -t template.yaml \\\n  -k speaker:\"Robert Smith\" \\\n  -f json \\\n  -o output.json \\\n  input.txt\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/","title":"TNH-FAB Command Line Tool Specification","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#overview","title":"Overview","text":"<p><code>tnh-fab</code> is a command-line text processing tool providing standalone but pipeable operations for Buddhist text processing. It is part of the <code>tnh-scholar</code> suite of tools, focusing on simplicity, flexibility, and consistent behavior.</p>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#core-functionality","title":"Core Functionality","text":"<ul> <li>Text punctuation and formatting</li> <li>Section creation and management</li> <li>Translation (line-by-line and block)</li> <li>Pattern-based text processing</li> <li>XML/structured output generation</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#command-structure","title":"Command Structure","text":"<pre><code>tnh-fab &lt;command&gt; [options] [input_file]\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#global-options","title":"Global Options","text":"<pre><code>Input/Output:\n  [input_file]                Input file (optional, uses STDIN if not provided)\n  -o, --output FILE          Output file (default: STDOUT)\n  -d, --output-dir DIR       Output directory (default: current)\n  -f, --format FORMAT        Output format: txt/json/yaml/xml (default varies by command)\n\nConfiguration:\n  -l, --language LANG        Source language code (auto-detect if not specified)\n  -t, --template FILE        Template values file (YAML format)\n  -k, --key-values PAIRS     Space-separated key:value pairs (e.g., speaker:\"Name\")\n  -p, --pattern NAME         Pattern name (command-specific default if not specified)\n  -c, --review-count NUM       Number of review passes (default: 3)\n\nLogging:\n  -v, --verbose             Enable detailed logging\n  --debug                   Enable debug output\n  --quiet                   Suppress all non-error output\n\nOther:\n  -h, --help               Show command-specific help\n  --version                Show version information\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#commands","title":"Commands","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#punctuate","title":"punctuate","text":"<p>Add punctuation and structure to text.</p> <pre><code>Additional Options:\n  -y, --style STYLE            Punctuation style (default: configuration file)\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#section","title":"section","text":"<p>Create and manage text sections.</p> <pre><code>Additional Options:\n  -n, --num-sections NUM   Target number of sections (default: auto)\n  --target-section-size NUM   Target section size in tokens (default: configuration file)\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#translate","title":"translate","text":"<p>Perform line-by-line or block translation.</p> <pre><code>Additional Options:\n  -r, --target LANG        Target language code (default: en)\n  -y, --style STYLE            Translation style (default: configuration file)\n  --context-lines NUM      Number of context lines (default: 3)\n  --segment-size NUM       Lines per translation segment (default: auto)\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#process","title":"process","text":"<p>Execute pattern-based text processing. Can work on sections of data or on the whole input stream.</p> <pre><code>Additional Options:\n  -s, --sections FILE      JSON file containing section data\n  -g, --paragraph         Use line-separated paragraphs as sections\n  --xml                   Wrap output in XML style document tags\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#inputoutput-handling","title":"Input/Output Handling","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#input-sources","title":"Input Sources","text":"<ul> <li>File specified as argument</li> <li>STDIN (piped input)</li> <li>Section data (JSON format)</li> <li>Template files (YAML format)</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#input-processing","title":"Input Processing","text":"<ol> <li>Text content priority:</li> <li>Named input file</li> <li>STDIN if no file specified</li> <li> <p>Error if neither available</p> </li> <li> <p>Section data priority:</p> </li> <li>JSON file specified with -s</li> <li>STDIN when paired with input file</li> <li> <p>Auto-generated sections if no pattern specified</p> </li> <li> <p>Template values priority:</p> </li> <li>Command line key-values (-k)</li> <li>Template file values (-t)</li> <li>Default values from pattern</li> <li>Environment variables (TNH_FAB_*)</li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#output-handling","title":"Output Handling","text":"<ol> <li>Output destination priority:</li> <li>File specified by -o</li> <li> <p>STDOUT if no file specified</p> </li> <li> <p>Format determination:</p> </li> <li>Format specified by -f</li> <li>Default format by command:<ul> <li>punctuate: txt</li> <li>section: json</li> <li>translate: txt</li> <li>process: txt</li> </ul> </li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#pattern-management","title":"Pattern Management","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#pattern-resolution","title":"Pattern Resolution","text":"<ol> <li>Pattern name sources (in order):</li> <li>Command line (-p)</li> <li> <p>Command defaults:</p> <ul> <li>punctuate: default_punctuate</li> <li>section: default_section</li> <li>translate: default_translate</li> <li>process: NO DEFAULT (must be specified)</li> </ul> </li> <li> <p>Pattern search paths:</p> </li> <li>Path specified in configuration</li> <li>~/.config/tnh_scholar/patterns/</li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#template-value-processing","title":"Template Value Processing","text":"<ol> <li>Key-Value Format:    <code>key:value key2:\"value with spaces\"</code></li> <li>Keys must be valid identifiers</li> <li>Values with spaces must be quoted</li> <li> <p>Invalid formats raise error</p> </li> <li> <p>Template File Format (YAML):    <code>yaml    key1: value1    key2: value2</code></p> </li> <li> <p>Environment Variables:</p> </li> <li>Format: TNH_FAB_{KEY}</li> <li>Lowest priority in template resolution</li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#pipeline-behavior","title":"Pipeline Behavior","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#data-flow","title":"Data Flow","text":"<ul> <li>All commands accept STDIN</li> <li>All commands can output to STDOUT</li> <li>Section data can flow through pipeline</li> <li>Binary data not supported</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#pipeline-examples","title":"Pipeline Examples","text":"<pre><code># Punctuate and section\ncat input.txt | tnh-fab punctuate | tnh-fab section &gt; sections.json\n\n# Section and process\ntnh-fab section input.txt | tnh-fab process -p format_xml &gt; output.xml\n\n# Complete pipeline\ncat input.txt | \\\n  tnh-fab punctuate -l vi | \\\n  tnh-fab section -n 5 | \\\n  tnh-fab process -p format_xml -k speaker:\"Thay\" &gt; output.xml\n</code></pre> <ol> <li>Single File Input <code>bash    tnh-fab process -p format_xml input.txt</code></li> <li> <p>Processes input.txt directly. No sectioning is performed.</p> </li> <li> <p>STDIN Only <code>bash    cat input.txt | tnh-fab process -p format_xml    cat input.txt | tnh-fab process -g -p format_xml  # process by paragraphs</code></p> </li> <li> <p>Processes text from STDIN</p> </li> <li> <p>File + Sections File <code>bash    tnh-fab process -p format_xml -s sections.json input.txt</code></p> </li> <li> <p>Processes input.txt using sections from sections.json</p> </li> <li> <p>STDIN + Sections File <code>bash    cat input.txt | tnh-fab process -p format_xml -s sections.json</code></p> </li> <li> <p>Processes STDIN text using sections from sections.json</p> </li> <li> <p>Section Stream + Input File <code>bash    tnh-fab section input.txt | tnh-fab process -p format_xml input.txt</code></p> </li> <li>Processes input.txt using sections from STDIN</li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#input-validation","title":"Input Validation","text":"<ul> <li>When sections are provided (via -s or STDIN):</li> <li>Validates JSON format matches TextObject schema</li> <li>Checks source_file field in TextObject if present</li> <li>Warns if source_file doesn't match input file name</li> <li>Validates section line ranges against input text</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#configuration","title":"Configuration","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#configuration-files","title":"Configuration Files","text":"<ol> <li>User: ~/.config/tnh_scholar/tnh-fab/config.yaml</li> <li>Project: ./.tnh-fab.yaml</li> <li>Priority: Project &gt; User</li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#configuration-format","title":"Configuration Format","text":"<pre><code>defaults:\n  language: auto\n  output_format: txt\n\npunctuate:\n  pattern: default_punctuate\n  style: APA\n  review_count: 3\n\nsection:\n  pattern: default_section\n  review_count: 3\n\ntranslate:\n  pattern: default_translate\n  target_language: en\n  style: \"American Dharma Teaching\"\n  context_lines: 3\n  review_count: 3\n\nprocess:\n  wrap_document: true\n\npatterns:\n  path: ~/.config/tnh_scholar/patterns\n\nlogging:\n  level: INFO\n  file: ~/.tnh-fab.log\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#error-handling","title":"Error Handling","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#error-categories","title":"Error Categories","text":"<ol> <li>Input Errors</li> <li>Missing required input</li> <li>Invalid file formats</li> <li>Encoding issues</li> <li> <p>Section/input mismatch</p> </li> <li> <p>Pattern Errors</p> </li> <li>Missing required pattern</li> <li>Pattern not found</li> <li> <p>Invalid pattern format</p> </li> <li> <p>Template Errors</p> </li> <li>Invalid template format</li> <li>Missing required values</li> <li> <p>Invalid key-value syntax</p> </li> <li> <p>Processing Errors</p> </li> <li>AI service errors</li> <li>Timeout errors</li> <li>Validation failures</li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#error-reporting","title":"Error Reporting","text":"<ul> <li>Standard error format</li> <li>Error codes for scripting</li> <li>Detailed logging with -v</li> <li>Stack traces with --debug</li> </ul>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#exit-codes","title":"Exit Codes","text":"<pre><code>0  Success\n1  General error\n2  Input error\n3  Pattern error\n4  Template error\n5  Processing error\n64-73  Command-specific errors\n</code></pre>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#development-notes","title":"Development Notes","text":""},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#key-decision-points","title":"Key Decision Points","text":"<ol> <li>Pattern Management:</li> <li>Consider pattern versioning</li> <li>Pattern validation requirements</li> <li> <p>Pattern update mechanism</p> </li> <li> <p>Pipeline Handling:</p> </li> <li>Memory management for large files</li> <li>Progress indication in pipelines</li> <li> <p>Error propagation in pipelines</p> </li> <li> <p>Configuration:</p> </li> <li>Environment variable handling</li> <li>Configuration validation</li> <li> <p>Configuration migration</p> </li> <li> <p>Testing Requirements:</p> </li> <li>Unit test coverage requirements</li> <li>Integration test scenarios</li> <li>Performance benchmarks</li> </ol>"},{"location":"design/tnh-fab/text%20processing%20cli%20design_v2/#future-considerations","title":"Future Considerations","text":"<ol> <li>Additional Commands:</li> <li>Format validation</li> <li>Pattern management</li> <li> <p>Batch processing</p> </li> <li> <p>Extensions:</p> </li> <li>Plugin system</li> <li>Custom pattern repositories</li> <li> <p>API integration</p> </li> <li> <p>Integration:</p> </li> <li>CI/CD requirements</li> <li>Packaging requirements</li> <li>Documentation generation</li> </ol>"},{"location":"design/ytt-fetch/ADR%20transcript%20source%20handling/","title":"ADR: YouTube Transcript Source Handling in yt-fetch CLI","text":""},{"location":"design/ytt-fetch/ADR%20transcript%20source%20handling/#status","title":"Status","text":"<p>Proposed (supplements ADR: YouTube Transcript Format Selection)</p>"},{"location":"design/ytt-fetch/ADR%20transcript%20source%20handling/#context","title":"Context","text":"<p>When requesting transcripts from YouTube videos:</p> <ul> <li>Videos may have manually uploaded subtitles</li> <li>Videos may have auto-generated captions</li> <li>Some videos may have both</li> <li>Quality and accuracy can vary significantly between sources</li> </ul> <p>Currently yt-dlp options:</p> <p>```python opts = {     \"writesubtitles\": True,      # Get manual subtitles     \"writeautomaticsub\": True,   # Get auto-generated captions     \"subtitleslangs\": [\"en\"]     # Language selection }</p>"},{"location":"design/ytt-fetch/ADR%20transcript%20source%20handling/#decision","title":"Decision","text":"<ol> <li> <p>Initially accept both sources (manual and auto-generated) with preference given to manual subtitles when available (yt-dlp's default behavior)</p> </li> <li> <p>Flag this as a known limitation/consideration:</p> </li> <li>Source of transcript (manual vs auto) may affect quality</li> <li>No current mechanism to force selection of specific source</li> <li>Transcript source not clearly indicated in output</li> </ol>"},{"location":"design/ytt-fetch/ADR%20transcript%20source%20handling/#future-considerations","title":"Future Considerations","text":"<p>Future versions should consider: - Adding transcript source metadata - Option to specify preferred source - Quality indicators in output - Logging which source was used</p>"},{"location":"design/ytt-fetch/ADR%20transcript%20source%20handling/#consequences","title":"Consequences","text":""},{"location":"design/ytt-fetch/ADR%20transcript%20source%20handling/#positive","title":"Positive","text":"<ul> <li>Simple initial implementation</li> <li>Works with all video types</li> <li>Maximum transcript availability</li> </ul>"},{"location":"design/ytt-fetch/ADR%20transcript%20source%20handling/#negative","title":"Negative","text":"<ul> <li>Uncertain transcript source</li> <li>No quality indicators</li> <li>May get auto-generated when manual exists</li> <li>May get manual when auto-generated preferred</li> </ul>"},{"location":"design/ytt-fetch/ADR%20transcript%20source%20handling/#notes","title":"Notes","text":"<p>This limitation is acceptable for prototyping but should be revisited when: - Transcript quality becomes critical - Source attribution needed - Specific use cases require specific transcript types</p> <p>Would you like me to explore any specific aspect of this further, or shall we move on to implementation?</p>"},{"location":"design/ytt-fetch/ADR%20yt-dlp%20transcripts/","title":"ADR: YouTube Transcript Format Selection for yt-fetch CLI","text":""},{"location":"design/ytt-fetch/ADR%20yt-dlp%20transcripts/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"design/ytt-fetch/ADR%20yt-dlp%20transcripts/#context","title":"Context","text":"<p>The yt-fetch CLI tool needs to download YouTube transcripts/captions. YouTube offers multiple formats: - VTT (Web Video Text Tracks) - TTML (Timed Text Markup Language)  - srv1/2/3 (YouTube internal formats) - json3 (YouTube JSON format)</p> <p>While yt-dlp offers format conversion capabilities, these are: - Poorly documented - Inconsistent in behavior - May change across versions</p>"},{"location":"design/ytt-fetch/ADR%20yt-dlp%20transcripts/#decision","title":"Decision","text":"<p>Standardize on VTT format output because: 1. It is a web standard format, likely to remain stable 2. Human readable and well-documented 3. Has wide library support if needed 4. Already the default format from yt-dlp 5. Available for both manual and auto-generated captions</p> <p>Implementation approach: - Use minimal yt-dlp options (writesubtitles, writeautomaticsub, subtitleslangs) - Accept VTT as default output without trying format conversion - Let downstream tools handle any needed format conversion</p> <p>```python</p>"},{"location":"design/ytt-fetch/ADR%20yt-dlp%20transcripts/#example-minimal-implementation","title":"Example minimal implementation","text":"<p>opts = {    \"writesubtitles\": True,    \"writeautomaticsub\": True,    \"subtitleslangs\": [\"en\"],    \"skip_download\": True }</p>"},{"location":"design/ytt-fetch/ADRs/","title":"All ADR records for yt-fetch CLI tool","text":""},{"location":"design/ytt-fetch/ADRs/#2024-01-15","title":"2024-01-15","text":""},{"location":"design/ytt-fetch/ADRs/#adr-1-youtube-transcript-format-selection","title":"ADR-1: YouTube Transcript Format Selection","text":""},{"location":"design/ytt-fetch/ADRs/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"design/ytt-fetch/ADRs/#context","title":"Context","text":"<p>The yt-fetch CLI tool needs to download YouTube transcripts/captions. YouTube offers multiple formats: - VTT (Web Video Text Tracks) - TTML (Timed Text Markup Language)  - srv1/2/3 (YouTube internal formats) - json3 (YouTube JSON format)</p> <p>While yt-dlp offers format conversion capabilities, these are: - Poorly documented - Inconsistent in behavior - May change across versions</p>"},{"location":"design/ytt-fetch/ADRs/#decision","title":"Decision","text":"<p>Standardize on VTT format output because: 1. It is a web standard format, likely to remain stable 2. Human readable and well-documented 3. Has wide library support if needed 4. Already the default format from yt-dlp 5. Available for both manual and auto-generated captions</p> <p>Implementation approach: - Use minimal yt-dlp options (writesubtitles, writeautomaticsub, subtitleslangs) - Accept VTT as default output without trying format conversion - Let downstream tools handle any needed format conversion</p> <pre><code># Example minimal implementation\nopts = {\n   \"writesubtitles\": True,\n   \"writeautomaticsub\": True,\n   \"subtitleslangs\": [\"en\"],\n   \"skip_download\": True\n}\n</code></pre>"},{"location":"design/ytt-fetch/ADRs/#2024-01-15_1","title":"2024-01-15","text":""},{"location":"design/ytt-fetch/ADRs/#adr-2-youtube-transcript-source-handling","title":"ADR-2: YouTube Transcript Source Handling","text":""},{"location":"design/ytt-fetch/ADRs/#status_1","title":"Status","text":"<p>Proposed (supplements ADR: YouTube Transcript Format Selection)</p>"},{"location":"design/ytt-fetch/ADRs/#context_1","title":"Context","text":"<p>When requesting transcripts from YouTube videos: - Videos may have manually uploaded subtitles - Videos may have auto-generated captions - Some videos may have both - Quality and accuracy can vary significantly between sources</p> <p>Currently yt-dlp options:</p> <pre><code>opts = {\n    \"writesubtitles\": True,      # Get manual subtitles\n    \"writeautomaticsub\": True,   # Get auto-generated captions\n    \"subtitleslangs\": [\"en\"]     # Language selection\n}\n</code></pre>"},{"location":"design/ytt-fetch/ADRs/#decision_1","title":"Decision","text":"<ol> <li> <p>Initially accept both sources (manual and auto-generated) with preference given to manual subtitles when available (yt-dlp's default behavior)</p> </li> <li> <p>Flag this as a known limitation/consideration:</p> </li> <li>Source of transcript (manual vs auto) may affect quality</li> <li>No current mechanism to force selection of specific source</li> <li>Transcript source not clearly indicated in output</li> </ol>"},{"location":"design/ytt-fetch/ADRs/#future-considerations","title":"Future Considerations","text":"<p>Future versions should consider: - Adding transcript source metadata - Option to specify preferred source - Quality indicators in output - Logging which source was used</p>"},{"location":"design/ytt-fetch/ADRs/#consequences","title":"Consequences","text":""},{"location":"design/ytt-fetch/ADRs/#positive","title":"Positive","text":"<ul> <li>Simple initial implementation</li> <li>Works with all video types</li> <li>Maximum transcript availability</li> </ul>"},{"location":"design/ytt-fetch/ADRs/#negative","title":"Negative","text":"<ul> <li>Uncertain transcript source</li> <li>No quality indicators</li> <li>May get auto-generated when manual exists</li> <li>May get manual when auto-generated preferred</li> </ul>"},{"location":"design/ytt-fetch/ADRs/#notes","title":"Notes","text":"<p>This limitation is acceptable for prototyping but should be revisited when: - Transcript quality becomes critical - Source attribution needed - Specific use cases require specific transcript types</p>"},{"location":"design/ytt-fetch/ADRs/#2024-01-16","title":"2024-01-16","text":""},{"location":"design/ytt-fetch/ADRs/#adr-3-update-youtube-transcript-format-selection-revision-2","title":"ADR-3 Update: YouTube Transcript Format Selection (Revision 2)","text":""},{"location":"design/ytt-fetch/ADRs/#context_2","title":"Context","text":"<p>After testing various formats with real YouTube videos: - VTT shows duplicate line issues - TTML format provides cleaner output - TTML includes timing and styling information in structured XML</p>"},{"location":"design/ytt-fetch/ADRs/#decision-update","title":"Decision Update","text":"<p>Changing preferred format from VTT to TTML based on: - Better transcript quality (fewer duplicates) - Clean, structured XML format - Includes all necessary timing data - Easy to parse with standard XML tools</p>"},{"location":"design/ytt-fetch/ADRs/#learning-process-note","title":"Learning Process Note","text":"<p>Initial decision was based on theoretical advantages of VTT. Actual testing revealed TTML as superior choice.</p>"},{"location":"design/ytt-fetch/ADRs/#adr-4-ttml-text-extraction-implementation","title":"ADR-4: TTML Text Extraction Implementation","text":""},{"location":"design/ytt-fetch/ADRs/#status_2","title":"Status","text":"<p>Accepted</p>"},{"location":"design/ytt-fetch/ADRs/#context_3","title":"Context","text":"<p>Need to extract raw text content from YouTube TTML transcript files: - TTML files contain timing and styling metadata in XML format - Only need raw text content for downstream processing - Must handle potentially malformed XML from YouTube - Prefer minimal dependencies and simple implementation</p>"},{"location":"design/ytt-fetch/ADRs/#decision_2","title":"Decision","text":"<p>Implement using Python's standard library xml.etree.ElementTree because: 1. No additional dependencies required (already part of Python standard library) 2. Simple, focused API sufficient for basic XML parsing 3. Built-in namespace handling 4. Lightweight and well-documented</p> <p>Implementation approach: - Strip XML namespaces to simplify parsing - Use ElementTree's findall() with XPath to locate text content - Extract and join raw text with newlines - Keep implementation minimal (under 20 lines)</p>"},{"location":"design/ytt-fetch/ADRs/#consequences_1","title":"Consequences","text":""},{"location":"design/ytt-fetch/ADRs/#positive_1","title":"Positive","text":"<ul> <li>Zero new dependencies</li> <li>Simple, maintainable code</li> <li>Handles basic TTML structure effectively</li> <li>Easy to test and debug</li> </ul>"},{"location":"design/ytt-fetch/ADRs/#negative_1","title":"Negative","text":"<ul> <li>Limited validation of XML structure</li> <li>May need additional error handling for malformed XML</li> <li>No preservation of timing information (acceptable for current needs)</li> <li>Could break if YouTube significantly changes TTML format</li> </ul>"},{"location":"design/ytt-fetch/ADRs/#notes_1","title":"Notes","text":"<p>While more robust XML parsing libraries exist (lxml, BeautifulSoup4), the standard library solution provides sufficient functionality for current requirements while maintaining simplicity. Error handling can be enhanced if needed based on production experience.</p>"},{"location":"design/ytt-fetch/ADRs/#future-considerations_1","title":"Future Considerations","text":"<ul> <li>Add error handling for malformed XML if needed</li> <li>Monitor YouTube TTML format changes</li> <li>Consider preserving timing data if needed for future features</li> </ul>"},{"location":"design/ytt-fetch/yt-dlp%20vs.%20Youtube%20API/","title":"YouTube API vs yt-dlp Evaluation","text":""},{"location":"design/ytt-fetch/yt-dlp%20vs.%20Youtube%20API/#youtube-data-api-google-cloud-approach","title":"YouTube Data API (Google Cloud) Approach","text":""},{"location":"design/ytt-fetch/yt-dlp%20vs.%20Youtube%20API/#advantages","title":"Advantages","text":"<ol> <li>Official solution with:</li> <li>Stable, documented API</li> <li>Clear rate limits and quotas</li> <li>Service level guarantees</li> <li> <p>Official support channels</p> </li> <li> <p>Additional capabilities:</p> </li> <li>Better metadata access</li> <li>Channel/playlist management</li> <li>Comment access</li> <li>Full YouTube ecosystem integration</li> </ol>"},{"location":"design/ytt-fetch/yt-dlp%20vs.%20Youtube%20API/#disadvantages","title":"Disadvantages","text":"<ol> <li>Setup overhead:</li> <li>Requires Google Cloud account</li> <li>API key/credentials management</li> <li>Project setup in Google Cloud Console</li> <li> <p>Quota management</p> </li> <li> <p>Cost considerations:</p> </li> <li>Free tier limits</li> <li>Usage-based pricing</li> <li> <p>Quota costs for transcript access</p> </li> <li> <p>Implementation complexity:</p> </li> <li>OAuth flow for some operations</li> <li>More complex credential management</li> <li>More code to maintain</li> </ol>"},{"location":"design/ytt-fetch/yt-dlp%20vs.%20Youtube%20API/#current-yt-dlp-approach","title":"Current yt-dlp Approach","text":""},{"location":"design/ytt-fetch/yt-dlp%20vs.%20Youtube%20API/#advantages_1","title":"Advantages","text":"<ol> <li>Simplicity:</li> <li>No authentication needed</li> <li>Minimal setup</li> <li> <p>Works immediately</p> </li> <li> <p>Cost:</p> </li> <li>Free to use</li> <li>No quota limits</li> <li> <p>No account needed</p> </li> <li> <p>Implementation:</p> </li> <li>Already working solution</li> <li>Minimal code</li> <li>Handles both manual and auto captions</li> </ol>"},{"location":"design/ytt-fetch/yt-dlp%20vs.%20Youtube%20API/#disadvantages_1","title":"Disadvantages","text":"<ol> <li>Unofficial:</li> <li>Could break with YouTube changes</li> <li>No guaranteed support</li> <li>Limited to public video access</li> </ol>"},{"location":"design/ytt-fetch/yt-dlp%20vs.%20Youtube%20API/#recommendation","title":"Recommendation","text":"<p>For this project's current needs (transcript downloading from public videos), yt-dlp remains the better choice because: 1. Matches current project scope 2. Zero setup overhead 3. No cost implications 4. Already working solution</p> <p>Consider YouTube API if project requirements expand to need: - Private video access - Channel management - Commercial deployment - Service level guarantees</p>"},{"location":"development/contributing/","title":"Contributing to TNH Scholar (Prototype Phase)","text":"<p>TNH Scholar is currently in rapid prototype phase, focusing on core functionality and basic usability. We welcome contributions that help validate and improve the prototype implementation.</p>"},{"location":"development/contributing/#current-focus-areas","title":"Current Focus Areas","text":"<ol> <li> <p>TNH-FAB Command Line Tool</p> <ul> <li>Basic functionality testing</li> <li>Error case identification</li> <li>Command pipeline testing</li> <li>Pattern system integration</li> </ul> </li> <li> <p>Pattern System</p> <ul> <li>Pattern usage testing</li> <li>Pattern creation testing</li> <li>Version control functionality</li> <li>Concurrent access testing</li> </ul> </li> <li> <p>AUDIO-TRANSCRIBE Command Line Tool</p> <ul> <li>Basic functionality testing</li> <li>Error case identification</li> </ul> </li> </ol>"},{"location":"development/contributing/#how-to-help","title":"How to Help","text":""},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#1-install-the-package","title":"1. Install the package","text":"<pre><code>   pip install tnh-scholar\n</code></pre>"},{"location":"development/contributing/#2-try-basic-operations","title":"2. Try basic operations","text":"<pre><code>    # Test basic commands\n    tnh-fab punctuate input.txt\n    tnh-fab section input.txt\n    tnh-fab translate input.txt\n    tnh-fab process -p pattern_name input.txt\n\n    # Test pipeline operations\n    cat input.txt | tnh-fab punctuate | tnh-fab section\n</code></pre>"},{"location":"development/contributing/#3-report-issues","title":"3. Report issues","text":"<ul> <li>Use GitHub Issues</li> <li>Include command used</li> <li>Provide minimal example that reproduces the issue</li> <li>Note your environment (OS, Python version)</li> </ul>"},{"location":"development/contributing/#pattern-testing","title":"Pattern Testing","text":"<ol> <li>Create test patterns in <code>~/.config/tnh-scholar/patterns/</code></li> <li>Test pattern loading and application</li> <li>Report any issues with pattern system</li> </ol>"},{"location":"development/contributing/#reporting-issues","title":"Reporting Issues","text":"<p>Create issues on GitHub with:</p> <ul> <li>Clear description of the problem</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Example files (if needed)</li> </ul>"},{"location":"development/contributing/#code-contributions","title":"Code Contributions","text":"<p>At this prototype stage:</p> <ul> <li>Start with bug fixes</li> <li>Keep changes focused</li> <li>Include tests for new functionality</li> <li>Follow existing code style</li> <li>See design guide for coding style and requirements.</li> </ul>"},{"location":"development/contributing/#questions-and-discussion","title":"Questions and Discussion","text":"<ul> <li>Use GitHub Issues for feature discussions</li> <li>Tag issues with 'question' or 'discussion'</li> <li>Focus on prototype phase functionality</li> </ul> <p>This is a project in rapid prototype - we're looking for practical feedback on core functionality as well as possible new feature additions and new tools.</p>"},{"location":"development/design-guide/","title":"TNH Scholar Design Guide","text":""},{"location":"development/design-guide/#overview","title":"Overview","text":"<p>This design guide establishes development standards for the TNH Scholar project. While the project is currently in a rapid prototyping phase, these guidelines aim to maintain code quality and consistency throughout development. The guide distinguishes between immediate prototyping requirements and standards for later production phases where appropriate.</p>"},{"location":"development/design-guide/#code-style-and-organization","title":"Code Style and Organization","text":""},{"location":"development/design-guide/#python-standards","title":"Python Standards","text":"<p>The project follows PEP 8 with some specific adaptations. All Python code should adhere to these standards regardless of development phase:</p> <p>The project uses Python 3.12.4 exclusively, taking advantage of modern Python features including strict typing. This version requirement ensures consistency across all components and enables use of the latest language features.</p> <p>Import organization follows this pattern:</p> <ol> <li>Standard library imports</li> <li>External package imports</li> <li>Internal package imports</li> <li>Relative imports</li> </ol> <p>For example:</p> <pre><code>from pathlib import Path\nfrom typing import Optional, Dict\n\nimport click\nfrom pydantic import BaseModel\n\nfrom tnh_scholar.utils import ensure_directory_exists\nfrom .environment import check_env\n</code></pre>"},{"location":"development/design-guide/#file-and-directory-naming","title":"File and Directory Naming","text":"<p>File naming conventions apply across all project phases:</p> <p>All Python files use lowercase with underscores, for example: <code>audio_processing.py</code>.</p> <p>Directory names follow the same lowercase with underscores pattern: <code>text_processing/</code>.</p> <p>Exception cases follow traditional conventions:</p> <ul> <li>README.md</li> <li>LICENSE</li> <li>CONTRIBUTING.md</li> <li>Requirements files (requirements.txt, dev-requirements.txt)</li> </ul>"},{"location":"development/design-guide/#module-structure","title":"Module Structure","text":"<p>Each module should maintain this general structure:</p> <pre><code>\"\"\"Module docstring providing overview and purpose.\"\"\"\n\n# Standard imports\n# External imports\n# Internal imports\n\n# Module-level constants\nDEFAULT_CHUNK_SIZE = 1024\n\n# Classes\nclass ExampleClass:\n    \"\"\"Class docstring.\"\"\"\n\n# Functions\ndef example_function():\n    \"\"\"Function docstring.\"\"\"\n</code></pre>"},{"location":"development/design-guide/#type-handling","title":"Type Handling","text":""},{"location":"development/design-guide/#type-annotations","title":"Type Annotations","text":"<p>The project emphasizes strong typing throughout:</p> <p>Basic type annotations are required even during prototyping:</p> <pre><code>def process_text(\n    text: str,\n    language: Optional[str] = None,\n    max_tokens: int = 0\n) -&gt; str:\n</code></pre> <p>Custom types should be defined for complex structures:</p> <pre><code>from typing import NewType\n\nMarkdownStr = NewType('MarkdownStr', str)\n</code></pre>"},{"location":"development/design-guide/#pydantic-models","title":"Pydantic Models","text":"<p>Data models use Pydantic for validation:</p> <pre><code>class TextObject(BaseModel):\n    \"\"\"Represents processed text with metadata.\"\"\"\n    language: str = Field(..., description=\"ISO 639-1 language code\")\n    sections: List[LogicalSection]\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"development/design-guide/#error-handling","title":"Error Handling","text":"<p>Error handling requirements differ between prototyping and production phases:</p>"},{"location":"development/design-guide/#prototyping-phase","title":"Prototyping Phase","text":"<p>During prototyping, error handling should prioritize visibility of failure cases over comprehensive handling. This approach helps identify and document necessary error cases early in development.</p> <p>Preferred approach - allow exceptions to propagate:</p> <pre><code># TODO: Add error handling for ValueError and PatternError\nresult = process_text(input_text)\n</code></pre> <p>When try blocks are needed, use minimal handling to maintain visibility:</p> <pre><code>try:\n    # TODO: Handle specific exceptions in production\n    result = process_text(input_text)\nexcept:\n    # Maintain stack trace while documenting intent\n    raise\n</code></pre> <p>This approach:</p> <ul> <li>Maintains clear visibility of failure modes</li> <li>Documents intended error handling through TODO comments</li> <li>Preserves full stack traces for debugging</li> <li>Avoids masking exceptions during development</li> </ul>"},{"location":"development/design-guide/#production-phase","title":"Production Phase","text":"<p>Production code requires comprehensive error handling:</p> <pre><code>try:\n    result = process_text(input_text)\nexcept ValueError as e:\n    logger.error(f\"Invalid input format: {e}\")\n    raise InvalidInputError(str(e)) from e\nexcept APIError as e:\n    logger.error(f\"API processing failed: {e}\")\n    raise ProcessingError(str(e)) from e\n</code></pre> <p>Do NOT write catch-all exception handling such as in:</p> <pre><code>except Exception as e:\n    logger.error(f\"Unexpected error: {e}\")\n    raise SystemError(f\"unexpected error: {e}\") from e\n</code></pre> <p>It is preferred to let unknown exceptions propagate.</p>"},{"location":"development/design-guide/#logging","title":"Logging","text":""},{"location":"development/design-guide/#prototyping-phase_1","title":"Prototyping Phase","text":"<p>Basic logging configuration is acceptable during prototyping:</p> <pre><code>logger = get_child_logger(__name__)\nlogger.info(\"Processing started\")\nlogger.error(\"Processing failed\")\n</code></pre> <p>Especially important is DEBUG level logging.</p>"},{"location":"development/design-guide/#production-phase_1","title":"Production Phase","text":"<p>Production logging should include:</p> <ul> <li>Log levels properly used</li> <li>Structured logging where appropriate</li> <li>Contextual information</li> <li>Error tracebacks</li> </ul>"},{"location":"development/design-guide/#testing","title":"Testing","text":""},{"location":"development/design-guide/#test-organization","title":"Test Organization","text":"<p>Tests follow this structure even during prototyping:</p> <pre><code>tests/\n\u251c\u2500\u2500 unit/\n\u2502   \u251c\u2500\u2500 test_text_processing.py\n\u2502   \u2514\u2500\u2500 test_audio_processing.py\n\u251c\u2500\u2500 integration/\n\u2502   \u2514\u2500\u2500 test_full_pipeline.py\n\u2514\u2500\u2500 conftest.py\n</code></pre>"},{"location":"development/design-guide/#test-requirements","title":"Test Requirements","text":"<p>Prototyping Phase:</p> <ul> <li>Basic unit tests for core functionality</li> <li>Critical path testing</li> <li>Basic integration tests</li> </ul> <p>Production Phase:</p> <ul> <li>Comprehensive unit test coverage</li> <li>Full integration test suite</li> <li>Performance testing</li> <li>Edge case handling</li> <li>Mock external services</li> </ul>"},{"location":"development/design-guide/#documentation","title":"Documentation","text":""},{"location":"development/design-guide/#code-documentation","title":"Code Documentation","text":"<p>The project follows Google's Python documentation style for all docstrings. This style provides clear structure while maintaining readability.</p> <p>Classes:</p> <pre><code>class TextProcessor:\n    \"\"\"A class that processes text using configurable patterns.\n\n    Implements pattern-based text processing with configurable token limits\n    and language support. Designed for extensibility through the pattern system.\n\n    Attributes:\n        pattern: A Pattern instance defining processing instructions.\n        max_tokens: An integer specifying maximum tokens for processing.\n\n    Note:\n        Pattern instances should be initialized with proper template validation.\n    \"\"\"\n</code></pre> <p>Functions:</p> <pre><code>def process_text(text: str, language: Optional[str] = None) -&gt; str:\n    \"\"\"Processes text according to pattern instructions.\n\n    Applies the configured pattern to input text, handling language-specific\n    requirements and token limitations.\n\n    Args:\n        text: Input text to process.\n        language: Optional ISO 639-1 language code. Defaults to None for\n            auto-detection.\n\n    Returns:\n        A string containing the processed text.\n\n    Raises:\n        ValueError: If text is empty or invalid.\n        PatternError: If pattern application fails.\n\n    Examples:\n        &gt;&gt;&gt; processor = TextProcessor(pattern)\n        &gt;&gt;&gt; result = process_text(\"Input text\", language=\"en\")\n        &gt;&gt;&gt; print(result)\n        Processed text output\n    \"\"\"\n</code></pre>"},{"location":"development/design-guide/#api-documentation","title":"API Documentation","text":"<p>API documentation requirements increase with development phase:</p> <p>Prototyping Phase:</p> <ul> <li>Basic function/class documentation</li> <li>Essential usage examples</li> <li>Known limitations noted</li> </ul> <p>Production Phase:</p> <ul> <li>Comprehensive API documentation</li> <li>Multiple usage examples</li> <li>Error handling documentation</li> <li>Performance considerations</li> <li>Security implications</li> </ul>"},{"location":"development/design-guide/#development-workflow","title":"Development Workflow","text":""},{"location":"development/design-guide/#version-control","title":"Version Control","text":"<p>Git workflow standards apply across all phases:</p> <ul> <li>Feature branches for development</li> <li>Clear commit messages</li> <li>Regular main branch updates</li> <li>Version tags for releases</li> </ul>"},{"location":"development/design-guide/#code-review","title":"Code Review","text":"<p>Review requirements increase with development phase:</p> <p>Prototyping Phase:</p> <ul> <li>Basic functionality review</li> <li>Core design review</li> <li>Critical security review</li> </ul> <p>Production Phase:</p> <ul> <li>Comprehensive code review</li> <li>Performance review</li> <li>Security audit</li> <li>Documentation review</li> <li>Test coverage review</li> </ul>"},{"location":"development/design-guide/#security-considerations","title":"Security Considerations","text":""},{"location":"development/design-guide/#api-key-management","title":"API Key Management","text":"<p>Consistent across all phases:</p> <ul> <li>No keys in code</li> <li>Environment variable usage</li> <li>Secure configuration loading</li> <li>Key rotation support</li> </ul>"},{"location":"development/design-guide/#input-validation","title":"Input Validation","text":"<p>Validation requirements increase with phase:</p> <p>Prototyping Phase:</p> <ul> <li>Basic input validation</li> <li>Type checking</li> <li>Simple sanitization</li> </ul> <p>Production Phase:</p> <ul> <li>Comprehensive validation</li> <li>Security scanning</li> <li>Input sanitization</li> <li>Output escaping</li> </ul>"},{"location":"development/design-guide/#performance-guidelines","title":"Performance Guidelines","text":""},{"location":"development/design-guide/#resource-management","title":"Resource Management","text":"<p>Basic guidelines apply across phases:</p> <p>Memory Management:</p> <ul> <li>Stream large files</li> <li>Clean up temporary files</li> <li>Monitor memory usage</li> </ul> <p>Processing Optimization:</p> <ul> <li>Batch operations where possible</li> <li>Cache frequently used data</li> <li>Monitor API usage</li> </ul>"},{"location":"development/design-guide/#future-considerations","title":"Future Considerations","text":"<p>Areas marked for future development:</p> <ul> <li>Plugin system architecture</li> <li>Configuration handling</li> <li>Rebuild of ai_text_processing suite</li> <li>Extended API integration<ul> <li>Batch processing</li> <li>Alternate API model services</li> </ul> </li> <li>Enhanced security features</li> <li>Performance optimization</li> <li>Extended pattern capabilities</li> <li>Additional CLI processing tools</li> <li>Model training tools</li> <li>Natural language processing tools</li> </ul>"},{"location":"docs-design/design/documentation/","title":"Documentation Design","text":""},{"location":"docs-design/design/documentation/#tools-technology-choices","title":"Tools &amp; Technology Choices","text":""},{"location":"docs-design/design/documentation/#static-site-generator","title":"Static Site Generator","text":"<ul> <li>Choice: MkDocs with Material theme</li> <li>Rationale: </li> <li>Markdown-based for ease of writing</li> <li>Good integration with Python tooling</li> <li>Modern, responsive design</li> <li>Active community support</li> </ul>"},{"location":"docs-design/design/documentation/#api-documentation","title":"API Documentation","text":"<ul> <li>Choice: MkDocstrings</li> <li>Rationale:</li> <li>Native MkDocs integration</li> <li>Support for Google-style docstrings</li> <li>Clean, hierarchical output</li> <li>Good code navigation features</li> </ul>"},{"location":"docs-design/design/documentation/#version-control","title":"Version Control","text":"<ul> <li>Choice: Git + GitHub Pages</li> <li>Rationale:</li> <li>Free hosting</li> <li>Automatic deployment</li> <li>Version tracking</li> <li>PR-based reviews</li> </ul>"},{"location":"docs-design/design/documentation/#structure-decisions","title":"Structure Decisions","text":""},{"location":"docs-design/design/documentation/#documentation-types","title":"Documentation Types","text":"<ol> <li>User Documentation (Markdown)</li> <li>Installation guides</li> <li>User manuals</li> <li> <p>Tutorials</p> </li> <li> <p>API Documentation (Docstrings)</p> </li> <li>Class/function documentation</li> <li>Code examples</li> <li> <p>Type hints</p> </li> <li> <p>Development Documentation</p> </li> <li>Architecture decisions</li> <li>Contribution guides</li> <li>Development setup</li> </ol>"},{"location":"docs-design/design/documentation/#file-organization","title":"File Organization","text":"<ul> <li>User-facing content in /user_guide</li> <li>API reference in /api</li> <li>Development docs in /development</li> </ul>"},{"location":"docs-design/planning/maintenance/","title":"Documentation Maintenance","text":""},{"location":"docs-design/planning/maintenance/#regular-tasks","title":"Regular Tasks","text":"<ul> <li>Monthly link checking</li> <li>Quarterly content review</li> <li>Version synchronization</li> </ul>"},{"location":"docs-design/planning/maintenance/#review-process","title":"Review Process","text":"<ul> <li>PR documentation requirements</li> <li>Documentation testing</li> <li>Style guide compliance</li> </ul>"},{"location":"docs-design/planning/maintenance/#update-procedures","title":"Update Procedures","text":"<ul> <li>Version update process</li> <li>Deprecation notices</li> <li>Breaking change documentation</li> </ul>"},{"location":"docs-design/planning/roadmap/","title":"TNH Scholar Documentation Development Plan","text":""},{"location":"docs-design/planning/roadmap/#phase-1-rapid-prototype-current","title":"Phase 1: Rapid Prototype (Current)","text":"<p>Estimated effort: 1-2 coder-days</p> <p>Focus: Essential documentation for testers and early adopters</p> <pre><code>docs/\n\u251c\u2500\u2500 index.md                 # (2 hours) Project overview, quick start\n\u251c\u2500\u2500 user_guide/\n\u2502   \u251c\u2500\u2500 tnh_fab.md          # (4 hours) Core CLI usage \n\u2502   \u2514\u2500\u2500 patterns.md         # (2 hours) Basic pattern system\n\u2514\u2500\u2500 contributing.md         # (2 hours) Initial testing guidelines\n</code></pre> <p>Key Deliverables: - Basic MkDocs setup with minimal theme - Essential command documentation - Testing focus areas defined - Example patterns documented</p>"},{"location":"docs-design/planning/roadmap/#phase-2-full-development","title":"Phase 2: Full Development","text":"<p>Estimated effort: 3-4 coder-days</p> <p>Focus: Comprehensive developer documentation and architecture</p> <pre><code>docs/\n\u251c\u2500\u2500 architecture/\n\u2502   \u251c\u2500\u2500 overview.md         # (4 hours) System architecture\n\u2502   \u251c\u2500\u2500 patterns.md         # (4 hours) Pattern system design\n\u2502   \u2514\u2500\u2500 ai_interface.md     # (4 hours) AI integration design\n\u251c\u2500\u2500 developer_guide/\n\u2502   \u251c\u2500\u2500 setup.md           # (2 hours) Development environment\n\u2502   \u251c\u2500\u2500 patterns.md        # (4 hours) Pattern development guide\n\u2502   \u2514\u2500\u2500 testing.md         # (4 hours) Testing framework guide\n\u2514\u2500\u2500 api/\n    \u2514\u2500\u2500 reference.md       # (8 hours) API documentation\n</code></pre> <p>Key Deliverables: - Full architecture documentation - Development environment setup - Testing framework documentation - API reference documentation - Pattern development guidelines</p>"},{"location":"docs-design/planning/roadmap/#phase-3-testing-integration","title":"Phase 3: Testing &amp; Integration","text":"<p>Estimated effort: 2-3 coder-days</p> <p>Focus: Testing documentation and integration guides</p> <pre><code>docs/\n\u251c\u2500\u2500 testing/\n\u2502   \u251c\u2500\u2500 test_suite.md      # (4 hours) Test suite documentation\n\u2502   \u251c\u2500\u2500 coverage.md        # (2 hours) Coverage requirements\n\u2502   \u2514\u2500\u2500 integration.md     # (4 hours) Integration testing\n\u251c\u2500\u2500 deployment/\n\u2502   \u251c\u2500\u2500 ci_cd.md          # (4 hours) CI/CD pipeline docs\n\u2502   \u2514\u2500\u2500 environments.md    # (2 hours) Environment setup\n\u2514\u2500\u2500 tutorials/\n    \u251c\u2500\u2500 basic.md          # (2 hours) Basic usage tutorials\n    \u2514\u2500\u2500 advanced.md       # (4 hours) Advanced patterns\n</code></pre> <p>Key Deliverables: - Comprehensive test documentation - CI/CD pipeline documentation - Environment setup guides - Tutorial series</p>"},{"location":"docs-design/planning/roadmap/#phase-4-production-deployment","title":"Phase 4: Production Deployment","text":"<p>Estimated effort: 4-5 coder-days</p> <p>Focus: Production-ready documentation and maintenance</p> <pre><code>docs/\n\u251c\u2500\u2500 deployment/\n\u2502   \u251c\u2500\u2500 production.md      # (8 hours) Production deployment\n\u2502   \u251c\u2500\u2500 monitoring.md      # (4 hours) Monitoring guide\n\u2502   \u2514\u2500\u2500 troubleshooting.md # (8 hours) Production issues\n\u251c\u2500\u2500 user_guide/\n\u2502   \u251c\u2500\u2500 best_practices.md  # (6 hours) Production usage\n\u2502   \u2514\u2500\u2500 optimization.md    # (4 hours) Performance guide\n\u251c\u2500\u2500 maintenance/\n\u2502   \u251c\u2500\u2500 upgrades.md       # (4 hours) Upgrade procedures\n\u2502   \u2514\u2500\u2500 backup.md         # (4 hours) Backup/restore\n\u2514\u2500\u2500 examples/\n    \u2514\u2500\u2500 production.md     # (4 hours) Production examples\n</code></pre> <p>Key Deliverables: - Production deployment guides - Monitoring and maintenance docs - Performance optimization guides - Production-grade examples</p>"},{"location":"docs-design/planning/roadmap/#general-guidelines-across-phases","title":"General Guidelines Across Phases","text":"<p>Documentation Updates: - Update with each significant feature addition (1-2 hours per feature) - Review and refresh documentation monthly (2-4 hours) - Address documentation issues from user feedback (1-2 hours per issue)</p> <p>Quality Maintenance: - Regular link checking (automated) - Example testing (2-4 hours per major version) - Documentation testing (2-4 hours per major version)</p> <p>Each Phase Transition Checklist: 1. Review and update existing docs (4-8 hours) 2. Plan new documentation needs (2-4 hours) 3. Get user feedback on current docs (2-4 hours) 4. Implement improvements (varies by feedback)</p> <p>This phased approach allows documentation to grow naturally with the project while maintaining focus on current needs. Time estimates are for documentation work only and assume familiarity with the codebase.</p> <p>Would you like me to expand on any particular phase or create detailed outlines for specific documents?</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>TNH Scholar requires some initial configuration to function properly. This guide covers the essential configuration steps and options.</p>"},{"location":"getting-started/configuration/#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/configuration/#openai-api-key","title":"OpenAI API Key","text":"<p>TNH Scholar's AI functionality requires an OpenAI API key. To configure this:</p> <ol> <li>Obtain an API key from OpenAI's platform</li> <li>Set the environment variable:</li> </ol> <p>```bash    # Linux/Mac    export OPENAI_API_KEY='your-api-key-here'</p> <p># Windows    set OPENAI_API_KEY=your-api-key-here    ```</p> <p>In development configuration (where you have downloaded the tnh-scholar repository) you can also use a <code>.env</code> file in your project directory:</p> <pre><code>OPENAI_API_KEY=your-api-key-here\n</code></pre>"},{"location":"getting-started/configuration/#directory-structure","title":"Directory Structure","text":"<p>TNH Scholar creates and uses the following directory structure:</p> <pre><code>~/.config/tnh_scholar/\n\u251c\u2500\u2500 patterns/         # Pattern storage\n\u2514\u2500\u2500 logs/            # Log files\n</code></pre>"},{"location":"getting-started/configuration/#pattern-configuration","title":"Pattern Configuration","text":""},{"location":"getting-started/configuration/#pattern-directory","title":"Pattern Directory","text":"<p>Patterns can be stored in:</p> <ol> <li>Default location: <code>~/.config/tnh_scholar/patterns/</code></li> <li>Custom location specified by <code>TNH_PATTERN_DIR</code> environment variable</li> </ol> <p>To use a custom pattern directory:</p> <pre><code>export TNH_PATTERN_DIR=/path/to/patterns\n</code></pre>"},{"location":"getting-started/configuration/#default-patterns","title":"Default Patterns","text":"<p>The system includes several default patterns:</p> <ul> <li>default_punctuate.md</li> <li>default_section.md</li> <li>default_line_translation.md</li> <li>default_xml_format.md</li> <li>default_xml_paragraph_format.md</li> </ul> <p>These can be downloaded during setup or manually added later.</p>"},{"location":"getting-started/configuration/#configuration-file","title":"Configuration File","text":"<p>The system looks for configuration in this order:</p> <ol> <li>Command line arguments</li> <li>Environment variables</li> <li>Project-level config: <code>./.tnh-fab.yaml</code></li> <li>User-level config: <code>~/.config/tnh_scholar/tnh-fab/config.yaml</code></li> </ol> <p>Example configuration file:</p> <pre><code>defaults:\n  language: auto\n  output_format: txt\n\npunctuate:\n  pattern: default_punctuate\n  style: APA\n  review_count: 3\n\nsection:\n  pattern: default_section\n  review_count: 3\n\ntranslate:\n  pattern: default_line_translation\n  target_language: en\n  style: \"American Dharma Teaching\"\n  context_lines: 3\n  review_count: 3\n\nprocess:\n  wrap_document: true\n\npatterns:\n  path: ~/.config/tnh_scholar/patterns\n\nlogging:\n  level: INFO\n  file: ~/.tnh-fab.log\n</code></pre>"},{"location":"getting-started/configuration/#initial-setup","title":"Initial Setup","text":"<p>The <code>tnh-setup</code> command automates configuration:</p> <pre><code># Full setup\ntnh-setup\n\n# Skip specific steps\ntnh-setup --skip-env        # Skip API key check\ntnh-setup --skip-patterns   # Skip pattern download\n</code></pre> <p>This will:</p> <ol> <li>Create necessary directories</li> <li>Offer to download default patterns</li> <li>Check for OpenAI API key</li> <li>Set up basic configuration</li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>TNH Scholar--Python package for text processing and analysis. Install it using pip:</p> <pre><code>pip install tnh-scholar\n</code></pre>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.12.4</li> <li>OpenAI API key for AI-powered features</li> <li>Git (for pattern version control)</li> </ul>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>For basic usage:</p> <pre><code>pip install tnh-scholar\ntnh-setup  # Configure default patterns and directories\n</code></pre>"},{"location":"getting-started/installation/#feature-specific-installation","title":"Feature-Specific Installation","text":"<p>Install optional components based on your needs:</p> <ul> <li>OCR capabilities: <code>pip install \"tnh-scholar[ocr]\"</code></li> <li>GUI tools: <code>pip install \"tnh-scholar[gui]\"</code></li> <li>Query features: <code>pip install \"tnh-scholar[query]\"</code></li> <li>Development tools: <code>pip install \"tnh-scholar[dev]\"</code></li> </ul>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<ol> <li>Set your OpenAI API key:</li> </ol> <p><code>bash    export OPENAI_API_KEY='your-api-key'</code></p> <p>Or add it to your .env file.</p> <ol> <li>Run the setup tool:</li> </ol> <p><code>bash    tnh-setup</code></p> <p>This will:    - Create the configuration directory (~/.config/tnh-scholar)    - Download default patterns    - Set up initial configuration</p>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code>tnh-fab --help\n</code></pre>"},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Missing API Key    If you see authentication errors, ensure OPENAI_API_KEY is set correctly.</p> </li> <li> <p>Python Version Mismatch    TNH Scholar requires Python 3.12.4 exactly. Check your version:</p> </li> </ol> <p><code>bash    python --version</code></p> <p>For troubleshooting, see our GitHub Issues.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>TNH Scholar provides powerful text processing capabilities through several command-line tools. This guide will help you get started with the basic workflows.</p>"},{"location":"getting-started/quick-start/#initial-setup","title":"Initial Setup","text":"<p>After installation, run the setup tool:</p> <pre><code>tnh-setup\n</code></pre> <p>This creates necessary directories and downloads default patterns.</p>"},{"location":"getting-started/quick-start/#core-tools","title":"Core Tools","text":"<p>TNH Scholar includes several specialized tools:</p>"},{"location":"getting-started/quick-start/#tnh-fab","title":"tnh-fab","text":"<p>The main text processing tool, providing functions for:</p> <ul> <li>Text punctuation and formatting</li> <li>Section analysis</li> <li>Translation</li> <li>Pattern-based processing</li> </ul> <p>Example usage:</p> <pre><code># Add punctuation to text\ntnh-fab punctuate input.txt &gt; punctuated.txt\n\n# Translate Vietnamese text to English\ntnh-fab translate -l vi input.txt &gt; translated.txt\n</code></pre>"},{"location":"getting-started/quick-start/#audio-transcribe","title":"audio-transcribe","text":"<p>Process and transcribe audio content:</p> <pre><code># Transcribe from YouTube\naudio-transcribe --yt_url \"https://youtube.com/watch?v=example\" --split --transcribe\n\n# Process local audio\naudio-transcribe -f recording.mp3 --split --transcribe\n</code></pre>"},{"location":"getting-started/quick-start/#ytt-fetch","title":"ytt-fetch","text":"<p>Download YouTube transcripts:</p> <pre><code># Get English transcript\nytt-fetch \"https://youtube.com/watch?v=example\" -l en -o transcript.txt\n</code></pre>"},{"location":"getting-started/quick-start/#nfmt","title":"nfmt","text":"<p>Format text file newlines:</p> <pre><code># Normalize newlines in a file\nnfmt input.txt &gt; formatted.txt\n</code></pre>"},{"location":"getting-started/quick-start/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quick-start/#text-processing-pipeline","title":"Text Processing Pipeline","text":"<pre><code># Complete processing pipeline\ncat input.txt | \\\ntnh-fab punctuate | \\\ntnh-fab section | \\\ntnh-fab translate -l vi | \\\ntnh-fab process -p format_xml &gt; output.xml\n</code></pre>"},{"location":"getting-started/quick-start/#audio-processing","title":"Audio Processing","text":"<pre><code># Download and transcribe\naudio-transcribe --yt_url \"https://example.com/video\" --split --transcribe\n\n# Post-process transcription\ntnh-fab punctuate transcript.txt | \\\ntnh-fab section &gt; processed.txt\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Review the Pattern System documentation</li> <li>Explore detailed CLI documentation</li> <li>Check out example notebooks in the repository</li> </ul>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/","title":"Query Generation","text":""},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#input-test-1","title":"Input test 1:","text":"<p>Generate 30 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The queries should be a mix of short (30%), medium (50%), and long lengths (20%): - 30% short queries (1-3 words) - 50% medium queries (4-10 words) - 20% long queries (10+ words)</p> <p>The project focuses on helping users find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Text:</p> <p>title Transformation and Healing: Sutra on the Four Establishments of Mindfulness \\title</p> <p>section Mindfulness Exercises \\section</p> <p>exercise Exercise 7| Parts of the Body exercise</p> <p>Further, the practitioner meditates on his very own body from the soles of the feet upwards and then from the hair on top of the head downwards, a body contained inside the skin and full of all the impurities which belong to the body: \u2018Here is the hair of the head, the hairs on the body, the nails, teeth, skin, flesh, sinews, bones, bone marrow, kidneys, heart, liver, diaphragm, spleen, lungs, intestines, bowels, excrement, bile, phlegm, pus, blood, sweat, fat, tears, grease, saliva, mucus, synovic fluid, urine.\u2019</p> <p>This exercise brings us into even deeper contact with our body. Here we observe the body in all its parts, from the hair on the head to the skin on the soles of the feet. In the process of our observation, we scan all the parts of the body, including the brain, heart, lungs, gall bladder, spleen, blood, urine, and so forth. The Buddha gives us the example of a farmer pouring the contents of a sack filled with a variety of seeds onto the floor and then observing and identifying each kind of seed: \u201cThis is rice, these are beans, these are sesame seeds.\u201d</p> <p>We use our conscious breathing in order to observe mindfully all the parts of the body. For example: \u201cBreathing in, I am aware of the hair on my head. Breathing out, I know that this is the hair on my head.\u201d Breathing consciously helps us dwell in mindfulness more easily and sustain the work of observing each part of the body. In addition to the conscious breathing, we can use the method of silently calling each part of the body by name to enable these parts to become increasingly clear in the light of mindfulness.</p> <p>Why do we need to observe in mindfulness the different parts of the body? First of all, it is to be in contact with the body. We often have the impression that we\u2019re already totally in touch with our body, but often we\u2019re wrong. Between us and our body there can be a large separation, and our body remains a stranger to us. Sometimes we hate our body. There are even people who see their body as a prison and a place of punishment. To come back to our body is to become familiar with it and to establish harmony with it. We know that if our body isn\u2019t happy, we\u2019re not happy, and so we want our body to be calm and peaceful. To do so, we come back to our body and make peace with it.</p> <p>We can try touching the different parts of our body to make their acquaintance. We should touch each part in an affectionate and caring way. For several decades, our eyes, feet, and heart have done their work devotedly and faithfully with us and for us, but we never really give them much attention or express our gratitude to them. It\u2019s necessary to establish a close relationship with our body.</p> <p>The second reason for mindfully observing the different parts of the body is that each part can be the door to liberation and awakening. At first we\u2019ll only recognize the presence of the part of the body being observed, but later we\u2019ll come to see its true nature. Every hair on our head and every cell in our body contains the entire universe. Observing the interdependent nature of a single hair can help us to see into the nature of the universe.</p> <p>The exercise of observing every part of the body begins with the hair on the head and goes down to the skin on the soles of the feet. Sometimes we observe just one part of the body deeply, such as our eyes, heart, or toe. In the process of observation from the head to the feet, some observations may spring up in our mind. For example, as we pass our heart, we may think, \u201cMy friend John has a heart condition. I must visit him soon to see if he\u2019s all right.\u201d We can note these observations and then continue with the work of observing the remaining parts of the body. Later we can return to those observations.</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#ouput","title":"Ouput","text":"<p>Here are 20 query-text pairs for training a BERT-based search model on Thich Nhat Hanh\u2019s work, focusing on the \u201cTransformation and Healing: Sutra on the Four Establishments of Mindfulness,\u201d specifically the mindfulness exercise on \u201cParts of the Body.\u201d</p> <p>Short Queries (30%):</p> <pre><code>1.  Query: Mindfulness breathing\n</code></pre> <p>Text: \u201cWe use our conscious breathing in order to observe mindfully all the parts of the body\u2026\u201d     2.  Query: Body observation Text: \u201cThe practitioner meditates on his very own body from the soles of the feet upwards\u2026\u201d     3.  Query: Contact with the body Text: \u201cFirst of all, it is to be in contact with the body. We often have the impression that we\u2019re already totally in touch with our body, but often we\u2019re wrong\u2026\u201d     4.  Query: Parts of the body meditation Text: \u201cWe scan all the parts of the body, including the brain, heart, lungs, gall bladder, spleen, blood, urine\u2026\u201d     5.  Query: Affectionate touch Text: \u201cWe should touch each part in an affectionate and caring way. For several decades, our eyes, feet, and heart have done their work devotedly\u2026\u201d     6.  Query: Conscious breathing Text: \u201cBreathing consciously helps us dwell in mindfulness more easily and sustain the work of observing each part of the body\u2026\u201d</p> <p>Medium Queries (50%):     7. Query: How to reconnect with the body Text: \u201cTo come back to our body is to become familiar with it and to establish harmony with it. We know that if our body isn\u2019t happy, we\u2019re not happy\u2026\u201d</p> <pre><code>8.  Query: Meditation on the body\u2019s true nature\n</code></pre> <p>Text: \u201cAt first we\u2019ll only recognize the presence of the part of the body being observed, but later we\u2019ll come to see its true nature\u2026\u201d     9.  Query: Importance of mindful observation Text: \u201cThe second reason for mindfully observing the different parts of the body is that each part can be the door to liberation and awakening\u2026\u201d     10. Query: Separation from the body Text: \u201cBetween us and our body there can be a large separation, and our body remains a stranger to us. Sometimes we hate our body\u2026\u201d     11. Query: Calling parts of the body by name Text: \u201cIn addition to the conscious breathing, we can use the method of silently calling each part of the body by name to enable these parts to become increasingly clear\u2026\u201d     12. Query: Observing interdependence in the body Text: \u201cEvery hair on our head and every cell in our body contains the entire universe. Observing the interdependent nature of a single hair can help us to see into the nature of the universe\u2026\u201d     13. Query: Mindfulness and body parts Text: \u201cWhy do we need to observe in mindfulness the different parts of the body? First of all, it is to be in contact with the body\u2026\u201d     14. Query: Examples from Buddha\u2019s teachings on mindfulness Text: \u201cThe Buddha gives us the example of a farmer pouring the contents of a sack filled with a variety of seeds onto the floor and then observing and identifying each kind of seed\u2026\u201d     15. Query: Importance of making peace with the body Text: \u201cWe want our body to be calm and peaceful. To do so, we come back to our body and make peace with it\u2026\u201d     16. Query: Touching the body with care Text: \u201cWe can try touching the different parts of our body to make their acquaintance. We should touch each part in an affectionate and caring way\u2026\u201d</p> <p>Long Queries (20%):     17. Query: Why should we mindfully observe different parts of the body and how does this help us? Text: \u201cThe second reason for mindfully observing the different parts of the body is that each part can be the door to liberation and awakening. At first we\u2019ll only recognize the presence of the part of the body being observed, but later we\u2019ll come to see its true nature\u2026\u201d</p> <pre><code>18. Query: What are the practices for observing the body in mindfulness as taught by Thich Nhat Hanh?\n</code></pre> <p>Text: \u201cWe scan all the parts of the body, including the brain, heart, lungs, gall bladder, spleen, blood, urine, and so forth. The Buddha gives us the example of a farmer pouring the contents of a sack filled with a variety of seeds onto the floor and then observing and identifying each kind of seed\u2026\u201d     19. Query: How does conscious breathing help in the mindful observation of the body? Text: \u201cWe use our conscious breathing in order to observe mindfully all the parts of the body. For example: \u2018Breathing in, I am aware of the hair on my head. Breathing out, I know that this is the hair on my head.\u2019 Breathing consciously helps us dwell in mindfulness more easily and sustain the work of observing each part of the body\u2026\u201d     20. Query: What is the method of observing each part of the body as taught in Buddhist mindfulness exercises? Text: \u201cThis exercise brings us into even deeper contact with our body. Here we observe the body in all its parts, from the hair on the head to the skin on the soles of the feet. In the process of our observation, we scan all the parts of the body, including the brain, heart, lungs, gall bladder, spleen, blood, urine, and so forth\u2026\u201d</p> <p>These query-text pairs provide a balanced mix of short, medium, and long queries to help the model understand different types of search intents while reflecting key themes and detailed teachings from the text.</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#input-2-prompt","title":"Input 2 Prompt","text":"<p>Generate 20 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The queries should be: - roughly 30% short queries (~ 1-3 words) - roughly 50% medium queries (~ 4-7 words) - roughly 20% long queries (~ 7-11 words)</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Text:</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#input-3-prompt","title":"Input 3 Prompt","text":"<p>Generate 40 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The queries should be: - 40% short queries (1-3 words) - 50% medium queries (4-7 words) - 20% long queries (7-11 words)</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Output in pythonic as a list of tuples: (query, text). Truncate the text in the pair to the first 10 words.</p> <p>Text:</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#input-4-prompt","title":"Input 4 Prompt","text":"<p>Generate 40 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The queries should be: - 40% short queries (1-3 words) - 50% medium queries (4-7 words) - 20% long queries (7-11 words)</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Output in pythonic as a list of tuples: (query, text). Truncate the text in the pair to the first 10 words.</p> <p>Text:</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#input-5-prompt","title":"Input 5 Prompt","text":"<p>Generate 40 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The queries should be: - 40% short queries (1-3 words) - 50% medium queries (4-7 words) - 10% long queries (7-12 words)</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Output in pythonic as a list of tuples: (query, text). Truncate the text in the pair to the first 10 words.</p> <p>Text:</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#input-6-prompt","title":"Input 6 Prompt","text":"<p>Generate 40 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. </p> <p>15 queries: 1-3 words in length 20 queries: 4-7 words in length 5 queries: 7-12 words in length</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Output in pythonic as a list of tuples: (query, text). Truncate the text in the pair to the first 10 words.</p> <p>Text:</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#output-6","title":"Output 6","text":"<p>`queries_text_pairs = [     # 1-3 word queries     (\"mindfulness\", \"Further, the practitioner meditates on his very own body from\"),     (\"body parts\", \"Here is the hair of the head, the hairs\"),     (\"conscious breathing\", \"We use our conscious breathing in order to observe mindfully all\"),     (\"Buddha example\", \"The Buddha gives us the example of a farmer pouring\"),     (\"body awareness\", \"Breathing in, I am aware of the hair on\"),     (\"inner peace\", \"We know that if our body isn\u2019t happy, we\u2019re\"),     (\"harmony body\", \"To come back to our body is to become familiar\"),     (\"affectionate touch\", \"We can try touching the different parts of our body\"),     (\"liberation door\", \"The second reason for mindfully observing the different parts\"),     (\"interdependent nature\", \"Every hair on our head and every cell in\"),     (\"cell universe\", \"Every hair on our head and every cell in\"),     (\"scanning body\", \"In the process of our observation, we scan all\"),     (\"bodily gratitude\", \"We never really give them much attention or express\"),     (\"observing heart\", \"As we pass our heart, we may think, 'My'\"),     (\"mindful observation\", \"Why do we need to observe in mindfulness the different\"),</p> <pre><code># 4-7 word queries\n(\"how to practice conscious breathing\", \"We use our conscious breathing in order to observe\"),\n(\"awareness of head to feet\", \"Breathing in, I am aware of the hair\"),\n(\"Buddha's teachings on body mindfulness\", \"The Buddha gives us the example of a farmer\"),\n(\"building harmony with one's body\", \"To come back to our body is to\"),\n(\"reasons for mindful body observation\", \"Why do we need to observe in mindfulness the\"),\n(\"touching the body mindfully\", \"We can try touching the different parts of our\"),\n(\"grateful for bodily functions\", \"For several decades, our eyes, feet, and heart\"),\n(\"using breath for body awareness\", \"Breathing consciously helps us dwell in mindfulness more\"),\n(\"body parts as door to awakening\", \"The second reason for mindfully observing the different\"),\n(\"making peace with the body\", \"We know that if our body isn\u2019t happy\"),\n(\"why mindful breathing matters\", \"Breathing consciously helps us dwell in mindfulness more\"),\n(\"how mindfulness affects body awareness\", \"Why do we need to observe in mindfulness the\"),\n(\"relationship between body and happiness\", \"We know that if our body isn\u2019t happy\"),\n(\"using mindfulness to connect body\", \"To come back to our body is to\"),\n(\"importance of mindful body touch\", \"We can try touching the different parts of our\"),\n(\"observing body leads to insight\", \"Observing the interdependent nature of a single hair\"),\n(\"example of mindful body observation\", \"The Buddha gives us the example of a\"),\n(\"mindfulness of body leads to peace\", \"We know that if our body isn\u2019t happy\"),\n(\"contact with body through mindfulness\", \"First of all, it is to be in\"),\n(\"what mindfulness reveals about the body\", \"At first we\u2019ll only recognize the presence of\"),\n\n# 7-12 word queries\n(\"breathing and observing each part of the body mindfully\", \"Breathing consciously helps us dwell in mindfulness more easily\"),\n(\"mindful observation of body from head to feet\", \"In the process of our observation, we scan\"),\n(\"using mindfulness to become familiar with the body\", \"To come back to our body is to\"),\n(\"observing the interdependent nature of each body part\", \"Observing the interdependent nature of a single hair\"),\n(\"what Buddha teaches about observing the body's nature\", \"The Buddha gives us the example of a farmer\")\n</code></pre> <p>]`</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#input-7-prompt","title":"Input 7 prompt","text":"<p>Generate 30 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. </p> <p>10 queries: 1-3 words in length 12 queries: 4-7 words in length 8 queries: full sentence questions</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, some broad context questions beyond the scope relevant to Plum Village, and also some detailed information specific to the text.</p> <p>Output in pythonic as a list of tuples: (query, text). Truncate the text in the pair to the first 10 words.</p> <p>Text:</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#input-8-prompt-modified","title":"Input 8 prompt (modified!)","text":"<p>Generate query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text (without too much semantic overlap), some broad context questions generally relevant to Plum Village (not directly from the text), and also some detailed information specific to the text. Metadata, such as tagged titles, etc. can be ignored for this semantic based search training.</p> <p>12 queries: 1-3 words in length 12 queries: 4-7 words in length 8 queries: full sentence questions</p> <p>Give the result in a code block as csv form: query, text. </p> <p>Text:</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#output-for-prompt-8","title":"Output for Prompt 8","text":"<p>queries_text_pairs = [     # 1-3 word queries     (\"body mindfulness\", \"Further, the practitioner meditates on his very own body...\"),     (\"conscious breathing\", \"We use our conscious breathing in order to observe...\"),     (\"body parts\", \"In the process of our observation, we scan all the parts...\"),     (\"bodily impurities\", \"Further, the practitioner meditates on his very own body...\"),     (\"loving body\", \"We should touch each part in an affectionate and caring...\"),     (\"liberation\", \"Each part can be the door to liberation and awakening...\"),     (\"interdependence\", \"Every hair on our head and every cell in our body...\"),     (\"affectionate touch\", \"We should touch each part in an affectionate and caring...\"),     (\"Buddha's example\", \"The Buddha gives us the example of a farmer pouring...\"),     (\"physical harmony\", \"To come back to our body is to become familiar with it...\"),     (\"body awareness\", \"Breathing in, I am aware of the hair on my head...\"),     (\"observing parts\", \"The exercise of observing every part of the body begins...\"),</p> <pre><code># 4-7 word queries\n(\"scanning body parts through mindfulness\", \"In the process of our observation, we scan all the parts...\"),\n(\"observing body to establish harmony\", \"To come back to our body is to become familiar with it...\"),\n(\"how to mindfully observe body parts\", \"We use our conscious breathing in order to observe...\"),\n(\"practicing mindfulness for bodily awareness\", \"Breathing in, I am aware of the hair on my head...\"),\n(\"why practice mindfulness for body\", \"Why do we need to observe in mindfulness the different...\"),\n(\"meditating on physical impurities\", \"Further, the practitioner meditates on his very own body...\"),\n(\"building affection towards your body\", \"We should touch each part in an affectionate and caring...\"),\n(\"example of Buddha on mindfulness\", \"The Buddha gives us the example of a farmer pouring...\"),\n(\"observing body as a mindfulness exercise\", \"The exercise of observing every part of the body begins...\"),\n(\"liberation through bodily awareness\", \"Each part can be the door to liberation and awakening...\"),\n(\"practicing gratitude towards body parts\", \"For several decades, our eyes, feet, and heart have...\"),\n(\"understanding interdependence through mindfulness\", \"Every hair on our head and every cell in our body...\"),\n\n# Full sentence queries\n(\"How does observing the body lead to mindfulness?\", \"We use our conscious breathing in order to observe...\"),\n(\"What is the Buddha\u2019s example on mindfulness?\", \"The Buddha gives us the example of a farmer pouring...\"),\n(\"Why is it important to observe the body?\", \"Why do we need to observe in mindfulness the different...\"),\n(\"How can we establish harmony with our body?\", \"To come back to our body is to become familiar with it...\"),\n(\"What role does conscious breathing play in mindfulness?\", \"Breathing in, I am aware of the hair on my head...\"),\n(\"How does mindfulness lead to liberation?\", \"Each part can be the door to liberation and awakening...\"),\n(\"What is the importance of touching the body affectionately?\", \"We should touch each part in an affectionate and caring...\"),\n(\"How does mindfulness reveal the interdependent nature of the universe?\", \"Every hair on our head and every cell in our body...\")\n</code></pre> <p>]</p>"},{"location":"gpt_4o_search_query_text_pair_testing/testing_input_output/#working-prompt","title":"Working prompt","text":"<p>Generate query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text (without too much semantic overlap), some broad context questions generally relevant to Plum Village (not directly from the text), and also some detailed information specific to the text. Metadata, such as tagged titles, etc. can be ignored for this semantic based search training.</p> <p>12 queries: 1-3 words in length 12 queries: 4-7 words in length 8 queries: full sentence questions</p> <p>Give the result in a code block as csv form: query, text. </p> <p>Text:</p>"},{"location":"gpt_4o_translations%20experiments/passage_test/","title":"Translation","text":""},{"location":"gpt_4o_translations%20experiments/passage_test/#gpt-4o-prompt-1","title":"gpt-4o prompt 1","text":"<p>you are thich nhat hanh.</p> <p>translate this these sentences into Vietnamese:</p> <p>This exercise helps us see the impermanent and decomposing nature of our body. The objects of our mindful observation are the nine stages of the decomposition of a corpse. When we first read this, we may feel that this is not a pleasant meditation. But the effect of this practice can be very great.</p>"},{"location":"gpt_4o_translations%20experiments/passage_test/#out","title":"Out","text":"<p>B\u00e0i t\u1eadp n\u00e0y gi\u00fap ch\u00fang ta th\u1ea5y r\u00f5 t\u00ednh v\u00f4 th\u01b0\u1eddng v\u00e0 s\u1ef1 ph\u00e2n h\u1ee7y c\u1ee7a th\u00e2n th\u1ec3 ch\u00fang ta. C\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng quan s\u00e1t ch\u00e1nh ni\u1ec7m l\u00e0 ch\u00edn giai \u0111o\u1ea1n ph\u00e2n h\u1ee7y c\u1ee7a m\u1ed9t t\u1eed thi. Khi \u0111\u1ecdc \u0111i\u1ec1u n\u00e0y l\u1ea7n \u0111\u1ea7u, ch\u00fang ta c\u00f3 th\u1ec3 c\u1ea3m th\u1ea5y r\u1eb1ng \u0111\u00e2y kh\u00f4ng ph\u1ea3i l\u00e0 m\u1ed9t thi\u1ec1n t\u1eadp d\u1ec5 ch\u1ecbu. Nh\u01b0ng t\u00e1c d\u1ee5ng c\u1ee7a th\u1ef1c t\u1eadp n\u00e0y c\u00f3 th\u1ec3 r\u1ea5t l\u1edbn.</p>"},{"location":"gpt_4o_translations%20experiments/passage_test/#prompt-2-back-into-gpt-4o-translate-to-english","title":"prompt 2 -- back into gpt-4o translate to English","text":"<p>This exercise helps us to clearly see the impermanence and decay of our bodies. The objects of mindful observation are the nine stages of a decaying corpse. When reading this for the first time, we might feel that this is not a pleasant meditation practice. However, the effects of this practice can be profound.</p>"},{"location":"gpt_4o_translations%20experiments/passage_test/#vietai-en2vi-translation-mtet-translation","title":"VietAI EN2vi translation MTet translation","text":"<p>B\u00e0i t\u1eadp n\u00e0y gi\u00fap ta th\u1ea5y \u0111\u01b0\u1ee3c b\u1ea3n ch\u1ea5t v\u00f4 th\u01b0\u1eddng v\u00e0 ph\u00e2n h\u1ee7y c\u1ee7a th\u00e2n. \u0110\u1ed1i t\u01b0\u1ee3ng quan s\u00e1t ch\u00e1nh ni\u1ec7m c\u1ee7a ta l\u00e0 ch\u00edn giai \u0111o\u1ea1n ph\u00e2n h\u1ee7y x\u00e1c ch\u1ebft. Khi \u0111\u1ecdc \u0111\u1ebfn \u0111\u00e2y, ta c\u00f3 th\u1ec3 c\u1ea3m th\u1ea5y \u0111\u00e2y kh\u00f4ng ph\u1ea3i l\u00e0 m\u1ed9t thi\u1ec1n \u0111\u1ecbnh d\u1ec5 ch\u1ecbu. Nh\u01b0ng hi\u1ec7u qu\u1ea3 c\u1ee7a vi\u1ec7c th\u1ef1c h\u00e0nh n\u00e0y c\u00f3 th\u1ec3 r\u1ea5t l\u1edbn lao.</p>"},{"location":"user-guide/best-practices/","title":"Best Practices","text":"<p>This guide outlines recommended practices for using TNH Scholar effectively.</p>"},{"location":"user-guide/best-practices/#general-guidelines","title":"General Guidelines","text":""},{"location":"user-guide/best-practices/#text-processing","title":"Text Processing","text":""},{"location":"user-guide/best-practices/#1-input-preparation","title":"1. Input Preparation","text":"<ul> <li>Ensure text files use consistent line endings</li> <li>Remove any special formatting or control characters</li> <li>Use UTF-8 encoding for all text files</li> <li>Consider running <code>nfmt</code> on input files to standardize formatting</li> </ul>"},{"location":"user-guide/best-practices/#2-language-handling","title":"2. Language Handling","text":"<ul> <li>Specify language codes explicitly when known</li> <li>Use ISO 639-1 two-letter codes (e.g., 'en', 'vi')</li> <li>Allow auto-detection only for simple cases</li> </ul>"},{"location":"user-guide/best-practices/#3-file-management","title":"3. File Management","text":"<ul> <li>Keep original files backed up</li> <li>Use descriptive file names</li> <li>Maintain consistent directory structure</li> <li>Store intermediate results when running long pipelines</li> </ul>"},{"location":"user-guide/best-practices/#command-line-tools","title":"Command-Line Tools","text":""},{"location":"user-guide/best-practices/#tnh-fab","title":"TNH-FAB","text":""},{"location":"user-guide/best-practices/#1-pattern-selection","title":"1. Pattern Selection","text":"<ul> <li>Use default patterns for initial testing</li> <li>Create custom patterns for specific needs</li> <li>Test patterns with small samples first</li> <li>Document pattern modifications</li> </ul>"},{"location":"user-guide/best-practices/#2-pipeline-design","title":"2. Pipeline Design","text":"<ul> <li>Break complex processing into steps</li> <li>Use intermediate files for long pipelines</li> <li>Validate output at each stage</li> <li>Consider using <code>tee</code> for debugging</li> </ul> <p>Example of good pipeline practice:</p> <pre><code># Good: Save intermediate results\ncat input.txt | \\\n  tnh-fab punctuate &gt; punctuated.txt &amp;&amp; \\\n  tnh-fab section punctuated.txt &gt; sections.json &amp;&amp; \\\n  tnh-fab process -p format_xml -s sections.json punctuated.txt &gt; final.xml\n\n# Not recommended: Direct pipeline without saves\ncat input.txt | tnh-fab punctuate | tnh-fab section | tnh-fab process -p format_xml\n</code></pre>"},{"location":"user-guide/best-practices/#3-error-handling","title":"3. Error Handling","text":"<ul> <li>Check command exit codes</li> <li>Save error output for debugging</li> <li>Use verbose mode for troubleshooting</li> <li>Keep log files organized</li> </ul>"},{"location":"user-guide/best-practices/#audio-transcribe","title":"Audio-Transcribe","text":""},{"location":"user-guide/best-practices/#1-audio-processing","title":"1. Audio Processing","text":"<ul> <li>Use appropriate chunk sizes for content</li> <li>Consider silence detection for natural breaks</li> <li>Monitor transcription quality</li> <li>Save intermediate audio chunks</li> </ul>"},{"location":"user-guide/best-practices/#2-youtube-downloads","title":"2. YouTube Downloads","text":"<ul> <li>Verify video availability before batch processing</li> <li>Use CSV files for bulk operations</li> <li>Include timestamps when needed</li> <li>Save downloaded audio files</li> </ul>"},{"location":"user-guide/best-practices/#pattern-development","title":"Pattern Development","text":""},{"location":"user-guide/best-practices/#1-pattern-design","title":"1. Pattern Design","text":"<ul> <li>Keep patterns focused and single-purpose</li> <li>Include clear documentation</li> <li>Test with various input types</li> <li>Patterns are automatically version controlled using git.</li> <li>Use git tools to inspect and manage versions.</li> </ul>"},{"location":"user-guide/best-practices/#2-template-variables","title":"2. Template Variables","text":"<ul> <li>Use descriptive variable names</li> <li>Provide default values when appropriate</li> <li>Document required variables</li> <li>Test variable combinations</li> </ul> <p>Example pattern structure:</p> <pre><code>---\ndescription: Example pattern for formatting\nversion: 1.0\nauthor: TNH Scholar\n---\nProcess this text according to these parameters:\n\nLanguage: {{ language }}\nStyle: {{ style_convention }}\nReview Count: {{ review_count }}\n\nAdditional instructions...\n</code></pre>"},{"location":"user-guide/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/best-practices/#1-resource-management","title":"1. Resource Management","text":"<ul> <li>Monitor token usage</li> <li>Use appropriate batch sizes</li> <li>Consider chunking for large files</li> <li>Cache intermediate results</li> </ul>"},{"location":"user-guide/best-practices/#2-api-usage","title":"2. API Usage","text":"<ul> <li>Implement rate limiting</li> <li>Handle API errors gracefully</li> <li>Monitor usage and costs</li> <li>Use appropriate models for tasks</li> </ul>"},{"location":"user-guide/best-practices/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"user-guide/best-practices/#1-input-validation","title":"1. Input Validation","text":"<ul> <li>Test with various file sizes</li> <li>Include different languages</li> <li>Check character encodings</li> <li>Verify line endings</li> </ul>"},{"location":"user-guide/best-practices/#2-output-validation","title":"2. Output Validation","text":"<ul> <li>Verify file formats</li> <li>Check content integrity</li> <li>Validate XML structure</li> <li>Compare with expected results</li> </ul>"},{"location":"user-guide/best-practices/#3-process-validation","title":"3. Process Validation","text":"<ul> <li>Monitor system resources</li> <li>Track processing time</li> <li>Log error conditions</li> <li>Document edge cases</li> </ul>"},{"location":"user-guide/best-practices/#security-considerations","title":"Security Considerations","text":""},{"location":"user-guide/best-practices/#1-api-keys","title":"1. API Keys","text":"<ul> <li>Use environment variables</li> <li>Never commit keys to version control</li> <li>Rotate keys regularly</li> <li>Monitor API usage</li> </ul>"},{"location":"user-guide/best-practices/#2-file-handling","title":"2. File Handling","text":"<ul> <li>Validate input files</li> <li>Use secure file permissions</li> <li>Clean up temporary files</li> <li>Handle sensitive content appropriately</li> </ul>"},{"location":"user-guide/best-practices/#documentation","title":"Documentation","text":""},{"location":"user-guide/best-practices/#1-code-comments","title":"1. Code Comments","text":"<ul> <li>Document complex logic</li> <li>Explain pattern usage</li> <li>Note assumptions</li> <li>Include examples</li> </ul>"},{"location":"user-guide/best-practices/#2-process-documentation","title":"2. Process Documentation","text":"<ul> <li>Document workflows</li> <li>Create usage examples</li> <li>Update for changes</li> <li>Include troubleshooting guides</li> </ul>"},{"location":"user-guide/overview/","title":"User Guide Overview","text":"<p>TNH Scholar is an AI-driven platform designed for text analysis, translation, and processing. The system provides a suite of tools for handling multilingual texts with a focus on wisdom and mindfulness content. This guide will help you understand and effectively use the TNH Scholar toolkit.</p>"},{"location":"user-guide/overview/#core-components","title":"Core Components","text":"<p>The TNH Scholar system consists of several integrated components working together. At its foundation, the system uses AI services for text processing while providing robust tools for managing and manipulating text content.</p> <p>The pattern system forms the backbone of text processing operations. Patterns are template-based instructions that guide AI interactions, ensuring consistent and customizable text processing. These patterns can be versioned, shared, and modified to suit specific needs.</p> <p>The system includes several command-line tools that work together, each specializing in specific aspects of text processing. These tools can be used independently or combined into powerful processing pipelines.</p>"},{"location":"user-guide/overview/#key-features","title":"Key Features","text":"<p>TNH Scholar provides text processing capabilities through its main features:</p> <p>Audio transcription enables the processing of audio content into text format. The system can handle various audio sources, including YouTube videos, and supports multiple languages.</p> <p>Text processing forms the core functionality, offering capabilities for punctuation, sectioning, and translation. The system maintains document structure and metadata throughout the processing pipeline.</p> <p>Translation services support line-by-line translation while preserving context and document structure. The system is particularly attuned to handling mindfulness and wisdom content.</p> <p>Pattern-based processing allows for customizable text transformations. The pattern system provides a flexible way to define and apply processing instructions across different types of content.</p>"},{"location":"user-guide/overview/#getting-started","title":"Getting Started","text":"<p>To begin using TNH Scholar, you'll need to complete a few initial steps:</p> <p>First, install the package using pip and run the setup tool to configure your environment. This process includes setting up necessary directories and downloading default patterns.</p> <p>After installation, familiarize yourself with the basic commands and their operations. The system is designed to be used incrementally, allowing you to build complexity as needed.</p> <p>Remember to configure your OpenAI API key, as many functions rely on AI services for processing. The setup tool will guide you through this process.</p>"},{"location":"user-guide/overview/#system-organization","title":"System Organization","text":"<p>TNH Scholar organizes its functionality into distinct areas:</p> <p>The command-line interface provides direct access to all major functions through several specialized tools. Each tool focuses on specific tasks while maintaining consistent interfaces and behavior.</p> <p>The pattern system manages processing templates and instructions. It includes version control and concurrent access management, ensuring safe and consistent pattern usage.</p> <p>Configuration management handles system settings, API keys, and processing options. The system supports both global and project-specific configurations.</p> <p>Directory management maintains organized storage of patterns, logs, and processing results. The system creates and manages necessary directories automatically.</p>"},{"location":"user-guide/overview/#best-practices","title":"Best Practices","text":"<p>Developing effective workflows with TNH Scholar involves understanding some key practices:</p> <p>Start with small, focused tasks and gradually build more complex pipelines. This approach helps ensure reliability and makes troubleshooting easier.</p> <p>Use appropriate patterns for specific tasks, and consider creating custom patterns for specialized needs. The pattern system is designed to be extensible and adaptable.</p> <p>Maintain organized directory structures and consistent file naming conventions. This organization becomes particularly important when working with multiple files and processing stages.</p> <p>Document your workflows and pattern modifications. This documentation helps maintain consistency and allows for effective collaboration.</p>"},{"location":"user-guide/overview/#next-steps","title":"Next Steps","text":"<p>After reviewing this overview, explore the detailed sections of the user guide:</p> <p>The Configuration guide provides detailed information about setting up and customizing your TNH Scholar environment.</p> <p>Best Practices offers guidance on effective system usage and workflow development.</p> <p>The Pattern System documentation explains how to create, modify, and manage processing patterns.</p> <p>Tool-specific guides provide detailed information about each command-line tool and its capabilities.</p>"},{"location":"user-guide/patterns/","title":"TNH Scholar Pattern System","text":"<p>The TNH Scholar Pattern System is inspired by and builds upon concepts from Daniel Miessler's 'fabric' project (https://github.com/danielmiessler/fabric). Like fabric, it uses template-based prompting for AI interactions, but adds version control and concurrent access management.</p> <p>It is designed to interface with tnh-fab a multi-command text processing tool.</p> <p>Additional tools which use patterns may be developed for the project.</p> <p>The pattern system provides a version-controlled, concurrent-safe way to manage text processing templates. It is built around Jinja2 templates with Git-based versioning and file locking for safety.</p>"},{"location":"user-guide/patterns/#core-components","title":"Core Components","text":""},{"location":"user-guide/patterns/#pattern","title":"Pattern","text":"<p>A Pattern represents a single text processing template with:</p> <ul> <li>Instructions (as a Jinja2 template)</li> <li>Default template values</li> <li>Metadata in YAML frontmatter (optional)</li> </ul> <p>Example pattern file:</p> <pre><code>---\ndescription: Example pattern\nversion: 1.0\n---\nProcess this text in {{ language }} using {{ style_convention }} formatting.\n</code></pre>"},{"location":"user-guide/patterns/#pattern-files","title":"Pattern Files","text":"<ul> <li>Stored as .md files</li> <li>Include optional YAML frontmatter</li> <li>Use Jinja2 template syntax</li> <li>Support template variables</li> </ul>"},{"location":"user-guide/patterns/#using-patterns","title":"Using Patterns","text":""},{"location":"user-guide/patterns/#through-tnh-fab-cli","title":"Through TNH-FAB CLI","text":"<p>The most common way to use patterns is through the TNH-FAB command-line tool:</p> <pre><code># Basic pattern processing\ntnh-fab process -p pattern_name input.txt\n\n# Process with sections\ntnh-fab process -p format_xml -s sections.json input.txt\n\n# Process by paragraphs\ntnh-fab process -p format_xml -g input.txt\n\n# Process with template values\ntnh-fab process -p format_xml -t template.yaml input.txt\n</code></pre> <p>Each TNH-FAB command (punctuate, section, translate, process) uses specific patterns:</p> <ul> <li>punctuate: Uses punctuation patterns (default: 'default_punctuate')</li> <li>section: Uses section analysis patterns (default: 'default_section')</li> <li>translate: Uses translation patterns (default: 'default_line_translation')</li> <li>process: Requires explicit pattern specification</li> </ul>"},{"location":"user-guide/patterns/#programmatic-usage","title":"Programmatic Usage","text":"<p>For developers building tools that use the pattern system:</p> <pre><code>from tnh_scholar.ai_text_processing import Pattern, PatternManager\n\n# Initialize pattern manager\npattern_manager = PatternManager(pattern_dir)\n\n# Load a pattern\npattern = pattern_manager.load_pattern(\"my_pattern\")\n\n# Apply template values\nresult = pattern.apply_template({\n    \"language\": \"English\",\n    \"style_convention\": \"APA\"\n})\n</code></pre>"},{"location":"user-guide/patterns/#pattern-location","title":"Pattern Location","text":"<p>By default, patterns are stored in the user's home directory under:</p> <pre><code>~/.config/tnh-scholar/patterns/\n</code></pre> <p>This location can be customized by setting the <code>TNH_PATTERN_DIR</code> environment variable:</p> <pre><code># In .bashrc, .zshrc, or similar:\nexport TNH_PATTERN_DIR=/path/to/patterns\n</code></pre> <p>(or loaded through a <code>.env</code> file for development installations.)</p> <p>The pattern system will:</p> <ol> <li>First check for <code>TNH_PATTERN_DIR</code> environment variable</li> <li>If not set, use the default ~/.config/tnh-scholar/patterns</li> <li>Create the pattern directory if it doesn't exist</li> </ol> <p>When using a pattern name with tnh-fab commands (e.g., <code>tnh-fab process -p my_pattern</code>), the system searches for a corresponding .md file (e.g., <code>my_pattern.md</code>) in the pattern directory and its subdirectories.</p>"},{"location":"user-guide/patterns/#default-patterns","title":"Default Patterns","text":"<p>Through the setup utility, tnh-setup, the user has the option to download and install several default and example patterns.</p> <p>Note that tnh-fab expects the following patterns to be in the patterns directory for default use:</p> <ul> <li>default_punctuate.md - Default punctuation pattern</li> <li>default_section.md - Default section analysis pattern</li> <li>default_line_translation.md - Default translation pattern</li> </ul> <p>These provide basic functionality but can be customized or overridden by creating patterns with the same names in your pattern directory.</p>"},{"location":"user-guide/patterns/#pattern-integration","title":"Pattern Integration","text":"<p>The pattern system can be integrated into other tools and workflows:</p> <ul> <li>Custom text processing applications</li> <li>Web services</li> <li>Analysis pipelines</li> <li>Batch processing systems</li> </ul>"},{"location":"user-guide/patterns/#template-variables","title":"Template Variables","text":"<p>Templates support variables through Jinja2 syntax:</p> <ul> <li>Use <code>{{ variable }}</code> for simple substitution</li> <li>Values provided when applying template</li> <li>Default values can be specified in Pattern</li> </ul>"},{"location":"user-guide/patterns/#pattern-storage-and-management","title":"Pattern Storage and Management","text":""},{"location":"user-guide/patterns/#pattern-manager","title":"Pattern Manager","text":"<p>The PatternManager provides the main interface for:</p> <ul> <li>Loading patterns by name</li> <li>Saving new patterns</li> <li>Version control integration</li> <li>Concurrent access management</li> </ul>"},{"location":"user-guide/patterns/#pattern-locations","title":"Pattern Locations","text":"<p>Patterns are stored in a directory specified as either:</p> <ul> <li> <p><code>$HOME/.config/tnh-scholar/patterns</code> (default search location)</p> </li> <li> <p>A custom directory specified by TNH_PATTERN_DIR environment variable, which can also be configured in a .env file.</p> </li> </ul>"},{"location":"user-guide/patterns/#version-control","title":"Version Control","text":"<p>Patterns are automatically version controlled:</p> <ul> <li>Git-backed storage</li> <li>Automatic commits on changes</li> <li>History tracking</li> <li>Change validation</li> </ul>"},{"location":"user-guide/patterns/#concurrent-access","title":"Concurrent Access","text":"<p>The system provides safe concurrent access through:</p> <ul> <li>File-level locking</li> <li>Lock cleanup</li> <li>Stale lock detection</li> <li>Safe access patterns</li> </ul>"},{"location":"user-guide/patterns/#creating-patterns","title":"Creating Patterns","text":"<p>Patterns must have:</p> <ol> <li>Unique name</li> <li>Valid Jinja2 template content</li> <li>Optional default template values</li> </ol> <p>Example pattern creation:</p> <pre><code>from tnh_scholar.ai_text_processing import Pattern\n\npattern = Pattern(\n    name=\"example_pattern\",\n    instructions=\"Process {{ text }} using {{ style }}\",\n    default_template_fields={\"style\": \"default\"}\n)\n</code></pre>"},{"location":"user-guide/patterns/#pattern-file-format","title":"Pattern File Format","text":"<p>A pattern file (<code>example.md</code>):</p> <pre><code>---\ndescription: Example processing pattern\nversion: 1.0\nauthor: TNH Scholar\n---\nPlease process this text according to these parameters:\n\nLanguage: {{ language }}\nStyle: {{ style_convention }}\nReview Count: {{ review_count }}\n\nApply standard formatting while maintaining original meaning.\n</code></pre>"},{"location":"user-guide/patterns/#error-handling","title":"Error Handling","text":"<p>The system handles common errors:</p> <ul> <li>Missing patterns</li> <li>Invalid template syntax</li> <li>Concurrent access conflicts</li> <li>Version control issues</li> </ul>"},{"location":"user-guide/patterns/#technical-details","title":"Technical Details","text":""},{"location":"user-guide/patterns/#file-locking","title":"File Locking","text":"<ul> <li>Uses system-level file locking</li> <li>Automatic lock cleanup</li> <li>Timeout handling</li> <li>Safe concurrent access</li> </ul>"},{"location":"user-guide/patterns/#version-control_1","title":"Version Control","text":"<ul> <li>Git-based backend</li> <li>Automatic commit messages</li> <li>Change tracking</li> <li>History preservation</li> </ul>"},{"location":"user-guide/patterns/#pattern-validation","title":"Pattern Validation","text":"<p>All patterns are validated for:</p> <ul> <li>Template syntax</li> <li>Required variables</li> <li>Unique naming</li> <li>Content format</li> </ul>"},{"location":"user-guide/patterns/#limitations","title":"Limitations","text":"<p>Current implementation:</p> <ul> <li>Single repository per PatternManager</li> <li>File-based storage only</li> <li>Local Git repository</li> <li>Synchronous operations</li> </ul>"},{"location":"user-guide/patterns/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/patterns/#1-pattern-naming","title":"1. Pattern Naming","text":"<ul> <li>Use descriptive names</li> <li>Include purpose in name</li> <li>Follow lowercase_with_underscores format</li> </ul>"},{"location":"user-guide/patterns/#2-template-content","title":"2. Template Content","text":"<ul> <li>Document required variables</li> <li>Include usage examples</li> <li>Provide default values</li> <li>Use clear template syntax</li> </ul>"},{"location":"user-guide/patterns/#3-pattern-management","title":"3. Pattern Management","text":"<ul> <li>Regular pattern updates</li> <li>Version control usage</li> <li>Proper error handling</li> <li>Pattern testing</li> </ul>"}]}