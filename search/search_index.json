{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TNH Scholar","text":"<p>TNH Scholar is a long-term, community-aligned initiative to build a trusted multilingual digital ecosystem for studying, translating, and engaging with the teachings of Th\u00edch Nh\u1ea5t H\u1ea1nh and the Plum Village tradition.</p> <p>This document contains deeper onboarding and architectural context. For a more concise intro to the project, see the README.</p>"},{"location":"#vision-aspirations","title":"Vision &amp; Aspirations","text":"<p>TNH Scholar is a long-term effort to support the living Plum Village tradition with trustworthy, transparent digital tools. This work is intended, both in development and usage, to deeply respect the tradition and practice of Thay Nh\u1ea5t H\u1ea1nh and the Plum Village community.</p> <ul> <li>Build a canonical, multilingual corpus of Th\u00edch Nh\u1ea5t H\u1ea1nh's teachings with high-fidelity text, rich metadata, and sentence-level alignment across languages.</li> <li>Provide AI-assisted research tools that expose their reasoning and keep human judgment central, serving monastics, practitioners, teachers, and researchers.</li> <li>Support cross-lingual research across Vietnamese, English, French, Chinese, P\u0101li, Sanskrit, Tibetan, and other sources.</li> <li>Enable rich interactive environments like bilingual readers combining scans, text, translations, and audio.</li> <li>Enable human-supervised AI workflows for corpus processing, translation, and evaluation.</li> </ul> <p>This work is envisioned on a multi-year to multi-decade timescale. The CLI tools and GenAI Service in this repository are the early infrastructure for that larger arc.</p> <p>For the full vision, including scope, non-scope, relationship to spin-offs, and time horizon, see:</p> <ul> <li>TNH Scholar Project Vision (in project/vision)</li> </ul> <p>Note on Terminology: Earlier versions of TNH Scholar referred to engineered AI prompts as \"Patterns\" to emphasize their engineering pattern nature. Current documentation uses \"Prompt\" to align with industry standards. References to \"Pattern\" in legacy documentation should be read as \"Prompt\".</p>"},{"location":"#what-tnh-scholar-makes-possible","title":"What TNH Scholar Makes Possible","text":"<p>TNH Scholar aims to support the community in:</p> <ul> <li>Exploring teachings with bilingual text and translation side-by-side</li> <li>Searching themes and teachings across languages and periods</li> <li>Reviewing and refining translations collaboratively with transparent history</li> <li>Connecting practitioners, researchers, and teachers with reliable digital resources</li> <li>Preserving teaching materials for future generations with clarity and care</li> </ul> <p>These are aspirational but active development goals aligned with the needs of the Plum Village community.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Audio and transcript processing:  with diarization and YouTube support</li> <li>Text formatting and translation:  for punctuation, translation, sectioning, and prompt-driven processing</li> <li>Acquisition utilities:  for transcripts;  and  for prep and planning</li> <li>Setup and configuration:  plus guided config in Getting Started</li> <li>Prompt system: See Prompt System Architecture and ADR-PT03 for current status and roadmap</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Choose your path based on your primary interest:</p>"},{"location":"#path-1-use-the-tools","title":"Path 1: Use the Tools","text":"<p>For practitioners, translators, and researchers ready to work with TNH Scholar:</p> <p>Get up and running with TNH Scholar's CLI tools for transcription, translation, and text processing:</p> <ul> <li>Install from PyPI: </li> <li>Configure credentials per Configuration</li> <li>Follow the Quick Start Guide for your first workflow</li> <li>Explore task-oriented workflows in the User Guide</li> </ul>"},{"location":"#path-2-understand-the-vision-principles","title":"Path 2: Understand the Vision &amp; Principles","text":"<p>For community members, stakeholders, and and those exploring how this project fits within Plum Village initiatives:</p> <p>Explore the project's foundation, values, and long-term direction:</p> <ul> <li>Vision &amp; Scope: Project Vision \u2013 multi-year aspirations, community alignment, and what's in/out of scope</li> <li>Philosophy: Philosophy \u2013 ethical foundations and mindful technology principles</li> <li>Principles: Design Principles \u2013 transparency, human judgment, and architectural values</li> <li>Community Context: Parallax Overview \u2013 relationship to broader Plum Village digital initiatives</li> </ul>"},{"location":"#path-3-contribute-to-development","title":"Path 3: Contribute to Development","text":"<p>For developers, architects, and contributors:</p> <p>Understand the technical foundation and start contributing:</p> <ul> <li>Setup: DEV_SETUP.md \u2013 development environment and workflows</li> <li>Architecture: System Design and Architecture Overview \u2013 core patterns and technical decisions</li> <li>Standards: Style Guide and Contributing \u2013 code quality and PR workflow</li> <li>Key ADRs: Start with GenAI Service Strategy and Prompt System Status</li> <li>Research: Research Index \u2013 experiments, evaluations, and exploratory work</li> <li>Future Directions: Long-term Vision \u2013 planned research directions and architectural horizons</li> <li>Common commands: , , , , </li> </ul>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"<ul> <li>Getting Started: Installation, configuration, first-run guidance</li> <li>User Guide: Task-oriented workflows and practical how-tos</li> <li>CLI Reference: Auto-generated command documentation for every CLI entry point</li> <li>API: Python API reference (mkdocstrings)</li> <li>Architecture: ADRs, design docs, system diagrams by component</li> <li>Development: Contributor guides, design principles, engineering practices</li> <li>Docs Ops: Style guides, ADR template, documentation maintenance</li> <li>Research: Experiments, evaluations, exploratory notes</li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>TNH Scholar is currently in alpha stage. Expect ongoing API and workflow changes during active development.</p>"},{"location":"#support-community","title":"Support &amp; Community","text":"<ul> <li>Bug reports &amp; feature requests: GitHub Issues</li> <li>Questions &amp; discussions: GitHub Discussions</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the GPL-3.0 License.</p>"},{"location":"#documentation-map","title":"Documentation Map","text":""},{"location":"#getting-started_1","title":"Getting Started","text":"<ul> <li>Configuration</li> <li>Installation</li> <li>Quick Start Guide</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Best Practices</li> <li>TNH Scholar Prompt System</li> <li>User Guide Overview</li> </ul>"},{"location":"#project","title":"Project","text":"<ul> <li>conceptual-architecture</li> <li>future-directions</li> <li>philosophy</li> <li>principles</li> <li>TNH Scholar CHANGELOG</li> <li>TNH Scholar CONTRIBUTING</li> <li>TNH Scholar README</li> <li>TNH Scholar Release Checklist</li> <li>TNH Scholar TODO List</li> <li>TNH Scholar Versioning Policy</li> <li>TNH-Scholar DEV_SETUP</li> <li>vision</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>parallax-overview</li> </ul>"},{"location":"#cli-reference","title":"CLI Reference","text":"<ul> <li>audio-transcribe</li> <li>Command Line Tools Overview</li> <li>json-to-srt</li> <li>nfmt</li> <li>sent-split</li> <li>srt-translate</li> <li>tnh-fab</li> <li>tnh-setup</li> <li>token-count</li> <li>ytt-fetch</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<ul> <li>ADR-A01: Adopt Object-Service for GenAI Interactions</li> <li>ADR-A02: PatternCatalog Integration (V1)</li> <li>ADR-A08: Configuration / Parameters / Policy Taxonomy</li> <li>ADR-A09: V1 Simplified Implementation Pathway</li> <li>ADR-A11: Model Parameters and Strong Typing Fix</li> <li>ADR-A12: Prompt System &amp; Fingerprinting Architecture (V1)</li> <li>ADR-A13: Migrate All OpenAI Interactions to GenAIService</li> <li>ADR-A14: File-Based Registry System for Provider Metadata</li> <li>ADR-AT01: AI Text Processing Pipeline Redesign</li> <li>ADR-AT02: TextObject Architecture Decision Records</li> <li>ADR-AT03: AI Text Processing Object-Service Refactor</li> <li>ADR-DD01: Documentation System Reorganization Strategy</li> <li>ADR-DD02: Documentation Main Content and Navigation Strategy</li> <li>adr-dd03-phase1-punchlist</li> <li>ADR-DD03: Pattern to Prompt Terminology Standardization</li> <li>ADR-K01: Preliminary Architectural Strategy for TNH Scholar Knowledge Base</li> <li>ADR-MD01: Adoption of JSON-LD for Metadata Management</li> <li>ADR-MD02: Metadata Infrastructure Object-Service Integration</li> <li>ADR-OS01: Object-Service Design Architecture V3</li> <li>ADR-PP01: Rapid Prototype Versioning Policy</li> <li>ADR-PT01: Pattern Access Strategy</li> <li>ADR-PT02: Adopt Pattern and PatternCatalog as Core Concepts</li> <li>ADR-PT03: Prompt System Current Status &amp; Roadmap</li> <li>ADR-PT04: Prompt System Refactor Plan</li> <li>ADR-TG01: tnh-gen CLI Architecture</li> <li>ADR-TG02: TNH-Gen CLI Prompt System Integration</li> <li>ADR-TR01: AssemblyAI Integration for Transcription Service</li> <li>ADR-TR02: Optimized SRT Generation Design</li> <li>ADR-TR03: Standardizing Timestamps to Milliseconds</li> <li>ADR-TR04: AssemblyAI Service Implementation Improvements</li> <li>ADR-VP01: Video Processing Return Types and Configuration</li> <li>adr-vsc01-vscode-integration-strategy</li> <li>ADR-VSC02: VS Code Extension Integration with tnh-gen CLI</li> <li>ADR-YF00: Early yt-fetch Transcript Decisions (Historical)</li> <li>ADR-YF01: YouTube Transcript Source Handling</li> <li>ADR-YF02: YouTube Transcript Format Selection</li> <li>Architecture Overview</li> <li>Audio Chunking Algorithm Design Document</li> <li>Core Pattern Architecture: Meta-patterns, Textual Expansion Processing</li> <li>Design Strategy: VS Code as UI/UX Platform for TNH Scholar</li> <li>Diarization Algorithms</li> <li>Diarization Chunker Module Design Strategy</li> <li>Diarization System Design</li> <li>Documentation Design</li> <li>GenAI Service \u2014 Design Strategy</li> <li>Generate Markdown Translation JSON Pairs</li> <li>Generate Markdown Vietnamese</li> <li>Interval-to-Segment Mapping Algorithm</li> <li>JVB Viewer \u2014 Version 2 Strategy &amp; High\u2011Level Design</li> <li>Language-Aware Chunking Orchestrator Notes</li> <li>LU\u00c2N-H\u1ed2I</li> <li>minimal but extensible setup tool for the prototyping phase</li> <li>Modular Pipeline Design: Best Practices for Audio Transcription and Diarization</li> <li>Object-Service Design Gaps</li> <li>Object-Service Design Overview</li> <li>Object-Service Implementation Status</li> <li>OpenAI Interface Migration Plan</li> <li>Package Version Checker Design Document</li> <li>Practical Language-Aware Chunking Design</li> <li>Prompt System Architecture</li> <li>Simplified Language-Aware Chunking Design</li> <li>Speaker Diarization Algorithm Design</li> <li>Speaker Diarization and Time-Mapped Transcription System Design</li> <li>TextObject Original Design</li> <li>TextObject System Design Document</li> <li>TimelineMapper Design Document</li> <li>TNH Configuration Management</li> <li>TNH FAB Design Document</li> <li>TNH-FAB Command Line Tool Specification</li> <li>TNH\u2011Scholar Utilities Catalog</li> <li>Versioning Policy Documentation Additions</li> <li>YouTube API vs yt-dlp Evaluation</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Contributing to TNH Scholar (Prototype Phase)</li> <li>Development Documentation</li> <li>Fine Tuning Strategy</li> <li>forensic-analysis</li> <li>Git Workflow &amp; Safety Guide</li> <li>Human-AI Software Engineering Principles</li> <li>Implementation Summary: Git Safety Improvements</li> <li>Improvements / Initial structure</li> <li>Incident Report: Git Recovery - December 7, 2025</li> <li>incident-report-updates</li> <li>Release Workflow</li> <li>TNH Scholar Design Principles</li> <li>TNH Scholar Style Guide</li> <li>TNH Scholar System Design</li> </ul>"},{"location":"#docs-ops","title":"Docs Ops","text":"<ul> <li>ADR Template</li> <li>Markdown Standards</li> <li>MkDocs Strict Warning Backlog</li> <li>Preview TNH Scholar Theme</li> <li>TNH Scholar Theme Design</li> </ul>"},{"location":"#research","title":"Research","text":"<ul> <li>1-3 Word Queries</li> <li>GPT Development Convos</li> <li>Passage Test</li> <li>Preliminary Feasibility Study</li> <li>RAG Research Directions for TNH Scholar</li> <li>Structural-Informed Adaptive Processing (SIAP) Methodology</li> <li>Summary Report on Metadata Extraction, Source Parsing, and Model Training for TNH-Scholar</li> <li>TNH Scholar Knowledge Base: Design Document</li> </ul>"},{"location":"docs-nav/","title":"Docs nav","text":"<ul> <li>TNH Scholar</li> <li>Getting Started<ul> <li>Configuration</li> <li>Installation</li> <li>Quick Start Guide</li> </ul> </li> <li>User Guide<ul> <li>Best Practices</li> <li>User Guide Overview</li> <li>TNH Scholar Prompt System</li> </ul> </li> <li>Project<ul> <li>Conceptual Architecture</li> <li>Future Directions</li> <li>Philosophy</li> <li>Principles</li> <li>Vision</li> </ul> </li> <li>Community &amp; Outreach<ul> <li>Parallax Overview</li> </ul> </li> <li>Repo Root<ul> <li>Overview</li> <li>TNH Scholar CHANGELOG</li> <li>TNH Scholar CONTRIBUTING</li> <li>TNH-Scholar DEV_SETUP</li> <li>TNH Scholar Release Checklist</li> <li>TNH Scholar README</li> <li>TNH Scholar TODO List</li> <li>TNH Scholar Versioning Policy</li> </ul> </li> <li>CLI Reference<ul> <li>audio-transcribe</li> <li>json-to-srt</li> <li>nfmt</li> <li>Command Line Tools Overview</li> <li>sent-split</li> <li>srt-translate</li> <li>tnh-fab</li> <li>tnh-setup</li> <li>token-count</li> <li>ytt-fetch</li> </ul> </li> <li>API Reference</li> <li>Architecture<ul> <li>Architecture Overview</li> <li>Ai Text Processing</li> <li>AI Text Processing<ul> <li>Adr<ul> <li>ADR-AT01: AI Text Processing Pipeline Redesign</li> <li>ADR-AT02: TextObject Architecture Decision Records</li> <li>ADR-AT03: AI Text Processing Object-Service Refactor</li> </ul> </li> <li>Design<ul> <li>TextObject Original Design</li> <li>TextObject System Design Document</li> </ul> </li> </ul> </li> <li>Configuration<ul> <li>TNH Configuration Management</li> </ul> </li> <li>Docs System<ul> <li>Adr<ul> <li>ADR-DD01: Documentation System Reorganization Strategy</li> <li>ADR-DD02: Documentation Main Content and Navigation Strategy</li> <li>ADR-DD03: Pattern to Prompt Terminology Standardization</li> <li>Adr Dd03 Phase1 Punchlist</li> </ul> </li> <li>Design<ul> <li>Documentation Design</li> </ul> </li> </ul> </li> <li>Gen Ai Service</li> <li>GenAI Service<ul> <li>Adr<ul> <li>ADR-A01: Adopt Object-Service for GenAI Interactions</li> <li>ADR-A02: PatternCatalog Integration (V1)</li> <li>ADR-A08: Configuration / Parameters / Policy Taxonomy</li> <li>ADR-A09: V1 Simplified Implementation Pathway</li> <li>ADR-A11: Model Parameters and Strong Typing Fix</li> <li>ADR-A12: Prompt System &amp; Fingerprinting Architecture (V1)</li> <li>ADR-A13: Migrate All OpenAI Interactions to GenAIService</li> <li>ADR-A14: File-Based Registry System for Provider Metadata</li> </ul> </li> <li>Design<ul> <li>GenAI Service \u2014 Design Strategy</li> <li>OpenAI Interface Migration Plan</li> </ul> </li> </ul> </li> <li>Jvb Viewer<ul> <li>Design<ul> <li>Generate Markdown Translation JSON Pairs</li> <li>Generate Markdown Vietnamese</li> <li>JVB Viewer \u2014 Version 2 Strategy &amp; High\u2011Level Design</li> <li>LU\u00c2N-H\u1ed2I</li> </ul> </li> </ul> </li> <li>Knowledge Base<ul> <li>Adr<ul> <li>ADR-K01: Preliminary Architectural Strategy for TNH Scholar Knowledge Base</li> </ul> </li> </ul> </li> <li>Metadata<ul> <li>Adr<ul> <li>ADR-MD01: Adoption of JSON-LD for Metadata Management</li> <li>ADR-MD02: Metadata Infrastructure Object-Service Integration</li> </ul> </li> </ul> </li> <li>Object Service<ul> <li>Object-Service Design Gaps</li> <li>Object-Service Design Overview</li> <li>Object-Service Implementation Status</li> <li>Adr<ul> <li>ADR-OS01: Object-Service Design Architecture V3</li> </ul> </li> </ul> </li> <li>Project Policies<ul> <li>Versioning Policy Documentation Additions</li> <li>Adr<ul> <li>ADR-PP01: Rapid Prototype Versioning Policy</li> </ul> </li> </ul> </li> <li>Prompt System<ul> <li>Prompt System Architecture</li> <li>Adr<ul> <li>ADR-PT03: Prompt System Current Status &amp; Roadmap</li> <li>ADR-PT04: Prompt System Refactor Plan</li> </ul> </li> <li>Archive<ul> <li>Core Pattern Architecture: Meta-patterns, Textual Expansion Processing</li> <li>Adr<ul> <li>ADR-PT01: Pattern Access Strategy</li> <li>ADR-PT02: Adopt Pattern and PatternCatalog as Core Concepts</li> </ul> </li> </ul> </li> </ul> </li> <li>Setup Tnh<ul> <li>Design<ul> <li>minimal but extensible setup tool for the prototyping phase</li> </ul> </li> </ul> </li> <li>TNH-Gen CLI</li> <li>Tnh Gen<ul> <li>Adr<ul> <li>ADR-TG01: tnh-gen CLI Architecture</li> <li>ADR-TG02: TNH-Gen CLI Prompt System Integration</li> </ul> </li> <li>Design<ul> <li>Archive<ul> <li>TNH-FAB Command Line Tool Specification</li> <li>TNH FAB Design Document</li> </ul> </li> </ul> </li> </ul> </li> <li>Transcription<ul> <li>Adr<ul> <li>ADR-TR01: AssemblyAI Integration for Transcription Service</li> <li>ADR-TR02: Optimized SRT Generation Design</li> <li>ADR-TR03: Standardizing Timestamps to Milliseconds</li> <li>ADR-TR04: AssemblyAI Service Implementation Improvements</li> </ul> </li> <li>Design<ul> <li>Audio Chunking Algorithm Design Document</li> <li>Diarization Algorithms</li> <li>Diarization Chunker Module Design Strategy</li> <li>Diarization System Design</li> <li>Interval-to-Segment Mapping Algorithm</li> <li>Simplified Language-Aware Chunking Design</li> <li>Language-Aware Chunking Orchestrator Notes</li> <li>Modular Pipeline Design: Best Practices for Audio Transcription and Diarization</li> <li>Practical Language-Aware Chunking Design</li> <li>Speaker Diarization Algorithm Design</li> <li>Speaker Diarization and Time-Mapped Transcription System Design</li> <li>TimelineMapper Design Document</li> </ul> </li> </ul> </li> <li>Ui Ux<ul> <li>Design<ul> <li>Design Strategy: VS Code as UI/UX Platform for TNH Scholar</li> </ul> </li> <li>Vs Code Integration<ul> <li>Adr Vsc01 Vscode Integration Strategy</li> <li>ADR-VSC02: VS Code Extension Integration with tnh-gen CLI</li> </ul> </li> </ul> </li> <li>Utilities<ul> <li>Design<ul> <li>Package Version Checker Design Document</li> <li>TNH\u2011Scholar Utilities Catalog</li> </ul> </li> </ul> </li> <li>Video Processing<ul> <li>Adr<ul> <li>ADR-VP01: Video Processing Return Types and Configuration</li> </ul> </li> </ul> </li> <li>Ytt Fetch</li> <li>YTT Fetch<ul> <li>Adr<ul> <li>ADR-YF00: Early yt-fetch Transcript Decisions (Historical)</li> <li>ADR-YF01: YouTube Transcript Source Handling</li> <li>ADR-YF02: YouTube Transcript Format Selection</li> </ul> </li> <li>Design<ul> <li>YouTube API vs yt-dlp Evaluation</li> </ul> </li> </ul> </li> </ul> </li> <li>Development<ul> <li>Contributing to TNH Scholar (Prototype Phase)</li> <li>TNH Scholar Design Principles</li> <li>Fine Tuning Strategy</li> <li>Git Workflow &amp; Safety Guide</li> <li>Human-AI Software Engineering Principles</li> <li>Improvements / Initial structure</li> <li>Development Documentation</li> <li>Release Workflow</li> <li>TNH Scholar Style Guide</li> <li>TNH Scholar System Design</li> <li>Incident Reports<ul> <li>Incident Report: Git Recovery - December 7, 2025</li> <li>2025 12 07 Reference<ul> <li>Forensic Analysis</li> <li>Implementation Summary: Git Safety Improvements</li> <li>Incident Report Updates</li> </ul> </li> </ul> </li> </ul> </li> <li>Docs Ops<ul> <li>ADR Template</li> <li>Markdown Standards</li> <li>MkDocs Strict Warning Backlog</li> <li>Preview TNH Scholar Theme</li> <li>TNH Scholar Theme Design</li> </ul> </li> <li>Research<ul> <li>GPT Development Convos</li> <li>TNH Scholar Knowledge Base: Design Document</li> <li>Summary Report on Metadata Extraction, Source Parsing, and Model Training for TNH-Scholar</li> <li>Preliminary Feasibility Study</li> <li>RAG Research Directions for TNH Scholar</li> <li>Structural-Informed Adaptive Processing (SIAP) Methodology</li> <li>Gpt4O Search Query Testing<ul> <li>1-3 Word Queries</li> </ul> </li> <li>Gpt4O Translation Experiments<ul> <li>Passage Test</li> </ul> </li> </ul> </li> <li>Documentation Index</li> <li>Archive</li> </ul>"},{"location":"documentation_index/","title":"Documentation Index","text":"<p>This is a comprehensive, searchable index of all TNH Scholar documentation with descriptions and metadata.</p> <p>For a simpler hierarchical view, see the Documentation Map section at the bottom of the main index.</p>"},{"location":"documentation_index/#getting-started","title":"Getting Started","text":"Title Description Created Path Configuration TNH Scholar requires some initial configuration to function properly. This guide covers the essential configuration steps and options. 2025-02-01 <code>docs/getting-started/configuration.md</code> Getting Started Table of contents for getting-started 2025-12-08 <code>docs/getting-started/index.md</code> Installation Install instructions for TNH Scholar, a Python package for text processing and analysis, using <code>pip</code>. 2025-02-01 <code>docs/getting-started/installation.md</code> Quick Start Guide TNH Scholar provides powerful text processing capabilities through several command-line tools. This guide will help you get started with the basic workflows. 2025-02-01 <code>docs/getting-started/quick-start-guide.md</code>"},{"location":"documentation_index/#user-guide","title":"User Guide","text":"Title Description Created Path Best Practices This guide outlines recommended practices for using TNH Scholar effectively. 2025-02-01 <code>docs/user-guide/best-practices.md</code> TNH Scholar Prompt System This document describes the TNH Scholar Prompt System (formerly called patterns). The system allows for template-based prompting of AI interactions, with version control and concurrent access management. 2025-01-19 <code>docs/user-guide/prompt-system.md</code> User Guide Table of contents for user-guide 2025-12-08 <code>docs/user-guide/index.md</code> User Guide Overview Practical guide for using TNH Scholar as a tool user or workflow designer, covering main workflows and how the pieces fit together. 2025-12-02 <code>docs/user-guide/overview.md</code>"},{"location":"documentation_index/#project","title":"Project","text":"Title Description Created Path conceptual-architecture <code>docs/project/conceptual-architecture.md</code> future-directions <code>docs/project/future-directions.md</code> philosophy <code>docs/project/philosophy.md</code> principles <code>docs/project/principles.md</code> Project Table of contents for project 2025-12-08 <code>docs/project/index.md</code> Repo Root Repository root documentation surfaced in the MkDocs site. <code>docs/project/repo-root/index.md</code> TNH Scholar CHANGELOG Chronological log of notable TNH Scholar changes. 2025-02-28 <code>docs/project/repo-root/changelog.md</code> TNH Scholar CONTRIBUTING TNH Scholar is rapidly evolving, but we strive for a predictable, reproducible development workflow. 2024-10-21 <code>docs/project/repo-root/contributing-root.md</code> TNH Scholar README TNH Scholar is an AI-driven project designed to explore, query, process and translate the teachings of Thich Nhat Hanh and the Plum Village community. The project provides tools for practitioners and scholars to engage with mindfulness and spiritual wisdom through natural language processing and machine learning models. 2024-10-21 <code>docs/project/repo-root/repo-readme.md</code> TNH Scholar Release Checklist Checklist of tasks required before publishing a TNH Scholar release. 2025-01-22 <code>docs/project/repo-root/release_checklist.md</code> TNH Scholar TODO List Roadmap tracking the highest-priority TNH Scholar tasks and release blockers. 2025-01-20 <code>docs/project/repo-root/todo-list.md</code> TNH Scholar Versioning Policy Versioning policy for TNH Scholar during rapid prototype phase (0.x) and post-1.0 stable releases 2025-12-06 <code>docs/project/repo-root/versioning.md</code> TNH-Scholar DEV_SETUP This document outlines the standard development environment for TNH\u2011Scholar. 2025-11-19 <code>docs/project/repo-root/dev-setup-guide.md</code> vision <code>docs/project/vision.md</code>"},{"location":"documentation_index/#community","title":"Community","text":"Title Description Created Path Community &amp; Outreach Documentation for community partners, stakeholders, and collaborators in the Plum Village tradition. <code>docs/community/index.md</code> parallax-overview <code>docs/community/parallax-overview.md</code>"},{"location":"documentation_index/#cli-reference","title":"CLI Reference","text":"Title Description Created Path audio-transcribe Command-line tool for audio transcription tasks. 2025-01-21 <code>docs/cli-reference/audio-transcribe.md</code> CLI Reference Table of contents for cli-reference 2025-12-08 <code>docs/cli-reference/index.md</code> Command Line Tools Overview TNH Scholar provides a suite of command-line tools designed to work together for text processing. Each tool focuses on specific tasks while maintaining consistent interfaces and behavior. This overview introduces the available tools and their primary functions. 2025-02-01 <code>docs/cli-reference/overview.md</code> json-to-srt Convert JSONL transcription output (from audio-transcribe) into SRT subtitle files. 2025-12-10 <code>docs/cli-reference/json-to-srt.md</code> nfmt <code>nfmt</code>, a newline formatting utility, standardizes line endings and spacing in text files. 2025-02-01 <code>docs/cli-reference/nfmt.md</code> sent-split Split text into sentences using NLTK, with newline or space separators. 2025-12-10 <code>docs/cli-reference/sent-split.md</code> srt-translate Translate SRT subtitle files while preserving timecodes, using TNH Scholar translation patterns. 2025-12-10 <code>docs/cli-reference/srt-translate.md</code> tnh-fab User-facing reference for the <code>tnh-fab</code> CLI covering commands, options, and example workflows. 2025-01-19 <code>docs/cli-reference/tnh-fab.md</code> tnh-setup The <code>tnh-setup</code> command configures the TNH Scholar environment, setting up necessary directories and downloading default patterns. 2025-02-01 <code>docs/cli-reference/tnh-setup.md</code> token-count The <code>token-count</code> command calculates the OpenAI API token count for text input. This is useful for ensuring that a text is within maximum token limits for the API model and also for estimating API costs. 2025-02-01 <code>docs/cli-reference/token-count.md</code> ytt-fetch (Y)ou(T)ube (T)ranscript-(Fetch)ing utility. 2025-01-21 <code>docs/cli-reference/ytt-fetch.md</code>"},{"location":"documentation_index/#api","title":"API","text":"Title Description Created Path API Reference ::: tnh_scholar 2025-01-19 <code>docs/api/index.md</code>"},{"location":"documentation_index/#architecture","title":"Architecture","text":"Title Description Created Path Adr Table of contents for architecture/video-processing/adr 2025-12-08 <code>docs/architecture/video-processing/adr/index.md</code> Adr Table of contents for architecture/tnh-gen/adr 2025-12-08 <code>docs/architecture/tnh-gen/adr/index.md</code> Adr Table of contents for architecture/transcription/adr 2025-12-08 <code>docs/architecture/transcription/adr/index.md</code> Adr Table of contents for architecture/knowledge-base/adr 2025-12-08 <code>docs/architecture/knowledge-base/adr/index.md</code> Adr Table of contents for architecture/docs-system/adr 2025-12-08 <code>docs/architecture/docs-system/adr/index.md</code> Adr Table of contents for architecture/project-policies/adr 2025-12-08 <code>docs/architecture/project-policies/adr/index.md</code> Adr Table of contents for architecture/ai-text-processing/adr 2025-12-08 <code>docs/architecture/ai-text-processing/adr/index.md</code> Adr Table of contents for architecture/metadata/adr 2025-12-08 <code>docs/architecture/metadata/adr/index.md</code> Adr Table of contents for architecture/ytt-fetch/adr 2025-12-08 <code>docs/architecture/ytt-fetch/adr/index.md</code> Adr Table of contents for architecture/object-service/adr 2025-12-08 <code>docs/architecture/object-service/adr/index.md</code> Adr Table of contents for architecture/prompt-system/adr 2025-12-08 <code>docs/architecture/prompt-system/adr/index.md</code> Adr Table of contents for architecture/prompt-system/archive/adr 2025-12-08 <code>docs/architecture/prompt-system/archive/adr/index.md</code> Adr Table of contents for architecture/gen-ai-service/adr 2025-12-08 <code>docs/architecture/gen-ai-service/adr/index.md</code> ADR-A01: Adopt Object-Service for GenAI Interactions Standardizes GenAI interactions with an Object-Service pattern that separates domain shapes from provider orchestration. 2025-11-15 <code>docs/architecture/gen-ai-service/adr/adr-a01-object-service-genai.md</code> ADR-A02: PatternCatalog Integration (V1) Describes the V1 contract for plugging the legacy PatternCatalog into GenAI Service via rendered system prompts. 2025-11-15 <code>docs/architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1.md</code> ADR-A08: Configuration / Parameters / Policy Taxonomy Establishes the Config/Params/Policy taxonomy for GenAI Service to prevent parameter soup and clarify ownership. 2025-11-15 <code>docs/architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy.md</code> ADR-A09: V1 Simplified Implementation Pathway Defines the minimum viable GenAI Service implementation that preserves architectural seams while shipping quickly. 2025-11-15 <code>docs/architecture/gen-ai-service/adr/adr-a09-v1-simplified.md</code> ADR-A11: Model Parameters and Strong Typing Fix Enforces typed parameter objects and removes literals from GenAI Service so provider flows stay consistent. 2025-11-15 <code>docs/architecture/gen-ai-service/adr/adr-a11-model-parameters-fix.md</code> ADR-A12: Prompt System &amp; Fingerprinting Architecture (V1) Replaces the Pattern Catalog adapter with a Prompt-first design that yields domain objects plus fingerprints. 2025-02-04 <code>docs/architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1.md</code> ADR-A13: Migrate All OpenAI Interactions to GenAIService Retires the legacy OpenAI client and standardizes every caller on the typed GenAI Service pipeline. 2025-11-17 <code>docs/architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice.md</code> ADR-A14: File-Based Registry System for Provider Metadata Establishes a JSONC-based registry system for model capabilities, pricing, and provider metadata with auto-update mechanisms, aligned with VS Code's native configuration format. 2025-12-10 <code>docs/architecture/gen-ai-service/adr/adr-a14-file-based-registry-system.md</code> ADR-AT01: AI Text Processing Pipeline Redesign Defines the modular TextObject pipeline, metadata handling, and configuration strategy for AI processing. 2025-02-26 <code>docs/architecture/ai-text-processing/adr/adr-at01-ai-text-processing.md</code> ADR-AT02: TextObject Architecture Decision Records Captures the historical TextObject design comparisons and links to the original/new design documents. 2025-02-01 <code>docs/architecture/ai-text-processing/adr/adr-at02-sectioning-textobject.md</code> ADR-AT03: AI Text Processing Object-Service Refactor Three-tier refactor of ai_text_processing module for object-service compliance, GenAIService integration, and prompt system adoption 2025-12-07 <code>docs/architecture/ai-text-processing/adr/adr-at03-object-service-refactor.md</code> ADR-DD01: Documentation System Reorganization Strategy Rebuilds the documentation architecture with new directories, automation, and Prompt terminology. 2024-11-09 <code>docs/architecture/docs-system/adr/adr-dd01-docs-reorg-strategy.md</code> ADR-DD02: Documentation Main Content and Navigation Strategy Defines content architecture, sync mechanisms, and navigation patterns for README.md, docs/index.md, and filesystem-driven documentation. 2025-11-23 <code>docs/architecture/docs-system/adr/adr-dd02-main-content-nav.md</code> adr-dd03-phase1-punchlist <code>docs/architecture/docs-system/adr/adr-dd03-phase1-punchlist.md</code> ADR-DD03: Pattern to Prompt Terminology Standardization Standardizes documentation terminology from 'Pattern' to 'Prompt' to align with industry conventions and gen-ai-service refactoring. 2025-11-28 <code>docs/architecture/docs-system/adr/adr-dd03-pattern-to-prompt.md</code> ADR-K01: Preliminary Architectural Strategy for TNH Scholar Knowledge Base Proposes a phased managed-to-open-source knowledge base rollout to validate retrieval before scaling. 2025-11-15 <code>docs/architecture/knowledge-base/adr/adr-k01-kb-architecture-strategy.md</code> ADR-MD01: Adoption of JSON-LD for Metadata Management Chooses JSON-LD as the canonical metadata format to capture provenance, relationships, and future semantic queries. 2025-02-01 <code>docs/architecture/metadata/adr/adr-md01-json-ld-metadata.md</code> ADR-MD02: Metadata Infrastructure Object-Service Integration Defines metadata system's role as foundational infrastructure in the object-service architecture, establishing patterns for cross-layer usage and ensuring compliance with design principles. 2025-12-07 <code>docs/architecture/metadata/adr/adr-md02-metadata-object-service-integration.md</code> ADR-OS01: Object-Service Design Architecture V3 Establishes layered architecture for complex objects and API-backed services across TNH Scholar, defining clear boundaries between domain, service, and transport layers. 2025-10-24 <code>docs/architecture/object-service/adr/adr-os01-object-service-architecture-v3.md</code> ADR-PP01: Rapid Prototype Versioning Policy Establishes versioning policy for TNH Scholar during 0.x releases, allowing breaking changes in any release to enable fast iteration and architectural improvements. 2025-12-06 <code>docs/architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning.md</code> ADR-PT01: Pattern Access Strategy Introduces a two-phase plan for accessing PromptTemplate metadata, moving from a singleton to injected managers. 2025-02-26 <code>docs/architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy.md</code> ADR-PT02: Adopt Pattern and PatternCatalog as Core Concepts Keeps the Pattern terminology while clarifying how PatternCatalog functions in TNH Scholar. 2025-09-22 <code>docs/architecture/prompt-system/archive/adr/adr-pt02-adopt-pattern-and-patterncatalog.md</code> ADR-PT03: Prompt System Current Status &amp; Roadmap Current as-built status of the TNH Scholar prompt system, documentation terminology standardization, and planned enhancements. 2025-11-29 <code>docs/architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap.md</code> ADR-PT04: Prompt System Refactor Plan Refactors the legacy pattern-based prompt system into a modular, object-service compliant PromptCatalog with validation, transport isolation, and clean dependency injection seams. 2025-12-05 <code>docs/architecture/prompt-system/adr/adr-pt04-prompt-system-refactor.md</code> ADR-TG01: tnh-gen CLI Architecture Core command structure, error handling, and configuration for the unified TNH Scholar CLI tool 2025-12-07 <code>docs/architecture/tnh-gen/adr/adr-tg01-cli-architecture.md</code> ADR-TG02: TNH-Gen CLI Prompt System Integration Integration pattern for tnh-gen CLI with prompt system via PromptsAdapter 2025-12-07 <code>docs/architecture/tnh-gen/adr/adr-tg02-prompt-integration.md</code> ADR-TR01: AssemblyAI Integration for Transcription Service Introduces a pluggable transcription interface with AssemblyAI and Whisper providers. 2025-05-01 <code>docs/architecture/transcription/adr/adr-tr01-assemblyai-integration.md</code> ADR-TR02: Optimized SRT Generation Design Uses provider-native SRT generation to simplify the transcription pipeline. 2025-05-01 <code>docs/architecture/transcription/adr/adr-tr02-optimized-srt-design.md</code> ADR-TR03: Standardizing Timestamps to Milliseconds Aligns all transcription providers on millisecond timestamps to avoid float drift. 2025-05-01 <code>docs/architecture/transcription/adr/adr-tr03-ms-timestamps.md</code> ADR-TR04: AssemblyAI Service Implementation Improvements Refactors the AssemblyAI adapter to use the official SDK, richer options, and better error handling. 2025-05-01 <code>docs/architecture/transcription/adr/adr-tr04-assemblyai-improvements.md</code> ADR-VP01: Video Processing Return Types and Configuration Centralizes yt-dlp configuration and return types so video tooling emits consistent metadata. 2025-02-01 <code>docs/architecture/video-processing/adr/adr-vp01-video-processing.md</code> adr-vsc01-vscode-integration-strategy <code>docs/architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy.md</code> ADR-VSC02: VS Code Extension Integration with tnh-gen CLI VS Code extension strategy for consuming tnh-gen CLI and providing GenAI text processing UI 2025-01-28 <code>docs/architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation.md</code> ADR-YF00: Early yt-fetch Transcript Decisions (Historical) Consolidates the original transcript ADR notes for yt-fetch before they were split into discrete records. 2025-01-17 <code>docs/architecture/ytt-fetch/adr/adr-yf00-early-decisions.md</code> ADR-YF01: YouTube Transcript Source Handling Documents how yt-fetch prioritizes manual subtitles while accepting auto captions for maximum coverage. 2025-01-21 <code>docs/architecture/ytt-fetch/adr/adr-yf01-yt-transcript-source-handling.md</code> ADR-YF02: YouTube Transcript Format Selection Locks yt-fetch to a single transcript format (initially VTT) for predictable downstream processing. 2025-01-21 <code>docs/architecture/ytt-fetch/adr/adr-yf02-yt-transcript-format-selection.md</code> Ai Text Processing Table of contents for architecture/ai-text-processing 2025-12-08 <code>docs/architecture/ai-text-processing/index.md</code> Architecture Table of contents for architecture 2025-12-08 <code>docs/architecture/index.md</code> Architecture Overview High-level conceptual view of TNH Scholar's layered, object-service oriented architecture with links to detailed design documents and ADRs. 2025-12-02 <code>docs/architecture/overview.md</code> Archive Table of contents for architecture/tnh-gen/design/archive 2025-12-08 <code>docs/architecture/tnh-gen/design/archive/index.md</code> Archive Table of contents for architecture/prompt-system/archive 2025-12-08 <code>docs/architecture/prompt-system/archive/index.md</code> Audio Chunking Algorithm Design Document Design for splitting diarization segments into five-minute audio chunks using greedy accumulation and speaker-aware boundaries. 2025-05-01 <code>docs/architecture/transcription/design/audio-chunking-design.md</code> Configuration Table of contents for architecture/configuration 2025-12-08 <code>docs/architecture/configuration/index.md</code> Core Pattern Architecture: Meta-patterns, Textual Expansion Processing ```mermaid 2025-02-26 <code>docs/architecture/prompt-system/archive/core-pattern-architecture.md</code> Design Table of contents for architecture/transcription/design 2025-12-08 <code>docs/architecture/transcription/design/index.md</code> Design Table of contents for architecture/docs-system/design 2025-12-08 <code>docs/architecture/docs-system/design/index.md</code> Design Table of contents for architecture/ai-text-processing/design 2025-12-08 <code>docs/architecture/ai-text-processing/design/index.md</code> Design Table of contents for architecture/utilities/design 2025-12-08 <code>docs/architecture/utilities/design/index.md</code> Design Table of contents for architecture/ui-ux/design 2025-12-08 <code>docs/architecture/ui-ux/design/index.md</code> Design Table of contents for architecture/ytt-fetch/design 2025-12-08 <code>docs/architecture/ytt-fetch/design/index.md</code> Design Table of contents for architecture/setup-tnh/design 2025-12-08 <code>docs/architecture/setup-tnh/design/index.md</code> Design Table of contents for architecture/gen-ai-service/design 2025-12-08 <code>docs/architecture/gen-ai-service/design/index.md</code> Design Table of contents for architecture/jvb-viewer/design 2025-12-08 <code>docs/architecture/jvb-viewer/design/index.md</code> Design Strategy: VS Code as UI/UX Platform for TNH Scholar --- 2025-11-15 <code>docs/architecture/ui-ux/design/vs-code-as-ui-platform.md</code> Diarization Algorithms This document details the key algorithms in the diarization system, focusing on high-level design without implementation details. Each algorithm is presented with its inputs, outputs, and process flow. 2025-05-01 <code>docs/architecture/transcription/design/diarization-algorithms.md</code> Diarization Chunker Module Design Strategy I've analyzed the current system and PoC code to propose a modular, extensible design for integrating the diarization chunking functionality into your tnh-scholar project. 2025-05-05 <code>docs/architecture/transcription/design/diarization-chunker-design.md</code> Diarization System Design Detailed architecture for the diarization pipeline, covering segmentation, track extraction, and transcript remapping. 2025-05-01 <code>docs/architecture/transcription/design/diarization-system-design.md</code> Docs System Table of contents for architecture/docs-system 2025-12-08 <code>docs/architecture/docs-system/index.md</code> Documentation Design Reference for the documentation stack, covering tooling choices, information architecture, and publishing workflow. 2025-01-19 <code>docs/architecture/docs-system/design/documentation-design.md</code> Gen Ai Service Table of contents for architecture/gen-ai-service 2025-12-08 <code>docs/architecture/gen-ai-service/index.md</code> GenAI Service \u2014 Design Strategy Strategy for unifying GenAI Service capabilities, personas, and phased releases. 2025-11-15 <code>docs/architecture/gen-ai-service/design/genai-service-design-strategy.md</code> Generate Markdown Translation JSON Pairs Instructions for producing paired Vietnamese-English Markdown JSON lines from scanned journal pages with strict formatting. 2025-11-15 <code>docs/architecture/jvb-viewer/design/generate-md-translation-json-pairs.md</code> Generate Markdown Vietnamese Guidelines for rewriting Vietnamese journal pages into structured Markdown with one sentence per line and preserved metadata. 2025-11-15 <code>docs/architecture/jvb-viewer/design/generate-md-vietnamese.md</code> Interval-to-Segment Mapping Algorithm Algorithm for mapping chunk-relative transcription intervals back to diarization segments using overlap and proximity. 2025-05-08 <code>docs/architecture/transcription/design/interval-to-segment-mapping.md</code> Jvb Viewer Table of contents for architecture/jvb-viewer 2025-12-08 <code>docs/architecture/jvb-viewer/index.md</code> JVB Viewer \u2014 Version 2 Strategy &amp; High\u2011Level Design Strategy for a projection-first VS Code-based viewer/editor that reconciles OCR outputs into a canonical JSON artifact. 2025-11-15 <code>docs/architecture/jvb-viewer/design/jvb-viewer-v2-strategy.md</code> Knowledge Base Table of contents for architecture/knowledge-base 2025-12-08 <code>docs/architecture/knowledge-base/index.md</code> Language-Aware Chunking Orchestrator Notes Working notes for extending the DiarizationChunker orchestrator with language-aware strategies. 2025-06-24 <code>docs/architecture/transcription/design/language-aware-chunking-orchestrator-notes.md</code> LU\u00c2N-H\u1ed2I Transcribed Markdown sample of the 'Lu\u00e2n-H\u1ed3i M\u1ed9t Th\u1ef1c-Th\u1ec3' article used to validate viewer output. 2025-11-15 <code>docs/architecture/jvb-viewer/design/luan-hoi.md</code> Metadata Table of contents for architecture/metadata 2025-12-08 <code>docs/architecture/metadata/index.md</code> minimal but extensible setup tool for the prototyping phase Core Requirements: 2025-01-21 <code>docs/architecture/setup-tnh/design/setup-tnh-minimal-extensible-tool.md</code> Modular Pipeline Design: Best Practices for Audio Transcription and Diarization This document summarizes a detailed design and refactoring discussion on building a clean, modular, and production-ready audio transcription pipeline, with a focus on diarization chunking and robust system structure. It includes architectural patterns, file organization, and code hygiene practices. 2025-06-10 <code>docs/architecture/transcription/design/modular-pipeline-best-practices.md</code> Object Service Table of contents for architecture/object-service 2025-12-08 <code>docs/architecture/object-service/index.md</code> Object-Service Design Gaps Gaps, resolved items, and outstanding work needed to fully satisfy the Object-Service design blueprint. 2025-10-24 <code>docs/architecture/object-service/object-service-design-gaps.md</code> Object-Service Design Overview High-level overview of TNH Scholar's layered architecture for complex objects and API-backed services. 2025-11-29 <code>docs/architecture/object-service/object-service-design-overview.md</code> Object-Service Implementation Status Implementation status, resolved gaps, and outstanding work for the Object-Service design architecture. 2025-10-24 <code>docs/architecture/object-service/object-service-implementation-status.md</code> OpenAI Interface Migration Plan Step-by-step plan for migrating from the legacy <code>openai_interface</code> module to the typed GenAI Service. 2025-11-17 <code>docs/architecture/gen-ai-service/design/openai-interface-migration-plan.md</code> Package Version Checker Design Document Specification for a reusable package version checking utility with flexible strategies and clear reporting. 2025-06-10 <code>docs/architecture/utilities/design/package-version-checker-design.md</code> Practical Language-Aware Chunking Design Practical heuristics for detecting language changes during chunking when diarization output is noisy. 2025-06-24 <code>docs/architecture/transcription/design/practical-language-aware-chunking.md</code> Project Policies Cross-cutting architectural policies and decisions affecting the entire TNH Scholar codebase 2025-12-06 <code>docs/architecture/project-policies/index.md</code> Prompt System Table of contents for architecture/prompt-system 2025-12-08 <code>docs/architecture/prompt-system/index.md</code> Prompt System Architecture Current and planned architecture for the TNH Scholar prompt system, including VS Code integration and PromptCatalog service design. 2025-11-29 <code>docs/architecture/prompt-system/prompt-system-architecture.md</code> Setup Tnh Table of contents for architecture/setup-tnh 2025-12-08 <code>docs/architecture/setup-tnh/index.md</code> Simplified Language-Aware Chunking Design Language-aware chunking strategy that augments diarization splits with practical language detection heuristics. 2025-06-24 <code>docs/architecture/transcription/design/language-aware-chunking-design.md</code> Speaker Diarization Algorithm Design This document details the key algorithms referenced in the main diarization system design. Each algorithm is presented with a clear breakdown of its inputs, outputs, and processing steps. 2025-05-01 <code>docs/architecture/transcription/design/speaker-diarization-algorithm-design.md</code> Speaker Diarization and Time-Mapped Transcription System Design System design for mapping diarization outputs to speaker-specific transcriptions with accurate global timelines. 2025-05-01 <code>docs/architecture/transcription/design/speaker-diarization-time-mapped-design.md</code> TextObject Original Design Legacy TextObject design notes capturing the original sectioning models, metadata strategy, and validation approach. 2025-02-01 <code>docs/architecture/ai-text-processing/design/textobject-original-design.md</code> TextObject System Design Document Detailed blueprint for the modern TextObject pipeline, outlining segmentation models, metadata, and API surfaces. 2025-02-01 <code>docs/architecture/ai-text-processing/design/textobject-system-design.md</code> TimelineMapper Design Document Design for the TimelineMapper component that reprojects chunk-level transcripts into the original audio timeline. 2025-05-08 <code>docs/architecture/transcription/design/timelinemapper-design.md</code> TNH Configuration Management Architecture decisions and a phased plan for consolidating TNH Scholar configuration across modules, CLIs, and environments. 2025-02-01 <code>docs/architecture/configuration/tnh-configuration-management.md</code> TNH FAB Design Document First-generation design of the <code>tnh-fab</code> CLI covering core commands, usage patterns, and processing goals. 2025-01-21 <code>docs/architecture/tnh-gen/design/archive/tnh-fab-design-document.md</code> TNH-FAB Command Line Tool Specification Expanded specification for the <code>tnh-fab</code> CLI with refined command behaviors, options, and workflow integration. 2025-01-21 <code>docs/architecture/tnh-gen/design/archive/tnh-fab-cli-spec.md</code> TNH-Gen CLI Unified command-line interface for TNH Scholar GenAI operations 2025-12-07 <code>docs/architecture/tnh-gen/index.md</code> TNH\u2011Scholar Utilities Catalog This catalog lists core utility modules used across the TNH\u2011Scholar codebase. It provides a quick reference for shared abstractions, their purpose, API highlights, and stability. All modules are currently in Prototype phase, with most stable in active use. 2025-11-15 <code>docs/architecture/utilities/design/utilities-catalog.md</code> Transcription Table of contents for architecture/transcription 2025-12-08 <code>docs/architecture/transcription/index.md</code> Ui Ux Table of contents for architecture/ui-ux 2025-12-08 <code>docs/architecture/ui-ux/index.md</code> Utilities Table of contents for architecture/utilities 2025-12-08 <code>docs/architecture/utilities/index.md</code> Versioning Policy Documentation Additions Summary of documentation updates to clarify rapid prototype versioning policy 2025-12-06 <code>docs/architecture/project-policies/versioning-policy-implementation-summary.md</code> Video Processing Table of contents for architecture/video-processing 2025-12-08 <code>docs/architecture/video-processing/index.md</code> Vs Code Integration Table of contents for architecture/ui-ux/vs-code-integration 2025-12-08 <code>docs/architecture/ui-ux/vs-code-integration/index.md</code> YouTube API vs yt-dlp Evaluation Comparison of using the YouTube Data API versus yt-dlp for fetching Plum Village media assets. 2025-01-21 <code>docs/architecture/ytt-fetch/design/youtube-api-vs-yt-dlp-eval.md</code> Ytt Fetch Table of contents for architecture/ytt-fetch 2025-12-08 <code>docs/architecture/ytt-fetch/index.md</code>"},{"location":"documentation_index/#development","title":"Development","text":"Title Description Created Path 2025 12 07 Reference Table of contents for development/incident-reports/2025-12-07-reference 2025-12-08 <code>docs/development/incident-reports/2025-12-07-reference/index.md</code> Contributing to TNH Scholar (Prototype Phase) TNH Scholar is currently in rapid prototype phase, focusing on core functionality and basic usability. We welcome contributions that help validate and improve the prototype implementation. 2025-01-19 <code>docs/development/contributing-prototype-phase.md</code> Development Table of contents for development 2025-12-08 <code>docs/development/index.md</code> Development Documentation Landing page for contributor guides, design principles, and engineering practices for TNH Scholar. 2025-12-03 <code>docs/development/overview.md</code> Fine Tuning Strategy Strategy outline and development plan for fine-tuning foundation models on Thich Nhat Hanh translations. 2025-02-26 <code>docs/development/fine-tuning-strategy.md</code> forensic-analysis <code>docs/development/incident-reports/2025-12-07-reference/forensic-analysis.md</code> Git Workflow &amp; Safety Guide Safe git practices for TNH Scholar development to prevent data loss 2025-12-07 <code>docs/development/git-workflow.md</code> Human-AI Software Engineering Principles This document presents the Human-AI Software Engineering Principles, a comprehensive framework that builds upon established software engineering, architecture, and design principles from human-only teams and extends them to optimize collaboration between humans and AI agents. Central to this framework is the clear distinction between the Design Phase and the Coding Phase, each with distinct goals, modes, and workflows. It also addresses challenges such as context window limitations and maintaining alignment despite session resets. In addition to general principles, this framework incorporates concrete documentation and planning strategies designed to support long-term, sustainable human-AI collaboration. 2025-11-15 <code>docs/development/human-ai-software-engineering-principles.md</code> Implementation Summary: Git Safety Improvements Summary of remediation work completed after the 2025-12-07 git recovery incident. 2025-12-08 <code>docs/development/incident-reports/2025-12-07-reference/implementation-summary.md</code> Improvements / Initial structure Initial high-level view of the TNH Scholar ecosystem. 2025-02-01 <code>docs/development/improvements-initial-structure.md</code> Incident Report: Git Recovery - December 7, 2025 Post-mortem analysis of orphaned commits and successful recovery of prompt system implementation (ADR-PT04) <code>docs/development/incident-reports/2025-12-07-git-recovery.md</code> Incident Reports Table of contents for development/incident-reports 2025-12-08 <code>docs/development/incident-reports/index.md</code> incident-report-updates <code>docs/development/incident-reports/2025-12-07-reference/incident-report-updates.md</code> Release Workflow Automated release process for TNH Scholar with biweekly cadence during rapid prototyping. 2025-12-06 <code>docs/development/release-workflow.md</code> TNH Scholar Design Principles Architectural patterns, design philosophy, and system organization principles for TNH Scholar development. 2025-11-29 <code>docs/development/design-principles.md</code> TNH Scholar Style Guide Code formatting, naming conventions, and Python standards for TNH Scholar development. 2025-11-29 <code>docs/development/style-guide.md</code> TNH Scholar System Design High-level system design describing the cyclical AI processing architecture powering TNH Scholar. 2025-02-01 <code>docs/development/system-design.md</code>"},{"location":"documentation_index/#docs-ops","title":"Docs Ops","text":"Title Description Created Path ADR Template Reusable template for TNH Scholar architecture decision records. 2025-02-27 <code>docs/docs-ops/adr-template.md</code> Docs Ops Table of contents for docs-ops 2025-12-08 <code>docs/docs-ops/index.md</code> Markdown Standards House style, linting, and structure requirements for TNH Scholar documentation. 2025-02-27 <code>docs/docs-ops/markdown-standards.md</code> MkDocs Strict Warning Backlog Checklist to drive MkDocs builds to zero warnings in strict mode. 2025-11-25 <code>docs/docs-ops/mkdocs-strict-warning-backlog.md</code> Preview TNH Scholar Theme Quick guide to previewing the custom zen theme locally 2025-12-02 <code>docs/docs-ops/preview-theme.md</code> TNH Scholar Theme Design Zen-inspired documentation theme blending mindfulness aesthetics with modern AI tooling 2025-12-02 <code>docs/docs-ops/theme-design.md</code>"},{"location":"documentation_index/#research","title":"Research","text":"Title Description Created Path 1-3 Word Queries Prompt experiments for generating search query and passage pairs used to train retrieval models. 2024-10-28 <code>docs/research/gpt4o-search-query-testing/queries-1-3-words.md</code> GPT Development Convos Link log of early GPT design, feasibility, and data processing conversations for the project. 2024-10-23 <code>docs/research/gpt_development_convos.md</code> Gpt4O Search Query Testing Table of contents for research/gpt4o-search-query-testing 2025-12-08 <code>docs/research/gpt4o-search-query-testing/index.md</code> Gpt4O Translation Experiments Table of contents for research/gpt4o-translation-experiments 2025-12-08 <code>docs/research/gpt4o-translation-experiments/index.md</code> Passage Test Spot-checks comparing GPT-4o translation outputs with bilingual baselines on short passages. 2024-11-08 <code>docs/research/gpt4o-translation-experiments/passage_test.md</code> Preliminary Feasibility Study Feasibility study exploring an interactive translation, search, and conversation system built on Thich Nhat Hanh\u2019s teachings. 2024-10-21 <code>docs/research/preliminary-feasibility-study.md</code> RAG Research Directions for TNH Scholar Exploratory roadmap for retrieval-augmented generation (RAG) within the TNH Scholar project, with emphasis on multilingual Buddhist corpora and Plum Village practice contexts. 2025-12-05 <code>docs/research/rag-research-directions.md</code> Research Entry points to research notes, experiments, and feasibility studies. <code>docs/research/index.md</code> Structural-Informed Adaptive Processing (SIAP) Methodology Methodology for structure-aware adaptive processing that selects AI strategies based on content fingerprints. 2025-06-24 <code>docs/research/siap-methodology.md</code> Summary Report on Metadata Extraction, Source Parsing, and Model Training for TNH-Scholar Summary of metadata extraction lessons, tooling, and training implications across Thich Nhat Hanh sources. 2024-10-24 <code>docs/research/metadata-summary-report.md</code> TNH Scholar Knowledge Base: Design Document Design document for the TNH Scholar knowledge base and semantic search stack. 2025-06-10 <code>docs/research/kb-design-document.md</code>"},{"location":"documentation_index/#root","title":"Root","text":"Title Description Created Path TNH Scholar Comprehensive documentation for TNH Scholar, an AI-driven project exploring, processing, and translating the teachings of Thich Nhat Hanh. <code>docs/index.md</code>"},{"location":"documentation_index/#archive","title":"Archive","text":"Title Description Created Path Archive Table of contents for archive 2025-12-08 <code>docs/archive/index.md</code>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#tnh_scholar","title":"<code>tnh_scholar</code>","text":"<p>TNH Scholar: Text Processing and Analysis Tools</p> <p>TNH Scholar is an AI-driven project designed to explore, query, process and translate the teachings of Thich Nhat Hanh and other Plum Village Dharma Teachers. The project aims to create a resource for practitioners and scholars to deeply engage with mindfulness and spiritual wisdom through natural language processing and machine learning models.</p> Core Features <ul> <li>Audio transcription and processing</li> <li>Multi-lingual text processing and translation</li> <li>Pattern-based text analysis</li> <li>OCR processing for historical documents</li> <li>CLI tools for batch processing</li> </ul> Package Structure <ul> <li>tnh_scholar/</li> <li>CLI_tools/          - Command line interface tools</li> <li>audio_processing/   - Audio file handling and transcription</li> <li>journal_processing/ - Journal and publication processing</li> <li>ocr_processing/     - Optical character recognition tools</li> <li>text_processing/    - Core text processing utilities</li> <li>video_processing/   - Video file handling and transcription</li> <li>utils/             - Shared utility functions</li> <li>xml_processing/    - XML parsing and generation</li> </ul> Environment Configuration <ul> <li>The package uses environment variables for configuration, including:</li> <li>TNH_PATTERN_DIR - Directory for text processing patterns</li> <li>OPENAI_API_KEY     - OpenAI API authentication</li> <li>GOOGLE_VISION_KEY  - Google Cloud Vision API key for OCR</li> </ul> CLI Tools <ul> <li>audio-transcribe  - Audio file transcription utility</li> <li>tnh-fab          - Text processing and analysis toolkit</li> </ul> <p>For more information, see:     - Documentation: https://aaronksolomon.github.io/tnh-scholar/     - Source: https://github.com/aaronksolomon/tnh-scholar     - Issues: https://github.com/aaronksolomon/tnh-scholar/issues</p> Dependencies <ul> <li>Core: click, pydantic, openai, yt-dlp</li> <li>Optional: streamlit (GUI), spacy (NLP), google-cloud-vision (OCR)</li> </ul>"},{"location":"api/#tnh_scholar.TNH_CLI_TOOLS_DIR","title":"<code>TNH_CLI_TOOLS_DIR = TNH_ROOT_SRC_DIR / 'cli_tools'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_CONFIG_DIR","title":"<code>TNH_CONFIG_DIR = Path.home() / '.config' / 'tnh-scholar'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_DEFAULT_PATTERN_DIR","title":"<code>TNH_DEFAULT_PATTERN_DIR = TNH_PROJECT_ROOT_DIR / 'patterns'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_LOG_DIR","title":"<code>TNH_LOG_DIR = TNH_CONFIG_DIR / 'logs'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_METADATA_PROCESS_FIELD","title":"<code>TNH_METADATA_PROCESS_FIELD = 'tnh_processing'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_PROJECT_ROOT_DIR","title":"<code>TNH_PROJECT_ROOT_DIR = TNH_ROOT_SRC_DIR.resolve().parent.parent</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.TNH_ROOT_SRC_DIR","title":"<code>TNH_ROOT_SRC_DIR = Path(__file__).resolve().parent</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.__version__","title":"<code>__version__ = '0.1.3'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing","title":"<code>ai_text_processing</code>","text":"<p>Public surface for <code>tnh_scholar.ai_text_processing</code>.</p> <p>Historically this module eagerly imported multiple submodules with heavy dependencies (e.g., audio codecs, ML toolkits) which made importing lightweight components such as <code>Prompt</code> surprisingly expensive and brittle in test environments.  We now lazily import the concrete implementations on demand so that callers can depend on just the pieces they need.</p>"},{"location":"api/#tnh_scholar.ai_text_processing.__all__","title":"<code>__all__ = ['OpenAIProcessor', 'SectionParser', 'SectionProcessor', 'find_sections', 'process_text', 'process_text_by_paragraphs', 'process_text_by_sections', 'get_pattern', 'translate_text_by_lines', 'openai_process_text', 'GitBackedRepository', 'LocalPromptManager', 'Prompt', 'PromptCatalog', 'AIResponse', 'LogicalSection', 'SectionEntry', 'TextObject', 'TextObjectInfo']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.AIResponse","title":"<code>AIResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for dividing large texts into AI-processable segments while maintaining broader document context.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class AIResponse(BaseModel):\n    \"\"\"Class for dividing large texts into AI-processable segments while\n    maintaining broader document context.\"\"\"\n    document_summary: str = Field(\n        ...,\n        description=\"Concise, comprehensive overview of the text's content and purpose\"\n    )\n    document_metadata: str = Field(\n        ...,\n        description=\"Available Dublin Core standard metadata in human-readable YAML format\" # noqa: E501\n    )\n    key_concepts: str = Field(\n        ...,\n        description=\"Important terms, ideas, or references that appear throughout the text\"  # noqa: E501\n    )\n    narrative_context: str = Field(\n        ...,\n        description=\"Concise overview of how the text develops or progresses as a whole\"\n    )\n    language: str = Field(..., description=\"ISO 639-1 language code\")\n    sections: List[LogicalSection]\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.AIResponse.document_metadata","title":"<code>document_metadata = Field(..., description='Available Dublin Core standard metadata in human-readable YAML format')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.AIResponse.document_summary","title":"<code>document_summary = Field(..., description=\"Concise, comprehensive overview of the text's content and purpose\")</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.AIResponse.key_concepts","title":"<code>key_concepts = Field(..., description='Important terms, ideas, or references that appear throughout the text')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.AIResponse.language","title":"<code>language = Field(..., description='ISO 639-1 language code')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.AIResponse.narrative_context","title":"<code>narrative_context = Field(..., description='Concise overview of how the text develops or progresses as a whole')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.AIResponse.sections","title":"<code>sections</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.GitBackedRepository","title":"<code>GitBackedRepository</code>","text":"<p>Manages versioned storage of prompts using Git.</p> <p>Provides basic Git operations while hiding complexity: - Automatic versioning of changes - Basic conflict resolution - History tracking</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>class GitBackedRepository:\n    \"\"\"\n    Manages versioned storage of prompts using Git.\n\n    Provides basic Git operations while hiding complexity:\n    - Automatic versioning of changes\n    - Basic conflict resolution\n    - History tracking\n    \"\"\"\n\n    def __init__(self, repo_path: Path):\n        \"\"\"\n        Initialize or connect to Git repository.\n\n        Args:\n            repo_path: Path to repository directory\n\n        Raises:\n            GitCommandError: If Git operations fail\n        \"\"\"\n        self.repo_path = repo_path\n\n        try:\n            # Try to connect to existing repository\n            self.repo = Repo(repo_path)\n            logger.debug(f\"Connected to existing Git repository at {repo_path}\")\n\n        except InvalidGitRepositoryError:\n            # Initialize new repository if none exists\n            logger.info(f\"Initializing new Git repository at {repo_path}\")\n            self.repo = Repo.init(repo_path)\n\n            # Create initial commit if repo is empty\n            if not self.repo.head.is_valid():\n                # Create and commit .gitignore\n                gitignore = repo_path / \".gitignore\"\n                gitignore.write_text(\"*.lock\\n.DS_Store\\n\")\n                self.repo.index.add([\".gitignore\"])\n                self.repo.index.commit(\"Initial repository setup\")\n\n    def update_file(self, file_path: Path) -&gt; str:\n        \"\"\"\n        Stage and commit changes to a file in the Git repository.\n\n        Args:\n            file_path: Absolute or relative path to the file.\n\n        Returns:\n            str: Commit hash if changes were made.\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n            ValueError: If the file is outside the repository.\n            GitCommandError: If Git operations fail.\n        \"\"\"\n        file_path = file_path.resolve()\n\n        # Ensure the file is within the repository\n        try:\n            rel_path = file_path.relative_to(self.repo_path)\n        except ValueError as e:\n            raise ValueError(\n                f\"File {file_path} is not under the repository root {self.repo_path}\"\n            ) from e\n\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File does not exist: {file_path}\")\n\n        try:\n            return self._commit_file_update(rel_path, file_path)\n        except GitCommandError as e:\n            logger.error(f\"Git operation failed: {e}\")\n            raise\n\n    def _commit_file_update(self, rel_path, file_path):\n        if self._is_file_clean(rel_path):\n            # Return the current commit hash if no changes\n            return self.repo.head.commit.hexsha\n\n        logger.info(f\"Detected changes in {rel_path}, updating version control.\")\n        self.repo.index.add([str(rel_path)])\n        commit = self.repo.index.commit(\n            f\"{MANAGER_UPDATE_MESSAGE} {rel_path.stem}\",\n            author=Actor(\"PromptManager\", \"\"),\n        )\n        logger.info(f\"Committed changes to {file_path}: {commit.hexsha}\")\n        return commit.hexsha\n\n    def _get_file_revisions(self, file_path: Path) -&gt; List[Commit]:\n        \"\"\"\n        Get ordered list of commits that modified a file, most recent first.\n\n        Args:\n            file_path: Path to file relative to repository root\n\n        Returns:\n            List of Commit objects affecting this file\n\n        Raises:\n            GitCommandError: If Git operations fail\n        \"\"\"\n        rel_path = file_path.relative_to(self.repo_path)\n        try:\n            return list(self.repo.iter_commits(paths=str(rel_path)))\n        except GitCommandError as e:\n            logger.error(f\"Failed to get commits for {rel_path}: {e}\")\n            return []\n\n    def _get_commit_diff(\n        self, commit: Commit, file_path: Path, prev_commit: Optional[Commit] = None\n    ) -&gt; Tuple[str, str]:\n        \"\"\"\n        Get both stat and detailed diff for a commit.\n\n        Args:\n            commit: Commit to diff\n            file_path: Path relative to repository root\n            prev_commit: Previous commit for diff, defaults to commit's parent\n\n        Returns:\n            Tuple of (stat_diff, detailed_diff) where:\n                stat_diff: Summary of changes (files changed, insertions/deletions)\n                detailed_diff: Colored word-level diff with context\n\n        Raises:\n            GitCommandError: If Git operations fail\n        \"\"\"\n        prev_hash = prev_commit.hexsha if prev_commit else f\"{commit.hexsha}^\"\n        rel_path = file_path.relative_to(self.repo_path)\n\n        try:\n            # Get stats diff\n            stat = self.repo.git.diff(prev_hash, commit.hexsha, rel_path, stat=True)\n\n            # Get detailed diff\n            diff = self.repo.git.diff(\n                prev_hash,\n                commit.hexsha,\n                rel_path,\n                unified=2,\n                word_diff=\"plain\",\n                color=\"always\",\n                ignore_space_change=True,\n            )\n\n            return stat, diff\n        except GitCommandError as e:\n            logger.error(f\"Failed to get diff for {commit.hexsha}: {e}\")\n            return \"\", \"\"\n\n    def display_history(self, file_path: Path, max_versions: int = 0) -&gt; None:\n        \"\"\"\n        Display history of changes for a file with diffs between versions.\n\n        Shows most recent changes first, limited to max_versions entries.\n        For each change shows:\n        - Commit info and date\n        - Stats summary of changes\n        - Detailed color diff with 2 lines of context\n\n        Args:\n            file_path: Path to file in repository\n            max_versions: Maximum number of versions to show; zero shows all revisions.\n\n        Example:\n            &gt;&gt;&gt; repo.display_history(Path(\"prompts/format_dharma_talk.yaml\"))\n            Commit abc123def (2024-12-28 14:30:22):\n            1 file changed, 5 insertions(+), 2 deletions(-)\n\n            diff --git a/prompts/format_dharma_talk.yaml ...\n            ...\n        \"\"\"\n\n        try:\n            # Get commit history\n            commits = self._get_file_revisions(file_path)\n            if not commits:\n                print(f\"No history found for {file_path}\")\n                return\n\n            if max_versions == 0:\n                max_versions = len(commits)  # look at all commits.\n\n            # Display limited history with diffs\n            for i, commit in enumerate(commits[:max_versions]):\n                # Print commit header\n                date_str = commit.committed_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n                print(f\"\\nCommit {commit.hexsha[:8]} ({date_str}):\")\n                print(f\"Message: {commit.message.strip()}\")\n\n                # Get and display diffs\n                prev_commit = commits[i + 1] if i + 1 &lt; len(commits) else None\n                stat_diff, detailed_diff = self._get_commit_diff(\n                    commit, file_path, prev_commit\n                )\n\n                if stat_diff:\n                    print(\"\\nChanges:\")\n                    print(stat_diff)\n                if detailed_diff:\n                    print(\"\\nDetailed diff:\")\n                    print(detailed_diff)\n\n                print(\"\\033[0m\", end=\"\")\n                print(\"-\" * 80)  # Visual separator between commits\n\n        except Exception as e:\n            logger.error(f\"Failed to display history for {file_path}: {e}\")\n            print(f\"Error displaying history: {e}\")\n            raise\n\n    def _is_file_clean(self, rel_path: Path) -&gt; bool:\n        \"\"\"\n        Check if file has uncommitted changes.\n\n        Args:\n            rel_path: Path relative to repository root\n\n        Returns:\n            bool: True if file has no changes\n        \"\"\"\n        return str(rel_path) not in (\n            [item.a_path for item in self.repo.index.diff(None)]\n            + self.repo.untracked_files\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.GitBackedRepository.repo","title":"<code>repo = Repo(repo_path)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.GitBackedRepository.repo_path","title":"<code>repo_path = repo_path</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.GitBackedRepository.__init__","title":"<code>__init__(repo_path)</code>","text":"<p>Initialize or connect to Git repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to repository directory</p> required <p>Raises:</p> Type Description <code>GitCommandError</code> <p>If Git operations fail</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __init__(self, repo_path: Path):\n    \"\"\"\n    Initialize or connect to Git repository.\n\n    Args:\n        repo_path: Path to repository directory\n\n    Raises:\n        GitCommandError: If Git operations fail\n    \"\"\"\n    self.repo_path = repo_path\n\n    try:\n        # Try to connect to existing repository\n        self.repo = Repo(repo_path)\n        logger.debug(f\"Connected to existing Git repository at {repo_path}\")\n\n    except InvalidGitRepositoryError:\n        # Initialize new repository if none exists\n        logger.info(f\"Initializing new Git repository at {repo_path}\")\n        self.repo = Repo.init(repo_path)\n\n        # Create initial commit if repo is empty\n        if not self.repo.head.is_valid():\n            # Create and commit .gitignore\n            gitignore = repo_path / \".gitignore\"\n            gitignore.write_text(\"*.lock\\n.DS_Store\\n\")\n            self.repo.index.add([\".gitignore\"])\n            self.repo.index.commit(\"Initial repository setup\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.GitBackedRepository.display_history","title":"<code>display_history(file_path, max_versions=0)</code>","text":"<p>Display history of changes for a file with diffs between versions.</p> <p>Shows most recent changes first, limited to max_versions entries. For each change shows: - Commit info and date - Stats summary of changes - Detailed color diff with 2 lines of context</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file in repository</p> required <code>max_versions</code> <code>int</code> <p>Maximum number of versions to show; zero shows all revisions.</p> <code>0</code> Example <p>repo.display_history(Path(\"prompts/format_dharma_talk.yaml\")) Commit abc123def (2024-12-28 14:30:22): 1 file changed, 5 insertions(+), 2 deletions(-)</p> <p>diff --git a/prompts/format_dharma_talk.yaml ... ...</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def display_history(self, file_path: Path, max_versions: int = 0) -&gt; None:\n    \"\"\"\n    Display history of changes for a file with diffs between versions.\n\n    Shows most recent changes first, limited to max_versions entries.\n    For each change shows:\n    - Commit info and date\n    - Stats summary of changes\n    - Detailed color diff with 2 lines of context\n\n    Args:\n        file_path: Path to file in repository\n        max_versions: Maximum number of versions to show; zero shows all revisions.\n\n    Example:\n        &gt;&gt;&gt; repo.display_history(Path(\"prompts/format_dharma_talk.yaml\"))\n        Commit abc123def (2024-12-28 14:30:22):\n        1 file changed, 5 insertions(+), 2 deletions(-)\n\n        diff --git a/prompts/format_dharma_talk.yaml ...\n        ...\n    \"\"\"\n\n    try:\n        # Get commit history\n        commits = self._get_file_revisions(file_path)\n        if not commits:\n            print(f\"No history found for {file_path}\")\n            return\n\n        if max_versions == 0:\n            max_versions = len(commits)  # look at all commits.\n\n        # Display limited history with diffs\n        for i, commit in enumerate(commits[:max_versions]):\n            # Print commit header\n            date_str = commit.committed_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n            print(f\"\\nCommit {commit.hexsha[:8]} ({date_str}):\")\n            print(f\"Message: {commit.message.strip()}\")\n\n            # Get and display diffs\n            prev_commit = commits[i + 1] if i + 1 &lt; len(commits) else None\n            stat_diff, detailed_diff = self._get_commit_diff(\n                commit, file_path, prev_commit\n            )\n\n            if stat_diff:\n                print(\"\\nChanges:\")\n                print(stat_diff)\n            if detailed_diff:\n                print(\"\\nDetailed diff:\")\n                print(detailed_diff)\n\n            print(\"\\033[0m\", end=\"\")\n            print(\"-\" * 80)  # Visual separator between commits\n\n    except Exception as e:\n        logger.error(f\"Failed to display history for {file_path}: {e}\")\n        print(f\"Error displaying history: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.GitBackedRepository.update_file","title":"<code>update_file(file_path)</code>","text":"<p>Stage and commit changes to a file in the Git repository.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Absolute or relative path to the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Commit hash if changes were made.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> <code>ValueError</code> <p>If the file is outside the repository.</p> <code>GitCommandError</code> <p>If Git operations fail.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def update_file(self, file_path: Path) -&gt; str:\n    \"\"\"\n    Stage and commit changes to a file in the Git repository.\n\n    Args:\n        file_path: Absolute or relative path to the file.\n\n    Returns:\n        str: Commit hash if changes were made.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the file is outside the repository.\n        GitCommandError: If Git operations fail.\n    \"\"\"\n    file_path = file_path.resolve()\n\n    # Ensure the file is within the repository\n    try:\n        rel_path = file_path.relative_to(self.repo_path)\n    except ValueError as e:\n        raise ValueError(\n            f\"File {file_path} is not under the repository root {self.repo_path}\"\n        ) from e\n\n    if not file_path.exists():\n        raise FileNotFoundError(f\"File does not exist: {file_path}\")\n\n    try:\n        return self._commit_file_update(rel_path, file_path)\n    except GitCommandError as e:\n        logger.error(f\"Git operation failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.LocalPromptManager","title":"<code>LocalPromptManager</code>","text":"<p>A simple singleton implementation of PromptManager that ensures only one instance is created and reused throughout the application lifecycle.</p> <p>This class wraps the PromptManager to provide efficient prompt loading by maintaining a single reusable instance.</p> <p>Attributes:</p> Name Type Description <code>_instance</code> <code>Optional[SingletonPromptManager]</code> <p>The singleton instance</p> <code>_prompt_manager</code> <code>Optional[PromptManager]</code> <p>The wrapped PromptManager instance</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>class LocalPromptManager:\n    \"\"\"\n    A simple singleton implementation of PromptManager that ensures only one instance\n    is created and reused throughout the application lifecycle.\n\n    This class wraps the PromptManager to provide efficient prompt loading by\n    maintaining a single reusable instance.\n\n    Attributes:\n        _instance (Optional[SingletonPromptManager]): The singleton instance\n        _prompt_manager (Optional[PromptManager]): The wrapped PromptManager instance\n    \"\"\"\n\n    _instance: Optional[\"LocalPromptManager\"] = None\n\n    def __new__(cls) -&gt; \"LocalPromptManager\":\n        \"\"\"\n        Create or return the singleton instance.\n\n        Returns:\n            SingletonPromptManager: The singleton instance\n        \"\"\"\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._prompt_manager = None\n        return cls._instance\n\n    @property\n    def prompt_manager(self) -&gt; \"PromptCatalog\":\n        \"\"\"\n        Lazy initialization of the PromptManager instance.\n\n        Returns:\n            PromptManager: The wrapped PromptManager instance\n\n        Raises:\n            RuntimeError: If PATTERN_REPO is not properly configured\n        \"\"\"\n        if self._prompt_manager is None:  # type: ignore\n            try:\n                load_dotenv()\n                if prompt_path_name := os.getenv(\"TNH_PATTERN_DIR\"):\n                    prompt_dir = Path(prompt_path_name)\n                    logger.debug(f\"prompt dir: {prompt_path_name}\")\n                else:\n                    prompt_dir = TNH_DEFAULT_PATTERN_DIR\n                self._prompt_manager = PromptCatalog(prompt_dir)\n            except ImportError as err:\n                raise RuntimeError(\n                    \"Failed to initialize PromptManager. Ensure prompt_manager \"\n                    f\"module and PATTERN_REPO are properly configured: {err}\"\n                ) from err\n        return self._prompt_manager\n\n    def get_prompt(self, name: str) -&gt; Prompt:\n        \"\"\"Get a prompt by name.\"\"\"\n        return self.prompt_manager.load(Prompt._normalize_name(name))\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.LocalPromptManager.prompt_manager","title":"<code>prompt_manager</code>  <code>property</code>","text":"<p>Lazy initialization of the PromptManager instance.</p> <p>Returns:</p> Name Type Description <code>PromptManager</code> <code>PromptCatalog</code> <p>The wrapped PromptManager instance</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If PATTERN_REPO is not properly configured</p>"},{"location":"api/#tnh_scholar.ai_text_processing.LocalPromptManager.__new__","title":"<code>__new__()</code>","text":"<p>Create or return the singleton instance.</p> <p>Returns:</p> Name Type Description <code>SingletonPromptManager</code> <code>LocalPromptManager</code> <p>The singleton instance</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __new__(cls) -&gt; \"LocalPromptManager\":\n    \"\"\"\n    Create or return the singleton instance.\n\n    Returns:\n        SingletonPromptManager: The singleton instance\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n        cls._instance._prompt_manager = None\n    return cls._instance\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.LocalPromptManager.get_prompt","title":"<code>get_prompt(name)</code>","text":"<p>Get a prompt by name.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def get_prompt(self, name: str) -&gt; Prompt:\n    \"\"\"Get a prompt by name.\"\"\"\n    return self.prompt_manager.load(Prompt._normalize_name(name))\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.LogicalSection","title":"<code>LogicalSection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a contextually meaningful segment of a larger text.</p> <p>Sections should preserve natural breaks in content  (explicit section markers, topic shifts, argument development, narrative progression)  while staying within specified size limits in order to create chunks suitable for AI processing.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class LogicalSection(BaseModel):\n    \"\"\"\n    Represents a contextually meaningful segment of a larger text.\n\n    Sections should preserve natural breaks in content \n    (explicit section markers, topic shifts, argument development, narrative progression) \n    while staying within specified size limits in order to create chunks suitable for AI processing.\n    \"\"\"  # noqa: E501\n    start_line: int = Field(\n        ..., \n        description=\"Starting line number that begins this logical segment\"\n    )\n    title: str = Field(\n        ...,\n        description=\"Descriptive title of section's key content\"\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.LogicalSection.start_line","title":"<code>start_line = Field(..., description='Starting line number that begins this logical segment')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.LogicalSection.title","title":"<code>title = Field(..., description=\"Descriptive title of section's key content\")</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.OpenAIProcessor","title":"<code>OpenAIProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>OpenAI-based text processor implementation.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class OpenAIProcessor(TextProcessor):\n    \"\"\"OpenAI-based text processor implementation.\"\"\"\n    def __init__(self, model: Optional[str] = None, max_tokens: int = 0):\n        if not model:\n            model = DEFAULT_OPENAI_MODEL\n        self.model = model\n        self.max_tokens = max_tokens\n\n    def process_text(\n        self,\n        input_str: str,\n        instructions: str,\n        response_format: Optional[Type[BaseModel]] = None,\n        max_tokens: int = 0,\n        **kwargs: Any,\n    ) -&gt; ProcessorResult:\n        \"\"\"Process text using OpenAI API with optional structured output.\"\"\"\n\n        if max_tokens == 0 and self.max_tokens &gt; 0:\n            max_tokens = self.max_tokens\n\n        return openai_process_text(\n            input_str,\n            instructions,\n            model=self.model,\n            max_tokens=max_tokens,\n            response_format=response_format,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.OpenAIProcessor.max_tokens","title":"<code>max_tokens = max_tokens</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.OpenAIProcessor.model","title":"<code>model = model</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.OpenAIProcessor.__init__","title":"<code>__init__(model=None, max_tokens=0)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(self, model: Optional[str] = None, max_tokens: int = 0):\n    if not model:\n        model = DEFAULT_OPENAI_MODEL\n    self.model = model\n    self.max_tokens = max_tokens\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.OpenAIProcessor.process_text","title":"<code>process_text(input_str, instructions, response_format=None, max_tokens=0, **kwargs)</code>","text":"<p>Process text using OpenAI API with optional structured output.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text(\n    self,\n    input_str: str,\n    instructions: str,\n    response_format: Optional[Type[BaseModel]] = None,\n    max_tokens: int = 0,\n    **kwargs: Any,\n) -&gt; ProcessorResult:\n    \"\"\"Process text using OpenAI API with optional structured output.\"\"\"\n\n    if max_tokens == 0 and self.max_tokens &gt; 0:\n        max_tokens = self.max_tokens\n\n    return openai_process_text(\n        input_str,\n        instructions,\n        model=self.model,\n        max_tokens=max_tokens,\n        response_format=response_format,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt","title":"<code>Prompt</code>","text":"<p>Base Prompt class for version-controlled template prompts.</p> <p>Prompts contain: - Instructions: The main prompt instructions as a Jinja2 template.    Note: Instructions are intended to be saved in markdown format in a .md file. - Template fields: Default values for template variables - Metadata: Name and identifier information</p> <p>Version control is handled externally through Git, not in the prompt itself. Prompt identity is determined by the combination of identifiers.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the prompt</p> <code>instructions</code> <code>str</code> <p>The Jinja2 template string for this prompt</p> <code>default_template_fields</code> <code>Dict[str, str]</code> <p>Default values for template variables</p> <code>_allow_empty_vars</code> <code>bool</code> <p>Whether to allow undefined template variables</p> <code>_env</code> <code>Environment</code> <p>Configured Jinja2 environment instance</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>class Prompt:\n    \"\"\"\n    Base Prompt class for version-controlled template prompts.\n\n    Prompts contain:\n    - Instructions: The main prompt instructions as a Jinja2 template.\n       Note: Instructions are intended to be saved in markdown format in a .md file.\n    - Template fields: Default values for template variables\n    - Metadata: Name and identifier information\n\n    Version control is handled externally through Git, not in the prompt itself.\n    Prompt identity is determined by the combination of identifiers.\n\n    Attributes:\n        name (str): The name of the prompt\n        instructions (str): The Jinja2 template string for this prompt\n        default_template_fields (Dict[str, str]): Default values for template variables\n        _allow_empty_vars (bool): Whether to allow undefined template variables\n        _env (Environment): Configured Jinja2 environment instance\n    \"\"\"\n\n    @staticmethod\n    def _normalize_name(value: str) -&gt; str:\n        \"\"\"Canonicalize prompt names for case-insensitive handling.\n\n        Currently: strip() + lower(). If future rules are needed (e.g.,\n        removing punctuation, limiting length), implement them here.\n        \"\"\"\n        return value.strip().lower()\n\n    def __init__(\n        self,\n        name: str,\n        instructions: MarkdownStr,\n        path: Optional[Path] = None,\n        default_template_fields: Optional[Dict[str, str]] = None,\n        allow_empty_vars: bool = False,        \n    ) -&gt; None:\n        \"\"\"\n        Initialize a new Prompt instance.\n\n        Args:\n            name: Unique name identifying the prompt\n            instructions: Jinja2 template string containing the prompt\n            default_template_fields: Optional default values for template variables\n            allow_empty_vars: Whether to allow undefined template variables\n\n        Raises:\n            ValueError: If name or instructions are empty\n            TemplateError: If template syntax is invalid\n        \"\"\"\n        if not name or not instructions:\n            raise ValueError(\"Name and instructions must not be empty\")\n\n        # Normalize prompt name to lowercase for case-insensitive handling\n        name = Prompt._normalize_name(name)\n\n        self.name = name\n        self.instructions = instructions\n        self.path = path\n        self.default_template_fields = default_template_fields or {}\n        self._allow_empty_vars = allow_empty_vars\n        self._env = self._create_environment()\n\n        # Validate template syntax on initialization\n        self._validate_template()\n\n    @staticmethod\n    def _create_environment() -&gt; Environment:\n        \"\"\"\n        Create and configure a Jinja2 environment with optimal settings.\n\n        Returns:\n            Environment: Configured Jinja2 environment \n            with security and formatting options\n        \"\"\"\n        return Environment(\n            undefined=StrictUndefined,  # Raise errors for undefined variables\n            trim_blocks=True,  # Remove first newline after a block\n            lstrip_blocks=True,  # Strip tabs and spaces from the start of lines\n            autoescape=True,  # Enable autoescaping for security\n        )\n\n    def _validate_template(self) -&gt; None:\n        \"\"\"\n        Validate the template syntax without rendering.\n\n        Raises:\n            TemplateError: If template syntax is invalid\n        \"\"\"\n        try:\n            self._env.parse(self.instructions)\n        except TemplateError as e:\n            raise TemplateError(\n                f\"Invalid template syntax in prompt '{self.name}': {str(e)}\"\n            ) from e\n\n    def apply_template(self, field_values: Optional[Dict[str, str]] = None) -&gt; str:\n        \"\"\"\n        Apply template values to prompt instructions using Jinja2.\n\n        Values precedence (highest to lowest):\n        1. field_values (explicitly passed)\n        2. frontmatter values (from prompt file)\n        3. default_template_fields (prompt defaults)\n\n        Args:\n            field_values: Values to substitute into the template.\n                        If None, uses frontmatter/defaults.\n\n        Returns:\n            str: Rendered instructions with template values applied.\n\n        Raises:\n            TemplateError: If template rendering fails\n            ValueError: If required template variables are missing\n        \"\"\"\n        # Get frontmatter values\n        frontmatter = self.extract_frontmatter() or {}\n\n        # Combine values with correct precedence using | operator\n        template_values = self.default_template_fields | \\\n            frontmatter | (field_values or {})\n\n        instructions = self.get_content_without_frontmatter()\n        logger.debug(f\"instructions without frontmatter:\\n{instructions}\")\n\n        try:\n            return self._render_template_with_values(instructions, template_values)\n        except TemplateError as e:\n            raise TemplateError(\n                f\"Template rendering failed for prompt '{self.name}': {str(e)}\"\n                ) from e\n\n    def _render_template_with_values(\n        self, \n        instructions: str, \n        template_values: dict\n        ) -&gt; str:\n        \"\"\"\n        Validate and render template with provided values.\n\n        Args:\n            instructions: Template content without frontmatter\n            template_values: Values to substitute into template\n\n        Returns:\n            Rendered template string\n\n        Raises:\n            ValueError: If required template variables are missing\n        \"\"\"\n        # Parse for validation\n        parsed_content = self._env.parse(instructions)\n        required_vars = find_undeclared_variables(parsed_content)\n\n        # Validate variables\n        missing_vars = required_vars - set(template_values.keys())\n        if missing_vars and not self._allow_empty_vars:\n            raise ValueError(\n                f\"Missing required template variables in prompt '{self.name}': \"\n                f\"{', '.join(sorted(missing_vars))}\"\n            )\n\n        # Create and render template\n        template = self._env.from_string(instructions)\n        return template.render(**template_values)\n\n    def extract_frontmatter(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Extract and validate YAML frontmatter from markdown instructions.\n\n        Returns:\n            Optional[Dict]: Frontmatter data if found and valid, None otherwise\n\n        Note:\n            Frontmatter must be at the very start of the file and properly formatted.\n        \"\"\"\n\n        prompt = r\"\\A---\\s*\\n(.*?)\\n---\\s*(?:\\n|$)\"\n        if match := re.match(prompt, self.instructions, re.DOTALL):\n            try:\n                frontmatter = yaml.safe_load(match[1])\n                if frontmatter is None:\n                    return None\n                if not isinstance(frontmatter, dict):\n                    logger.warning(f\"Frontmatter must be a YAML dictionary: \"\n                                   f\"{frontmatter}\")\n                    return None\n                return frontmatter\n            except yaml.YAMLError as e:\n                logger.warning(f\"Invalid YAML in frontmatter: {e}\")\n                return None\n        return None\n\n    def get_content_without_frontmatter(self) -&gt; str:\n        \"\"\"\n        Get markdown content with frontmatter removed.\n\n        Returns:\n            str: Markdown content without frontmatter\n        \"\"\"\n        prompt = r\"\\A---\\s*\\n.*?\\n---\\s*\\n\"\n        return re.sub(prompt, \"\", self.instructions, flags=re.DOTALL)\n\n    def update_frontmatter(self, new_data: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Update or add frontmatter to the markdown content.\n\n        Args:\n            new_data: Dictionary of frontmatter fields to update\n        \"\"\"\n\n        current_frontmatter = self.extract_frontmatter() or {}\n        updated_frontmatter = {**current_frontmatter, **new_data}\n\n        # Create YAML string\n        yaml_str = yaml.dump(\n            updated_frontmatter, default_flow_style=False, allow_unicode=True\n        )\n\n        # Remove existing frontmatter if present\n        content = self.get_content_without_frontmatter()\n\n        # Combine new frontmatter with content\n        self.instructions = f\"---\\n{yaml_str}---\\n\\n{content}\"\n\n\n    def source_bytes(self) -&gt; bytes:\n        \"\"\"\n        Best-effort raw bytes for prompt hashing.\n\n        Prefers hashing exact on-disk bytes including front-matter.\n        We therefore first try to read from `prompt_path`. If that fails, we fall back\n        to hashing the concatenation of known templates. In V1, only\n        the instructions (system template) are used for rendering.\n        \"\"\"\n        # Preferred path: use on-disk bytes when available.\n        if self.path is not None:\n            return self.path.read_bytes()\n\n        # Fallback: concatenate known templates deterministically\n        sys_part = self.instructions or \"\"\n        return sys_part.encode(\"utf-8\")\n\n    def content_hash(self) -&gt; str:\n        \"\"\"\n        Generate a SHA-256 hash of the prompt content.\n\n        Useful for quick content comparison and change detection.\n\n        Returns:\n            str: Hexadecimal string of the SHA-256 hash\n        \"\"\"\n        content = (\n            f\"{self.name}{self.instructions}\"\n            f\"{sorted(self.default_template_fields.items())}\"\n            )\n        return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Convert prompt to dictionary for serialization.\n\n        Returns:\n            Dict containing all prompt data in serializable format\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"instructions\": self.instructions,\n            \"default_template_fields\": self.default_template_fields,\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"Prompt\":\n        \"\"\"\n        Create prompt instance from dictionary data.\n\n        Args:\n            data: Dictionary containing prompt data\n\n        Returns:\n            Prompt: New prompt instance\n\n        Raises:\n            ValueError: If required fields are missing\n        \"\"\"\n        required_fields = {\"name\", \"instructions\"}\n        if missing_fields := required_fields - set(data.keys()):\n            raise ValueError(f\"Missing required fields: {', '.join(missing_fields)}\")\n\n        return cls(\n            name=Prompt._normalize_name(str(data[\"name\"])),\n            instructions=data[\"instructions\"],\n            path=None,\n            default_template_fields=data.get(\"default_template_fields\", {}),\n        )\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Compare prompts based on their content.\"\"\"\n        if not isinstance(other, Prompt):\n            return NotImplemented\n        return self.content_hash() == other.content_hash()\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Hash based on content hash for container operations.\"\"\"\n        return hash(self.content_hash())\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.default_template_fields","title":"<code>default_template_fields = default_template_fields or {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.instructions","title":"<code>instructions = instructions</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.name","title":"<code>name = name</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.path","title":"<code>path = path</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare prompts based on their content.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Compare prompts based on their content.\"\"\"\n    if not isinstance(other, Prompt):\n        return NotImplemented\n    return self.content_hash() == other.content_hash()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash based on content hash for container operations.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Hash based on content hash for container operations.\"\"\"\n    return hash(self.content_hash())\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.__init__","title":"<code>__init__(name, instructions, path=None, default_template_fields=None, allow_empty_vars=False)</code>","text":"<p>Initialize a new Prompt instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name identifying the prompt</p> required <code>instructions</code> <code>MarkdownStr</code> <p>Jinja2 template string containing the prompt</p> required <code>default_template_fields</code> <code>Optional[Dict[str, str]]</code> <p>Optional default values for template variables</p> <code>None</code> <code>allow_empty_vars</code> <code>bool</code> <p>Whether to allow undefined template variables</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If name or instructions are empty</p> <code>TemplateError</code> <p>If template syntax is invalid</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    instructions: MarkdownStr,\n    path: Optional[Path] = None,\n    default_template_fields: Optional[Dict[str, str]] = None,\n    allow_empty_vars: bool = False,        \n) -&gt; None:\n    \"\"\"\n    Initialize a new Prompt instance.\n\n    Args:\n        name: Unique name identifying the prompt\n        instructions: Jinja2 template string containing the prompt\n        default_template_fields: Optional default values for template variables\n        allow_empty_vars: Whether to allow undefined template variables\n\n    Raises:\n        ValueError: If name or instructions are empty\n        TemplateError: If template syntax is invalid\n    \"\"\"\n    if not name or not instructions:\n        raise ValueError(\"Name and instructions must not be empty\")\n\n    # Normalize prompt name to lowercase for case-insensitive handling\n    name = Prompt._normalize_name(name)\n\n    self.name = name\n    self.instructions = instructions\n    self.path = path\n    self.default_template_fields = default_template_fields or {}\n    self._allow_empty_vars = allow_empty_vars\n    self._env = self._create_environment()\n\n    # Validate template syntax on initialization\n    self._validate_template()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.apply_template","title":"<code>apply_template(field_values=None)</code>","text":"<p>Apply template values to prompt instructions using Jinja2.</p> <p>Values precedence (highest to lowest): 1. field_values (explicitly passed) 2. frontmatter values (from prompt file) 3. default_template_fields (prompt defaults)</p> <p>Parameters:</p> Name Type Description Default <code>field_values</code> <code>Optional[Dict[str, str]]</code> <p>Values to substitute into the template.         If None, uses frontmatter/defaults.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered instructions with template values applied.</p> <p>Raises:</p> Type Description <code>TemplateError</code> <p>If template rendering fails</p> <code>ValueError</code> <p>If required template variables are missing</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def apply_template(self, field_values: Optional[Dict[str, str]] = None) -&gt; str:\n    \"\"\"\n    Apply template values to prompt instructions using Jinja2.\n\n    Values precedence (highest to lowest):\n    1. field_values (explicitly passed)\n    2. frontmatter values (from prompt file)\n    3. default_template_fields (prompt defaults)\n\n    Args:\n        field_values: Values to substitute into the template.\n                    If None, uses frontmatter/defaults.\n\n    Returns:\n        str: Rendered instructions with template values applied.\n\n    Raises:\n        TemplateError: If template rendering fails\n        ValueError: If required template variables are missing\n    \"\"\"\n    # Get frontmatter values\n    frontmatter = self.extract_frontmatter() or {}\n\n    # Combine values with correct precedence using | operator\n    template_values = self.default_template_fields | \\\n        frontmatter | (field_values or {})\n\n    instructions = self.get_content_without_frontmatter()\n    logger.debug(f\"instructions without frontmatter:\\n{instructions}\")\n\n    try:\n        return self._render_template_with_values(instructions, template_values)\n    except TemplateError as e:\n        raise TemplateError(\n            f\"Template rendering failed for prompt '{self.name}': {str(e)}\"\n            ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.content_hash","title":"<code>content_hash()</code>","text":"<p>Generate a SHA-256 hash of the prompt content.</p> <p>Useful for quick content comparison and change detection.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Hexadecimal string of the SHA-256 hash</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def content_hash(self) -&gt; str:\n    \"\"\"\n    Generate a SHA-256 hash of the prompt content.\n\n    Useful for quick content comparison and change detection.\n\n    Returns:\n        str: Hexadecimal string of the SHA-256 hash\n    \"\"\"\n    content = (\n        f\"{self.name}{self.instructions}\"\n        f\"{sorted(self.default_template_fields.items())}\"\n        )\n    return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.extract_frontmatter","title":"<code>extract_frontmatter()</code>","text":"<p>Extract and validate YAML frontmatter from markdown instructions.</p> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict]: Frontmatter data if found and valid, None otherwise</p> Note <p>Frontmatter must be at the very start of the file and properly formatted.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def extract_frontmatter(self) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract and validate YAML frontmatter from markdown instructions.\n\n    Returns:\n        Optional[Dict]: Frontmatter data if found and valid, None otherwise\n\n    Note:\n        Frontmatter must be at the very start of the file and properly formatted.\n    \"\"\"\n\n    prompt = r\"\\A---\\s*\\n(.*?)\\n---\\s*(?:\\n|$)\"\n    if match := re.match(prompt, self.instructions, re.DOTALL):\n        try:\n            frontmatter = yaml.safe_load(match[1])\n            if frontmatter is None:\n                return None\n            if not isinstance(frontmatter, dict):\n                logger.warning(f\"Frontmatter must be a YAML dictionary: \"\n                               f\"{frontmatter}\")\n                return None\n            return frontmatter\n        except yaml.YAMLError as e:\n            logger.warning(f\"Invalid YAML in frontmatter: {e}\")\n            return None\n    return None\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create prompt instance from dictionary data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing prompt data</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>New prompt instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are missing</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"Prompt\":\n    \"\"\"\n    Create prompt instance from dictionary data.\n\n    Args:\n        data: Dictionary containing prompt data\n\n    Returns:\n        Prompt: New prompt instance\n\n    Raises:\n        ValueError: If required fields are missing\n    \"\"\"\n    required_fields = {\"name\", \"instructions\"}\n    if missing_fields := required_fields - set(data.keys()):\n        raise ValueError(f\"Missing required fields: {', '.join(missing_fields)}\")\n\n    return cls(\n        name=Prompt._normalize_name(str(data[\"name\"])),\n        instructions=data[\"instructions\"],\n        path=None,\n        default_template_fields=data.get(\"default_template_fields\", {}),\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.get_content_without_frontmatter","title":"<code>get_content_without_frontmatter()</code>","text":"<p>Get markdown content with frontmatter removed.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Markdown content without frontmatter</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def get_content_without_frontmatter(self) -&gt; str:\n    \"\"\"\n    Get markdown content with frontmatter removed.\n\n    Returns:\n        str: Markdown content without frontmatter\n    \"\"\"\n    prompt = r\"\\A---\\s*\\n.*?\\n---\\s*\\n\"\n    return re.sub(prompt, \"\", self.instructions, flags=re.DOTALL)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.source_bytes","title":"<code>source_bytes()</code>","text":"<p>Best-effort raw bytes for prompt hashing.</p> <p>Prefers hashing exact on-disk bytes including front-matter. We therefore first try to read from <code>prompt_path</code>. If that fails, we fall back to hashing the concatenation of known templates. In V1, only the instructions (system template) are used for rendering.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def source_bytes(self) -&gt; bytes:\n    \"\"\"\n    Best-effort raw bytes for prompt hashing.\n\n    Prefers hashing exact on-disk bytes including front-matter.\n    We therefore first try to read from `prompt_path`. If that fails, we fall back\n    to hashing the concatenation of known templates. In V1, only\n    the instructions (system template) are used for rendering.\n    \"\"\"\n    # Preferred path: use on-disk bytes when available.\n    if self.path is not None:\n        return self.path.read_bytes()\n\n    # Fallback: concatenate known templates deterministically\n    sys_part = self.instructions or \"\"\n    return sys_part.encode(\"utf-8\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert prompt to dictionary for serialization.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing all prompt data in serializable format</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert prompt to dictionary for serialization.\n\n    Returns:\n        Dict containing all prompt data in serializable format\n    \"\"\"\n    return {\n        \"name\": self.name,\n        \"instructions\": self.instructions,\n        \"default_template_fields\": self.default_template_fields,\n    }\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.Prompt.update_frontmatter","title":"<code>update_frontmatter(new_data)</code>","text":"<p>Update or add frontmatter to the markdown content.</p> <p>Parameters:</p> Name Type Description Default <code>new_data</code> <code>Dict[str, Any]</code> <p>Dictionary of frontmatter fields to update</p> required Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def update_frontmatter(self, new_data: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Update or add frontmatter to the markdown content.\n\n    Args:\n        new_data: Dictionary of frontmatter fields to update\n    \"\"\"\n\n    current_frontmatter = self.extract_frontmatter() or {}\n    updated_frontmatter = {**current_frontmatter, **new_data}\n\n    # Create YAML string\n    yaml_str = yaml.dump(\n        updated_frontmatter, default_flow_style=False, allow_unicode=True\n    )\n\n    # Remove existing frontmatter if present\n    content = self.get_content_without_frontmatter()\n\n    # Combine new frontmatter with content\n    self.instructions = f\"---\\n{yaml_str}---\\n\\n{content}\"\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.PromptCatalog","title":"<code>PromptCatalog</code>","text":"<p>Main interface for prompt management system.</p> <p>Provides high-level operations: - Prompt creation and loading - Automatic versioning - Safe concurrent access - Basic history tracking - Case-insensitive prompt names (stored as lowercase)</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>class PromptCatalog:\n    \"\"\"\n    Main interface for prompt management system.\n\n    Provides high-level operations:\n    - Prompt creation and loading\n    - Automatic versioning\n    - Safe concurrent access\n    - Basic history tracking\n    - Case-insensitive prompt names (stored as lowercase)\n    \"\"\"\n\n    def __init__(self, base_path: Path):\n        \"\"\"\n        Initialize prompt management system.\n\n        Args:\n            base_path: Base directory for prompt storage\n        \"\"\"\n        self.base_path = Path(base_path).resolve()\n        self.base_path.mkdir(parents=True, exist_ok=True)\n\n        # Initialize subsystems\n        self.repo = GitBackedRepository(self.base_path)\n        self.access_manager = ConcurrentAccessManager(self.base_path / \".locks\")\n\n        logger.info(f\"Initialized prompt management system at {base_path}\")\n\n    def _normalize_path(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Normalize a path to be absolute under the repository base path.\n\n        Handles these cases to same result:\n        - \"my_file\" -&gt; &lt;base_path&gt;/my_file\n        - \"&lt;base_path&gt;/my_file\" -&gt; &lt;base_path&gt;/my_file\n\n        Args:\n            path: Input path as string or Path\n\n        Returns:\n            Path: Absolute path under base_path\n\n        Raises:\n            ValueError: If path would resolve outside repository\n        \"\"\"\n        path = Path(path)  # ensure we have a path\n\n        # Join with base_path as needed: always interpret relative\n        # paths as relative to the repository base path. This avoids\n        # incorrectly handling nested relative paths like \"a/b\"\n        # which may not have the same parent as self.base_path.\n        if not path.is_absolute():\n            path = self.base_path / path\n\n        # Safety check after resolution\n        resolved = path.resolve()\n        try:\n            resolved.relative_to(self.base_path)\n        except ValueError as e:\n            raise ValueError(\n                f\"Path {path} resolves outside repository: {self.base_path}\"\n            ) from e\n\n        return resolved\n\n    def get_path(self, prompt_name: str) -&gt; Optional[Path]:\n        \"\"\"\n        Recursively search for a prompt file with the given name (case-insensitive)\n        in base_path and all subdirectories.\n\n        Args:\n            prompt_name: prompt name (without extension) to search for\n\n        Returns:\n            Optional[Path]: Full path to the found prompt file, or None if not found\n        \"\"\"\n        target = Prompt._normalize_name(prompt_name)\n        with suppress(StopIteration):\n            for path in self.base_path.rglob(\"*.md\"):\n                if path.is_file() and path.stem.lower() == target:\n                    logger.debug(\n                        f\"Found prompt file for name {prompt_name} at: {path}\"\n                    )\n                    return self._normalize_path(path)\n        logger.debug(f\"No prompt file found with name: {prompt_name}\")\n        return None\n\n    def save(self, prompt: Prompt, subdir: Optional[Path] = None) -&gt; Path:\n        prompt_name = Prompt._normalize_name(prompt.name)\n        instructions = prompt.instructions\n\n        if subdir is None:\n            path = self.base_path / f\"{prompt_name}.md\"\n        else:\n            path = self.base_path / subdir / f\"{prompt_name}.md\"\n\n        path = self._normalize_path(path)\n\n        # Check for existing prompt by case-insensitive match\n        existing_path = self.get_path(prompt_name)\n\n        try:\n            # Lock on the destination path name (lowercase) to avoid races\n            with self.access_manager.file_lock(path):\n                # If an existing file is present but at a different case/path, rename it\n                if existing_path is not None and existing_path != path:\n                    path.parent.mkdir(parents=True, exist_ok=True)\n                    logger.info(\n                        f\"Renaming existing prompt file from {existing_path} to {path} \"\n                        \"to enforce lowercase naming.\"\n                    )\n                    existing_path.rename(path)\n\n                write_str_to_file(path, instructions, overwrite=True)\n                self.repo.update_file(path)\n                logger.info(f\"Prompt saved at {path}\")\n                return path.relative_to(self.base_path)\n\n        except Exception as e:\n            logger.error(f\"Failed to save prompt {prompt_name}: {e}\")\n            raise\n\n    def load(self, prompt_name: str) -&gt; Prompt:\n        \"\"\"\n        Load the .md prompt file by name, extract placeholders, and\n        return a fully constructed Prompt object.\n\n        Args:\n            prompt_name: Name of the prompt (without .md extension).\n\n        Returns:\n            A new Prompt object whose 'instructions' is the file's text\n            and whose 'template_fields' are inferred from placeholders in\n            those instructions.\n        \"\"\"\n        prompt_name = Prompt._normalize_name(prompt_name)\n        # Locate the .md file; raise if missing\n        path = self.get_path(prompt_name)\n        if not path:\n            raise FileNotFoundError(f\"No prompt file named {prompt_name}.md found in prompt catalog:\\n\"\n                                    f\"{self.base_path}\"\n                                    )\n\n        # Acquire lock before reading\n        with self.access_manager.file_lock(path):\n            instructions = read_str_from_file(path)\n\n        instructions = MarkdownStr(instructions)\n\n        # Create the prompt from the raw .md text (name is already lowercase)\n        prompt = Prompt(name=prompt_name, instructions=instructions, path=path)\n\n        # Check for local uncommitted changes, updating file:\n        self.repo.update_file(path)\n\n        return prompt\n\n    def show_history(self, prompt_name: str) -&gt; None:\n        if path := self.get_path(prompt_name):\n            self.repo.display_history(path)\n        else:\n            logger.error(f\"Path to {prompt_name} not found.\")\n            return\n\n    # def get_prompt_history_from_path(self, path: Path) -&gt; List[Dict[str, Any]]:\n    #     \"\"\"\n    #     Get version history for a prompt.\n\n    #     Args:\n    #         path: Path to prompt file\n\n    #     Returns:\n    #         List of version information\n    #     \"\"\"\n    #     path = self._normalize_path(path)\n\n    #     return self.repo.get_history(path)\n\n    @classmethod\n    def verify_repository(cls, base_path: Path) -&gt; bool:\n        \"\"\"\n        Verify repository integrity and uniqueness of prompt names.\n\n        Performs the following checks:\n        1. Validates Git repository structure.\n        2. Ensures no duplicate prompt names exist.\n\n        Args:\n            base_path: Repository path to verify.\n\n        Returns:\n            bool: True if the repository is valid \n            and contains no duplicate prompt files.\n        \"\"\"\n        try:\n            # Check if it's a valid Git repository\n            repo = Repo(base_path)\n\n            # Verify basic repository structure\n            basic_valid = (\n                repo.head.is_valid()\n                and not repo.bare\n                and (base_path / \".git\").is_dir()\n                and (base_path / \".locks\").is_dir()\n            )\n\n            if not basic_valid:\n                return False\n\n            prompt_files = list(base_path.rglob(\"*.md\"))\n            seen_names: Dict[str, Path] = {}\n\n            for prompt_file in prompt_files:\n                # Skip files in .git directory\n                if \".git\" in prompt_file.parts:\n                    continue\n\n                # Case-insensitive key\n                key = Prompt._normalize_name(prompt_file.stem)\n\n                if key in seen_names:\n                    logger.error(\n                        f\"Duplicate prompt file detected (case-insensitive):\\n\"\n                        f\"  First occurrence: {seen_names[key]}\\n\"\n                        f\"  Second occurrence: {prompt_file}\"\n                    )\n                    return False\n\n                seen_names[key] = prompt_file\n\n            return True\n\n        except (InvalidGitRepositoryError, Exception) as e:\n            logger.error(f\"Repository verification failed: {e}\")\n            return False\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.PromptCatalog.access_manager","title":"<code>access_manager = ConcurrentAccessManager(self.base_path / '.locks')</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.PromptCatalog.base_path","title":"<code>base_path = Path(base_path).resolve()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.PromptCatalog.repo","title":"<code>repo = GitBackedRepository(self.base_path)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.PromptCatalog.__init__","title":"<code>__init__(base_path)</code>","text":"<p>Initialize prompt management system.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>Base directory for prompt storage</p> required Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __init__(self, base_path: Path):\n    \"\"\"\n    Initialize prompt management system.\n\n    Args:\n        base_path: Base directory for prompt storage\n    \"\"\"\n    self.base_path = Path(base_path).resolve()\n    self.base_path.mkdir(parents=True, exist_ok=True)\n\n    # Initialize subsystems\n    self.repo = GitBackedRepository(self.base_path)\n    self.access_manager = ConcurrentAccessManager(self.base_path / \".locks\")\n\n    logger.info(f\"Initialized prompt management system at {base_path}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.PromptCatalog.get_path","title":"<code>get_path(prompt_name)</code>","text":"<p>Recursively search for a prompt file with the given name (case-insensitive) in base_path and all subdirectories.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_name</code> <code>str</code> <p>prompt name (without extension) to search for</p> required <p>Returns:</p> Type Description <code>Optional[Path]</code> <p>Optional[Path]: Full path to the found prompt file, or None if not found</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def get_path(self, prompt_name: str) -&gt; Optional[Path]:\n    \"\"\"\n    Recursively search for a prompt file with the given name (case-insensitive)\n    in base_path and all subdirectories.\n\n    Args:\n        prompt_name: prompt name (without extension) to search for\n\n    Returns:\n        Optional[Path]: Full path to the found prompt file, or None if not found\n    \"\"\"\n    target = Prompt._normalize_name(prompt_name)\n    with suppress(StopIteration):\n        for path in self.base_path.rglob(\"*.md\"):\n            if path.is_file() and path.stem.lower() == target:\n                logger.debug(\n                    f\"Found prompt file for name {prompt_name} at: {path}\"\n                )\n                return self._normalize_path(path)\n    logger.debug(f\"No prompt file found with name: {prompt_name}\")\n    return None\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.PromptCatalog.load","title":"<code>load(prompt_name)</code>","text":"<p>Load the .md prompt file by name, extract placeholders, and return a fully constructed Prompt object.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_name</code> <code>str</code> <p>Name of the prompt (without .md extension).</p> required <p>Returns:</p> Type Description <code>Prompt</code> <p>A new Prompt object whose 'instructions' is the file's text</p> <code>Prompt</code> <p>and whose 'template_fields' are inferred from placeholders in</p> <code>Prompt</code> <p>those instructions.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def load(self, prompt_name: str) -&gt; Prompt:\n    \"\"\"\n    Load the .md prompt file by name, extract placeholders, and\n    return a fully constructed Prompt object.\n\n    Args:\n        prompt_name: Name of the prompt (without .md extension).\n\n    Returns:\n        A new Prompt object whose 'instructions' is the file's text\n        and whose 'template_fields' are inferred from placeholders in\n        those instructions.\n    \"\"\"\n    prompt_name = Prompt._normalize_name(prompt_name)\n    # Locate the .md file; raise if missing\n    path = self.get_path(prompt_name)\n    if not path:\n        raise FileNotFoundError(f\"No prompt file named {prompt_name}.md found in prompt catalog:\\n\"\n                                f\"{self.base_path}\"\n                                )\n\n    # Acquire lock before reading\n    with self.access_manager.file_lock(path):\n        instructions = read_str_from_file(path)\n\n    instructions = MarkdownStr(instructions)\n\n    # Create the prompt from the raw .md text (name is already lowercase)\n    prompt = Prompt(name=prompt_name, instructions=instructions, path=path)\n\n    # Check for local uncommitted changes, updating file:\n    self.repo.update_file(path)\n\n    return prompt\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.PromptCatalog.save","title":"<code>save(prompt, subdir=None)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def save(self, prompt: Prompt, subdir: Optional[Path] = None) -&gt; Path:\n    prompt_name = Prompt._normalize_name(prompt.name)\n    instructions = prompt.instructions\n\n    if subdir is None:\n        path = self.base_path / f\"{prompt_name}.md\"\n    else:\n        path = self.base_path / subdir / f\"{prompt_name}.md\"\n\n    path = self._normalize_path(path)\n\n    # Check for existing prompt by case-insensitive match\n    existing_path = self.get_path(prompt_name)\n\n    try:\n        # Lock on the destination path name (lowercase) to avoid races\n        with self.access_manager.file_lock(path):\n            # If an existing file is present but at a different case/path, rename it\n            if existing_path is not None and existing_path != path:\n                path.parent.mkdir(parents=True, exist_ok=True)\n                logger.info(\n                    f\"Renaming existing prompt file from {existing_path} to {path} \"\n                    \"to enforce lowercase naming.\"\n                )\n                existing_path.rename(path)\n\n            write_str_to_file(path, instructions, overwrite=True)\n            self.repo.update_file(path)\n            logger.info(f\"Prompt saved at {path}\")\n            return path.relative_to(self.base_path)\n\n    except Exception as e:\n        logger.error(f\"Failed to save prompt {prompt_name}: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.PromptCatalog.show_history","title":"<code>show_history(prompt_name)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def show_history(self, prompt_name: str) -&gt; None:\n    if path := self.get_path(prompt_name):\n        self.repo.display_history(path)\n    else:\n        logger.error(f\"Path to {prompt_name} not found.\")\n        return\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.PromptCatalog.verify_repository","title":"<code>verify_repository(base_path)</code>  <code>classmethod</code>","text":"<p>Verify repository integrity and uniqueness of prompt names.</p> <p>Performs the following checks: 1. Validates Git repository structure. 2. Ensures no duplicate prompt names exist.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>Repository path to verify.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the repository is valid </p> <code>bool</code> <p>and contains no duplicate prompt files.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>@classmethod\ndef verify_repository(cls, base_path: Path) -&gt; bool:\n    \"\"\"\n    Verify repository integrity and uniqueness of prompt names.\n\n    Performs the following checks:\n    1. Validates Git repository structure.\n    2. Ensures no duplicate prompt names exist.\n\n    Args:\n        base_path: Repository path to verify.\n\n    Returns:\n        bool: True if the repository is valid \n        and contains no duplicate prompt files.\n    \"\"\"\n    try:\n        # Check if it's a valid Git repository\n        repo = Repo(base_path)\n\n        # Verify basic repository structure\n        basic_valid = (\n            repo.head.is_valid()\n            and not repo.bare\n            and (base_path / \".git\").is_dir()\n            and (base_path / \".locks\").is_dir()\n        )\n\n        if not basic_valid:\n            return False\n\n        prompt_files = list(base_path.rglob(\"*.md\"))\n        seen_names: Dict[str, Path] = {}\n\n        for prompt_file in prompt_files:\n            # Skip files in .git directory\n            if \".git\" in prompt_file.parts:\n                continue\n\n            # Case-insensitive key\n            key = Prompt._normalize_name(prompt_file.stem)\n\n            if key in seen_names:\n                logger.error(\n                    f\"Duplicate prompt file detected (case-insensitive):\\n\"\n                    f\"  First occurrence: {seen_names[key]}\\n\"\n                    f\"  Second occurrence: {prompt_file}\"\n                )\n                return False\n\n            seen_names[key] = prompt_file\n\n        return True\n\n    except (InvalidGitRepositoryError, Exception) as e:\n        logger.error(f\"Repository verification failed: {e}\")\n        return False\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.SectionEntry","title":"<code>SectionEntry</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Represents a section with its content during iteration.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class SectionEntry(NamedTuple):\n    \"\"\"Represents a section with its content during iteration.\"\"\"\n    number: int         # Logical Section number (1 based index)\n    title: str          # Section title \n    content: str        # Section content\n    range: SectionRange # Section range\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.SectionEntry.content","title":"<code>content</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionEntry.number","title":"<code>number</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionEntry.range","title":"<code>range</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionEntry.title","title":"<code>title</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionParser","title":"<code>SectionParser</code>","text":"<p>Generates structured section breakdowns of text content.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class SectionParser:\n    \"\"\"Generates structured section breakdowns of text content.\"\"\"\n\n    def __init__(\n        self,\n        section_scanner: TextProcessor,\n        section_pattern: Prompt,\n        review_count: int = DEFAULT_REVIEW_COUNT,\n    ):\n        \"\"\"\n        Initialize section generator.\n\n        Args:\n            section_scanner: Text processor used to extract sections\n            section_pattern: Pattern object containing section generation instructions\n            review_count: Number of review passes\n        \"\"\"\n        self.section_scanner = section_scanner\n        self.section_pattern = section_pattern\n        self.review_count = review_count\n\n    def find_sections(\n        self,\n        text: TextObject,\n        section_count_target: Optional[int] = None,\n        segment_size_target: Optional[int] = None,\n        template_dict: Optional[Dict[str, str]] = None,\n    ) -&gt; TextObject:\n        \"\"\"\n        Generate section breakdown of input text. The text must be split up by newlines.\n\n        Args:\n            text: Input TextObject to process\n            section_count_target: the target for the number of sections to find\n            segment_size_target: the target for the number of lines per section\n                (if section_count_target is specified, \n                this value will be set to generate correct segments)\n            template_dict: Optional additional template variables\n\n        Returns:\n            TextObject containing section breakdown\n        \"\"\"\n\n        # Prepare numbered text, each line is numbered\n        num_text = text.num_text\n\n        if num_text.size &lt; SECTION_SEGMENT_SIZE_WARNING_LIMIT:\n            logger.warning(\n                f\"find_sections: Text has only {num_text.size} lines. \"\n                \"This may lead to unexpected sectioning results.\"\n            )\n\n        # Get language if not specified\n        source_language = get_language_from_code(text.language)\n\n        # determine section count if not specified\n        if not section_count_target:\n            segment_size_target, section_count_target = self._get_section_count_info(\n                text.content\n            )\n        elif not segment_size_target:\n            segment_size_target = round(num_text.size / section_count_target)\n\n        section_count_range = self._get_section_count_range(section_count_target)\n\n        current_metadata = text.metadata\n\n        # Prepare template variables\n        template_values = {\n            \"metadata\": current_metadata.to_yaml(),\n            \"source_language\": source_language,\n            \"section_count\": section_count_range,\n            \"line_count\": segment_size_target,\n            \"review_count\": self.review_count,\n        }\n\n        if template_dict:\n            template_values |= template_dict\n\n        # Get and apply processing instructions\n        instructions = self.section_pattern.apply_template(template_values)\n        logger.debug(f\"Finding sections with pattern instructions:\\n {instructions}\")\n\n        logger.info(\n            f\"Finding sections for {source_language} text \"\n            f\"(target sections: {section_count_target})\"\n        )\n\n        # Process text with structured output\n        result = self.section_scanner.process_text(\n            num_text.numbered_content, instructions, response_format=AIResponse\n        )\n\n        ai_response = cast(AIResponse, result)\n        text_result = TextObject.from_response(ai_response, current_metadata, num_text)\n\n        logger.info(f\"Generated {text_result.section_count} sections.\")\n\n        return text_result\n\n    def _get_section_count_info(self, text: str) -&gt; Tuple[int, int]:\n        num_text = NumberedText(text)\n        segment_size = _calculate_segment_size(num_text, DEFAULT_SECTION_TOKEN_SIZE)\n        section_count_target = round(num_text.size / segment_size)\n        return segment_size, section_count_target\n\n    def _get_section_count_range(\n        self,\n        section_count_target: int,\n        section_range_var: int = DEFAULT_SECTION_RANGE_VAR,\n    ) -&gt; str:\n        low = max(1, section_count_target - section_range_var)\n        high = section_count_target + section_range_var\n        return f\"{low}-{high}\"\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.SectionParser.review_count","title":"<code>review_count = review_count</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionParser.section_pattern","title":"<code>section_pattern = section_pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionParser.section_scanner","title":"<code>section_scanner = section_scanner</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionParser.__init__","title":"<code>__init__(section_scanner, section_pattern, review_count=DEFAULT_REVIEW_COUNT)</code>","text":"<p>Initialize section generator.</p> <p>Parameters:</p> Name Type Description Default <code>section_scanner</code> <code>TextProcessor</code> <p>Text processor used to extract sections</p> required <code>section_pattern</code> <code>Prompt</code> <p>Pattern object containing section generation instructions</p> required <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(\n    self,\n    section_scanner: TextProcessor,\n    section_pattern: Prompt,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n):\n    \"\"\"\n    Initialize section generator.\n\n    Args:\n        section_scanner: Text processor used to extract sections\n        section_pattern: Pattern object containing section generation instructions\n        review_count: Number of review passes\n    \"\"\"\n    self.section_scanner = section_scanner\n    self.section_pattern = section_pattern\n    self.review_count = review_count\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.SectionParser.find_sections","title":"<code>find_sections(text, section_count_target=None, segment_size_target=None, template_dict=None)</code>","text":"<p>Generate section breakdown of input text. The text must be split up by newlines.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>TextObject</code> <p>Input TextObject to process</p> required <code>section_count_target</code> <code>Optional[int]</code> <p>the target for the number of sections to find</p> <code>None</code> <code>segment_size_target</code> <code>Optional[int]</code> <p>the target for the number of lines per section (if section_count_target is specified,  this value will be set to generate correct segments)</p> <code>None</code> <code>template_dict</code> <code>Optional[Dict[str, str]]</code> <p>Optional additional template variables</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject containing section breakdown</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def find_sections(\n    self,\n    text: TextObject,\n    section_count_target: Optional[int] = None,\n    segment_size_target: Optional[int] = None,\n    template_dict: Optional[Dict[str, str]] = None,\n) -&gt; TextObject:\n    \"\"\"\n    Generate section breakdown of input text. The text must be split up by newlines.\n\n    Args:\n        text: Input TextObject to process\n        section_count_target: the target for the number of sections to find\n        segment_size_target: the target for the number of lines per section\n            (if section_count_target is specified, \n            this value will be set to generate correct segments)\n        template_dict: Optional additional template variables\n\n    Returns:\n        TextObject containing section breakdown\n    \"\"\"\n\n    # Prepare numbered text, each line is numbered\n    num_text = text.num_text\n\n    if num_text.size &lt; SECTION_SEGMENT_SIZE_WARNING_LIMIT:\n        logger.warning(\n            f\"find_sections: Text has only {num_text.size} lines. \"\n            \"This may lead to unexpected sectioning results.\"\n        )\n\n    # Get language if not specified\n    source_language = get_language_from_code(text.language)\n\n    # determine section count if not specified\n    if not section_count_target:\n        segment_size_target, section_count_target = self._get_section_count_info(\n            text.content\n        )\n    elif not segment_size_target:\n        segment_size_target = round(num_text.size / section_count_target)\n\n    section_count_range = self._get_section_count_range(section_count_target)\n\n    current_metadata = text.metadata\n\n    # Prepare template variables\n    template_values = {\n        \"metadata\": current_metadata.to_yaml(),\n        \"source_language\": source_language,\n        \"section_count\": section_count_range,\n        \"line_count\": segment_size_target,\n        \"review_count\": self.review_count,\n    }\n\n    if template_dict:\n        template_values |= template_dict\n\n    # Get and apply processing instructions\n    instructions = self.section_pattern.apply_template(template_values)\n    logger.debug(f\"Finding sections with pattern instructions:\\n {instructions}\")\n\n    logger.info(\n        f\"Finding sections for {source_language} text \"\n        f\"(target sections: {section_count_target})\"\n    )\n\n    # Process text with structured output\n    result = self.section_scanner.process_text(\n        num_text.numbered_content, instructions, response_format=AIResponse\n    )\n\n    ai_response = cast(AIResponse, result)\n    text_result = TextObject.from_response(ai_response, current_metadata, num_text)\n\n    logger.info(f\"Generated {text_result.section_count} sections.\")\n\n    return text_result\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.SectionProcessor","title":"<code>SectionProcessor</code>","text":"<p>Handles section-based XML text processing with configurable output handling.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class SectionProcessor:\n    \"\"\"Handles section-based XML text processing with configurable output handling.\"\"\"\n\n    def __init__(\n        self,\n        processor: TextProcessor,\n        pattern: Prompt,\n        template_dict: Dict,\n        wrap_in_document: bool = True,\n    ):\n        \"\"\"\n        Initialize the XML section processor.\n\n        Args:\n            processor: Implementation of TextProcessor to use\n            pattern: Pattern object containing processing instructions\n            template_dict: Dictionary for template substitution\n            wrap_in_document: Whether to wrap output in &lt;document&gt; tags\n        \"\"\"\n        self.processor = processor\n        self.pattern = pattern\n        self.template_dict = template_dict\n        self.wrap_in_document = wrap_in_document\n\n    def process_sections(\n        self,\n        text_object: TextObject,\n    ) -&gt; Generator[ProcessedSection, None, None]:\n        \"\"\"\n        Process transcript sections and yield results one section at a time.\n\n        Args:\n            text_object: Object containing section definitions\n\n        Yields:\n            ProcessedSection: One processed section at a time, containing:\n                - title: Section title (English or original language)\n                - original_text: Raw text segment\n                - processed_text: Processed text content\n                - start_line: Starting line number\n        \"\"\"\n        # numbered_transcript = NumberedText(transcript) \n        # transcript is now stored in the TextObject\n        sections = text_object.sections\n\n        logger.info(\n            f\"Processing {len(sections)} sections with pattern: {self.pattern.name}\"\n        )\n\n        for section_entry in text_object:\n            logger.info(f\"Processing section {section_entry.number} \"\n                        f\"'{section_entry.title}':\")\n\n            # Get text segment for section\n            text_segment = section_entry.content\n\n            # Prepare template variables\n            template_values = {\n                \"metadata\": text_object.metadata.to_yaml(),\n                \"section_title\": section_entry.title,\n                \"source_language\": get_language_from_code(text_object.language),\n                \"review_count\": DEFAULT_REVIEW_COUNT,\n            }\n\n            if self.template_dict:\n                template_values |= self.template_dict\n\n            # Get and apply processing instructions\n            instructions = self.pattern.apply_template(template_values)\n            processed_str = self.processor.process_text(text_segment, instructions)\n\n            yield ProcessedSection(\n                title=section_entry.title,\n                original_str=text_segment,\n                processed_str=processed_str,\n            )\n\n    def process_paragraphs(\n        self,\n        text: TextObject,\n    ) -&gt; Generator[ProcessedSection, None, None]:\n        \"\"\"\n        Process transcript by paragraphs (as sections), yielding ProcessedSection objects.\n        Paragraphs are assumed to be given as newline separated.\n\n        Args:\n            text: TextObject to process\n\n        Yields:\n            ProcessedSection: One processed paragraph at a time, containing:\n                - title: Paragraph number (e.g., 'Paragraph 1')\n                - original_str: Raw paragraph text\n                - processed_str: Processed paragraph text\n                - metadata: Optional metadata dict\n        \"\"\"\n        num_text = text.num_text\n\n        logger.info(f\"Processing lines as paragraphs with pattern: {self.pattern.name}\")\n\n        for i, line in num_text:\n            # If line is empty or whitespace, continue\n            if not line.strip():\n                continue\n\n            instructions = self.pattern.apply_template(self.template_dict)\n\n            if i &lt;= 1:\n                logger.debug(f\"Process instructions (first paragraph):\\n{instructions}\")\n\n            processed_str = self.processor.process_text(line, instructions)\n            yield ProcessedSection(\n                title=f\"Paragraph {i}\",\n                original_str=line,\n                processed_str=processed_str,\n                metadata={\"paragraph_number\": i}\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.SectionProcessor.pattern","title":"<code>pattern = pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionProcessor.processor","title":"<code>processor = processor</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionProcessor.template_dict","title":"<code>template_dict = template_dict</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionProcessor.wrap_in_document","title":"<code>wrap_in_document = wrap_in_document</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.SectionProcessor.__init__","title":"<code>__init__(processor, pattern, template_dict, wrap_in_document=True)</code>","text":"<p>Initialize the XML section processor.</p> <p>Parameters:</p> Name Type Description Default <code>processor</code> <code>TextProcessor</code> <p>Implementation of TextProcessor to use</p> required <code>pattern</code> <code>Prompt</code> <p>Pattern object containing processing instructions</p> required <code>template_dict</code> <code>Dict</code> <p>Dictionary for template substitution</p> required <code>wrap_in_document</code> <code>bool</code> <p>Whether to wrap output in  tags <code>True</code> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(\n    self,\n    processor: TextProcessor,\n    pattern: Prompt,\n    template_dict: Dict,\n    wrap_in_document: bool = True,\n):\n    \"\"\"\n    Initialize the XML section processor.\n\n    Args:\n        processor: Implementation of TextProcessor to use\n        pattern: Pattern object containing processing instructions\n        template_dict: Dictionary for template substitution\n        wrap_in_document: Whether to wrap output in &lt;document&gt; tags\n    \"\"\"\n    self.processor = processor\n    self.pattern = pattern\n    self.template_dict = template_dict\n    self.wrap_in_document = wrap_in_document\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.SectionProcessor.process_paragraphs","title":"<code>process_paragraphs(text)</code>","text":"<p>Process transcript by paragraphs (as sections), yielding ProcessedSection objects. Paragraphs are assumed to be given as newline separated.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>TextObject</code> <p>TextObject to process</p> required <p>Yields:</p> Name Type Description <code>ProcessedSection</code> <code>ProcessedSection</code> <p>One processed paragraph at a time, containing: - title: Paragraph number (e.g., 'Paragraph 1') - original_str: Raw paragraph text - processed_str: Processed paragraph text - metadata: Optional metadata dict</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_paragraphs(\n    self,\n    text: TextObject,\n) -&gt; Generator[ProcessedSection, None, None]:\n    \"\"\"\n    Process transcript by paragraphs (as sections), yielding ProcessedSection objects.\n    Paragraphs are assumed to be given as newline separated.\n\n    Args:\n        text: TextObject to process\n\n    Yields:\n        ProcessedSection: One processed paragraph at a time, containing:\n            - title: Paragraph number (e.g., 'Paragraph 1')\n            - original_str: Raw paragraph text\n            - processed_str: Processed paragraph text\n            - metadata: Optional metadata dict\n    \"\"\"\n    num_text = text.num_text\n\n    logger.info(f\"Processing lines as paragraphs with pattern: {self.pattern.name}\")\n\n    for i, line in num_text:\n        # If line is empty or whitespace, continue\n        if not line.strip():\n            continue\n\n        instructions = self.pattern.apply_template(self.template_dict)\n\n        if i &lt;= 1:\n            logger.debug(f\"Process instructions (first paragraph):\\n{instructions}\")\n\n        processed_str = self.processor.process_text(line, instructions)\n        yield ProcessedSection(\n            title=f\"Paragraph {i}\",\n            original_str=line,\n            processed_str=processed_str,\n            metadata={\"paragraph_number\": i}\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.SectionProcessor.process_sections","title":"<code>process_sections(text_object)</code>","text":"<p>Process transcript sections and yield results one section at a time.</p> <p>Parameters:</p> Name Type Description Default <code>text_object</code> <code>TextObject</code> <p>Object containing section definitions</p> required <p>Yields:</p> Name Type Description <code>ProcessedSection</code> <code>ProcessedSection</code> <p>One processed section at a time, containing: - title: Section title (English or original language) - original_text: Raw text segment - processed_text: Processed text content - start_line: Starting line number</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_sections(\n    self,\n    text_object: TextObject,\n) -&gt; Generator[ProcessedSection, None, None]:\n    \"\"\"\n    Process transcript sections and yield results one section at a time.\n\n    Args:\n        text_object: Object containing section definitions\n\n    Yields:\n        ProcessedSection: One processed section at a time, containing:\n            - title: Section title (English or original language)\n            - original_text: Raw text segment\n            - processed_text: Processed text content\n            - start_line: Starting line number\n    \"\"\"\n    # numbered_transcript = NumberedText(transcript) \n    # transcript is now stored in the TextObject\n    sections = text_object.sections\n\n    logger.info(\n        f\"Processing {len(sections)} sections with pattern: {self.pattern.name}\"\n    )\n\n    for section_entry in text_object:\n        logger.info(f\"Processing section {section_entry.number} \"\n                    f\"'{section_entry.title}':\")\n\n        # Get text segment for section\n        text_segment = section_entry.content\n\n        # Prepare template variables\n        template_values = {\n            \"metadata\": text_object.metadata.to_yaml(),\n            \"section_title\": section_entry.title,\n            \"source_language\": get_language_from_code(text_object.language),\n            \"review_count\": DEFAULT_REVIEW_COUNT,\n        }\n\n        if self.template_dict:\n            template_values |= self.template_dict\n\n        # Get and apply processing instructions\n        instructions = self.pattern.apply_template(template_values)\n        processed_str = self.processor.process_text(text_segment, instructions)\n\n        yield ProcessedSection(\n            title=section_entry.title,\n            original_str=text_segment,\n            processed_str=processed_str,\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject","title":"<code>TextObject</code>","text":"<p>Manages text content with section organization and metadata tracking.</p> <p>TextObject serves as the core container for text processing, providing: - Line-numbered text content management - Language identification - Section organization and access - Metadata tracking including incorporated processing stages</p> <p>The class allows for section boundaries through line numbering, allowing sections to be defined by start lines without explicit end lines. Subsequent sections implicitly end where the next section begins. SectionObjects are utilized to represent sections.</p> <p>Attributes:</p> Name Type Description <code>num_text</code> <code>NumberedText</code> <p>Line-numbered text content manager</p> <code>language</code> <code>str</code> <p>ISO 639-1 language code for the text content</p> <code>_sections</code> <code>List[SectionObject]</code> <p>Internal list of text sections with boundaries</p> <code>_metadata</code> <code>Metadata</code> <p>Processing and content metadata container</p> Example <p>content = NumberedText(\"Line 1\\nLine 2\\nLine 3\") obj = TextObject(content, language=\"en\")</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class TextObject:\n    \"\"\"\n    Manages text content with section organization and metadata tracking.\n\n    TextObject serves as the core container for text processing, providing:\n    - Line-numbered text content management\n    - Language identification\n    - Section organization and access\n    - Metadata tracking including incorporated processing stages\n\n    The class allows for section boundaries through line numbering,\n    allowing sections to be defined by start lines without explicit end lines.\n    Subsequent sections implicitly end where the next section begins.\n    SectionObjects are utilized to represent sections.\n\n    Attributes:\n        num_text: Line-numbered text content manager\n        language: ISO 639-1 language code for the text content\n        _sections: Internal list of text sections with boundaries\n        _metadata: Processing and content metadata container\n\n    Example:\n        &gt;&gt;&gt; content = NumberedText(\"Line 1\\\\nLine 2\\\\nLine 3\")\n        &gt;&gt;&gt; obj = TextObject(content, language=\"en\")\n    \"\"\"\n    num_text: NumberedText \n    language: str \n    _sections: List[SectionObject]\n    _metadata: Metadata\n\n    def __init__(self, \n        num_text: NumberedText, \n        language: Optional[str] = None, \n        sections: Optional[List[SectionObject]] = None,\n        metadata: Optional[Metadata] = None):\n        \"\"\"\n        Initialize a TextObject with content and optional organizing components.\n\n        Args:\n            num_text: Text content with line numbering\n            language: ISO 639-1 language code. If None, auto-detected from content\n            sections: Initial sections defining text organization. If None, \n                      text is considered un-sectioned.\n            metadata: Initial metadata. If None, creates empty metadata container\n\n        Note:\n            Until sections are established, section-based methods will raise a value\n            error if called.\n        \"\"\"\n        self.num_text = num_text\n        self.language = language or get_language_code_from_text(num_text.content)\n        self._sections = sections or []\n        self._metadata = metadata or Metadata()\n\n        if sections:\n            self.validate_sections()\n\n\n    def __iter__(self) -&gt; Iterator[SectionEntry]:\n        \"\"\"Iterate through sections, yielding full section information.\"\"\"\n        if not self._sections:\n            raise ValueError(\"No Sections available.\")\n\n        for i, section in enumerate(self._sections):\n            content = self.num_text.get_segment(\n                section.section_range.start, \n                section.section_range.end\n            )\n            yield SectionEntry(\n                number=i+1,\n                title=section.title,\n                range=section.section_range,\n                content=content\n            )\n\n    def __str__(self) -&gt; str:\n        return Frontmatter.embed(self.metadata, self.content)\n\n    @staticmethod\n    def _build_section_objects(\n        logical_sections: List[LogicalSection], \n        last_line: int,\n        metadata: Optional[Metadata] = None\n    ) -&gt; List[SectionObject]:\n        \"\"\"Convert LogicalSections to SectionObjects with proper ranges.\"\"\"\n        section_objects = []\n\n        for i, section in enumerate(logical_sections):\n            # For each section, end is either next section's start or last line + 1\n            end_line = (logical_sections[i + 1].start_line \n                    if i &lt; len(logical_sections) - 1 \n                    else last_line + 1)\n\n            section_objects.append(\n                SectionObject.from_logical_section(section, end_line, metadata)\n            )\n\n        return section_objects\n\n    @classmethod\n    def from_str(\n        cls,\n        text: str,\n        language: Optional[str] = None,\n        sections: Optional[List[SectionObject]] = None,\n        metadata: Optional[Metadata] = None\n    ) -&gt; 'TextObject':\n        \"\"\"\n        Create a TextObject from a string, extracting any frontmatter.\n\n        Args:\n            text: Input text string, potentially containing frontmatter\n            language: ISO language code\n            sections: List of section objects\n            metadata: Optional base metadata to merge with frontmatter\n\n        Returns:\n            TextObject instance with combined metadata\n        \"\"\"\n        # Extract any frontmatter and merge with provided metadata\n        frontmatter_metadata, content = Frontmatter.extract(text)\n\n        # Create NumberedText from content without frontmatter\n        numbered_text = NumberedText(content)\n\n        obj = cls(\n            num_text=numbered_text,\n            language=language,\n            sections=sections,\n            metadata=frontmatter_metadata\n        )\n        if metadata:\n            obj.merge_metadata(metadata)\n\n        return obj\n\n\n    @classmethod\n    def from_response(\n        cls, \n        response: AIResponse,\n        existing_metadata: Metadata,\n        num_text: 'NumberedText'\n    ) -&gt; 'TextObject':\n        \"\"\"Create TextObject from AI response format.\"\"\"\n        # Create metadata from response\n        ai_metadata = response.document_metadata\n        new_metadata = Metadata({\n            \"ai_summary\": response.document_summary,\n            \"ai_concepts\": response.key_concepts,\n            \"ai_context\": response.narrative_context\n        })\n\n        # Convert LogicalSections to SectionObjects\n        sections = cls._build_section_objects(\n            response.sections, \n            num_text.size,\n        )\n\n        text = cls(\n            num_text=num_text,\n            language=response.language,\n            sections=sections,\n            metadata=existing_metadata\n        )\n        text.merge_metadata(new_metadata)\n        text.merge_metadata(Metadata.from_yaml(ai_metadata))\n        return text\n\n    def merge_metadata(self, new_metadata: Metadata, override=False) -&gt; None:\n        \"\"\"\n        Merge new metadata with existing metadata.\n\n        For now, performs simple dict-like union (|=) but can be extended \n        to handle more complex merging logic in the future (e.g., merging \n        nested structures, handling conflicts, merging arrays).\n\n        Args:\n        new_metadata: Metadata to merge with existing metadata\n        override: If True, new_metadata values override existing values\n                            If False, existing values are preserved\n        \"\"\"\n        # Currently using simple dict union\n        # Future implementations might handle:\n        # - Deep merging of nested structures\n        # - Special handling of specific fields\n        # - Array/list merging strategies\n        # - Conflict resolution\n        # - Metadata versioning\n        if not new_metadata:\n            return\n\n        if override:\n            self._metadata |= new_metadata  # new overrides existing\n        else:\n            self._metadata = new_metadata | self._metadata # existing values preserved\n\n        logger.debug(\"Merging new metadata into TextObject\")\n\n    def update_metadata(self, **kwargs) -&gt; None:\n        \"\"\"Update metadata with new key-value pairs.\"\"\"\n        new_metadata = Metadata(kwargs)\n        self.merge_metadata(new_metadata)\n\n    def validate_sections(self) -&gt; None:\n        \"\"\"Basic validation of section integrity.\"\"\"\n        if not self._sections:\n            raise ValueError(\"No sections set.\")\n\n        # Check section ordering and bounds\n        for i, section in enumerate(self._sections):\n            if section.section_range.start &lt; 1:\n                logger.warning(f\"Section {i}: start line must be &gt;= 1\")\n            if section.section_range.start &gt; self.num_text.size:\n                logger.warning(f\"Section {i}: start line exceeds text length\")\n            if i &gt; 0 and \\\n                section.section_range.start &lt;= self._sections[i-1].section_range.start:\n                logger.warning(f\"Section {i}: non-sequential start line\")\n\n    def get_section_content(self, index: int) -&gt; str:     \n        if not self._sections:\n            raise ValueError(\"No Sections available.\")\n        \"\"\"Get content for a section.\"\"\"            \n        if index &lt; 0 or index &gt;= len(self._sections):\n            raise IndexError(\"Section index out of range\")\n\n        section = self._sections[index]\n        return self.num_text.get_segment(\n            section.section_range.start, \n            section.section_range.end\n        )\n\n    def export_info(self, source_file: Optional[Path] = None) -&gt; TextObjectInfo:\n        \"\"\"Export serializable state.\"\"\"\n        if source_file:\n            source_file = source_file.resolve() # use absolute path for info\n\n        return TextObjectInfo(\n            source_file=source_file,\n            language=self.language,\n            sections=self.sections,\n            metadata=self.metadata\n        )\n\n    @classmethod\n    def from_info(\n        cls, \n        info: TextObjectInfo, \n        metadata: Metadata, \n        num_text: 'NumberedText'\n        ) -&gt; 'TextObject':\n        \"\"\"Create TextObject from info and content.\"\"\"\n        text_obj = cls(\n            num_text=num_text, \n            language=info.language, \n            sections=info.sections, \n            metadata=info.metadata\n            )\n\n        text_obj.merge_metadata(metadata)\n        return text_obj\n\n    @classmethod\n    def from_text_file(\n        cls,\n        file: Path\n    ) -&gt; 'TextObject':\n        text_str = read_str_from_file(file)\n        return cls.from_str(text_str)\n\n    @classmethod\n    def from_section_file(\n        cls, \n        section_file: Path, \n        source: Optional[str] = None\n        ) -&gt; 'TextObject':\n        \"\"\"\n        Create TextObject from a section info file, loading content from source_file.\n        Metadata is extracted from the source_file or from content.\n\n        Args:\n            section_file: Path to JSON file containing TextObjectInfo\n            source: Optional source string in case no source file is found.\n\n        Returns:\n            TextObject instance\n\n        Raises:\n            ValueError: If source_file is missing from section info\n            FileNotFoundError: If either section_file or source_file not found\n        \"\"\"\n        # Check section file exists\n        if not section_file.exists():\n            raise FileNotFoundError(f\"Section file not found: {section_file}\")\n\n        # Load and parse section info\n        info = TextObjectInfo.model_validate_json(read_str_from_file(section_file))\n\n        if not source:  # passed content always takes precedence over source_file\n            # check if source file exists\n            if not info.source_file:\n                raise ValueError(f\"No content available: no source_file specified \"\n                                 f\"in section info: {section_file}\")\n\n            source_path = Path(info.source_file)\n            if not source_path.exists():\n                raise FileNotFoundError(\n                    f\"No content available: Source file not found: {source_path}\"\n                    )\n\n            # Load source from path\n            source = read_str_from_file(source_path)\n\n        metadata, content = Frontmatter.extract(source)\n\n        # Create TextObject\n        return cls.from_info(info=info, \n                             metadata=metadata, \n                             num_text=NumberedText(content)\n                             )\n\n    def save(\n        self,\n        path: Path,\n        output_format: StorageFormatType = StorageFormat.TEXT,\n        source_file: Optional[Path] = None,\n        pretty: bool = True\n        ) -&gt; None:\n        \"\"\"\n        Save TextObject to file in specified format.\n\n        Args:\n            path: Output file path\n            output_format: \"text\" for full content+metadata or \"json\" for serialized state\n            source_file: Optional source file to record in metadata\n            pretty: For JSON output, whether to pretty print\n        \"\"\"\n        if isinstance(output_format, str):\n            output_format = StorageFormat(output_format)\n\n        if output_format == StorageFormat.TEXT:\n            # Full text output with metadata as frontmatter\n            write_str_to_file(path, str(self))\n\n        elif output_format == StorageFormat.JSON:\n            # Export serializable state\n            info = self.export_info(source_file)\n            json_str = info.model_dump_json(indent=2 if pretty else None)\n            write_str_to_file(path, json_str)\n\n    @classmethod\n    def load(\n        cls,\n        path: Path,\n        config: Optional[LoadConfig] = None\n    ) -&gt; 'TextObject':\n        \"\"\"\n        Load TextObject from file with optional configuration.\n\n        Args:\n            path: Input file path\n            config: Optional loading configuration. If not provided,\n                loads directly from text file.\n\n        Returns:\n            TextObject instance\n\n        Usage:\n            # Load from text file with frontmatter\n            obj = TextObject.load(Path(\"content.txt\"))\n\n            # Load state from JSON with source content string\n            config = LoadConfig(\n                format=StorageFormat.JSON,\n                source_content=\"Text content...\"\n            )\n            obj = TextObject.load(Path(\"state.json\"), config)\n\n            # Load state from JSON with source content file\n            config = LoadConfig(\n                format=StorageFormat.JSON,\n                source_content=Path(\"content.txt\")\n            )\n            obj = TextObject.load(Path(\"state.json\"), config)\n        \"\"\"\n        # Use default config if none provided\n        config = config or LoadConfig()\n\n        if config.format == StorageFormat.TEXT:\n            return cls.from_text_file(path)\n\n        elif config.format == StorageFormat.JSON:\n            return cls.from_section_file(path, source=config.get_source_text())\n\n        else:\n            raise ValueError(\"Unknown load configuration format.\")\n\n    def transform(\n        self,\n        data_str: Optional[str] = None,\n        language: Optional[str] = None, \n        metadata: Optional[Metadata] = None,\n        process_metadata: Optional[ProcessMetadata] = None,\n        sections: Optional[List[SectionObject]] = None\n    ) -&gt; Self:\n        \"\"\"Update TextObject content and metadata in place.\n\n        Optionally modifies the object's content, language, and adds process tracking.\n        Process history is maintained in metadata.\n\n        Args:\n            data_str: New text content\n            language: New language code  \n            metadata: Metadata to merge into the object\n            process_metadata: Identifier and details for the process performed\n            sections: Optional replacement list of sections\n        \"\"\"\n        # Update potentially changed elements\n        if data_str:\n            self.num_text = NumberedText(data_str)\n        if language:\n            self.language = language\n        if metadata:\n            self.merge_metadata(metadata)\n        if process_metadata:    \n            self._metadata.add_process_info(process_metadata)\n        if sections:\n            self._sections = sections\n\n        return self\n\n    @property\n    def metadata(self) -&gt; Metadata:\n        \"\"\"Access to metadata dictionary.\"\"\"\n        return self._metadata  \n\n    @property\n    def section_count(self) -&gt; int:\n        return len(self._sections) if self._sections else 0\n\n    @property\n    def last_line_num(self) -&gt; int:\n        return self.num_text.size\n\n    @property\n    def sections(self) -&gt; List[SectionObject]:\n        \"\"\"Access to sections list.\"\"\"\n        return self._sections or []\n\n    @property\n    def content(self) -&gt; str:\n        return self.num_text.content\n\n    @property\n    def metadata_str(self) -&gt; str:\n        return self.metadata.to_yaml()\n\n    @property\n    def numbered_content(self) -&gt; str:\n        return self.num_text.numbered_content\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.content","title":"<code>content</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.language","title":"<code>language = language or get_language_code_from_text(num_text.content)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.last_line_num","title":"<code>last_line_num</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Access to metadata dictionary.</p>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.metadata_str","title":"<code>metadata_str</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.num_text","title":"<code>num_text = num_text</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.numbered_content","title":"<code>numbered_content</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.section_count","title":"<code>section_count</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.sections","title":"<code>sections</code>  <code>property</code>","text":"<p>Access to sections list.</p>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.__init__","title":"<code>__init__(num_text, language=None, sections=None, metadata=None)</code>","text":"<p>Initialize a TextObject with content and optional organizing components.</p> <p>Parameters:</p> Name Type Description Default <code>num_text</code> <code>NumberedText</code> <p>Text content with line numbering</p> required <code>language</code> <code>Optional[str]</code> <p>ISO 639-1 language code. If None, auto-detected from content</p> <code>None</code> <code>sections</code> <code>Optional[List[SectionObject]]</code> <p>Initial sections defining text organization. If None,        text is considered un-sectioned.</p> <code>None</code> <code>metadata</code> <code>Optional[Metadata]</code> <p>Initial metadata. If None, creates empty metadata container</p> <code>None</code> Note <p>Until sections are established, section-based methods will raise a value error if called.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def __init__(self, \n    num_text: NumberedText, \n    language: Optional[str] = None, \n    sections: Optional[List[SectionObject]] = None,\n    metadata: Optional[Metadata] = None):\n    \"\"\"\n    Initialize a TextObject with content and optional organizing components.\n\n    Args:\n        num_text: Text content with line numbering\n        language: ISO 639-1 language code. If None, auto-detected from content\n        sections: Initial sections defining text organization. If None, \n                  text is considered un-sectioned.\n        metadata: Initial metadata. If None, creates empty metadata container\n\n    Note:\n        Until sections are established, section-based methods will raise a value\n        error if called.\n    \"\"\"\n    self.num_text = num_text\n    self.language = language or get_language_code_from_text(num_text.content)\n    self._sections = sections or []\n    self._metadata = metadata or Metadata()\n\n    if sections:\n        self.validate_sections()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate through sections, yielding full section information.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def __iter__(self) -&gt; Iterator[SectionEntry]:\n    \"\"\"Iterate through sections, yielding full section information.\"\"\"\n    if not self._sections:\n        raise ValueError(\"No Sections available.\")\n\n    for i, section in enumerate(self._sections):\n        content = self.num_text.get_segment(\n            section.section_range.start, \n            section.section_range.end\n        )\n        yield SectionEntry(\n            number=i+1,\n            title=section.title,\n            range=section.section_range,\n            content=content\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.__str__","title":"<code>__str__()</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def __str__(self) -&gt; str:\n    return Frontmatter.embed(self.metadata, self.content)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.export_info","title":"<code>export_info(source_file=None)</code>","text":"<p>Export serializable state.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def export_info(self, source_file: Optional[Path] = None) -&gt; TextObjectInfo:\n    \"\"\"Export serializable state.\"\"\"\n    if source_file:\n        source_file = source_file.resolve() # use absolute path for info\n\n    return TextObjectInfo(\n        source_file=source_file,\n        language=self.language,\n        sections=self.sections,\n        metadata=self.metadata\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.from_info","title":"<code>from_info(info, metadata, num_text)</code>  <code>classmethod</code>","text":"<p>Create TextObject from info and content.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_info(\n    cls, \n    info: TextObjectInfo, \n    metadata: Metadata, \n    num_text: 'NumberedText'\n    ) -&gt; 'TextObject':\n    \"\"\"Create TextObject from info and content.\"\"\"\n    text_obj = cls(\n        num_text=num_text, \n        language=info.language, \n        sections=info.sections, \n        metadata=info.metadata\n        )\n\n    text_obj.merge_metadata(metadata)\n    return text_obj\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.from_response","title":"<code>from_response(response, existing_metadata, num_text)</code>  <code>classmethod</code>","text":"<p>Create TextObject from AI response format.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_response(\n    cls, \n    response: AIResponse,\n    existing_metadata: Metadata,\n    num_text: 'NumberedText'\n) -&gt; 'TextObject':\n    \"\"\"Create TextObject from AI response format.\"\"\"\n    # Create metadata from response\n    ai_metadata = response.document_metadata\n    new_metadata = Metadata({\n        \"ai_summary\": response.document_summary,\n        \"ai_concepts\": response.key_concepts,\n        \"ai_context\": response.narrative_context\n    })\n\n    # Convert LogicalSections to SectionObjects\n    sections = cls._build_section_objects(\n        response.sections, \n        num_text.size,\n    )\n\n    text = cls(\n        num_text=num_text,\n        language=response.language,\n        sections=sections,\n        metadata=existing_metadata\n    )\n    text.merge_metadata(new_metadata)\n    text.merge_metadata(Metadata.from_yaml(ai_metadata))\n    return text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.from_section_file","title":"<code>from_section_file(section_file, source=None)</code>  <code>classmethod</code>","text":"<p>Create TextObject from a section info file, loading content from source_file. Metadata is extracted from the source_file or from content.</p> <p>Parameters:</p> Name Type Description Default <code>section_file</code> <code>Path</code> <p>Path to JSON file containing TextObjectInfo</p> required <code>source</code> <code>Optional[str]</code> <p>Optional source string in case no source file is found.</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source_file is missing from section info</p> <code>FileNotFoundError</code> <p>If either section_file or source_file not found</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_section_file(\n    cls, \n    section_file: Path, \n    source: Optional[str] = None\n    ) -&gt; 'TextObject':\n    \"\"\"\n    Create TextObject from a section info file, loading content from source_file.\n    Metadata is extracted from the source_file or from content.\n\n    Args:\n        section_file: Path to JSON file containing TextObjectInfo\n        source: Optional source string in case no source file is found.\n\n    Returns:\n        TextObject instance\n\n    Raises:\n        ValueError: If source_file is missing from section info\n        FileNotFoundError: If either section_file or source_file not found\n    \"\"\"\n    # Check section file exists\n    if not section_file.exists():\n        raise FileNotFoundError(f\"Section file not found: {section_file}\")\n\n    # Load and parse section info\n    info = TextObjectInfo.model_validate_json(read_str_from_file(section_file))\n\n    if not source:  # passed content always takes precedence over source_file\n        # check if source file exists\n        if not info.source_file:\n            raise ValueError(f\"No content available: no source_file specified \"\n                             f\"in section info: {section_file}\")\n\n        source_path = Path(info.source_file)\n        if not source_path.exists():\n            raise FileNotFoundError(\n                f\"No content available: Source file not found: {source_path}\"\n                )\n\n        # Load source from path\n        source = read_str_from_file(source_path)\n\n    metadata, content = Frontmatter.extract(source)\n\n    # Create TextObject\n    return cls.from_info(info=info, \n                         metadata=metadata, \n                         num_text=NumberedText(content)\n                         )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.from_str","title":"<code>from_str(text, language=None, sections=None, metadata=None)</code>  <code>classmethod</code>","text":"<p>Create a TextObject from a string, extracting any frontmatter.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text string, potentially containing frontmatter</p> required <code>language</code> <code>Optional[str]</code> <p>ISO language code</p> <code>None</code> <code>sections</code> <code>Optional[List[SectionObject]]</code> <p>List of section objects</p> <code>None</code> <code>metadata</code> <code>Optional[Metadata]</code> <p>Optional base metadata to merge with frontmatter</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject instance with combined metadata</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_str(\n    cls,\n    text: str,\n    language: Optional[str] = None,\n    sections: Optional[List[SectionObject]] = None,\n    metadata: Optional[Metadata] = None\n) -&gt; 'TextObject':\n    \"\"\"\n    Create a TextObject from a string, extracting any frontmatter.\n\n    Args:\n        text: Input text string, potentially containing frontmatter\n        language: ISO language code\n        sections: List of section objects\n        metadata: Optional base metadata to merge with frontmatter\n\n    Returns:\n        TextObject instance with combined metadata\n    \"\"\"\n    # Extract any frontmatter and merge with provided metadata\n    frontmatter_metadata, content = Frontmatter.extract(text)\n\n    # Create NumberedText from content without frontmatter\n    numbered_text = NumberedText(content)\n\n    obj = cls(\n        num_text=numbered_text,\n        language=language,\n        sections=sections,\n        metadata=frontmatter_metadata\n    )\n    if metadata:\n        obj.merge_metadata(metadata)\n\n    return obj\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.from_text_file","title":"<code>from_text_file(file)</code>  <code>classmethod</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_text_file(\n    cls,\n    file: Path\n) -&gt; 'TextObject':\n    text_str = read_str_from_file(file)\n    return cls.from_str(text_str)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.get_section_content","title":"<code>get_section_content(index)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def get_section_content(self, index: int) -&gt; str:     \n    if not self._sections:\n        raise ValueError(\"No Sections available.\")\n    \"\"\"Get content for a section.\"\"\"            \n    if index &lt; 0 or index &gt;= len(self._sections):\n        raise IndexError(\"Section index out of range\")\n\n    section = self._sections[index]\n    return self.num_text.get_segment(\n        section.section_range.start, \n        section.section_range.end\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.load","title":"<code>load(path, config=None)</code>  <code>classmethod</code>","text":"<p>Load TextObject from file with optional configuration.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Input file path</p> required <code>config</code> <code>Optional[LoadConfig]</code> <p>Optional loading configuration. If not provided, loads directly from text file.</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject instance</p> Usage Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    path: Path,\n    config: Optional[LoadConfig] = None\n) -&gt; 'TextObject':\n    \"\"\"\n    Load TextObject from file with optional configuration.\n\n    Args:\n        path: Input file path\n        config: Optional loading configuration. If not provided,\n            loads directly from text file.\n\n    Returns:\n        TextObject instance\n\n    Usage:\n        # Load from text file with frontmatter\n        obj = TextObject.load(Path(\"content.txt\"))\n\n        # Load state from JSON with source content string\n        config = LoadConfig(\n            format=StorageFormat.JSON,\n            source_content=\"Text content...\"\n        )\n        obj = TextObject.load(Path(\"state.json\"), config)\n\n        # Load state from JSON with source content file\n        config = LoadConfig(\n            format=StorageFormat.JSON,\n            source_content=Path(\"content.txt\")\n        )\n        obj = TextObject.load(Path(\"state.json\"), config)\n    \"\"\"\n    # Use default config if none provided\n    config = config or LoadConfig()\n\n    if config.format == StorageFormat.TEXT:\n        return cls.from_text_file(path)\n\n    elif config.format == StorageFormat.JSON:\n        return cls.from_section_file(path, source=config.get_source_text())\n\n    else:\n        raise ValueError(\"Unknown load configuration format.\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.load--load-from-text-file-with-frontmatter","title":"Load from text file with frontmatter","text":"<p>obj = TextObject.load(Path(\"content.txt\"))</p>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.load--load-state-from-json-with-source-content-string","title":"Load state from JSON with source content string","text":"<p>config = LoadConfig(     format=StorageFormat.JSON,     source_content=\"Text content...\" ) obj = TextObject.load(Path(\"state.json\"), config)</p>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.load--load-state-from-json-with-source-content-file","title":"Load state from JSON with source content file","text":"<p>config = LoadConfig(     format=StorageFormat.JSON,     source_content=Path(\"content.txt\") ) obj = TextObject.load(Path(\"state.json\"), config)</p>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.merge_metadata","title":"<code>merge_metadata(new_metadata, override=False)</code>","text":"<p>Merge new metadata with existing metadata.</p> <p>For now, performs simple dict-like union (|=) but can be extended  to handle more complex merging logic in the future (e.g., merging  nested structures, handling conflicts, merging arrays).</p> <p>Args: new_metadata: Metadata to merge with existing metadata override: If True, new_metadata values override existing values                     If False, existing values are preserved</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def merge_metadata(self, new_metadata: Metadata, override=False) -&gt; None:\n    \"\"\"\n    Merge new metadata with existing metadata.\n\n    For now, performs simple dict-like union (|=) but can be extended \n    to handle more complex merging logic in the future (e.g., merging \n    nested structures, handling conflicts, merging arrays).\n\n    Args:\n    new_metadata: Metadata to merge with existing metadata\n    override: If True, new_metadata values override existing values\n                        If False, existing values are preserved\n    \"\"\"\n    # Currently using simple dict union\n    # Future implementations might handle:\n    # - Deep merging of nested structures\n    # - Special handling of specific fields\n    # - Array/list merging strategies\n    # - Conflict resolution\n    # - Metadata versioning\n    if not new_metadata:\n        return\n\n    if override:\n        self._metadata |= new_metadata  # new overrides existing\n    else:\n        self._metadata = new_metadata | self._metadata # existing values preserved\n\n    logger.debug(\"Merging new metadata into TextObject\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.save","title":"<code>save(path, output_format=StorageFormat.TEXT, source_file=None, pretty=True)</code>","text":"<p>Save TextObject to file in specified format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output file path</p> required <code>output_format</code> <code>StorageFormatType</code> <p>\"text\" for full content+metadata or \"json\" for serialized state</p> <code>TEXT</code> <code>source_file</code> <code>Optional[Path]</code> <p>Optional source file to record in metadata</p> <code>None</code> <code>pretty</code> <code>bool</code> <p>For JSON output, whether to pretty print</p> <code>True</code> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def save(\n    self,\n    path: Path,\n    output_format: StorageFormatType = StorageFormat.TEXT,\n    source_file: Optional[Path] = None,\n    pretty: bool = True\n    ) -&gt; None:\n    \"\"\"\n    Save TextObject to file in specified format.\n\n    Args:\n        path: Output file path\n        output_format: \"text\" for full content+metadata or \"json\" for serialized state\n        source_file: Optional source file to record in metadata\n        pretty: For JSON output, whether to pretty print\n    \"\"\"\n    if isinstance(output_format, str):\n        output_format = StorageFormat(output_format)\n\n    if output_format == StorageFormat.TEXT:\n        # Full text output with metadata as frontmatter\n        write_str_to_file(path, str(self))\n\n    elif output_format == StorageFormat.JSON:\n        # Export serializable state\n        info = self.export_info(source_file)\n        json_str = info.model_dump_json(indent=2 if pretty else None)\n        write_str_to_file(path, json_str)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.transform","title":"<code>transform(data_str=None, language=None, metadata=None, process_metadata=None, sections=None)</code>","text":"<p>Update TextObject content and metadata in place.</p> <p>Optionally modifies the object's content, language, and adds process tracking. Process history is maintained in metadata.</p> <p>Parameters:</p> Name Type Description Default <code>data_str</code> <code>Optional[str]</code> <p>New text content</p> <code>None</code> <code>language</code> <code>Optional[str]</code> <p>New language code  </p> <code>None</code> <code>metadata</code> <code>Optional[Metadata]</code> <p>Metadata to merge into the object</p> <code>None</code> <code>process_metadata</code> <code>Optional[ProcessMetadata]</code> <p>Identifier and details for the process performed</p> <code>None</code> <code>sections</code> <code>Optional[List[SectionObject]]</code> <p>Optional replacement list of sections</p> <code>None</code> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def transform(\n    self,\n    data_str: Optional[str] = None,\n    language: Optional[str] = None, \n    metadata: Optional[Metadata] = None,\n    process_metadata: Optional[ProcessMetadata] = None,\n    sections: Optional[List[SectionObject]] = None\n) -&gt; Self:\n    \"\"\"Update TextObject content and metadata in place.\n\n    Optionally modifies the object's content, language, and adds process tracking.\n    Process history is maintained in metadata.\n\n    Args:\n        data_str: New text content\n        language: New language code  \n        metadata: Metadata to merge into the object\n        process_metadata: Identifier and details for the process performed\n        sections: Optional replacement list of sections\n    \"\"\"\n    # Update potentially changed elements\n    if data_str:\n        self.num_text = NumberedText(data_str)\n    if language:\n        self.language = language\n    if metadata:\n        self.merge_metadata(metadata)\n    if process_metadata:    \n        self._metadata.add_process_info(process_metadata)\n    if sections:\n        self._sections = sections\n\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.update_metadata","title":"<code>update_metadata(**kwargs)</code>","text":"<p>Update metadata with new key-value pairs.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def update_metadata(self, **kwargs) -&gt; None:\n    \"\"\"Update metadata with new key-value pairs.\"\"\"\n    new_metadata = Metadata(kwargs)\n    self.merge_metadata(new_metadata)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObject.validate_sections","title":"<code>validate_sections()</code>","text":"<p>Basic validation of section integrity.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def validate_sections(self) -&gt; None:\n    \"\"\"Basic validation of section integrity.\"\"\"\n    if not self._sections:\n        raise ValueError(\"No sections set.\")\n\n    # Check section ordering and bounds\n    for i, section in enumerate(self._sections):\n        if section.section_range.start &lt; 1:\n            logger.warning(f\"Section {i}: start line must be &gt;= 1\")\n        if section.section_range.start &gt; self.num_text.size:\n            logger.warning(f\"Section {i}: start line exceeds text length\")\n        if i &gt; 0 and \\\n            section.section_range.start &lt;= self._sections[i-1].section_range.start:\n            logger.warning(f\"Section {i}: non-sequential start line\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObjectInfo","title":"<code>TextObjectInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Serializable information about a text and its sections.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class TextObjectInfo(BaseModel):\n    \"\"\"Serializable information about a text and its sections.\"\"\"\n    source_file: Optional[Path] = None  # Original text file path\n    language: str\n    sections: List[SectionObject]\n    metadata: Metadata\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Ensure metadata is always a Metadata instance after initialization.\"\"\"\n        if isinstance(self.metadata, dict):\n            self.metadata = Metadata(self.metadata)\n        elif not isinstance(self.metadata, Metadata):\n            raise ValueError(f\"Unexpected type for metadata: {type(self.metadata)}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.TextObjectInfo.language","title":"<code>language</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObjectInfo.metadata","title":"<code>metadata</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObjectInfo.sections","title":"<code>sections</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObjectInfo.source_file","title":"<code>source_file = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.TextObjectInfo.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Ensure metadata is always a Metadata instance after initialization.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Ensure metadata is always a Metadata instance after initialization.\"\"\"\n    if isinstance(self.metadata, dict):\n        self.metadata = Metadata(self.metadata)\n    elif not isinstance(self.metadata, Metadata):\n        raise ValueError(f\"Unexpected type for metadata: {type(self.metadata)}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.__dir__","title":"<code>__dir__()</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/__init__.py</code> <pre><code>def __dir__() -&gt; list[str]:\n    return sorted(__all__)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.__getattr__","title":"<code>__getattr__(name)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/__init__.py</code> <pre><code>def __getattr__(name: str) -&gt; Any:\n    module_path = _LAZY_ATTRS.get(name)\n    if not module_path:\n        raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n    module = import_module(module_path)\n    value = getattr(module, name)\n    globals()[name] = value\n    return value\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.find_sections","title":"<code>find_sections(text, source_language=None, section_pattern=None, section_model=None, max_tokens=DEFAULT_SECTION_RESULT_MAX_SIZE, section_count=None, review_count=DEFAULT_REVIEW_COUNT, template_dict=None)</code>","text":"<p>High-level function for generating text sections.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>TextObject</code> <p>Input text</p> required <code>source_language</code> <code>Optional[str]</code> <p>ISO 639-1 language code</p> <code>None</code> <code>section_pattern</code> <code>Optional[Prompt]</code> <p>Optional custom pattern (uses default if None)</p> <code>None</code> <code>section_model</code> <code>Optional[str]</code> <p>Optional model identifier</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens for response</p> <code>DEFAULT_SECTION_RESULT_MAX_SIZE</code> <code>section_count</code> <code>Optional[int]</code> <p>Target number of sections</p> <code>None</code> <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> <code>template_dict</code> <code>Optional[Dict[str, str]]</code> <p>Optional additional template variables</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject containing section breakdown</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def find_sections(\n    text: TextObject,\n    source_language: Optional[str] = None,\n    section_pattern: Optional[Prompt] = None,\n    section_model: Optional[str] = None,\n    max_tokens: int = DEFAULT_SECTION_RESULT_MAX_SIZE,\n    section_count: Optional[int] = None,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n    template_dict: Optional[Dict[str, str]] = None,\n) -&gt; TextObject:\n    \"\"\"\n    High-level function for generating text sections.\n\n    Args:\n        text: Input text\n        source_language: ISO 639-1 language code\n        section_pattern: Optional custom pattern (uses default if None)\n        section_model: Optional model identifier\n        max_tokens: Maximum tokens for response\n        section_count: Target number of sections\n        review_count: Number of review passes\n        template_dict: Optional additional template variables\n\n    Returns:\n        TextObject containing section breakdown\n    \"\"\"\n    if section_pattern is None:\n        section_pattern = get_pattern(DEFAULT_SECTION_PATTERN)\n        logger.debug(f\"Using default section pattern: {DEFAULT_SECTION_PATTERN}.\")\n\n    section_scanner = OpenAIProcessor(model=section_model, max_tokens=max_tokens)\n    parser = SectionParser(\n        section_scanner=section_scanner,\n        section_pattern=section_pattern,\n        review_count=review_count,\n    )\n\n    process_metadata = ProcessMetadata(\n            step=\"find_sections\",\n            processor=\"SectionProcessor\", \n            source_language=source_language,\n            pattern=section_pattern.name,\n            model=section_model,\n            section_count=section_count,\n            review_count=review_count,\n            template_dict=template_dict,\n        )\n\n    result_text = parser.find_sections(\n        text,\n        section_count_target=section_count,\n        template_dict=template_dict,\n    )\n    result_text.transform(process_metadata=process_metadata)\n    return result_text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.get_pattern","title":"<code>get_pattern(name)</code>","text":"<p>Get a pattern by name using the singleton PatternManager.</p> <p>This is a more efficient version that reuses a single PatternManager instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pattern to load</p> required <p>Returns:</p> Type Description <code>Prompt</code> <p>The loaded pattern</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If pattern name is invalid</p> <code>FileNotFoundError</code> <p>If pattern file doesn't exist</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def get_pattern(name: str) -&gt; Prompt:\n    \"\"\"\n    Get a pattern by name using the singleton PatternManager.\n\n    This is a more efficient version that reuses a single PatternManager instance.\n\n    Args:\n        name: Name of the pattern to load\n\n    Returns:\n        The loaded pattern\n\n    Raises:\n        ValueError: If pattern name is invalid\n        FileNotFoundError: If pattern file doesn't exist\n    \"\"\"\n    return LocalPromptManager().get_prompt(name)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.openai_process_text","title":"<code>openai_process_text(text_input, process_instructions, model=None, response_format=None, batch=False, max_tokens=0)</code>","text":"<p>postprocessing a transcription.</p> Source code in <code>src/tnh_scholar/ai_text_processing/openai_process_interface.py</code> <pre><code>def openai_process_text(\n    text_input: str,\n    process_instructions: str,\n    model: Optional[str] = None,\n    response_format: Optional[Type[BaseModel]] = None,\n    batch: bool = False,\n    max_tokens: int = 0,\n) -&gt; Union[BaseModel, str]:\n    \"\"\"postprocessing a transcription.\"\"\"\n\n    user_prompts = [text_input]\n    system_message = process_instructions\n\n    logger.debug(f\"OpenAI Process Text with process instructions:\\n{system_message}\")\n    if max_tokens == 0:\n        tokens = token_count(text_input)\n        max_tokens = tokens + TOKEN_BUFFER\n\n    model_name = model or \"default\"\n\n    logger.info(\n        f\"Open AI Text Processing{' as batch process' if batch else ''} \"\n        f\"with model '{model_name}' initiated.\\n\"\n        f\"Requesting a maximum of {max_tokens} tokens.\"\n    )\n\n    if batch:\n        return _run_batch_process_text(\n            user_prompts, system_message, max_tokens, model_name, response_format\n        )\n\n    completion_result = simple_completion(\n        system_message=system_message,\n        user_message=text_input,\n        model=model,\n        max_tokens=max_tokens,\n        response_model=response_format,\n    )\n    logger.info(\"Processing completed.\")\n    return completion_result\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.process_text","title":"<code>process_text(text, pattern, source_language=None, model=None, template_dict=None)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text(\n    text: TextObject,\n    pattern: Prompt,\n    source_language: Optional[str] = None,\n    model: Optional[str] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; TextObject:\n\n    if not model:\n        model = DEFAULT_OPENAI_MODEL\n\n    processor = GeneralProcessor(\n        processor=OpenAIProcessor(model),\n        source_language=source_language,\n        pattern=pattern,\n    )\n\n    process_metadata = ProcessMetadata(\n            step=\"process_text\",\n            processor=\"GeneralProcessor\",\n            pattern=pattern.name,\n            model=model,\n            template_dict=template_dict,\n        )\n\n    result = processor.process_text(\n        text, template_dict=template_dict\n    )\n    text.transform(data_str=result, process_metadata=process_metadata)\n    return text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.process_text_by_paragraphs","title":"<code>process_text_by_paragraphs(text, template_dict, pattern=None, model=None)</code>","text":"<p>High-level function for processing text paragraphs, yielding ProcessedSection objects. Assumes paragraphs are separated by newlines. Uses DEFAULT_XML_FORMAT_PATTERN as default pattern for text processing.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>TextObject</code> <p>TextObject to process</p> required <code>template_dict</code> <code>Dict[str, str]</code> <p>Dictionary for template substitution</p> required <code>pattern</code> <code>Optional[Prompt]</code> <p>Pattern object containing processing instructions</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Optional model identifier for processor</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Generator for ProcessedSection objects (one per paragraph)</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text_by_paragraphs(\n    text: TextObject,\n    template_dict: Dict[str, str],\n    pattern: Optional[Prompt] = None,\n    model: Optional[str] = None,\n) -&gt; Generator[ProcessedSection, None, None]:\n    \"\"\"\n    High-level function for processing text paragraphs, yielding ProcessedSection objects.\n    Assumes paragraphs are separated by newlines.\n    Uses DEFAULT_XML_FORMAT_PATTERN as default pattern for text processing.\n\n    Args:\n        text: TextObject to process\n        template_dict: Dictionary for template substitution\n        pattern: Pattern object containing processing instructions\n        model: Optional model identifier for processor\n\n    Returns:\n        Generator for ProcessedSection objects (one per paragraph)\n    \"\"\"\n    processor = OpenAIProcessor(model)\n\n    if not pattern:\n        pattern = get_pattern(DEFAULT_PARAGRAPH_FORMAT_PATTERN)\n\n    section_processor = SectionProcessor(processor, pattern, template_dict)\n\n    process_metadata = ProcessMetadata(\n        step=\"process_text_by_paragraphs\",\n        processor=\"SectionProcessor\",\n        pattern=pattern.name,\n        model=model,\n        template_dict=template_dict,\n    )\n\n    result = section_processor.process_paragraphs(text)\n\n    text.transform(process_metadata=process_metadata)\n\n    return result\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.process_text_by_sections","title":"<code>process_text_by_sections(text_object, template_dict, pattern, model=None)</code>","text":"<p>High-level function for processing text sections with configurable output handling.</p> <p>Parameters:</p> Name Type Description Default <code>text_object</code> <code>TextObject</code> <p>Object containing section definitions</p> required <code>pattern</code> <code>Prompt</code> <p>Pattern object containing processing instructions</p> required <code>template_dict</code> <code>Dict</code> <p>Dictionary for template substitution</p> required <code>model</code> <code>Optional[str]</code> <p>Optional model identifier for processor</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Generator for ProcessedSections</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text_by_sections(\n    text_object: TextObject,\n    template_dict: Dict,\n    pattern: Prompt,\n    model: Optional[str] = None,\n) -&gt; Generator[ProcessedSection, None, None]:\n    \"\"\"\n    High-level function for processing text sections with configurable output handling.\n\n    Args:\n        text_object: Object containing section definitions\n        pattern: Pattern object containing processing instructions\n        template_dict: Dictionary for template substitution\n        model: Optional model identifier for processor\n\n    Returns:\n        Generator for ProcessedSections\n    \"\"\"\n    processor = OpenAIProcessor(model)\n\n    section_processor = SectionProcessor(processor, pattern, template_dict)\n\n    process_metadata = ProcessMetadata(\n            step=\"process_text_by_sections\",\n            processor=\"SectionProcessor\",\n            pattern=pattern.name,\n            model=model,\n            template_dict=template_dict,\n        )\n    result = section_processor.process_sections(text_object)\n\n    text_object.transform(process_metadata=process_metadata)\n\n    return result\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.translate_text_by_lines","title":"<code>translate_text_by_lines(text, source_language=None, target_language=DEFAULT_TARGET_LANGUAGE, pattern=None, model=None, style=None, segment_size=None, context_lines=None, review_count=None, template_dict=None)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/line_translator.py</code> <pre><code>def translate_text_by_lines(\n    text: TextObject,\n    source_language: Optional[str] = None,\n    target_language: str = DEFAULT_TARGET_LANGUAGE,\n    pattern: Optional[Prompt] = None,\n    model: Optional[str] = None,\n    style: Optional[str] = None,\n    segment_size: Optional[int] = None,\n    context_lines: Optional[int] = None,\n    review_count: Optional[int] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; TextObject:\n\n    if source_language is None:\n        source_language = text.language\n\n    if pattern is None:\n        pattern = get_pattern(DEFAULT_TRANSLATION_PATTERN)\n\n    processor = OpenAIProcessor(model)\n\n    translator = LineTranslator(\n        processor=processor,\n        pattern=pattern,\n        style=style or DEFAULT_TRANSLATE_STYLE,\n        context_lines=context_lines or DEFAULT_TRANSLATE_CONTEXT_LINES,\n        review_count=review_count or DEFAULT_REVIEW_COUNT,\n    )\n\n    process_metadata = ProcessMetadata(\n            step=\"translation\",\n            processor=\"LineTranslator\",\n            model=processor.model,\n            source_language=source_language,\n            target_language=target_language,\n            segment_size=segment_size,\n            context_lines=translator.context_lines,\n            review_count=translator.review_count,\n            style=translator.style,\n            template_dict=template_dict,\n        )\n\n    text = translator.translate_text(\n        text,\n        source_language=source_language,\n        target_language=target_language,\n        segment_size=segment_size,\n        template_dict=template_dict,\n    )\n    return text.transform(process_metadata=process_metadata)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing","title":"<code>ai_text_processing</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_MIN_SECTION_COUNT","title":"<code>DEFAULT_MIN_SECTION_COUNT = 3</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_OPENAI_MODEL","title":"<code>DEFAULT_OPENAI_MODEL = 'gpt-4o'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_PARAGRAPH_FORMAT_PATTERN","title":"<code>DEFAULT_PARAGRAPH_FORMAT_PATTERN = 'default_xml_paragraph_format'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_PUNCTUATE_MODEL","title":"<code>DEFAULT_PUNCTUATE_MODEL = 'gpt-4o'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_PUNCTUATE_PATTERN","title":"<code>DEFAULT_PUNCTUATE_PATTERN = 'default_punctuate'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_PUNCTUATE_STYLE","title":"<code>DEFAULT_PUNCTUATE_STYLE = 'APA'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_REVIEW_COUNT","title":"<code>DEFAULT_REVIEW_COUNT = 5</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_SECTION_PATTERN","title":"<code>DEFAULT_SECTION_PATTERN = 'default_section'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_SECTION_RANGE_VAR","title":"<code>DEFAULT_SECTION_RANGE_VAR = 2</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_SECTION_RESULT_MAX_SIZE","title":"<code>DEFAULT_SECTION_RESULT_MAX_SIZE = 4000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_SECTION_TOKEN_SIZE","title":"<code>DEFAULT_SECTION_TOKEN_SIZE = 650</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.DEFAULT_XML_FORMAT_PATTERN","title":"<code>DEFAULT_XML_FORMAT_PATTERN = 'default_xml_format'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SECTION_SEGMENT_SIZE_WARNING_LIMIT","title":"<code>SECTION_SEGMENT_SIZE_WARNING_LIMIT = 5</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor","title":"<code>GeneralProcessor</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class GeneralProcessor:\n    def __init__(\n        self,\n        processor: TextProcessor,\n        pattern: Prompt,\n        source_language: Optional[str] = None,\n        review_count: int = DEFAULT_REVIEW_COUNT,\n    ):\n        \"\"\"\n        Initialize general processor.\n\n        Args:\n            processor: Implementation of TextProcessor\n            pattern: Pattern object containing processing instructions\n            source_language: ISO code for the source language\n            review_count: Number of review passes\n        \"\"\"\n\n        self.source_language = source_language\n        self.processor = processor\n        self.pattern = pattern\n        self.review_count = review_count\n\n    def process_text(\n        self,\n        text: TextObject,\n        template_dict: Optional[Dict] = None,\n    ) -&gt; str:\n        \"\"\"\n        process a text based on a pattern and source language.\n        \"\"\"\n\n        source_language = get_language_from_code(text.language)\n\n        template_values = {\n            \"metadata\": text.metadata_str,\n            \"source_language\": source_language,\n            \"review_count\": self.review_count,\n        }\n\n        if template_dict:\n            template_values |= template_dict\n\n        logger.info(\"Processing text...\")\n        instructions = self.pattern.apply_template(template_values)\n\n        logger.debug(f\"Process instructions:\\n{instructions}\")\n\n        result = self.processor.process_text(text.content, instructions)\n\n        logger.info(\"Processing completed.\")\n\n        # normalize newline spacing to two newline between lines and return\n        # commented out to allow pattern to dictate newlines:\n        # return normalize_newlines(text)\n        return result\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.pattern","title":"<code>pattern = pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.processor","title":"<code>processor = processor</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.review_count","title":"<code>review_count = review_count</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.source_language","title":"<code>source_language = source_language</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.__init__","title":"<code>__init__(processor, pattern, source_language=None, review_count=DEFAULT_REVIEW_COUNT)</code>","text":"<p>Initialize general processor.</p> <p>Parameters:</p> Name Type Description Default <code>processor</code> <code>TextProcessor</code> <p>Implementation of TextProcessor</p> required <code>pattern</code> <code>Prompt</code> <p>Pattern object containing processing instructions</p> required <code>source_language</code> <code>Optional[str]</code> <p>ISO code for the source language</p> <code>None</code> <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(\n    self,\n    processor: TextProcessor,\n    pattern: Prompt,\n    source_language: Optional[str] = None,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n):\n    \"\"\"\n    Initialize general processor.\n\n    Args:\n        processor: Implementation of TextProcessor\n        pattern: Pattern object containing processing instructions\n        source_language: ISO code for the source language\n        review_count: Number of review passes\n    \"\"\"\n\n    self.source_language = source_language\n    self.processor = processor\n    self.pattern = pattern\n    self.review_count = review_count\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.GeneralProcessor.process_text","title":"<code>process_text(text, template_dict=None)</code>","text":"<p>process a text based on a pattern and source language.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text(\n    self,\n    text: TextObject,\n    template_dict: Optional[Dict] = None,\n) -&gt; str:\n    \"\"\"\n    process a text based on a pattern and source language.\n    \"\"\"\n\n    source_language = get_language_from_code(text.language)\n\n    template_values = {\n        \"metadata\": text.metadata_str,\n        \"source_language\": source_language,\n        \"review_count\": self.review_count,\n    }\n\n    if template_dict:\n        template_values |= template_dict\n\n    logger.info(\"Processing text...\")\n    instructions = self.pattern.apply_template(template_values)\n\n    logger.debug(f\"Process instructions:\\n{instructions}\")\n\n    result = self.processor.process_text(text.content, instructions)\n\n    logger.info(\"Processing completed.\")\n\n    # normalize newline spacing to two newline between lines and return\n    # commented out to allow pattern to dictate newlines:\n    # return normalize_newlines(text)\n    return result\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.OpenAIProcessor","title":"<code>OpenAIProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>OpenAI-based text processor implementation.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class OpenAIProcessor(TextProcessor):\n    \"\"\"OpenAI-based text processor implementation.\"\"\"\n    def __init__(self, model: Optional[str] = None, max_tokens: int = 0):\n        if not model:\n            model = DEFAULT_OPENAI_MODEL\n        self.model = model\n        self.max_tokens = max_tokens\n\n    def process_text(\n        self,\n        input_str: str,\n        instructions: str,\n        response_format: Optional[Type[BaseModel]] = None,\n        max_tokens: int = 0,\n        **kwargs: Any,\n    ) -&gt; ProcessorResult:\n        \"\"\"Process text using OpenAI API with optional structured output.\"\"\"\n\n        if max_tokens == 0 and self.max_tokens &gt; 0:\n            max_tokens = self.max_tokens\n\n        return openai_process_text(\n            input_str,\n            instructions,\n            model=self.model,\n            max_tokens=max_tokens,\n            response_format=response_format,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.OpenAIProcessor.max_tokens","title":"<code>max_tokens = max_tokens</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.OpenAIProcessor.model","title":"<code>model = model</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.OpenAIProcessor.__init__","title":"<code>__init__(model=None, max_tokens=0)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(self, model: Optional[str] = None, max_tokens: int = 0):\n    if not model:\n        model = DEFAULT_OPENAI_MODEL\n    self.model = model\n    self.max_tokens = max_tokens\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.OpenAIProcessor.process_text","title":"<code>process_text(input_str, instructions, response_format=None, max_tokens=0, **kwargs)</code>","text":"<p>Process text using OpenAI API with optional structured output.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text(\n    self,\n    input_str: str,\n    instructions: str,\n    response_format: Optional[Type[BaseModel]] = None,\n    max_tokens: int = 0,\n    **kwargs: Any,\n) -&gt; ProcessorResult:\n    \"\"\"Process text using OpenAI API with optional structured output.\"\"\"\n\n    if max_tokens == 0 and self.max_tokens &gt; 0:\n        max_tokens = self.max_tokens\n\n    return openai_process_text(\n        input_str,\n        instructions,\n        model=self.model,\n        max_tokens=max_tokens,\n        response_format=response_format,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection","title":"<code>ProcessedSection</code>  <code>dataclass</code>","text":"<p>Represents a processed section of text with its metadata.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>@dataclass\nclass ProcessedSection:\n    \"\"\"Represents a processed section of text with its metadata.\"\"\"\n    title: str\n    original_str: str\n    processed_str: str\n    metadata: Dict = field(default_factory=dict)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.metadata","title":"<code>metadata = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.original_str","title":"<code>original_str</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.processed_str","title":"<code>processed_str</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.title","title":"<code>title</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.ProcessedSection.__init__","title":"<code>__init__(title, original_str, processed_str, metadata=dict())</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser","title":"<code>SectionParser</code>","text":"<p>Generates structured section breakdowns of text content.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class SectionParser:\n    \"\"\"Generates structured section breakdowns of text content.\"\"\"\n\n    def __init__(\n        self,\n        section_scanner: TextProcessor,\n        section_pattern: Prompt,\n        review_count: int = DEFAULT_REVIEW_COUNT,\n    ):\n        \"\"\"\n        Initialize section generator.\n\n        Args:\n            section_scanner: Text processor used to extract sections\n            section_pattern: Pattern object containing section generation instructions\n            review_count: Number of review passes\n        \"\"\"\n        self.section_scanner = section_scanner\n        self.section_pattern = section_pattern\n        self.review_count = review_count\n\n    def find_sections(\n        self,\n        text: TextObject,\n        section_count_target: Optional[int] = None,\n        segment_size_target: Optional[int] = None,\n        template_dict: Optional[Dict[str, str]] = None,\n    ) -&gt; TextObject:\n        \"\"\"\n        Generate section breakdown of input text. The text must be split up by newlines.\n\n        Args:\n            text: Input TextObject to process\n            section_count_target: the target for the number of sections to find\n            segment_size_target: the target for the number of lines per section\n                (if section_count_target is specified, \n                this value will be set to generate correct segments)\n            template_dict: Optional additional template variables\n\n        Returns:\n            TextObject containing section breakdown\n        \"\"\"\n\n        # Prepare numbered text, each line is numbered\n        num_text = text.num_text\n\n        if num_text.size &lt; SECTION_SEGMENT_SIZE_WARNING_LIMIT:\n            logger.warning(\n                f\"find_sections: Text has only {num_text.size} lines. \"\n                \"This may lead to unexpected sectioning results.\"\n            )\n\n        # Get language if not specified\n        source_language = get_language_from_code(text.language)\n\n        # determine section count if not specified\n        if not section_count_target:\n            segment_size_target, section_count_target = self._get_section_count_info(\n                text.content\n            )\n        elif not segment_size_target:\n            segment_size_target = round(num_text.size / section_count_target)\n\n        section_count_range = self._get_section_count_range(section_count_target)\n\n        current_metadata = text.metadata\n\n        # Prepare template variables\n        template_values = {\n            \"metadata\": current_metadata.to_yaml(),\n            \"source_language\": source_language,\n            \"section_count\": section_count_range,\n            \"line_count\": segment_size_target,\n            \"review_count\": self.review_count,\n        }\n\n        if template_dict:\n            template_values |= template_dict\n\n        # Get and apply processing instructions\n        instructions = self.section_pattern.apply_template(template_values)\n        logger.debug(f\"Finding sections with pattern instructions:\\n {instructions}\")\n\n        logger.info(\n            f\"Finding sections for {source_language} text \"\n            f\"(target sections: {section_count_target})\"\n        )\n\n        # Process text with structured output\n        result = self.section_scanner.process_text(\n            num_text.numbered_content, instructions, response_format=AIResponse\n        )\n\n        ai_response = cast(AIResponse, result)\n        text_result = TextObject.from_response(ai_response, current_metadata, num_text)\n\n        logger.info(f\"Generated {text_result.section_count} sections.\")\n\n        return text_result\n\n    def _get_section_count_info(self, text: str) -&gt; Tuple[int, int]:\n        num_text = NumberedText(text)\n        segment_size = _calculate_segment_size(num_text, DEFAULT_SECTION_TOKEN_SIZE)\n        section_count_target = round(num_text.size / segment_size)\n        return segment_size, section_count_target\n\n    def _get_section_count_range(\n        self,\n        section_count_target: int,\n        section_range_var: int = DEFAULT_SECTION_RANGE_VAR,\n    ) -&gt; str:\n        low = max(1, section_count_target - section_range_var)\n        high = section_count_target + section_range_var\n        return f\"{low}-{high}\"\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser.review_count","title":"<code>review_count = review_count</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser.section_pattern","title":"<code>section_pattern = section_pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser.section_scanner","title":"<code>section_scanner = section_scanner</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser.__init__","title":"<code>__init__(section_scanner, section_pattern, review_count=DEFAULT_REVIEW_COUNT)</code>","text":"<p>Initialize section generator.</p> <p>Parameters:</p> Name Type Description Default <code>section_scanner</code> <code>TextProcessor</code> <p>Text processor used to extract sections</p> required <code>section_pattern</code> <code>Prompt</code> <p>Pattern object containing section generation instructions</p> required <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(\n    self,\n    section_scanner: TextProcessor,\n    section_pattern: Prompt,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n):\n    \"\"\"\n    Initialize section generator.\n\n    Args:\n        section_scanner: Text processor used to extract sections\n        section_pattern: Pattern object containing section generation instructions\n        review_count: Number of review passes\n    \"\"\"\n    self.section_scanner = section_scanner\n    self.section_pattern = section_pattern\n    self.review_count = review_count\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionParser.find_sections","title":"<code>find_sections(text, section_count_target=None, segment_size_target=None, template_dict=None)</code>","text":"<p>Generate section breakdown of input text. The text must be split up by newlines.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>TextObject</code> <p>Input TextObject to process</p> required <code>section_count_target</code> <code>Optional[int]</code> <p>the target for the number of sections to find</p> <code>None</code> <code>segment_size_target</code> <code>Optional[int]</code> <p>the target for the number of lines per section (if section_count_target is specified,  this value will be set to generate correct segments)</p> <code>None</code> <code>template_dict</code> <code>Optional[Dict[str, str]]</code> <p>Optional additional template variables</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject containing section breakdown</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def find_sections(\n    self,\n    text: TextObject,\n    section_count_target: Optional[int] = None,\n    segment_size_target: Optional[int] = None,\n    template_dict: Optional[Dict[str, str]] = None,\n) -&gt; TextObject:\n    \"\"\"\n    Generate section breakdown of input text. The text must be split up by newlines.\n\n    Args:\n        text: Input TextObject to process\n        section_count_target: the target for the number of sections to find\n        segment_size_target: the target for the number of lines per section\n            (if section_count_target is specified, \n            this value will be set to generate correct segments)\n        template_dict: Optional additional template variables\n\n    Returns:\n        TextObject containing section breakdown\n    \"\"\"\n\n    # Prepare numbered text, each line is numbered\n    num_text = text.num_text\n\n    if num_text.size &lt; SECTION_SEGMENT_SIZE_WARNING_LIMIT:\n        logger.warning(\n            f\"find_sections: Text has only {num_text.size} lines. \"\n            \"This may lead to unexpected sectioning results.\"\n        )\n\n    # Get language if not specified\n    source_language = get_language_from_code(text.language)\n\n    # determine section count if not specified\n    if not section_count_target:\n        segment_size_target, section_count_target = self._get_section_count_info(\n            text.content\n        )\n    elif not segment_size_target:\n        segment_size_target = round(num_text.size / section_count_target)\n\n    section_count_range = self._get_section_count_range(section_count_target)\n\n    current_metadata = text.metadata\n\n    # Prepare template variables\n    template_values = {\n        \"metadata\": current_metadata.to_yaml(),\n        \"source_language\": source_language,\n        \"section_count\": section_count_range,\n        \"line_count\": segment_size_target,\n        \"review_count\": self.review_count,\n    }\n\n    if template_dict:\n        template_values |= template_dict\n\n    # Get and apply processing instructions\n    instructions = self.section_pattern.apply_template(template_values)\n    logger.debug(f\"Finding sections with pattern instructions:\\n {instructions}\")\n\n    logger.info(\n        f\"Finding sections for {source_language} text \"\n        f\"(target sections: {section_count_target})\"\n    )\n\n    # Process text with structured output\n    result = self.section_scanner.process_text(\n        num_text.numbered_content, instructions, response_format=AIResponse\n    )\n\n    ai_response = cast(AIResponse, result)\n    text_result = TextObject.from_response(ai_response, current_metadata, num_text)\n\n    logger.info(f\"Generated {text_result.section_count} sections.\")\n\n    return text_result\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor","title":"<code>SectionProcessor</code>","text":"<p>Handles section-based XML text processing with configurable output handling.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class SectionProcessor:\n    \"\"\"Handles section-based XML text processing with configurable output handling.\"\"\"\n\n    def __init__(\n        self,\n        processor: TextProcessor,\n        pattern: Prompt,\n        template_dict: Dict,\n        wrap_in_document: bool = True,\n    ):\n        \"\"\"\n        Initialize the XML section processor.\n\n        Args:\n            processor: Implementation of TextProcessor to use\n            pattern: Pattern object containing processing instructions\n            template_dict: Dictionary for template substitution\n            wrap_in_document: Whether to wrap output in &lt;document&gt; tags\n        \"\"\"\n        self.processor = processor\n        self.pattern = pattern\n        self.template_dict = template_dict\n        self.wrap_in_document = wrap_in_document\n\n    def process_sections(\n        self,\n        text_object: TextObject,\n    ) -&gt; Generator[ProcessedSection, None, None]:\n        \"\"\"\n        Process transcript sections and yield results one section at a time.\n\n        Args:\n            text_object: Object containing section definitions\n\n        Yields:\n            ProcessedSection: One processed section at a time, containing:\n                - title: Section title (English or original language)\n                - original_text: Raw text segment\n                - processed_text: Processed text content\n                - start_line: Starting line number\n        \"\"\"\n        # numbered_transcript = NumberedText(transcript) \n        # transcript is now stored in the TextObject\n        sections = text_object.sections\n\n        logger.info(\n            f\"Processing {len(sections)} sections with pattern: {self.pattern.name}\"\n        )\n\n        for section_entry in text_object:\n            logger.info(f\"Processing section {section_entry.number} \"\n                        f\"'{section_entry.title}':\")\n\n            # Get text segment for section\n            text_segment = section_entry.content\n\n            # Prepare template variables\n            template_values = {\n                \"metadata\": text_object.metadata.to_yaml(),\n                \"section_title\": section_entry.title,\n                \"source_language\": get_language_from_code(text_object.language),\n                \"review_count\": DEFAULT_REVIEW_COUNT,\n            }\n\n            if self.template_dict:\n                template_values |= self.template_dict\n\n            # Get and apply processing instructions\n            instructions = self.pattern.apply_template(template_values)\n            processed_str = self.processor.process_text(text_segment, instructions)\n\n            yield ProcessedSection(\n                title=section_entry.title,\n                original_str=text_segment,\n                processed_str=processed_str,\n            )\n\n    def process_paragraphs(\n        self,\n        text: TextObject,\n    ) -&gt; Generator[ProcessedSection, None, None]:\n        \"\"\"\n        Process transcript by paragraphs (as sections), yielding ProcessedSection objects.\n        Paragraphs are assumed to be given as newline separated.\n\n        Args:\n            text: TextObject to process\n\n        Yields:\n            ProcessedSection: One processed paragraph at a time, containing:\n                - title: Paragraph number (e.g., 'Paragraph 1')\n                - original_str: Raw paragraph text\n                - processed_str: Processed paragraph text\n                - metadata: Optional metadata dict\n        \"\"\"\n        num_text = text.num_text\n\n        logger.info(f\"Processing lines as paragraphs with pattern: {self.pattern.name}\")\n\n        for i, line in num_text:\n            # If line is empty or whitespace, continue\n            if not line.strip():\n                continue\n\n            instructions = self.pattern.apply_template(self.template_dict)\n\n            if i &lt;= 1:\n                logger.debug(f\"Process instructions (first paragraph):\\n{instructions}\")\n\n            processed_str = self.processor.process_text(line, instructions)\n            yield ProcessedSection(\n                title=f\"Paragraph {i}\",\n                original_str=line,\n                processed_str=processed_str,\n                metadata={\"paragraph_number\": i}\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.pattern","title":"<code>pattern = pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.processor","title":"<code>processor = processor</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.template_dict","title":"<code>template_dict = template_dict</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.wrap_in_document","title":"<code>wrap_in_document = wrap_in_document</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.__init__","title":"<code>__init__(processor, pattern, template_dict, wrap_in_document=True)</code>","text":"<p>Initialize the XML section processor.</p> <p>Parameters:</p> Name Type Description Default <code>processor</code> <code>TextProcessor</code> <p>Implementation of TextProcessor to use</p> required <code>pattern</code> <code>Prompt</code> <p>Pattern object containing processing instructions</p> required <code>template_dict</code> <code>Dict</code> <p>Dictionary for template substitution</p> required <code>wrap_in_document</code> <code>bool</code> <p>Whether to wrap output in  tags <code>True</code> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def __init__(\n    self,\n    processor: TextProcessor,\n    pattern: Prompt,\n    template_dict: Dict,\n    wrap_in_document: bool = True,\n):\n    \"\"\"\n    Initialize the XML section processor.\n\n    Args:\n        processor: Implementation of TextProcessor to use\n        pattern: Pattern object containing processing instructions\n        template_dict: Dictionary for template substitution\n        wrap_in_document: Whether to wrap output in &lt;document&gt; tags\n    \"\"\"\n    self.processor = processor\n    self.pattern = pattern\n    self.template_dict = template_dict\n    self.wrap_in_document = wrap_in_document\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.process_paragraphs","title":"<code>process_paragraphs(text)</code>","text":"<p>Process transcript by paragraphs (as sections), yielding ProcessedSection objects. Paragraphs are assumed to be given as newline separated.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>TextObject</code> <p>TextObject to process</p> required <p>Yields:</p> Name Type Description <code>ProcessedSection</code> <code>ProcessedSection</code> <p>One processed paragraph at a time, containing: - title: Paragraph number (e.g., 'Paragraph 1') - original_str: Raw paragraph text - processed_str: Processed paragraph text - metadata: Optional metadata dict</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_paragraphs(\n    self,\n    text: TextObject,\n) -&gt; Generator[ProcessedSection, None, None]:\n    \"\"\"\n    Process transcript by paragraphs (as sections), yielding ProcessedSection objects.\n    Paragraphs are assumed to be given as newline separated.\n\n    Args:\n        text: TextObject to process\n\n    Yields:\n        ProcessedSection: One processed paragraph at a time, containing:\n            - title: Paragraph number (e.g., 'Paragraph 1')\n            - original_str: Raw paragraph text\n            - processed_str: Processed paragraph text\n            - metadata: Optional metadata dict\n    \"\"\"\n    num_text = text.num_text\n\n    logger.info(f\"Processing lines as paragraphs with pattern: {self.pattern.name}\")\n\n    for i, line in num_text:\n        # If line is empty or whitespace, continue\n        if not line.strip():\n            continue\n\n        instructions = self.pattern.apply_template(self.template_dict)\n\n        if i &lt;= 1:\n            logger.debug(f\"Process instructions (first paragraph):\\n{instructions}\")\n\n        processed_str = self.processor.process_text(line, instructions)\n        yield ProcessedSection(\n            title=f\"Paragraph {i}\",\n            original_str=line,\n            processed_str=processed_str,\n            metadata={\"paragraph_number\": i}\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.SectionProcessor.process_sections","title":"<code>process_sections(text_object)</code>","text":"<p>Process transcript sections and yield results one section at a time.</p> <p>Parameters:</p> Name Type Description Default <code>text_object</code> <code>TextObject</code> <p>Object containing section definitions</p> required <p>Yields:</p> Name Type Description <code>ProcessedSection</code> <code>ProcessedSection</code> <p>One processed section at a time, containing: - title: Section title (English or original language) - original_text: Raw text segment - processed_text: Processed text content - start_line: Starting line number</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_sections(\n    self,\n    text_object: TextObject,\n) -&gt; Generator[ProcessedSection, None, None]:\n    \"\"\"\n    Process transcript sections and yield results one section at a time.\n\n    Args:\n        text_object: Object containing section definitions\n\n    Yields:\n        ProcessedSection: One processed section at a time, containing:\n            - title: Section title (English or original language)\n            - original_text: Raw text segment\n            - processed_text: Processed text content\n            - start_line: Starting line number\n    \"\"\"\n    # numbered_transcript = NumberedText(transcript) \n    # transcript is now stored in the TextObject\n    sections = text_object.sections\n\n    logger.info(\n        f\"Processing {len(sections)} sections with pattern: {self.pattern.name}\"\n    )\n\n    for section_entry in text_object:\n        logger.info(f\"Processing section {section_entry.number} \"\n                    f\"'{section_entry.title}':\")\n\n        # Get text segment for section\n        text_segment = section_entry.content\n\n        # Prepare template variables\n        template_values = {\n            \"metadata\": text_object.metadata.to_yaml(),\n            \"section_title\": section_entry.title,\n            \"source_language\": get_language_from_code(text_object.language),\n            \"review_count\": DEFAULT_REVIEW_COUNT,\n        }\n\n        if self.template_dict:\n            template_values |= self.template_dict\n\n        # Get and apply processing instructions\n        instructions = self.pattern.apply_template(template_values)\n        processed_str = self.processor.process_text(text_segment, instructions)\n\n        yield ProcessedSection(\n            title=section_entry.title,\n            original_str=text_segment,\n            processed_str=processed_str,\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextProcessor","title":"<code>TextProcessor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text processors that can return Pydantic objects.</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>class TextProcessor(ABC):\n    \"\"\"Abstract base class for text processors that can return Pydantic objects.\"\"\"\n    @abstractmethod\n    def process_text(\n        self,\n        input_str: str,\n        instructions: str,\n        response_format: Optional[Type[BaseModel]] = None,\n        **kwargs: Any,\n    ) -&gt; ProcessorResult:\n        \"\"\"\n        Process text according to instructions.\n\n        Args:\n            input_str: Input text to process\n            instructions: Processing instructions\n            response_format: Optional Pydantic class for structured output\n            **kwargs: Additional processing parameters\n\n        Returns:\n            Either string or Pydantic model instance based on response_model\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.TextProcessor.process_text","title":"<code>process_text(input_str, instructions, response_format=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Process text according to instructions.</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>Input text to process</p> required <code>instructions</code> <code>str</code> <p>Processing instructions</p> required <code>response_format</code> <code>Optional[Type[BaseModel]]</code> <p>Optional Pydantic class for structured output</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional processing parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>ProcessorResult</code> <p>Either string or Pydantic model instance based on response_model</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>@abstractmethod\ndef process_text(\n    self,\n    input_str: str,\n    instructions: str,\n    response_format: Optional[Type[BaseModel]] = None,\n    **kwargs: Any,\n) -&gt; ProcessorResult:\n    \"\"\"\n    Process text according to instructions.\n\n    Args:\n        input_str: Input text to process\n        instructions: Processing instructions\n        response_format: Optional Pydantic class for structured output\n        **kwargs: Additional processing parameters\n\n    Returns:\n        Either string or Pydantic model instance based on response_model\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.find_sections","title":"<code>find_sections(text, source_language=None, section_pattern=None, section_model=None, max_tokens=DEFAULT_SECTION_RESULT_MAX_SIZE, section_count=None, review_count=DEFAULT_REVIEW_COUNT, template_dict=None)</code>","text":"<p>High-level function for generating text sections.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>TextObject</code> <p>Input text</p> required <code>source_language</code> <code>Optional[str]</code> <p>ISO 639-1 language code</p> <code>None</code> <code>section_pattern</code> <code>Optional[Prompt]</code> <p>Optional custom pattern (uses default if None)</p> <code>None</code> <code>section_model</code> <code>Optional[str]</code> <p>Optional model identifier</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens for response</p> <code>DEFAULT_SECTION_RESULT_MAX_SIZE</code> <code>section_count</code> <code>Optional[int]</code> <p>Target number of sections</p> <code>None</code> <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> <code>template_dict</code> <code>Optional[Dict[str, str]]</code> <p>Optional additional template variables</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject containing section breakdown</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def find_sections(\n    text: TextObject,\n    source_language: Optional[str] = None,\n    section_pattern: Optional[Prompt] = None,\n    section_model: Optional[str] = None,\n    max_tokens: int = DEFAULT_SECTION_RESULT_MAX_SIZE,\n    section_count: Optional[int] = None,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n    template_dict: Optional[Dict[str, str]] = None,\n) -&gt; TextObject:\n    \"\"\"\n    High-level function for generating text sections.\n\n    Args:\n        text: Input text\n        source_language: ISO 639-1 language code\n        section_pattern: Optional custom pattern (uses default if None)\n        section_model: Optional model identifier\n        max_tokens: Maximum tokens for response\n        section_count: Target number of sections\n        review_count: Number of review passes\n        template_dict: Optional additional template variables\n\n    Returns:\n        TextObject containing section breakdown\n    \"\"\"\n    if section_pattern is None:\n        section_pattern = get_pattern(DEFAULT_SECTION_PATTERN)\n        logger.debug(f\"Using default section pattern: {DEFAULT_SECTION_PATTERN}.\")\n\n    section_scanner = OpenAIProcessor(model=section_model, max_tokens=max_tokens)\n    parser = SectionParser(\n        section_scanner=section_scanner,\n        section_pattern=section_pattern,\n        review_count=review_count,\n    )\n\n    process_metadata = ProcessMetadata(\n            step=\"find_sections\",\n            processor=\"SectionProcessor\", \n            source_language=source_language,\n            pattern=section_pattern.name,\n            model=section_model,\n            section_count=section_count,\n            review_count=review_count,\n            template_dict=template_dict,\n        )\n\n    result_text = parser.find_sections(\n        text,\n        section_count_target=section_count,\n        template_dict=template_dict,\n    )\n    result_text.transform(process_metadata=process_metadata)\n    return result_text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.get_pattern","title":"<code>get_pattern(name)</code>","text":"<p>Get a pattern by name using the singleton PatternManager.</p> <p>This is a more efficient version that reuses a single PatternManager instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pattern to load</p> required <p>Returns:</p> Type Description <code>Prompt</code> <p>The loaded pattern</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If pattern name is invalid</p> <code>FileNotFoundError</code> <p>If pattern file doesn't exist</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def get_pattern(name: str) -&gt; Prompt:\n    \"\"\"\n    Get a pattern by name using the singleton PatternManager.\n\n    This is a more efficient version that reuses a single PatternManager instance.\n\n    Args:\n        name: Name of the pattern to load\n\n    Returns:\n        The loaded pattern\n\n    Raises:\n        ValueError: If pattern name is invalid\n        FileNotFoundError: If pattern file doesn't exist\n    \"\"\"\n    return LocalPromptManager().get_prompt(name)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.process_text","title":"<code>process_text(text, pattern, source_language=None, model=None, template_dict=None)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text(\n    text: TextObject,\n    pattern: Prompt,\n    source_language: Optional[str] = None,\n    model: Optional[str] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; TextObject:\n\n    if not model:\n        model = DEFAULT_OPENAI_MODEL\n\n    processor = GeneralProcessor(\n        processor=OpenAIProcessor(model),\n        source_language=source_language,\n        pattern=pattern,\n    )\n\n    process_metadata = ProcessMetadata(\n            step=\"process_text\",\n            processor=\"GeneralProcessor\",\n            pattern=pattern.name,\n            model=model,\n            template_dict=template_dict,\n        )\n\n    result = processor.process_text(\n        text, template_dict=template_dict\n    )\n    text.transform(data_str=result, process_metadata=process_metadata)\n    return text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.process_text_by_paragraphs","title":"<code>process_text_by_paragraphs(text, template_dict, pattern=None, model=None)</code>","text":"<p>High-level function for processing text paragraphs, yielding ProcessedSection objects. Assumes paragraphs are separated by newlines. Uses DEFAULT_XML_FORMAT_PATTERN as default pattern for text processing.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>TextObject</code> <p>TextObject to process</p> required <code>template_dict</code> <code>Dict[str, str]</code> <p>Dictionary for template substitution</p> required <code>pattern</code> <code>Optional[Prompt]</code> <p>Pattern object containing processing instructions</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Optional model identifier for processor</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Generator for ProcessedSection objects (one per paragraph)</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text_by_paragraphs(\n    text: TextObject,\n    template_dict: Dict[str, str],\n    pattern: Optional[Prompt] = None,\n    model: Optional[str] = None,\n) -&gt; Generator[ProcessedSection, None, None]:\n    \"\"\"\n    High-level function for processing text paragraphs, yielding ProcessedSection objects.\n    Assumes paragraphs are separated by newlines.\n    Uses DEFAULT_XML_FORMAT_PATTERN as default pattern for text processing.\n\n    Args:\n        text: TextObject to process\n        template_dict: Dictionary for template substitution\n        pattern: Pattern object containing processing instructions\n        model: Optional model identifier for processor\n\n    Returns:\n        Generator for ProcessedSection objects (one per paragraph)\n    \"\"\"\n    processor = OpenAIProcessor(model)\n\n    if not pattern:\n        pattern = get_pattern(DEFAULT_PARAGRAPH_FORMAT_PATTERN)\n\n    section_processor = SectionProcessor(processor, pattern, template_dict)\n\n    process_metadata = ProcessMetadata(\n        step=\"process_text_by_paragraphs\",\n        processor=\"SectionProcessor\",\n        pattern=pattern.name,\n        model=model,\n        template_dict=template_dict,\n    )\n\n    result = section_processor.process_paragraphs(text)\n\n    text.transform(process_metadata=process_metadata)\n\n    return result\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.ai_text_processing.process_text_by_sections","title":"<code>process_text_by_sections(text_object, template_dict, pattern, model=None)</code>","text":"<p>High-level function for processing text sections with configurable output handling.</p> <p>Parameters:</p> Name Type Description Default <code>text_object</code> <code>TextObject</code> <p>Object containing section definitions</p> required <code>pattern</code> <code>Prompt</code> <p>Pattern object containing processing instructions</p> required <code>template_dict</code> <code>Dict</code> <p>Dictionary for template substitution</p> required <code>model</code> <code>Optional[str]</code> <p>Optional model identifier for processor</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Generator for ProcessedSections</p> Source code in <code>src/tnh_scholar/ai_text_processing/ai_text_processing.py</code> <pre><code>def process_text_by_sections(\n    text_object: TextObject,\n    template_dict: Dict,\n    pattern: Prompt,\n    model: Optional[str] = None,\n) -&gt; Generator[ProcessedSection, None, None]:\n    \"\"\"\n    High-level function for processing text sections with configurable output handling.\n\n    Args:\n        text_object: Object containing section definitions\n        pattern: Pattern object containing processing instructions\n        template_dict: Dictionary for template substitution\n        model: Optional model identifier for processor\n\n    Returns:\n        Generator for ProcessedSections\n    \"\"\"\n    processor = OpenAIProcessor(model)\n\n    section_processor = SectionProcessor(processor, pattern, template_dict)\n\n    process_metadata = ProcessMetadata(\n            step=\"process_text_by_sections\",\n            processor=\"SectionProcessor\",\n            pattern=pattern.name,\n            model=model,\n            template_dict=template_dict,\n        )\n    result = section_processor.process_sections(text_object)\n\n    text_object.transform(process_metadata=process_metadata)\n\n    return result\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.general_processor","title":"<code>general_processor</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator","title":"<code>line_translator</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.DEFAULT_TARGET_LANGUAGE","title":"<code>DEFAULT_TARGET_LANGUAGE = 'en'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.DEFAULT_TRANSLATE_CONTEXT_LINES","title":"<code>DEFAULT_TRANSLATE_CONTEXT_LINES = 3</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.DEFAULT_TRANSLATE_STYLE","title":"<code>DEFAULT_TRANSLATE_STYLE = \"'American Dharma Teaching'\"</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.DEFAULT_TRANSLATION_PATTERN","title":"<code>DEFAULT_TRANSLATION_PATTERN = 'default_line_translate'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.DEFAULT_TRANSLATION_TARGET_TOKENS","title":"<code>DEFAULT_TRANSLATION_TARGET_TOKENS = 300</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.FOLLOWING_CONTEXT_MARKER","title":"<code>FOLLOWING_CONTEXT_MARKER = 'FOLLOWING_CONTEXT'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.MAX_RETRIES","title":"<code>MAX_RETRIES = 6</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.MIN_SEGMENT_SIZE","title":"<code>MIN_SEGMENT_SIZE = 4</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.PRECEDING_CONTEXT_MARKER","title":"<code>PRECEDING_CONTEXT_MARKER = 'PRECEDING_CONTEXT'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.TRANSCRIPT_SEGMENT_MARKER","title":"<code>TRANSCRIPT_SEGMENT_MARKER = 'TRANSCRIPT_SEGMENT'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.LineTranslator","title":"<code>LineTranslator</code>","text":"<p>Translates text line by line while maintaining line numbers and context.</p> Source code in <code>src/tnh_scholar/ai_text_processing/line_translator.py</code> <pre><code>class LineTranslator:\n    \"\"\"Translates text line by line while maintaining line numbers and context.\"\"\"\n\n    def __init__(\n        self,\n        processor: TextProcessor,\n        pattern: Prompt,\n        review_count: int = DEFAULT_REVIEW_COUNT,\n        style: str = DEFAULT_TRANSLATE_STYLE,\n        # Number of context lines before/after\n        context_lines: int = DEFAULT_TRANSLATE_CONTEXT_LINES,  \n    ):\n        \"\"\"\n        Initialize line translator.\n\n        Args:\n            processor: Implementation of TextProcessor\n            pattern: Pattern object containing translation instructions\n            review_count: Number of review passes\n            style: Translation style to apply\n            context_lines: Number of context lines to include before/after\n        \"\"\"\n        self.processor = processor\n        self.pattern = pattern\n        self.review_count = review_count\n        self.style = style\n        self.context_lines = context_lines\n\n    def translate_segment(\n        self,\n        num_text: NumberedText,\n        start_line: int,\n        end_line: int,\n        metadata: Metadata,\n        target_language: str,\n        source_language: str,\n        template_dict: Optional[Dict] = None,\n    ) -&gt; str:\n        \"\"\"\n        Translate a segment of text with context.\n\n        Args:\n            num_text: Numbered text to extract segment from\n            start_line: Starting line number of segment\n            end_line: Ending line number of segment\n            metadata: metadata for text\n            source_language: Source language code\n            target_language: Target language code (default: en for English)\n            template_dict: Optional additional template values\n\n        Returns:\n            Translated text segment with line numbers preserved\n        \"\"\"\n\n        # Calculate context ranges\n        preceding_start = max(1, start_line - self.context_lines)  # lines start on 1.\n        following_end = min(num_text.end + 1, end_line + self.context_lines)\n\n        # Extract context and segment\n        preceding_context = num_text.get_numbered_segment(preceding_start, start_line)\n        transcript_segment = num_text.get_numbered_segment(start_line, end_line)\n        following_context = num_text.get_numbered_segment(end_line, following_end)\n\n        # build input text\n        translation_input = self._build_translation_input(\n            preceding_context, transcript_segment, following_context\n        )\n\n        # Prepare template values\n        template_values = {\n            \"source_language\": get_language_from_code(source_language),\n            \"target_language\": get_language_from_code(target_language),\n            \"review_count\": self.review_count,\n            \"style\": self.style,\n            \"metadata\": metadata.to_yaml()\n        }\n\n        if template_dict:\n            template_values |= template_dict\n\n        # Get and apply translation instructions\n        logger.info(f\"Translating segment (lines {start_line}-{end_line})\")\n        translate_instructions = self.pattern.apply_template(template_values)\n\n        if start_line &lt;= 1:\n            logger.debug(\n                f\"Translate instructions (first segment):\\n{translate_instructions}\"\n            )\n        logger.debug(f\"Translation input:\\n{translation_input}\")\n\n        return self._translate_with_retries(\n            translation_input,\n            translate_instructions,\n            start_line,\n            end_line,\n            )\n\n    def _translate_with_retries(\n        self, \n        translation_input,\n        translate_instructions, \n        start_line, end_line\n        ) -&gt; str:\n\n        retries = 0\n        translated_lines = \"\"\n\n        while retries &lt; MAX_RETRIES:\n            translated_segment = self.processor.process_text(\n                translation_input, translate_instructions)\n            translated_lines = self._extract_lines(translated_segment)\n\n            if not self._validate_lines(translated_lines, start_line, end_line):\n                break  # Validation successful, exit loop\n\n            retries += 1\n            logger.warning(f\"Validation failed for segment {start_line}-{end_line}, \"\n                           f\"retrying (attempt {retries + 1}/{MAX_RETRIES})\")\n            # You might want to add a delay here, e.g., using time.sleep()\n\n        if retries == MAX_RETRIES:\n            logger.error(\n                f\"Validation failed after {MAX_RETRIES}\"\n                f\" retries for segment {start_line}-{end_line}\\n\"\n                \"Using last generated result.\"\n                )\n\n        return self._extract_content(translated_lines)\n\n    def _extract_content(self, lines: str) -&gt; str:\n        \"\"\"convert line-numbered format to un-numbered text.\"\"\"\n        # clean each line and return full clean segment\n        line_list = lines.splitlines()\n        # Remove line numbering and strip whitespace\n        stripped_lines = [line.split(':', 1)[-1].strip() for line in line_list]\n        return \"\\n\".join(stripped_lines)\n\n\n    def _build_translation_input(\n        self, preceding_context: str, transcript_segment: str, following_context: str\n    ) -&gt; str:\n        \"\"\"\n        Build input text in required XML-style format.\n\n        Args:\n            preceding_context: Context lines before segment\n            transcript_segment: Main segment to translate\n            following_context: Context lines after segment\n\n        Returns:\n            Formatted input text\n        \"\"\"\n        parts = []\n\n        # Add preceding context if exists\n        if preceding_context:\n            parts.extend(\n                [\n                    PRECEDING_CONTEXT_MARKER,\n                    preceding_context,\n                    PRECEDING_CONTEXT_MARKER,\n                    \"\",\n                ]\n            )\n\n        # Add main segment (always required)\n        parts.extend(\n            [\n                TRANSCRIPT_SEGMENT_MARKER,\n                transcript_segment,\n                TRANSCRIPT_SEGMENT_MARKER,\n                \"\",\n            ]\n        )\n\n        # Add following context if exists\n        if following_context:\n            parts.extend(\n                [\n                    FOLLOWING_CONTEXT_MARKER,\n                    following_context,\n                    FOLLOWING_CONTEXT_MARKER,\n                    \"\",\n                ]\n            )\n\n        return \"\\n\".join(parts)\n\n    def translate_text(\n        self,\n        text: TextObject,\n        source_language: str,\n        segment_size: Optional[int] = None,  \n        target_language: str = DEFAULT_TARGET_LANGUAGE,\n        template_dict: Optional[Dict] = None,\n    ) -&gt; TextObject:\n        \"\"\"\n        Translate entire text in segments while maintaining line continuity.\n\n        Args:\n            text: Text to translate\n            segment_size: Number of lines per translation segment\n            source_language: Source language code\n            target_language: Target language code (default: en for English)\n            template_dict: Optional additional template values\n\n        Returns:\n            Complete translated text with line numbers preserved\n        \"\"\"\n\n        # Use TextObject language if not specified\n        if not source_language:\n            source_language = text.language\n\n        # Convert text to numbered lines\n        num_text = text.num_text\n        total_lines = num_text.size\n\n        metadata = text.metadata\n\n        if not segment_size:\n            segment_size = _calculate_segment_size(\n                num_text, DEFAULT_TRANSLATION_TARGET_TOKENS\n            )\n\n        translated_segments = []\n\n        logger.debug(\n            f\"Total lines to translate: {total_lines} \"\n            f\" | Translation segment size: {segment_size}.\"\n        )\n        # Process text in segments using segment iteration\n        for start_idx, end_idx in num_text.iter_segments(\n            segment_size, min_segment_size=MIN_SEGMENT_SIZE\n        ):\n            translated_content = self.translate_segment(\n                num_text=num_text,\n                start_line=start_idx,\n                end_line=end_idx,\n                metadata=metadata,\n                source_language=source_language,\n                target_language=target_language,\n                template_dict=template_dict,\n            )\n\n            translated_segments.append(translated_content)\n\n        new_text =  \"\\n\".join(translated_segments).strip()\n\n        return text.transform(\n            data_str=new_text, \n            language=target_language, \n            )\n\n    def _extract_lines(self, segment: str) -&gt; str:\n        if segment.startswith(TRANSCRIPT_SEGMENT_MARKER) and segment.endswith(\n            TRANSCRIPT_SEGMENT_MARKER\n        ):\n            segment = segment[\n                len(TRANSCRIPT_SEGMENT_MARKER) : -len(TRANSCRIPT_SEGMENT_MARKER)\n            ]\n\n        else:\n            logger.warning(\"Translated segment missing transcript_segment tags\")\n\n        # clean each line and return full clean segment\n        lines = segment.splitlines()\n        stripped_lines = [line.strip() for line in lines]\n        segment = \"\\n\".join(stripped_lines)\n        return segment.strip()\n\n    def _validate_lines(\n        self, translated_content: str, start_index: int, end_index: int\n    ) -&gt; bool:\n        \"\"\"\n        Validate translated segment format, content, and line number sequence.\n        Issues warnings for validation issues rather than raising errors.\n\n        Args:\n            translated_segment: Translated text to validate\n            start_idx: the staring index of the range (inclusive)\n            end_line: then ending index of the range (exclusive)\n\n        Returns:\n            str: Content with segment tags removed\n        \"\"\"\n\n        # Validate lines\n\n        error_count = 0\n        lines = translated_content.splitlines()\n        line_numbers = []\n\n        start_line = start_index  # inclusive start\n        end_line = end_index - 1  # exclusive end\n\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            if \":\" not in line:\n                logger.warning(f\"Invalid line format: {line}\")\n                error_count += 1\n                continue\n\n            try:\n                line_num = int(line[: line.index(\":\")])\n                if line_num &lt; 0:\n                    logger.warning(f\"Invalid line number: {line}\")\n                    error_count += 1\n                    continue\n                line_numbers.append(line_num)\n            except ValueError:\n                logger.warning(f\"Line number parsing failed: {line}\")\n                error_count += 1\n                continue\n\n        # Validate sequence\n        if not line_numbers:\n            logger.warning(\"No valid line numbers found\")\n        else:\n            if line_numbers[0] != start_line:\n                logger.warning(\n                    f\"First line number {line_numbers[0]} \"\n                    f\" doesn't match expected {start_line}\"\n                )\n                error_count += 1\n\n            if line_numbers[-1] != end_line:\n                logger.warning(\n                    f\"Last line number {line_numbers[-1]} \"\n                    f\"doesn't match expected {end_line}\"\n                )\n                error_count += 1\n\n            expected = set(range(start_line, end_line + 1))\n            if missing := expected - set(line_numbers):\n                logger.warning(f\"Missing line numbers in sequence: {missing}\")\n                error_count += len(missing)\n\n        logger.debug(f\"Validated {len(lines)} lines from {start_line} to {end_line}\\n\"\n                     f\"{error_count} errors encountered.\")\n        return error_count &gt; 0\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.LineTranslator.context_lines","title":"<code>context_lines = context_lines</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.LineTranslator.pattern","title":"<code>pattern = pattern</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.LineTranslator.processor","title":"<code>processor = processor</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.LineTranslator.review_count","title":"<code>review_count = review_count</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.LineTranslator.style","title":"<code>style = style</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.LineTranslator.__init__","title":"<code>__init__(processor, pattern, review_count=DEFAULT_REVIEW_COUNT, style=DEFAULT_TRANSLATE_STYLE, context_lines=DEFAULT_TRANSLATE_CONTEXT_LINES)</code>","text":"<p>Initialize line translator.</p> <p>Parameters:</p> Name Type Description Default <code>processor</code> <code>TextProcessor</code> <p>Implementation of TextProcessor</p> required <code>pattern</code> <code>Prompt</code> <p>Pattern object containing translation instructions</p> required <code>review_count</code> <code>int</code> <p>Number of review passes</p> <code>DEFAULT_REVIEW_COUNT</code> <code>style</code> <code>str</code> <p>Translation style to apply</p> <code>DEFAULT_TRANSLATE_STYLE</code> <code>context_lines</code> <code>int</code> <p>Number of context lines to include before/after</p> <code>DEFAULT_TRANSLATE_CONTEXT_LINES</code> Source code in <code>src/tnh_scholar/ai_text_processing/line_translator.py</code> <pre><code>def __init__(\n    self,\n    processor: TextProcessor,\n    pattern: Prompt,\n    review_count: int = DEFAULT_REVIEW_COUNT,\n    style: str = DEFAULT_TRANSLATE_STYLE,\n    # Number of context lines before/after\n    context_lines: int = DEFAULT_TRANSLATE_CONTEXT_LINES,  \n):\n    \"\"\"\n    Initialize line translator.\n\n    Args:\n        processor: Implementation of TextProcessor\n        pattern: Pattern object containing translation instructions\n        review_count: Number of review passes\n        style: Translation style to apply\n        context_lines: Number of context lines to include before/after\n    \"\"\"\n    self.processor = processor\n    self.pattern = pattern\n    self.review_count = review_count\n    self.style = style\n    self.context_lines = context_lines\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.LineTranslator.translate_segment","title":"<code>translate_segment(num_text, start_line, end_line, metadata, target_language, source_language, template_dict=None)</code>","text":"<p>Translate a segment of text with context.</p> <p>Parameters:</p> Name Type Description Default <code>num_text</code> <code>NumberedText</code> <p>Numbered text to extract segment from</p> required <code>start_line</code> <code>int</code> <p>Starting line number of segment</p> required <code>end_line</code> <code>int</code> <p>Ending line number of segment</p> required <code>metadata</code> <code>Metadata</code> <p>metadata for text</p> required <code>source_language</code> <code>str</code> <p>Source language code</p> required <code>target_language</code> <code>str</code> <p>Target language code (default: en for English)</p> required <code>template_dict</code> <code>Optional[Dict]</code> <p>Optional additional template values</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Translated text segment with line numbers preserved</p> Source code in <code>src/tnh_scholar/ai_text_processing/line_translator.py</code> <pre><code>def translate_segment(\n    self,\n    num_text: NumberedText,\n    start_line: int,\n    end_line: int,\n    metadata: Metadata,\n    target_language: str,\n    source_language: str,\n    template_dict: Optional[Dict] = None,\n) -&gt; str:\n    \"\"\"\n    Translate a segment of text with context.\n\n    Args:\n        num_text: Numbered text to extract segment from\n        start_line: Starting line number of segment\n        end_line: Ending line number of segment\n        metadata: metadata for text\n        source_language: Source language code\n        target_language: Target language code (default: en for English)\n        template_dict: Optional additional template values\n\n    Returns:\n        Translated text segment with line numbers preserved\n    \"\"\"\n\n    # Calculate context ranges\n    preceding_start = max(1, start_line - self.context_lines)  # lines start on 1.\n    following_end = min(num_text.end + 1, end_line + self.context_lines)\n\n    # Extract context and segment\n    preceding_context = num_text.get_numbered_segment(preceding_start, start_line)\n    transcript_segment = num_text.get_numbered_segment(start_line, end_line)\n    following_context = num_text.get_numbered_segment(end_line, following_end)\n\n    # build input text\n    translation_input = self._build_translation_input(\n        preceding_context, transcript_segment, following_context\n    )\n\n    # Prepare template values\n    template_values = {\n        \"source_language\": get_language_from_code(source_language),\n        \"target_language\": get_language_from_code(target_language),\n        \"review_count\": self.review_count,\n        \"style\": self.style,\n        \"metadata\": metadata.to_yaml()\n    }\n\n    if template_dict:\n        template_values |= template_dict\n\n    # Get and apply translation instructions\n    logger.info(f\"Translating segment (lines {start_line}-{end_line})\")\n    translate_instructions = self.pattern.apply_template(template_values)\n\n    if start_line &lt;= 1:\n        logger.debug(\n            f\"Translate instructions (first segment):\\n{translate_instructions}\"\n        )\n    logger.debug(f\"Translation input:\\n{translation_input}\")\n\n    return self._translate_with_retries(\n        translation_input,\n        translate_instructions,\n        start_line,\n        end_line,\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.LineTranslator.translate_text","title":"<code>translate_text(text, source_language, segment_size=None, target_language=DEFAULT_TARGET_LANGUAGE, template_dict=None)</code>","text":"<p>Translate entire text in segments while maintaining line continuity.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>TextObject</code> <p>Text to translate</p> required <code>segment_size</code> <code>Optional[int]</code> <p>Number of lines per translation segment</p> <code>None</code> <code>source_language</code> <code>str</code> <p>Source language code</p> required <code>target_language</code> <code>str</code> <p>Target language code (default: en for English)</p> <code>DEFAULT_TARGET_LANGUAGE</code> <code>template_dict</code> <code>Optional[Dict]</code> <p>Optional additional template values</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>Complete translated text with line numbers preserved</p> Source code in <code>src/tnh_scholar/ai_text_processing/line_translator.py</code> <pre><code>def translate_text(\n    self,\n    text: TextObject,\n    source_language: str,\n    segment_size: Optional[int] = None,  \n    target_language: str = DEFAULT_TARGET_LANGUAGE,\n    template_dict: Optional[Dict] = None,\n) -&gt; TextObject:\n    \"\"\"\n    Translate entire text in segments while maintaining line continuity.\n\n    Args:\n        text: Text to translate\n        segment_size: Number of lines per translation segment\n        source_language: Source language code\n        target_language: Target language code (default: en for English)\n        template_dict: Optional additional template values\n\n    Returns:\n        Complete translated text with line numbers preserved\n    \"\"\"\n\n    # Use TextObject language if not specified\n    if not source_language:\n        source_language = text.language\n\n    # Convert text to numbered lines\n    num_text = text.num_text\n    total_lines = num_text.size\n\n    metadata = text.metadata\n\n    if not segment_size:\n        segment_size = _calculate_segment_size(\n            num_text, DEFAULT_TRANSLATION_TARGET_TOKENS\n        )\n\n    translated_segments = []\n\n    logger.debug(\n        f\"Total lines to translate: {total_lines} \"\n        f\" | Translation segment size: {segment_size}.\"\n    )\n    # Process text in segments using segment iteration\n    for start_idx, end_idx in num_text.iter_segments(\n        segment_size, min_segment_size=MIN_SEGMENT_SIZE\n    ):\n        translated_content = self.translate_segment(\n            num_text=num_text,\n            start_line=start_idx,\n            end_line=end_idx,\n            metadata=metadata,\n            source_language=source_language,\n            target_language=target_language,\n            template_dict=template_dict,\n        )\n\n        translated_segments.append(translated_content)\n\n    new_text =  \"\\n\".join(translated_segments).strip()\n\n    return text.transform(\n        data_str=new_text, \n        language=target_language, \n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.line_translator.translate_text_by_lines","title":"<code>translate_text_by_lines(text, source_language=None, target_language=DEFAULT_TARGET_LANGUAGE, pattern=None, model=None, style=None, segment_size=None, context_lines=None, review_count=None, template_dict=None)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/line_translator.py</code> <pre><code>def translate_text_by_lines(\n    text: TextObject,\n    source_language: Optional[str] = None,\n    target_language: str = DEFAULT_TARGET_LANGUAGE,\n    pattern: Optional[Prompt] = None,\n    model: Optional[str] = None,\n    style: Optional[str] = None,\n    segment_size: Optional[int] = None,\n    context_lines: Optional[int] = None,\n    review_count: Optional[int] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; TextObject:\n\n    if source_language is None:\n        source_language = text.language\n\n    if pattern is None:\n        pattern = get_pattern(DEFAULT_TRANSLATION_PATTERN)\n\n    processor = OpenAIProcessor(model)\n\n    translator = LineTranslator(\n        processor=processor,\n        pattern=pattern,\n        style=style or DEFAULT_TRANSLATE_STYLE,\n        context_lines=context_lines or DEFAULT_TRANSLATE_CONTEXT_LINES,\n        review_count=review_count or DEFAULT_REVIEW_COUNT,\n    )\n\n    process_metadata = ProcessMetadata(\n            step=\"translation\",\n            processor=\"LineTranslator\",\n            model=processor.model,\n            source_language=source_language,\n            target_language=target_language,\n            segment_size=segment_size,\n            context_lines=translator.context_lines,\n            review_count=translator.review_count,\n            style=translator.style,\n            template_dict=template_dict,\n        )\n\n    text = translator.translate_text(\n        text,\n        source_language=source_language,\n        target_language=target_language,\n        segment_size=segment_size,\n        template_dict=template_dict,\n    )\n    return text.transform(process_metadata=process_metadata)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.openai_process_interface","title":"<code>openai_process_interface</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.openai_process_interface.TOKEN_BUFFER","title":"<code>TOKEN_BUFFER = 500</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.openai_process_interface.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.openai_process_interface.openai_process_text","title":"<code>openai_process_text(text_input, process_instructions, model=None, response_format=None, batch=False, max_tokens=0)</code>","text":"<p>postprocessing a transcription.</p> Source code in <code>src/tnh_scholar/ai_text_processing/openai_process_interface.py</code> <pre><code>def openai_process_text(\n    text_input: str,\n    process_instructions: str,\n    model: Optional[str] = None,\n    response_format: Optional[Type[BaseModel]] = None,\n    batch: bool = False,\n    max_tokens: int = 0,\n) -&gt; Union[BaseModel, str]:\n    \"\"\"postprocessing a transcription.\"\"\"\n\n    user_prompts = [text_input]\n    system_message = process_instructions\n\n    logger.debug(f\"OpenAI Process Text with process instructions:\\n{system_message}\")\n    if max_tokens == 0:\n        tokens = token_count(text_input)\n        max_tokens = tokens + TOKEN_BUFFER\n\n    model_name = model or \"default\"\n\n    logger.info(\n        f\"Open AI Text Processing{' as batch process' if batch else ''} \"\n        f\"with model '{model_name}' initiated.\\n\"\n        f\"Requesting a maximum of {max_tokens} tokens.\"\n    )\n\n    if batch:\n        return _run_batch_process_text(\n            user_prompts, system_message, max_tokens, model_name, response_format\n        )\n\n    completion_result = simple_completion(\n        system_message=system_message,\n        user_message=text_input,\n        model=model,\n        max_tokens=max_tokens,\n        response_model=response_format,\n    )\n    logger.info(\"Processing completed.\")\n    return completion_result\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts","title":"<code>prompts</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.MANAGER_UPDATE_MESSAGE","title":"<code>MANAGER_UPDATE_MESSAGE = 'PromptManager Update:'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.MarkdownStr","title":"<code>MarkdownStr = NewType('MarkdownStr', str)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.ConcurrentAccessManager","title":"<code>ConcurrentAccessManager</code>","text":"<p>Manages concurrent access to prompt files.</p> <p>Provides: - File-level locking - Safe concurrent access prompts - Lock cleanup</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>class ConcurrentAccessManager:\n    \"\"\"\n    Manages concurrent access to prompt files.\n\n    Provides:\n    - File-level locking\n    - Safe concurrent access prompts\n    - Lock cleanup\n    \"\"\"\n\n    def __init__(self, lock_dir: Path):\n        \"\"\"\n        Initialize access manager.\n\n        Args:\n            lock_dir: Directory for lock files\n        \"\"\"\n        self.lock_dir = Path(lock_dir)\n        self._ensure_lock_dir()\n        self._cleanup_stale_locks()\n\n    def _ensure_lock_dir(self) -&gt; None:\n        \"\"\"Create lock directory if it doesn't exist.\"\"\"\n        self.lock_dir.mkdir(parents=True, exist_ok=True)\n\n    def _cleanup_stale_locks(self, max_age: timedelta = timedelta(hours=1)) -&gt; None:\n        \"\"\"\n        Remove stale lock files.\n\n        Args:\n            max_age: Maximum age for lock files before considered stale\n        \"\"\"\n        current_time = datetime.now()\n        for lock_file in self.lock_dir.glob(\"*.lock\"):\n            try:\n                mtime = datetime.fromtimestamp(lock_file.stat().st_mtime)\n                if current_time - mtime &gt; max_age:\n                    lock_file.unlink()\n                    logger.warning(f\"Removed stale lock file: {lock_file}\")\n            except FileNotFoundError:\n                # Lock was removed by another process\n                pass\n            except Exception as e:\n                logger.error(f\"Error cleaning up lock file {lock_file}: {e}\")\n\n    @contextmanager\n    def file_lock(self, file_path: Path) -&gt; Iterator[None]:\n        \"\"\"\n        Context manager for safely accessing files.\n\n        Args:\n            file_path: Path to file to lock\n\n        Yields:\n            None when lock is acquired\n\n        Raises:\n            RuntimeError: If file is already locked\n            OSError: If lock file operations fail\n        \"\"\"\n        file_path = Path(file_path)\n        lock_file_path = self.lock_dir / f\"{file_path.stem}.lock\"\n        lock_fd = None\n\n        try:\n            # Open or create lock file\n            lock_fd = os.open(str(lock_file_path), os.O_WRONLY | os.O_CREAT)\n\n            try:\n                # Attempt to acquire lock\n                fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\n\n                # Write process info to lock file\n                pid = os.getpid()\n                timestamp = datetime.now().isoformat()\n                os.write(lock_fd, f\"{pid} {timestamp}\\n\".encode())\n\n                logger.debug(f\"Acquired lock for {file_path}\")\n                yield\n\n            except BlockingIOError as e:\n                raise RuntimeError(\n                    f\"File {file_path} is locked by another process\"\n                ) from e\n\n        except OSError as e:\n            logger.error(f\"Lock operation failed for {file_path}: {e}\")\n            raise\n\n        finally:\n            if lock_fd is not None:\n                try:\n                    # Release lock and close file descriptor\n                    fcntl.flock(lock_fd, fcntl.LOCK_UN)\n                    os.close(lock_fd)\n\n                    # Remove lock file\n                    lock_file_path.unlink(missing_ok=True)\n                    logger.debug(f\"Released lock for {file_path}\")\n\n                except Exception as e:\n                    logger.error(f\"Error cleaning up lock for {file_path}: {e}\")\n\n    def is_locked(self, file_path: Path) -&gt; bool:\n        \"\"\"\n        Check if a file is currently locked.\n\n        Args:\n            file_path: Path to file to check\n\n        Returns:\n            bool: True if file is locked\n        \"\"\"\n        lock_file_path = self.lock_dir / f\"{file_path.stem}.lock\"\n\n        if not lock_file_path.exists():\n            return False\n\n        try:\n            with open(lock_file_path, \"r\") as f:\n                # Try to acquire and immediately release lock\n                fcntl.flock(f, fcntl.LOCK_EX | fcntl.LOCK_NB)\n                fcntl.flock(f, fcntl.LOCK_UN)\n                return False\n        except BlockingIOError:\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.ConcurrentAccessManager.lock_dir","title":"<code>lock_dir = Path(lock_dir)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.ConcurrentAccessManager.__init__","title":"<code>__init__(lock_dir)</code>","text":"<p>Initialize access manager.</p> <p>Parameters:</p> Name Type Description Default <code>lock_dir</code> <code>Path</code> <p>Directory for lock files</p> required Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __init__(self, lock_dir: Path):\n    \"\"\"\n    Initialize access manager.\n\n    Args:\n        lock_dir: Directory for lock files\n    \"\"\"\n    self.lock_dir = Path(lock_dir)\n    self._ensure_lock_dir()\n    self._cleanup_stale_locks()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.ConcurrentAccessManager.file_lock","title":"<code>file_lock(file_path)</code>","text":"<p>Context manager for safely accessing files.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file to lock</p> required <p>Yields:</p> Type Description <code>None</code> <p>None when lock is acquired</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If file is already locked</p> <code>OSError</code> <p>If lock file operations fail</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>@contextmanager\ndef file_lock(self, file_path: Path) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager for safely accessing files.\n\n    Args:\n        file_path: Path to file to lock\n\n    Yields:\n        None when lock is acquired\n\n    Raises:\n        RuntimeError: If file is already locked\n        OSError: If lock file operations fail\n    \"\"\"\n    file_path = Path(file_path)\n    lock_file_path = self.lock_dir / f\"{file_path.stem}.lock\"\n    lock_fd = None\n\n    try:\n        # Open or create lock file\n        lock_fd = os.open(str(lock_file_path), os.O_WRONLY | os.O_CREAT)\n\n        try:\n            # Attempt to acquire lock\n            fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\n\n            # Write process info to lock file\n            pid = os.getpid()\n            timestamp = datetime.now().isoformat()\n            os.write(lock_fd, f\"{pid} {timestamp}\\n\".encode())\n\n            logger.debug(f\"Acquired lock for {file_path}\")\n            yield\n\n        except BlockingIOError as e:\n            raise RuntimeError(\n                f\"File {file_path} is locked by another process\"\n            ) from e\n\n    except OSError as e:\n        logger.error(f\"Lock operation failed for {file_path}: {e}\")\n        raise\n\n    finally:\n        if lock_fd is not None:\n            try:\n                # Release lock and close file descriptor\n                fcntl.flock(lock_fd, fcntl.LOCK_UN)\n                os.close(lock_fd)\n\n                # Remove lock file\n                lock_file_path.unlink(missing_ok=True)\n                logger.debug(f\"Released lock for {file_path}\")\n\n            except Exception as e:\n                logger.error(f\"Error cleaning up lock for {file_path}: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.ConcurrentAccessManager.is_locked","title":"<code>is_locked(file_path)</code>","text":"<p>Check if a file is currently locked.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if file is locked</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def is_locked(self, file_path: Path) -&gt; bool:\n    \"\"\"\n    Check if a file is currently locked.\n\n    Args:\n        file_path: Path to file to check\n\n    Returns:\n        bool: True if file is locked\n    \"\"\"\n    lock_file_path = self.lock_dir / f\"{file_path.stem}.lock\"\n\n    if not lock_file_path.exists():\n        return False\n\n    try:\n        with open(lock_file_path, \"r\") as f:\n            # Try to acquire and immediately release lock\n            fcntl.flock(f, fcntl.LOCK_EX | fcntl.LOCK_NB)\n            fcntl.flock(f, fcntl.LOCK_UN)\n            return False\n    except BlockingIOError:\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.GitBackedRepository","title":"<code>GitBackedRepository</code>","text":"<p>Manages versioned storage of prompts using Git.</p> <p>Provides basic Git operations while hiding complexity: - Automatic versioning of changes - Basic conflict resolution - History tracking</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>class GitBackedRepository:\n    \"\"\"\n    Manages versioned storage of prompts using Git.\n\n    Provides basic Git operations while hiding complexity:\n    - Automatic versioning of changes\n    - Basic conflict resolution\n    - History tracking\n    \"\"\"\n\n    def __init__(self, repo_path: Path):\n        \"\"\"\n        Initialize or connect to Git repository.\n\n        Args:\n            repo_path: Path to repository directory\n\n        Raises:\n            GitCommandError: If Git operations fail\n        \"\"\"\n        self.repo_path = repo_path\n\n        try:\n            # Try to connect to existing repository\n            self.repo = Repo(repo_path)\n            logger.debug(f\"Connected to existing Git repository at {repo_path}\")\n\n        except InvalidGitRepositoryError:\n            # Initialize new repository if none exists\n            logger.info(f\"Initializing new Git repository at {repo_path}\")\n            self.repo = Repo.init(repo_path)\n\n            # Create initial commit if repo is empty\n            if not self.repo.head.is_valid():\n                # Create and commit .gitignore\n                gitignore = repo_path / \".gitignore\"\n                gitignore.write_text(\"*.lock\\n.DS_Store\\n\")\n                self.repo.index.add([\".gitignore\"])\n                self.repo.index.commit(\"Initial repository setup\")\n\n    def update_file(self, file_path: Path) -&gt; str:\n        \"\"\"\n        Stage and commit changes to a file in the Git repository.\n\n        Args:\n            file_path: Absolute or relative path to the file.\n\n        Returns:\n            str: Commit hash if changes were made.\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n            ValueError: If the file is outside the repository.\n            GitCommandError: If Git operations fail.\n        \"\"\"\n        file_path = file_path.resolve()\n\n        # Ensure the file is within the repository\n        try:\n            rel_path = file_path.relative_to(self.repo_path)\n        except ValueError as e:\n            raise ValueError(\n                f\"File {file_path} is not under the repository root {self.repo_path}\"\n            ) from e\n\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File does not exist: {file_path}\")\n\n        try:\n            return self._commit_file_update(rel_path, file_path)\n        except GitCommandError as e:\n            logger.error(f\"Git operation failed: {e}\")\n            raise\n\n    def _commit_file_update(self, rel_path, file_path):\n        if self._is_file_clean(rel_path):\n            # Return the current commit hash if no changes\n            return self.repo.head.commit.hexsha\n\n        logger.info(f\"Detected changes in {rel_path}, updating version control.\")\n        self.repo.index.add([str(rel_path)])\n        commit = self.repo.index.commit(\n            f\"{MANAGER_UPDATE_MESSAGE} {rel_path.stem}\",\n            author=Actor(\"PromptManager\", \"\"),\n        )\n        logger.info(f\"Committed changes to {file_path}: {commit.hexsha}\")\n        return commit.hexsha\n\n    def _get_file_revisions(self, file_path: Path) -&gt; List[Commit]:\n        \"\"\"\n        Get ordered list of commits that modified a file, most recent first.\n\n        Args:\n            file_path: Path to file relative to repository root\n\n        Returns:\n            List of Commit objects affecting this file\n\n        Raises:\n            GitCommandError: If Git operations fail\n        \"\"\"\n        rel_path = file_path.relative_to(self.repo_path)\n        try:\n            return list(self.repo.iter_commits(paths=str(rel_path)))\n        except GitCommandError as e:\n            logger.error(f\"Failed to get commits for {rel_path}: {e}\")\n            return []\n\n    def _get_commit_diff(\n        self, commit: Commit, file_path: Path, prev_commit: Optional[Commit] = None\n    ) -&gt; Tuple[str, str]:\n        \"\"\"\n        Get both stat and detailed diff for a commit.\n\n        Args:\n            commit: Commit to diff\n            file_path: Path relative to repository root\n            prev_commit: Previous commit for diff, defaults to commit's parent\n\n        Returns:\n            Tuple of (stat_diff, detailed_diff) where:\n                stat_diff: Summary of changes (files changed, insertions/deletions)\n                detailed_diff: Colored word-level diff with context\n\n        Raises:\n            GitCommandError: If Git operations fail\n        \"\"\"\n        prev_hash = prev_commit.hexsha if prev_commit else f\"{commit.hexsha}^\"\n        rel_path = file_path.relative_to(self.repo_path)\n\n        try:\n            # Get stats diff\n            stat = self.repo.git.diff(prev_hash, commit.hexsha, rel_path, stat=True)\n\n            # Get detailed diff\n            diff = self.repo.git.diff(\n                prev_hash,\n                commit.hexsha,\n                rel_path,\n                unified=2,\n                word_diff=\"plain\",\n                color=\"always\",\n                ignore_space_change=True,\n            )\n\n            return stat, diff\n        except GitCommandError as e:\n            logger.error(f\"Failed to get diff for {commit.hexsha}: {e}\")\n            return \"\", \"\"\n\n    def display_history(self, file_path: Path, max_versions: int = 0) -&gt; None:\n        \"\"\"\n        Display history of changes for a file with diffs between versions.\n\n        Shows most recent changes first, limited to max_versions entries.\n        For each change shows:\n        - Commit info and date\n        - Stats summary of changes\n        - Detailed color diff with 2 lines of context\n\n        Args:\n            file_path: Path to file in repository\n            max_versions: Maximum number of versions to show; zero shows all revisions.\n\n        Example:\n            &gt;&gt;&gt; repo.display_history(Path(\"prompts/format_dharma_talk.yaml\"))\n            Commit abc123def (2024-12-28 14:30:22):\n            1 file changed, 5 insertions(+), 2 deletions(-)\n\n            diff --git a/prompts/format_dharma_talk.yaml ...\n            ...\n        \"\"\"\n\n        try:\n            # Get commit history\n            commits = self._get_file_revisions(file_path)\n            if not commits:\n                print(f\"No history found for {file_path}\")\n                return\n\n            if max_versions == 0:\n                max_versions = len(commits)  # look at all commits.\n\n            # Display limited history with diffs\n            for i, commit in enumerate(commits[:max_versions]):\n                # Print commit header\n                date_str = commit.committed_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n                print(f\"\\nCommit {commit.hexsha[:8]} ({date_str}):\")\n                print(f\"Message: {commit.message.strip()}\")\n\n                # Get and display diffs\n                prev_commit = commits[i + 1] if i + 1 &lt; len(commits) else None\n                stat_diff, detailed_diff = self._get_commit_diff(\n                    commit, file_path, prev_commit\n                )\n\n                if stat_diff:\n                    print(\"\\nChanges:\")\n                    print(stat_diff)\n                if detailed_diff:\n                    print(\"\\nDetailed diff:\")\n                    print(detailed_diff)\n\n                print(\"\\033[0m\", end=\"\")\n                print(\"-\" * 80)  # Visual separator between commits\n\n        except Exception as e:\n            logger.error(f\"Failed to display history for {file_path}: {e}\")\n            print(f\"Error displaying history: {e}\")\n            raise\n\n    def _is_file_clean(self, rel_path: Path) -&gt; bool:\n        \"\"\"\n        Check if file has uncommitted changes.\n\n        Args:\n            rel_path: Path relative to repository root\n\n        Returns:\n            bool: True if file has no changes\n        \"\"\"\n        return str(rel_path) not in (\n            [item.a_path for item in self.repo.index.diff(None)]\n            + self.repo.untracked_files\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.GitBackedRepository.repo","title":"<code>repo = Repo(repo_path)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.GitBackedRepository.repo_path","title":"<code>repo_path = repo_path</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.GitBackedRepository.__init__","title":"<code>__init__(repo_path)</code>","text":"<p>Initialize or connect to Git repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to repository directory</p> required <p>Raises:</p> Type Description <code>GitCommandError</code> <p>If Git operations fail</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __init__(self, repo_path: Path):\n    \"\"\"\n    Initialize or connect to Git repository.\n\n    Args:\n        repo_path: Path to repository directory\n\n    Raises:\n        GitCommandError: If Git operations fail\n    \"\"\"\n    self.repo_path = repo_path\n\n    try:\n        # Try to connect to existing repository\n        self.repo = Repo(repo_path)\n        logger.debug(f\"Connected to existing Git repository at {repo_path}\")\n\n    except InvalidGitRepositoryError:\n        # Initialize new repository if none exists\n        logger.info(f\"Initializing new Git repository at {repo_path}\")\n        self.repo = Repo.init(repo_path)\n\n        # Create initial commit if repo is empty\n        if not self.repo.head.is_valid():\n            # Create and commit .gitignore\n            gitignore = repo_path / \".gitignore\"\n            gitignore.write_text(\"*.lock\\n.DS_Store\\n\")\n            self.repo.index.add([\".gitignore\"])\n            self.repo.index.commit(\"Initial repository setup\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.GitBackedRepository.display_history","title":"<code>display_history(file_path, max_versions=0)</code>","text":"<p>Display history of changes for a file with diffs between versions.</p> <p>Shows most recent changes first, limited to max_versions entries. For each change shows: - Commit info and date - Stats summary of changes - Detailed color diff with 2 lines of context</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file in repository</p> required <code>max_versions</code> <code>int</code> <p>Maximum number of versions to show; zero shows all revisions.</p> <code>0</code> Example <p>repo.display_history(Path(\"prompts/format_dharma_talk.yaml\")) Commit abc123def (2024-12-28 14:30:22): 1 file changed, 5 insertions(+), 2 deletions(-)</p> <p>diff --git a/prompts/format_dharma_talk.yaml ... ...</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def display_history(self, file_path: Path, max_versions: int = 0) -&gt; None:\n    \"\"\"\n    Display history of changes for a file with diffs between versions.\n\n    Shows most recent changes first, limited to max_versions entries.\n    For each change shows:\n    - Commit info and date\n    - Stats summary of changes\n    - Detailed color diff with 2 lines of context\n\n    Args:\n        file_path: Path to file in repository\n        max_versions: Maximum number of versions to show; zero shows all revisions.\n\n    Example:\n        &gt;&gt;&gt; repo.display_history(Path(\"prompts/format_dharma_talk.yaml\"))\n        Commit abc123def (2024-12-28 14:30:22):\n        1 file changed, 5 insertions(+), 2 deletions(-)\n\n        diff --git a/prompts/format_dharma_talk.yaml ...\n        ...\n    \"\"\"\n\n    try:\n        # Get commit history\n        commits = self._get_file_revisions(file_path)\n        if not commits:\n            print(f\"No history found for {file_path}\")\n            return\n\n        if max_versions == 0:\n            max_versions = len(commits)  # look at all commits.\n\n        # Display limited history with diffs\n        for i, commit in enumerate(commits[:max_versions]):\n            # Print commit header\n            date_str = commit.committed_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n            print(f\"\\nCommit {commit.hexsha[:8]} ({date_str}):\")\n            print(f\"Message: {commit.message.strip()}\")\n\n            # Get and display diffs\n            prev_commit = commits[i + 1] if i + 1 &lt; len(commits) else None\n            stat_diff, detailed_diff = self._get_commit_diff(\n                commit, file_path, prev_commit\n            )\n\n            if stat_diff:\n                print(\"\\nChanges:\")\n                print(stat_diff)\n            if detailed_diff:\n                print(\"\\nDetailed diff:\")\n                print(detailed_diff)\n\n            print(\"\\033[0m\", end=\"\")\n            print(\"-\" * 80)  # Visual separator between commits\n\n    except Exception as e:\n        logger.error(f\"Failed to display history for {file_path}: {e}\")\n        print(f\"Error displaying history: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.GitBackedRepository.update_file","title":"<code>update_file(file_path)</code>","text":"<p>Stage and commit changes to a file in the Git repository.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Absolute or relative path to the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Commit hash if changes were made.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> <code>ValueError</code> <p>If the file is outside the repository.</p> <code>GitCommandError</code> <p>If Git operations fail.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def update_file(self, file_path: Path) -&gt; str:\n    \"\"\"\n    Stage and commit changes to a file in the Git repository.\n\n    Args:\n        file_path: Absolute or relative path to the file.\n\n    Returns:\n        str: Commit hash if changes were made.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the file is outside the repository.\n        GitCommandError: If Git operations fail.\n    \"\"\"\n    file_path = file_path.resolve()\n\n    # Ensure the file is within the repository\n    try:\n        rel_path = file_path.relative_to(self.repo_path)\n    except ValueError as e:\n        raise ValueError(\n            f\"File {file_path} is not under the repository root {self.repo_path}\"\n        ) from e\n\n    if not file_path.exists():\n        raise FileNotFoundError(f\"File does not exist: {file_path}\")\n\n    try:\n        return self._commit_file_update(rel_path, file_path)\n    except GitCommandError as e:\n        logger.error(f\"Git operation failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.LocalPromptManager","title":"<code>LocalPromptManager</code>","text":"<p>A simple singleton implementation of PromptManager that ensures only one instance is created and reused throughout the application lifecycle.</p> <p>This class wraps the PromptManager to provide efficient prompt loading by maintaining a single reusable instance.</p> <p>Attributes:</p> Name Type Description <code>_instance</code> <code>Optional[SingletonPromptManager]</code> <p>The singleton instance</p> <code>_prompt_manager</code> <code>Optional[PromptManager]</code> <p>The wrapped PromptManager instance</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>class LocalPromptManager:\n    \"\"\"\n    A simple singleton implementation of PromptManager that ensures only one instance\n    is created and reused throughout the application lifecycle.\n\n    This class wraps the PromptManager to provide efficient prompt loading by\n    maintaining a single reusable instance.\n\n    Attributes:\n        _instance (Optional[SingletonPromptManager]): The singleton instance\n        _prompt_manager (Optional[PromptManager]): The wrapped PromptManager instance\n    \"\"\"\n\n    _instance: Optional[\"LocalPromptManager\"] = None\n\n    def __new__(cls) -&gt; \"LocalPromptManager\":\n        \"\"\"\n        Create or return the singleton instance.\n\n        Returns:\n            SingletonPromptManager: The singleton instance\n        \"\"\"\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._prompt_manager = None\n        return cls._instance\n\n    @property\n    def prompt_manager(self) -&gt; \"PromptCatalog\":\n        \"\"\"\n        Lazy initialization of the PromptManager instance.\n\n        Returns:\n            PromptManager: The wrapped PromptManager instance\n\n        Raises:\n            RuntimeError: If PATTERN_REPO is not properly configured\n        \"\"\"\n        if self._prompt_manager is None:  # type: ignore\n            try:\n                load_dotenv()\n                if prompt_path_name := os.getenv(\"TNH_PATTERN_DIR\"):\n                    prompt_dir = Path(prompt_path_name)\n                    logger.debug(f\"prompt dir: {prompt_path_name}\")\n                else:\n                    prompt_dir = TNH_DEFAULT_PATTERN_DIR\n                self._prompt_manager = PromptCatalog(prompt_dir)\n            except ImportError as err:\n                raise RuntimeError(\n                    \"Failed to initialize PromptManager. Ensure prompt_manager \"\n                    f\"module and PATTERN_REPO are properly configured: {err}\"\n                ) from err\n        return self._prompt_manager\n\n    def get_prompt(self, name: str) -&gt; Prompt:\n        \"\"\"Get a prompt by name.\"\"\"\n        return self.prompt_manager.load(Prompt._normalize_name(name))\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.LocalPromptManager.prompt_manager","title":"<code>prompt_manager</code>  <code>property</code>","text":"<p>Lazy initialization of the PromptManager instance.</p> <p>Returns:</p> Name Type Description <code>PromptManager</code> <code>PromptCatalog</code> <p>The wrapped PromptManager instance</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If PATTERN_REPO is not properly configured</p>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.LocalPromptManager.__new__","title":"<code>__new__()</code>","text":"<p>Create or return the singleton instance.</p> <p>Returns:</p> Name Type Description <code>SingletonPromptManager</code> <code>LocalPromptManager</code> <p>The singleton instance</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __new__(cls) -&gt; \"LocalPromptManager\":\n    \"\"\"\n    Create or return the singleton instance.\n\n    Returns:\n        SingletonPromptManager: The singleton instance\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n        cls._instance._prompt_manager = None\n    return cls._instance\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.LocalPromptManager.get_prompt","title":"<code>get_prompt(name)</code>","text":"<p>Get a prompt by name.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def get_prompt(self, name: str) -&gt; Prompt:\n    \"\"\"Get a prompt by name.\"\"\"\n    return self.prompt_manager.load(Prompt._normalize_name(name))\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt","title":"<code>Prompt</code>","text":"<p>Base Prompt class for version-controlled template prompts.</p> <p>Prompts contain: - Instructions: The main prompt instructions as a Jinja2 template.    Note: Instructions are intended to be saved in markdown format in a .md file. - Template fields: Default values for template variables - Metadata: Name and identifier information</p> <p>Version control is handled externally through Git, not in the prompt itself. Prompt identity is determined by the combination of identifiers.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the prompt</p> <code>instructions</code> <code>str</code> <p>The Jinja2 template string for this prompt</p> <code>default_template_fields</code> <code>Dict[str, str]</code> <p>Default values for template variables</p> <code>_allow_empty_vars</code> <code>bool</code> <p>Whether to allow undefined template variables</p> <code>_env</code> <code>Environment</code> <p>Configured Jinja2 environment instance</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>class Prompt:\n    \"\"\"\n    Base Prompt class for version-controlled template prompts.\n\n    Prompts contain:\n    - Instructions: The main prompt instructions as a Jinja2 template.\n       Note: Instructions are intended to be saved in markdown format in a .md file.\n    - Template fields: Default values for template variables\n    - Metadata: Name and identifier information\n\n    Version control is handled externally through Git, not in the prompt itself.\n    Prompt identity is determined by the combination of identifiers.\n\n    Attributes:\n        name (str): The name of the prompt\n        instructions (str): The Jinja2 template string for this prompt\n        default_template_fields (Dict[str, str]): Default values for template variables\n        _allow_empty_vars (bool): Whether to allow undefined template variables\n        _env (Environment): Configured Jinja2 environment instance\n    \"\"\"\n\n    @staticmethod\n    def _normalize_name(value: str) -&gt; str:\n        \"\"\"Canonicalize prompt names for case-insensitive handling.\n\n        Currently: strip() + lower(). If future rules are needed (e.g.,\n        removing punctuation, limiting length), implement them here.\n        \"\"\"\n        return value.strip().lower()\n\n    def __init__(\n        self,\n        name: str,\n        instructions: MarkdownStr,\n        path: Optional[Path] = None,\n        default_template_fields: Optional[Dict[str, str]] = None,\n        allow_empty_vars: bool = False,        \n    ) -&gt; None:\n        \"\"\"\n        Initialize a new Prompt instance.\n\n        Args:\n            name: Unique name identifying the prompt\n            instructions: Jinja2 template string containing the prompt\n            default_template_fields: Optional default values for template variables\n            allow_empty_vars: Whether to allow undefined template variables\n\n        Raises:\n            ValueError: If name or instructions are empty\n            TemplateError: If template syntax is invalid\n        \"\"\"\n        if not name or not instructions:\n            raise ValueError(\"Name and instructions must not be empty\")\n\n        # Normalize prompt name to lowercase for case-insensitive handling\n        name = Prompt._normalize_name(name)\n\n        self.name = name\n        self.instructions = instructions\n        self.path = path\n        self.default_template_fields = default_template_fields or {}\n        self._allow_empty_vars = allow_empty_vars\n        self._env = self._create_environment()\n\n        # Validate template syntax on initialization\n        self._validate_template()\n\n    @staticmethod\n    def _create_environment() -&gt; Environment:\n        \"\"\"\n        Create and configure a Jinja2 environment with optimal settings.\n\n        Returns:\n            Environment: Configured Jinja2 environment \n            with security and formatting options\n        \"\"\"\n        return Environment(\n            undefined=StrictUndefined,  # Raise errors for undefined variables\n            trim_blocks=True,  # Remove first newline after a block\n            lstrip_blocks=True,  # Strip tabs and spaces from the start of lines\n            autoescape=True,  # Enable autoescaping for security\n        )\n\n    def _validate_template(self) -&gt; None:\n        \"\"\"\n        Validate the template syntax without rendering.\n\n        Raises:\n            TemplateError: If template syntax is invalid\n        \"\"\"\n        try:\n            self._env.parse(self.instructions)\n        except TemplateError as e:\n            raise TemplateError(\n                f\"Invalid template syntax in prompt '{self.name}': {str(e)}\"\n            ) from e\n\n    def apply_template(self, field_values: Optional[Dict[str, str]] = None) -&gt; str:\n        \"\"\"\n        Apply template values to prompt instructions using Jinja2.\n\n        Values precedence (highest to lowest):\n        1. field_values (explicitly passed)\n        2. frontmatter values (from prompt file)\n        3. default_template_fields (prompt defaults)\n\n        Args:\n            field_values: Values to substitute into the template.\n                        If None, uses frontmatter/defaults.\n\n        Returns:\n            str: Rendered instructions with template values applied.\n\n        Raises:\n            TemplateError: If template rendering fails\n            ValueError: If required template variables are missing\n        \"\"\"\n        # Get frontmatter values\n        frontmatter = self.extract_frontmatter() or {}\n\n        # Combine values with correct precedence using | operator\n        template_values = self.default_template_fields | \\\n            frontmatter | (field_values or {})\n\n        instructions = self.get_content_without_frontmatter()\n        logger.debug(f\"instructions without frontmatter:\\n{instructions}\")\n\n        try:\n            return self._render_template_with_values(instructions, template_values)\n        except TemplateError as e:\n            raise TemplateError(\n                f\"Template rendering failed for prompt '{self.name}': {str(e)}\"\n                ) from e\n\n    def _render_template_with_values(\n        self, \n        instructions: str, \n        template_values: dict\n        ) -&gt; str:\n        \"\"\"\n        Validate and render template with provided values.\n\n        Args:\n            instructions: Template content without frontmatter\n            template_values: Values to substitute into template\n\n        Returns:\n            Rendered template string\n\n        Raises:\n            ValueError: If required template variables are missing\n        \"\"\"\n        # Parse for validation\n        parsed_content = self._env.parse(instructions)\n        required_vars = find_undeclared_variables(parsed_content)\n\n        # Validate variables\n        missing_vars = required_vars - set(template_values.keys())\n        if missing_vars and not self._allow_empty_vars:\n            raise ValueError(\n                f\"Missing required template variables in prompt '{self.name}': \"\n                f\"{', '.join(sorted(missing_vars))}\"\n            )\n\n        # Create and render template\n        template = self._env.from_string(instructions)\n        return template.render(**template_values)\n\n    def extract_frontmatter(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Extract and validate YAML frontmatter from markdown instructions.\n\n        Returns:\n            Optional[Dict]: Frontmatter data if found and valid, None otherwise\n\n        Note:\n            Frontmatter must be at the very start of the file and properly formatted.\n        \"\"\"\n\n        prompt = r\"\\A---\\s*\\n(.*?)\\n---\\s*(?:\\n|$)\"\n        if match := re.match(prompt, self.instructions, re.DOTALL):\n            try:\n                frontmatter = yaml.safe_load(match[1])\n                if frontmatter is None:\n                    return None\n                if not isinstance(frontmatter, dict):\n                    logger.warning(f\"Frontmatter must be a YAML dictionary: \"\n                                   f\"{frontmatter}\")\n                    return None\n                return frontmatter\n            except yaml.YAMLError as e:\n                logger.warning(f\"Invalid YAML in frontmatter: {e}\")\n                return None\n        return None\n\n    def get_content_without_frontmatter(self) -&gt; str:\n        \"\"\"\n        Get markdown content with frontmatter removed.\n\n        Returns:\n            str: Markdown content without frontmatter\n        \"\"\"\n        prompt = r\"\\A---\\s*\\n.*?\\n---\\s*\\n\"\n        return re.sub(prompt, \"\", self.instructions, flags=re.DOTALL)\n\n    def update_frontmatter(self, new_data: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Update or add frontmatter to the markdown content.\n\n        Args:\n            new_data: Dictionary of frontmatter fields to update\n        \"\"\"\n\n        current_frontmatter = self.extract_frontmatter() or {}\n        updated_frontmatter = {**current_frontmatter, **new_data}\n\n        # Create YAML string\n        yaml_str = yaml.dump(\n            updated_frontmatter, default_flow_style=False, allow_unicode=True\n        )\n\n        # Remove existing frontmatter if present\n        content = self.get_content_without_frontmatter()\n\n        # Combine new frontmatter with content\n        self.instructions = f\"---\\n{yaml_str}---\\n\\n{content}\"\n\n\n    def source_bytes(self) -&gt; bytes:\n        \"\"\"\n        Best-effort raw bytes for prompt hashing.\n\n        Prefers hashing exact on-disk bytes including front-matter.\n        We therefore first try to read from `prompt_path`. If that fails, we fall back\n        to hashing the concatenation of known templates. In V1, only\n        the instructions (system template) are used for rendering.\n        \"\"\"\n        # Preferred path: use on-disk bytes when available.\n        if self.path is not None:\n            return self.path.read_bytes()\n\n        # Fallback: concatenate known templates deterministically\n        sys_part = self.instructions or \"\"\n        return sys_part.encode(\"utf-8\")\n\n    def content_hash(self) -&gt; str:\n        \"\"\"\n        Generate a SHA-256 hash of the prompt content.\n\n        Useful for quick content comparison and change detection.\n\n        Returns:\n            str: Hexadecimal string of the SHA-256 hash\n        \"\"\"\n        content = (\n            f\"{self.name}{self.instructions}\"\n            f\"{sorted(self.default_template_fields.items())}\"\n            )\n        return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Convert prompt to dictionary for serialization.\n\n        Returns:\n            Dict containing all prompt data in serializable format\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"instructions\": self.instructions,\n            \"default_template_fields\": self.default_template_fields,\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"Prompt\":\n        \"\"\"\n        Create prompt instance from dictionary data.\n\n        Args:\n            data: Dictionary containing prompt data\n\n        Returns:\n            Prompt: New prompt instance\n\n        Raises:\n            ValueError: If required fields are missing\n        \"\"\"\n        required_fields = {\"name\", \"instructions\"}\n        if missing_fields := required_fields - set(data.keys()):\n            raise ValueError(f\"Missing required fields: {', '.join(missing_fields)}\")\n\n        return cls(\n            name=Prompt._normalize_name(str(data[\"name\"])),\n            instructions=data[\"instructions\"],\n            path=None,\n            default_template_fields=data.get(\"default_template_fields\", {}),\n        )\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Compare prompts based on their content.\"\"\"\n        if not isinstance(other, Prompt):\n            return NotImplemented\n        return self.content_hash() == other.content_hash()\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Hash based on content hash for container operations.\"\"\"\n        return hash(self.content_hash())\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.default_template_fields","title":"<code>default_template_fields = default_template_fields or {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.instructions","title":"<code>instructions = instructions</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.name","title":"<code>name = name</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.path","title":"<code>path = path</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare prompts based on their content.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Compare prompts based on their content.\"\"\"\n    if not isinstance(other, Prompt):\n        return NotImplemented\n    return self.content_hash() == other.content_hash()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash based on content hash for container operations.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Hash based on content hash for container operations.\"\"\"\n    return hash(self.content_hash())\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.__init__","title":"<code>__init__(name, instructions, path=None, default_template_fields=None, allow_empty_vars=False)</code>","text":"<p>Initialize a new Prompt instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name identifying the prompt</p> required <code>instructions</code> <code>MarkdownStr</code> <p>Jinja2 template string containing the prompt</p> required <code>default_template_fields</code> <code>Optional[Dict[str, str]]</code> <p>Optional default values for template variables</p> <code>None</code> <code>allow_empty_vars</code> <code>bool</code> <p>Whether to allow undefined template variables</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If name or instructions are empty</p> <code>TemplateError</code> <p>If template syntax is invalid</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    instructions: MarkdownStr,\n    path: Optional[Path] = None,\n    default_template_fields: Optional[Dict[str, str]] = None,\n    allow_empty_vars: bool = False,        \n) -&gt; None:\n    \"\"\"\n    Initialize a new Prompt instance.\n\n    Args:\n        name: Unique name identifying the prompt\n        instructions: Jinja2 template string containing the prompt\n        default_template_fields: Optional default values for template variables\n        allow_empty_vars: Whether to allow undefined template variables\n\n    Raises:\n        ValueError: If name or instructions are empty\n        TemplateError: If template syntax is invalid\n    \"\"\"\n    if not name or not instructions:\n        raise ValueError(\"Name and instructions must not be empty\")\n\n    # Normalize prompt name to lowercase for case-insensitive handling\n    name = Prompt._normalize_name(name)\n\n    self.name = name\n    self.instructions = instructions\n    self.path = path\n    self.default_template_fields = default_template_fields or {}\n    self._allow_empty_vars = allow_empty_vars\n    self._env = self._create_environment()\n\n    # Validate template syntax on initialization\n    self._validate_template()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.apply_template","title":"<code>apply_template(field_values=None)</code>","text":"<p>Apply template values to prompt instructions using Jinja2.</p> <p>Values precedence (highest to lowest): 1. field_values (explicitly passed) 2. frontmatter values (from prompt file) 3. default_template_fields (prompt defaults)</p> <p>Parameters:</p> Name Type Description Default <code>field_values</code> <code>Optional[Dict[str, str]]</code> <p>Values to substitute into the template.         If None, uses frontmatter/defaults.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered instructions with template values applied.</p> <p>Raises:</p> Type Description <code>TemplateError</code> <p>If template rendering fails</p> <code>ValueError</code> <p>If required template variables are missing</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def apply_template(self, field_values: Optional[Dict[str, str]] = None) -&gt; str:\n    \"\"\"\n    Apply template values to prompt instructions using Jinja2.\n\n    Values precedence (highest to lowest):\n    1. field_values (explicitly passed)\n    2. frontmatter values (from prompt file)\n    3. default_template_fields (prompt defaults)\n\n    Args:\n        field_values: Values to substitute into the template.\n                    If None, uses frontmatter/defaults.\n\n    Returns:\n        str: Rendered instructions with template values applied.\n\n    Raises:\n        TemplateError: If template rendering fails\n        ValueError: If required template variables are missing\n    \"\"\"\n    # Get frontmatter values\n    frontmatter = self.extract_frontmatter() or {}\n\n    # Combine values with correct precedence using | operator\n    template_values = self.default_template_fields | \\\n        frontmatter | (field_values or {})\n\n    instructions = self.get_content_without_frontmatter()\n    logger.debug(f\"instructions without frontmatter:\\n{instructions}\")\n\n    try:\n        return self._render_template_with_values(instructions, template_values)\n    except TemplateError as e:\n        raise TemplateError(\n            f\"Template rendering failed for prompt '{self.name}': {str(e)}\"\n            ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.content_hash","title":"<code>content_hash()</code>","text":"<p>Generate a SHA-256 hash of the prompt content.</p> <p>Useful for quick content comparison and change detection.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Hexadecimal string of the SHA-256 hash</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def content_hash(self) -&gt; str:\n    \"\"\"\n    Generate a SHA-256 hash of the prompt content.\n\n    Useful for quick content comparison and change detection.\n\n    Returns:\n        str: Hexadecimal string of the SHA-256 hash\n    \"\"\"\n    content = (\n        f\"{self.name}{self.instructions}\"\n        f\"{sorted(self.default_template_fields.items())}\"\n        )\n    return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.extract_frontmatter","title":"<code>extract_frontmatter()</code>","text":"<p>Extract and validate YAML frontmatter from markdown instructions.</p> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict]: Frontmatter data if found and valid, None otherwise</p> Note <p>Frontmatter must be at the very start of the file and properly formatted.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def extract_frontmatter(self) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract and validate YAML frontmatter from markdown instructions.\n\n    Returns:\n        Optional[Dict]: Frontmatter data if found and valid, None otherwise\n\n    Note:\n        Frontmatter must be at the very start of the file and properly formatted.\n    \"\"\"\n\n    prompt = r\"\\A---\\s*\\n(.*?)\\n---\\s*(?:\\n|$)\"\n    if match := re.match(prompt, self.instructions, re.DOTALL):\n        try:\n            frontmatter = yaml.safe_load(match[1])\n            if frontmatter is None:\n                return None\n            if not isinstance(frontmatter, dict):\n                logger.warning(f\"Frontmatter must be a YAML dictionary: \"\n                               f\"{frontmatter}\")\n                return None\n            return frontmatter\n        except yaml.YAMLError as e:\n            logger.warning(f\"Invalid YAML in frontmatter: {e}\")\n            return None\n    return None\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create prompt instance from dictionary data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing prompt data</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>New prompt instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are missing</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"Prompt\":\n    \"\"\"\n    Create prompt instance from dictionary data.\n\n    Args:\n        data: Dictionary containing prompt data\n\n    Returns:\n        Prompt: New prompt instance\n\n    Raises:\n        ValueError: If required fields are missing\n    \"\"\"\n    required_fields = {\"name\", \"instructions\"}\n    if missing_fields := required_fields - set(data.keys()):\n        raise ValueError(f\"Missing required fields: {', '.join(missing_fields)}\")\n\n    return cls(\n        name=Prompt._normalize_name(str(data[\"name\"])),\n        instructions=data[\"instructions\"],\n        path=None,\n        default_template_fields=data.get(\"default_template_fields\", {}),\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.get_content_without_frontmatter","title":"<code>get_content_without_frontmatter()</code>","text":"<p>Get markdown content with frontmatter removed.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Markdown content without frontmatter</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def get_content_without_frontmatter(self) -&gt; str:\n    \"\"\"\n    Get markdown content with frontmatter removed.\n\n    Returns:\n        str: Markdown content without frontmatter\n    \"\"\"\n    prompt = r\"\\A---\\s*\\n.*?\\n---\\s*\\n\"\n    return re.sub(prompt, \"\", self.instructions, flags=re.DOTALL)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.source_bytes","title":"<code>source_bytes()</code>","text":"<p>Best-effort raw bytes for prompt hashing.</p> <p>Prefers hashing exact on-disk bytes including front-matter. We therefore first try to read from <code>prompt_path</code>. If that fails, we fall back to hashing the concatenation of known templates. In V1, only the instructions (system template) are used for rendering.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def source_bytes(self) -&gt; bytes:\n    \"\"\"\n    Best-effort raw bytes for prompt hashing.\n\n    Prefers hashing exact on-disk bytes including front-matter.\n    We therefore first try to read from `prompt_path`. If that fails, we fall back\n    to hashing the concatenation of known templates. In V1, only\n    the instructions (system template) are used for rendering.\n    \"\"\"\n    # Preferred path: use on-disk bytes when available.\n    if self.path is not None:\n        return self.path.read_bytes()\n\n    # Fallback: concatenate known templates deterministically\n    sys_part = self.instructions or \"\"\n    return sys_part.encode(\"utf-8\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert prompt to dictionary for serialization.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing all prompt data in serializable format</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert prompt to dictionary for serialization.\n\n    Returns:\n        Dict containing all prompt data in serializable format\n    \"\"\"\n    return {\n        \"name\": self.name,\n        \"instructions\": self.instructions,\n        \"default_template_fields\": self.default_template_fields,\n    }\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.Prompt.update_frontmatter","title":"<code>update_frontmatter(new_data)</code>","text":"<p>Update or add frontmatter to the markdown content.</p> <p>Parameters:</p> Name Type Description Default <code>new_data</code> <code>Dict[str, Any]</code> <p>Dictionary of frontmatter fields to update</p> required Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def update_frontmatter(self, new_data: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Update or add frontmatter to the markdown content.\n\n    Args:\n        new_data: Dictionary of frontmatter fields to update\n    \"\"\"\n\n    current_frontmatter = self.extract_frontmatter() or {}\n    updated_frontmatter = {**current_frontmatter, **new_data}\n\n    # Create YAML string\n    yaml_str = yaml.dump(\n        updated_frontmatter, default_flow_style=False, allow_unicode=True\n    )\n\n    # Remove existing frontmatter if present\n    content = self.get_content_without_frontmatter()\n\n    # Combine new frontmatter with content\n    self.instructions = f\"---\\n{yaml_str}---\\n\\n{content}\"\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.PromptCatalog","title":"<code>PromptCatalog</code>","text":"<p>Main interface for prompt management system.</p> <p>Provides high-level operations: - Prompt creation and loading - Automatic versioning - Safe concurrent access - Basic history tracking - Case-insensitive prompt names (stored as lowercase)</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>class PromptCatalog:\n    \"\"\"\n    Main interface for prompt management system.\n\n    Provides high-level operations:\n    - Prompt creation and loading\n    - Automatic versioning\n    - Safe concurrent access\n    - Basic history tracking\n    - Case-insensitive prompt names (stored as lowercase)\n    \"\"\"\n\n    def __init__(self, base_path: Path):\n        \"\"\"\n        Initialize prompt management system.\n\n        Args:\n            base_path: Base directory for prompt storage\n        \"\"\"\n        self.base_path = Path(base_path).resolve()\n        self.base_path.mkdir(parents=True, exist_ok=True)\n\n        # Initialize subsystems\n        self.repo = GitBackedRepository(self.base_path)\n        self.access_manager = ConcurrentAccessManager(self.base_path / \".locks\")\n\n        logger.info(f\"Initialized prompt management system at {base_path}\")\n\n    def _normalize_path(self, path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Normalize a path to be absolute under the repository base path.\n\n        Handles these cases to same result:\n        - \"my_file\" -&gt; &lt;base_path&gt;/my_file\n        - \"&lt;base_path&gt;/my_file\" -&gt; &lt;base_path&gt;/my_file\n\n        Args:\n            path: Input path as string or Path\n\n        Returns:\n            Path: Absolute path under base_path\n\n        Raises:\n            ValueError: If path would resolve outside repository\n        \"\"\"\n        path = Path(path)  # ensure we have a path\n\n        # Join with base_path as needed: always interpret relative\n        # paths as relative to the repository base path. This avoids\n        # incorrectly handling nested relative paths like \"a/b\"\n        # which may not have the same parent as self.base_path.\n        if not path.is_absolute():\n            path = self.base_path / path\n\n        # Safety check after resolution\n        resolved = path.resolve()\n        try:\n            resolved.relative_to(self.base_path)\n        except ValueError as e:\n            raise ValueError(\n                f\"Path {path} resolves outside repository: {self.base_path}\"\n            ) from e\n\n        return resolved\n\n    def get_path(self, prompt_name: str) -&gt; Optional[Path]:\n        \"\"\"\n        Recursively search for a prompt file with the given name (case-insensitive)\n        in base_path and all subdirectories.\n\n        Args:\n            prompt_name: prompt name (without extension) to search for\n\n        Returns:\n            Optional[Path]: Full path to the found prompt file, or None if not found\n        \"\"\"\n        target = Prompt._normalize_name(prompt_name)\n        with suppress(StopIteration):\n            for path in self.base_path.rglob(\"*.md\"):\n                if path.is_file() and path.stem.lower() == target:\n                    logger.debug(\n                        f\"Found prompt file for name {prompt_name} at: {path}\"\n                    )\n                    return self._normalize_path(path)\n        logger.debug(f\"No prompt file found with name: {prompt_name}\")\n        return None\n\n    def save(self, prompt: Prompt, subdir: Optional[Path] = None) -&gt; Path:\n        prompt_name = Prompt._normalize_name(prompt.name)\n        instructions = prompt.instructions\n\n        if subdir is None:\n            path = self.base_path / f\"{prompt_name}.md\"\n        else:\n            path = self.base_path / subdir / f\"{prompt_name}.md\"\n\n        path = self._normalize_path(path)\n\n        # Check for existing prompt by case-insensitive match\n        existing_path = self.get_path(prompt_name)\n\n        try:\n            # Lock on the destination path name (lowercase) to avoid races\n            with self.access_manager.file_lock(path):\n                # If an existing file is present but at a different case/path, rename it\n                if existing_path is not None and existing_path != path:\n                    path.parent.mkdir(parents=True, exist_ok=True)\n                    logger.info(\n                        f\"Renaming existing prompt file from {existing_path} to {path} \"\n                        \"to enforce lowercase naming.\"\n                    )\n                    existing_path.rename(path)\n\n                write_str_to_file(path, instructions, overwrite=True)\n                self.repo.update_file(path)\n                logger.info(f\"Prompt saved at {path}\")\n                return path.relative_to(self.base_path)\n\n        except Exception as e:\n            logger.error(f\"Failed to save prompt {prompt_name}: {e}\")\n            raise\n\n    def load(self, prompt_name: str) -&gt; Prompt:\n        \"\"\"\n        Load the .md prompt file by name, extract placeholders, and\n        return a fully constructed Prompt object.\n\n        Args:\n            prompt_name: Name of the prompt (without .md extension).\n\n        Returns:\n            A new Prompt object whose 'instructions' is the file's text\n            and whose 'template_fields' are inferred from placeholders in\n            those instructions.\n        \"\"\"\n        prompt_name = Prompt._normalize_name(prompt_name)\n        # Locate the .md file; raise if missing\n        path = self.get_path(prompt_name)\n        if not path:\n            raise FileNotFoundError(f\"No prompt file named {prompt_name}.md found in prompt catalog:\\n\"\n                                    f\"{self.base_path}\"\n                                    )\n\n        # Acquire lock before reading\n        with self.access_manager.file_lock(path):\n            instructions = read_str_from_file(path)\n\n        instructions = MarkdownStr(instructions)\n\n        # Create the prompt from the raw .md text (name is already lowercase)\n        prompt = Prompt(name=prompt_name, instructions=instructions, path=path)\n\n        # Check for local uncommitted changes, updating file:\n        self.repo.update_file(path)\n\n        return prompt\n\n    def show_history(self, prompt_name: str) -&gt; None:\n        if path := self.get_path(prompt_name):\n            self.repo.display_history(path)\n        else:\n            logger.error(f\"Path to {prompt_name} not found.\")\n            return\n\n    # def get_prompt_history_from_path(self, path: Path) -&gt; List[Dict[str, Any]]:\n    #     \"\"\"\n    #     Get version history for a prompt.\n\n    #     Args:\n    #         path: Path to prompt file\n\n    #     Returns:\n    #         List of version information\n    #     \"\"\"\n    #     path = self._normalize_path(path)\n\n    #     return self.repo.get_history(path)\n\n    @classmethod\n    def verify_repository(cls, base_path: Path) -&gt; bool:\n        \"\"\"\n        Verify repository integrity and uniqueness of prompt names.\n\n        Performs the following checks:\n        1. Validates Git repository structure.\n        2. Ensures no duplicate prompt names exist.\n\n        Args:\n            base_path: Repository path to verify.\n\n        Returns:\n            bool: True if the repository is valid \n            and contains no duplicate prompt files.\n        \"\"\"\n        try:\n            # Check if it's a valid Git repository\n            repo = Repo(base_path)\n\n            # Verify basic repository structure\n            basic_valid = (\n                repo.head.is_valid()\n                and not repo.bare\n                and (base_path / \".git\").is_dir()\n                and (base_path / \".locks\").is_dir()\n            )\n\n            if not basic_valid:\n                return False\n\n            prompt_files = list(base_path.rglob(\"*.md\"))\n            seen_names: Dict[str, Path] = {}\n\n            for prompt_file in prompt_files:\n                # Skip files in .git directory\n                if \".git\" in prompt_file.parts:\n                    continue\n\n                # Case-insensitive key\n                key = Prompt._normalize_name(prompt_file.stem)\n\n                if key in seen_names:\n                    logger.error(\n                        f\"Duplicate prompt file detected (case-insensitive):\\n\"\n                        f\"  First occurrence: {seen_names[key]}\\n\"\n                        f\"  Second occurrence: {prompt_file}\"\n                    )\n                    return False\n\n                seen_names[key] = prompt_file\n\n            return True\n\n        except (InvalidGitRepositoryError, Exception) as e:\n            logger.error(f\"Repository verification failed: {e}\")\n            return False\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.PromptCatalog.access_manager","title":"<code>access_manager = ConcurrentAccessManager(self.base_path / '.locks')</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.PromptCatalog.base_path","title":"<code>base_path = Path(base_path).resolve()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.PromptCatalog.repo","title":"<code>repo = GitBackedRepository(self.base_path)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.prompts.PromptCatalog.__init__","title":"<code>__init__(base_path)</code>","text":"<p>Initialize prompt management system.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>Base directory for prompt storage</p> required Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def __init__(self, base_path: Path):\n    \"\"\"\n    Initialize prompt management system.\n\n    Args:\n        base_path: Base directory for prompt storage\n    \"\"\"\n    self.base_path = Path(base_path).resolve()\n    self.base_path.mkdir(parents=True, exist_ok=True)\n\n    # Initialize subsystems\n    self.repo = GitBackedRepository(self.base_path)\n    self.access_manager = ConcurrentAccessManager(self.base_path / \".locks\")\n\n    logger.info(f\"Initialized prompt management system at {base_path}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.PromptCatalog.get_path","title":"<code>get_path(prompt_name)</code>","text":"<p>Recursively search for a prompt file with the given name (case-insensitive) in base_path and all subdirectories.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_name</code> <code>str</code> <p>prompt name (without extension) to search for</p> required <p>Returns:</p> Type Description <code>Optional[Path]</code> <p>Optional[Path]: Full path to the found prompt file, or None if not found</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def get_path(self, prompt_name: str) -&gt; Optional[Path]:\n    \"\"\"\n    Recursively search for a prompt file with the given name (case-insensitive)\n    in base_path and all subdirectories.\n\n    Args:\n        prompt_name: prompt name (without extension) to search for\n\n    Returns:\n        Optional[Path]: Full path to the found prompt file, or None if not found\n    \"\"\"\n    target = Prompt._normalize_name(prompt_name)\n    with suppress(StopIteration):\n        for path in self.base_path.rglob(\"*.md\"):\n            if path.is_file() and path.stem.lower() == target:\n                logger.debug(\n                    f\"Found prompt file for name {prompt_name} at: {path}\"\n                )\n                return self._normalize_path(path)\n    logger.debug(f\"No prompt file found with name: {prompt_name}\")\n    return None\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.PromptCatalog.load","title":"<code>load(prompt_name)</code>","text":"<p>Load the .md prompt file by name, extract placeholders, and return a fully constructed Prompt object.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_name</code> <code>str</code> <p>Name of the prompt (without .md extension).</p> required <p>Returns:</p> Type Description <code>Prompt</code> <p>A new Prompt object whose 'instructions' is the file's text</p> <code>Prompt</code> <p>and whose 'template_fields' are inferred from placeholders in</p> <code>Prompt</code> <p>those instructions.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def load(self, prompt_name: str) -&gt; Prompt:\n    \"\"\"\n    Load the .md prompt file by name, extract placeholders, and\n    return a fully constructed Prompt object.\n\n    Args:\n        prompt_name: Name of the prompt (without .md extension).\n\n    Returns:\n        A new Prompt object whose 'instructions' is the file's text\n        and whose 'template_fields' are inferred from placeholders in\n        those instructions.\n    \"\"\"\n    prompt_name = Prompt._normalize_name(prompt_name)\n    # Locate the .md file; raise if missing\n    path = self.get_path(prompt_name)\n    if not path:\n        raise FileNotFoundError(f\"No prompt file named {prompt_name}.md found in prompt catalog:\\n\"\n                                f\"{self.base_path}\"\n                                )\n\n    # Acquire lock before reading\n    with self.access_manager.file_lock(path):\n        instructions = read_str_from_file(path)\n\n    instructions = MarkdownStr(instructions)\n\n    # Create the prompt from the raw .md text (name is already lowercase)\n    prompt = Prompt(name=prompt_name, instructions=instructions, path=path)\n\n    # Check for local uncommitted changes, updating file:\n    self.repo.update_file(path)\n\n    return prompt\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.PromptCatalog.save","title":"<code>save(prompt, subdir=None)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def save(self, prompt: Prompt, subdir: Optional[Path] = None) -&gt; Path:\n    prompt_name = Prompt._normalize_name(prompt.name)\n    instructions = prompt.instructions\n\n    if subdir is None:\n        path = self.base_path / f\"{prompt_name}.md\"\n    else:\n        path = self.base_path / subdir / f\"{prompt_name}.md\"\n\n    path = self._normalize_path(path)\n\n    # Check for existing prompt by case-insensitive match\n    existing_path = self.get_path(prompt_name)\n\n    try:\n        # Lock on the destination path name (lowercase) to avoid races\n        with self.access_manager.file_lock(path):\n            # If an existing file is present but at a different case/path, rename it\n            if existing_path is not None and existing_path != path:\n                path.parent.mkdir(parents=True, exist_ok=True)\n                logger.info(\n                    f\"Renaming existing prompt file from {existing_path} to {path} \"\n                    \"to enforce lowercase naming.\"\n                )\n                existing_path.rename(path)\n\n            write_str_to_file(path, instructions, overwrite=True)\n            self.repo.update_file(path)\n            logger.info(f\"Prompt saved at {path}\")\n            return path.relative_to(self.base_path)\n\n    except Exception as e:\n        logger.error(f\"Failed to save prompt {prompt_name}: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.PromptCatalog.show_history","title":"<code>show_history(prompt_name)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>def show_history(self, prompt_name: str) -&gt; None:\n    if path := self.get_path(prompt_name):\n        self.repo.display_history(path)\n    else:\n        logger.error(f\"Path to {prompt_name} not found.\")\n        return\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.prompts.PromptCatalog.verify_repository","title":"<code>verify_repository(base_path)</code>  <code>classmethod</code>","text":"<p>Verify repository integrity and uniqueness of prompt names.</p> <p>Performs the following checks: 1. Validates Git repository structure. 2. Ensures no duplicate prompt names exist.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>Repository path to verify.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the repository is valid </p> <code>bool</code> <p>and contains no duplicate prompt files.</p> Source code in <code>src/tnh_scholar/ai_text_processing/prompts.py</code> <pre><code>@classmethod\ndef verify_repository(cls, base_path: Path) -&gt; bool:\n    \"\"\"\n    Verify repository integrity and uniqueness of prompt names.\n\n    Performs the following checks:\n    1. Validates Git repository structure.\n    2. Ensures no duplicate prompt names exist.\n\n    Args:\n        base_path: Repository path to verify.\n\n    Returns:\n        bool: True if the repository is valid \n        and contains no duplicate prompt files.\n    \"\"\"\n    try:\n        # Check if it's a valid Git repository\n        repo = Repo(base_path)\n\n        # Verify basic repository structure\n        basic_valid = (\n            repo.head.is_valid()\n            and not repo.bare\n            and (base_path / \".git\").is_dir()\n            and (base_path / \".locks\").is_dir()\n        )\n\n        if not basic_valid:\n            return False\n\n        prompt_files = list(base_path.rglob(\"*.md\"))\n        seen_names: Dict[str, Path] = {}\n\n        for prompt_file in prompt_files:\n            # Skip files in .git directory\n            if \".git\" in prompt_file.parts:\n                continue\n\n            # Case-insensitive key\n            key = Prompt._normalize_name(prompt_file.stem)\n\n            if key in seen_names:\n                logger.error(\n                    f\"Duplicate prompt file detected (case-insensitive):\\n\"\n                    f\"  First occurrence: {seen_names[key]}\\n\"\n                    f\"  Second occurrence: {prompt_file}\"\n                )\n                return False\n\n            seen_names[key] = prompt_file\n\n        return True\n\n    except (InvalidGitRepositoryError, Exception) as e:\n        logger.error(f\"Repository verification failed: {e}\")\n        return False\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.response_format","title":"<code>response_format</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.TEXT_SECTIONS_DESCRIPTION","title":"<code>TEXT_SECTIONS_DESCRIPTION = 'Ordered list of logical sections for the text. The sequence of line ranges for the sections must cover every line from start to finish without any overlaps or gaps.'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.LogicalSection","title":"<code>LogicalSection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A logically coherent section of text.</p> Source code in <code>src/tnh_scholar/ai_text_processing/response_format.py</code> <pre><code>class LogicalSection(BaseModel):\n    \"\"\"\n    A logically coherent section of text.\n    \"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"Meaningful title for the section in the original language of the section.\",\n    )\n    start_line: int = Field(\n        ..., description=\"Starting line number of the section (inclusive).\"\n    )\n    end_line: int = Field(\n        ..., description=\"Ending line number of the section (inclusive).\"\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.response_format.LogicalSection.end_line","title":"<code>end_line = Field(..., description='Ending line number of the section (inclusive).')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.LogicalSection.start_line","title":"<code>start_line = Field(..., description='Starting line number of the section (inclusive).')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.LogicalSection.title","title":"<code>title = Field(..., description='Meaningful title for the section in the original language of the section.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.TextObject","title":"<code>TextObject</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a text in any language broken into coherent logical sections.</p> Source code in <code>src/tnh_scholar/ai_text_processing/response_format.py</code> <pre><code>class TextObject(BaseModel):\n    \"\"\"\n    Represents a text in any language broken into coherent logical sections.\n    \"\"\"\n\n    language: str = Field(..., description=\"ISO 639-1 language code of the text.\")\n    sections: List[LogicalSection] = Field(..., description=TEXT_SECTIONS_DESCRIPTION)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.response_format.TextObject.language","title":"<code>language = Field(..., description='ISO 639-1 language code of the text.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.response_format.TextObject.sections","title":"<code>sections = Field(..., description=TEXT_SECTIONS_DESCRIPTION)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.section_processor","title":"<code>section_processor</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object","title":"<code>text_object</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.StorageFormatType","title":"<code>StorageFormatType = Union[StorageFormat, Literal['text', 'json']]</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.AIResponse","title":"<code>AIResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for dividing large texts into AI-processable segments while maintaining broader document context.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class AIResponse(BaseModel):\n    \"\"\"Class for dividing large texts into AI-processable segments while\n    maintaining broader document context.\"\"\"\n    document_summary: str = Field(\n        ...,\n        description=\"Concise, comprehensive overview of the text's content and purpose\"\n    )\n    document_metadata: str = Field(\n        ...,\n        description=\"Available Dublin Core standard metadata in human-readable YAML format\" # noqa: E501\n    )\n    key_concepts: str = Field(\n        ...,\n        description=\"Important terms, ideas, or references that appear throughout the text\"  # noqa: E501\n    )\n    narrative_context: str = Field(\n        ...,\n        description=\"Concise overview of how the text develops or progresses as a whole\"\n    )\n    language: str = Field(..., description=\"ISO 639-1 language code\")\n    sections: List[LogicalSection]\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.AIResponse.document_metadata","title":"<code>document_metadata = Field(..., description='Available Dublin Core standard metadata in human-readable YAML format')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.AIResponse.document_summary","title":"<code>document_summary = Field(..., description=\"Concise, comprehensive overview of the text's content and purpose\")</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.AIResponse.key_concepts","title":"<code>key_concepts = Field(..., description='Important terms, ideas, or references that appear throughout the text')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.AIResponse.language","title":"<code>language = Field(..., description='ISO 639-1 language code')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.AIResponse.narrative_context","title":"<code>narrative_context = Field(..., description='Concise overview of how the text develops or progresses as a whole')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.AIResponse.sections","title":"<code>sections</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.LoadConfig","title":"<code>LoadConfig</code>  <code>dataclass</code>","text":"<p>Configuration for loading a TextObject.</p> <p>Attributes:</p> Name Type Description <code>format</code> <code>StorageFormat</code> <p>Storage format of the input file</p> <code>source_str</code> <code>Optional[str]</code> <p>Optional source content as string</p> <code>source_file</code> <code>Optional[Path]</code> <p>Optional path to source content file</p> Note <p>For JSON format, exactly one of source_str or source_file may be provided. Both fields are ignored for TEXT format.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@dataclass(frozen=True)\nclass LoadConfig:\n    \"\"\"Configuration for loading a TextObject.\n\n    Attributes:\n        format: Storage format of the input file\n        source_str: Optional source content as string\n        source_file: Optional path to source content file\n\n    Note:\n        For JSON format, exactly one of source_str or source_file may be provided.\n        Both fields are ignored for TEXT format.\n    \"\"\"\n    format: StorageFormat = StorageFormat.TEXT\n    source_str: Optional[str] = None\n    source_file: Optional[Path] = None\n\n    def __post_init__(self):\n        \"\"\"Validate configuration.\"\"\"\n        valid_source = (\n            (self.source_str is None) ^ (self.source_file is None)\n        )\n        if self.format == StorageFormat.JSON and not valid_source:\n            raise ValueError(\n                \"Either source_str or source_file (not both) \"\n                \"must be set for JSON format.\"\n            )\n\n    def get_source_text(self) -&gt; Optional[str]:\n        \"\"\"Get source content as text if provided.\"\"\"\n        if self.source_file is not None:\n            return read_str_from_file(self.source_file)\n        return self.source_str\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.LoadConfig.format","title":"<code>format = StorageFormat.TEXT</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.LoadConfig.source_file","title":"<code>source_file = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.LoadConfig.source_str","title":"<code>source_str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.LoadConfig.__init__","title":"<code>__init__(format=StorageFormat.TEXT, source_str=None, source_file=None)</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.LoadConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate configuration.\"\"\"\n    valid_source = (\n        (self.source_str is None) ^ (self.source_file is None)\n    )\n    if self.format == StorageFormat.JSON and not valid_source:\n        raise ValueError(\n            \"Either source_str or source_file (not both) \"\n            \"must be set for JSON format.\"\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.LoadConfig.get_source_text","title":"<code>get_source_text()</code>","text":"<p>Get source content as text if provided.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def get_source_text(self) -&gt; Optional[str]:\n    \"\"\"Get source content as text if provided.\"\"\"\n    if self.source_file is not None:\n        return read_str_from_file(self.source_file)\n    return self.source_str\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.LogicalSection","title":"<code>LogicalSection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a contextually meaningful segment of a larger text.</p> <p>Sections should preserve natural breaks in content  (explicit section markers, topic shifts, argument development, narrative progression)  while staying within specified size limits in order to create chunks suitable for AI processing.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class LogicalSection(BaseModel):\n    \"\"\"\n    Represents a contextually meaningful segment of a larger text.\n\n    Sections should preserve natural breaks in content \n    (explicit section markers, topic shifts, argument development, narrative progression) \n    while staying within specified size limits in order to create chunks suitable for AI processing.\n    \"\"\"  # noqa: E501\n    start_line: int = Field(\n        ..., \n        description=\"Starting line number that begins this logical segment\"\n    )\n    title: str = Field(\n        ...,\n        description=\"Descriptive title of section's key content\"\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.LogicalSection.start_line","title":"<code>start_line = Field(..., description='Starting line number that begins this logical segment')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.LogicalSection.title","title":"<code>title = Field(..., description=\"Descriptive title of section's key content\")</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionEntry","title":"<code>SectionEntry</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Represents a section with its content during iteration.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class SectionEntry(NamedTuple):\n    \"\"\"Represents a section with its content during iteration.\"\"\"\n    number: int         # Logical Section number (1 based index)\n    title: str          # Section title \n    content: str        # Section content\n    range: SectionRange # Section range\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionEntry.content","title":"<code>content</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionEntry.number","title":"<code>number</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionEntry.range","title":"<code>range</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionEntry.title","title":"<code>title</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionObject","title":"<code>SectionObject</code>  <code>dataclass</code>","text":"<p>Represents a section of text with metadata.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@dataclass\nclass SectionObject:\n    \"\"\"Represents a section of text with metadata.\"\"\"\n    title: str\n    section_range: SectionRange\n    metadata: Optional[Metadata] \n\n    @classmethod\n    def from_logical_section(\n        cls, \n        logical_section: LogicalSection, \n        end_line: int, \n        metadata: Optional[Metadata] = None\n        ) -&gt; \"SectionObject\":\n        \"\"\"Create a SectionObject from a LogicalSection model.\"\"\"\n        return cls(\n            title=logical_section.title,\n            section_range = SectionRange(logical_section.start_line, end_line),\n            metadata = metadata \n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionObject.metadata","title":"<code>metadata</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionObject.section_range","title":"<code>section_range</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionObject.title","title":"<code>title</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionObject.__init__","title":"<code>__init__(title, section_range, metadata)</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionObject.from_logical_section","title":"<code>from_logical_section(logical_section, end_line, metadata=None)</code>  <code>classmethod</code>","text":"<p>Create a SectionObject from a LogicalSection model.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_logical_section(\n    cls, \n    logical_section: LogicalSection, \n    end_line: int, \n    metadata: Optional[Metadata] = None\n    ) -&gt; \"SectionObject\":\n    \"\"\"Create a SectionObject from a LogicalSection model.\"\"\"\n    return cls(\n        title=logical_section.title,\n        section_range = SectionRange(logical_section.start_line, end_line),\n        metadata = metadata \n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionRange","title":"<code>SectionRange</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Represents the line range of a section.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class SectionRange(NamedTuple):\n    \"\"\"Represents the line range of a section.\"\"\"\n    start: int  # Start line (inclusive)\n    end: int    # End line (Exclusive)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionRange.end","title":"<code>end</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.SectionRange.start","title":"<code>start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.StorageFormat","title":"<code>StorageFormat</code>","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class StorageFormat(Enum):\n    TEXT = \"text\"\n    JSON = \"json\"\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.StorageFormat.JSON","title":"<code>JSON = 'json'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.StorageFormat.TEXT","title":"<code>TEXT = 'text'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject","title":"<code>TextObject</code>","text":"<p>Manages text content with section organization and metadata tracking.</p> <p>TextObject serves as the core container for text processing, providing: - Line-numbered text content management - Language identification - Section organization and access - Metadata tracking including incorporated processing stages</p> <p>The class allows for section boundaries through line numbering, allowing sections to be defined by start lines without explicit end lines. Subsequent sections implicitly end where the next section begins. SectionObjects are utilized to represent sections.</p> <p>Attributes:</p> Name Type Description <code>num_text</code> <code>NumberedText</code> <p>Line-numbered text content manager</p> <code>language</code> <code>str</code> <p>ISO 639-1 language code for the text content</p> <code>_sections</code> <code>List[SectionObject]</code> <p>Internal list of text sections with boundaries</p> <code>_metadata</code> <code>Metadata</code> <p>Processing and content metadata container</p> Example <p>content = NumberedText(\"Line 1\\nLine 2\\nLine 3\") obj = TextObject(content, language=\"en\")</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class TextObject:\n    \"\"\"\n    Manages text content with section organization and metadata tracking.\n\n    TextObject serves as the core container for text processing, providing:\n    - Line-numbered text content management\n    - Language identification\n    - Section organization and access\n    - Metadata tracking including incorporated processing stages\n\n    The class allows for section boundaries through line numbering,\n    allowing sections to be defined by start lines without explicit end lines.\n    Subsequent sections implicitly end where the next section begins.\n    SectionObjects are utilized to represent sections.\n\n    Attributes:\n        num_text: Line-numbered text content manager\n        language: ISO 639-1 language code for the text content\n        _sections: Internal list of text sections with boundaries\n        _metadata: Processing and content metadata container\n\n    Example:\n        &gt;&gt;&gt; content = NumberedText(\"Line 1\\\\nLine 2\\\\nLine 3\")\n        &gt;&gt;&gt; obj = TextObject(content, language=\"en\")\n    \"\"\"\n    num_text: NumberedText \n    language: str \n    _sections: List[SectionObject]\n    _metadata: Metadata\n\n    def __init__(self, \n        num_text: NumberedText, \n        language: Optional[str] = None, \n        sections: Optional[List[SectionObject]] = None,\n        metadata: Optional[Metadata] = None):\n        \"\"\"\n        Initialize a TextObject with content and optional organizing components.\n\n        Args:\n            num_text: Text content with line numbering\n            language: ISO 639-1 language code. If None, auto-detected from content\n            sections: Initial sections defining text organization. If None, \n                      text is considered un-sectioned.\n            metadata: Initial metadata. If None, creates empty metadata container\n\n        Note:\n            Until sections are established, section-based methods will raise a value\n            error if called.\n        \"\"\"\n        self.num_text = num_text\n        self.language = language or get_language_code_from_text(num_text.content)\n        self._sections = sections or []\n        self._metadata = metadata or Metadata()\n\n        if sections:\n            self.validate_sections()\n\n\n    def __iter__(self) -&gt; Iterator[SectionEntry]:\n        \"\"\"Iterate through sections, yielding full section information.\"\"\"\n        if not self._sections:\n            raise ValueError(\"No Sections available.\")\n\n        for i, section in enumerate(self._sections):\n            content = self.num_text.get_segment(\n                section.section_range.start, \n                section.section_range.end\n            )\n            yield SectionEntry(\n                number=i+1,\n                title=section.title,\n                range=section.section_range,\n                content=content\n            )\n\n    def __str__(self) -&gt; str:\n        return Frontmatter.embed(self.metadata, self.content)\n\n    @staticmethod\n    def _build_section_objects(\n        logical_sections: List[LogicalSection], \n        last_line: int,\n        metadata: Optional[Metadata] = None\n    ) -&gt; List[SectionObject]:\n        \"\"\"Convert LogicalSections to SectionObjects with proper ranges.\"\"\"\n        section_objects = []\n\n        for i, section in enumerate(logical_sections):\n            # For each section, end is either next section's start or last line + 1\n            end_line = (logical_sections[i + 1].start_line \n                    if i &lt; len(logical_sections) - 1 \n                    else last_line + 1)\n\n            section_objects.append(\n                SectionObject.from_logical_section(section, end_line, metadata)\n            )\n\n        return section_objects\n\n    @classmethod\n    def from_str(\n        cls,\n        text: str,\n        language: Optional[str] = None,\n        sections: Optional[List[SectionObject]] = None,\n        metadata: Optional[Metadata] = None\n    ) -&gt; 'TextObject':\n        \"\"\"\n        Create a TextObject from a string, extracting any frontmatter.\n\n        Args:\n            text: Input text string, potentially containing frontmatter\n            language: ISO language code\n            sections: List of section objects\n            metadata: Optional base metadata to merge with frontmatter\n\n        Returns:\n            TextObject instance with combined metadata\n        \"\"\"\n        # Extract any frontmatter and merge with provided metadata\n        frontmatter_metadata, content = Frontmatter.extract(text)\n\n        # Create NumberedText from content without frontmatter\n        numbered_text = NumberedText(content)\n\n        obj = cls(\n            num_text=numbered_text,\n            language=language,\n            sections=sections,\n            metadata=frontmatter_metadata\n        )\n        if metadata:\n            obj.merge_metadata(metadata)\n\n        return obj\n\n\n    @classmethod\n    def from_response(\n        cls, \n        response: AIResponse,\n        existing_metadata: Metadata,\n        num_text: 'NumberedText'\n    ) -&gt; 'TextObject':\n        \"\"\"Create TextObject from AI response format.\"\"\"\n        # Create metadata from response\n        ai_metadata = response.document_metadata\n        new_metadata = Metadata({\n            \"ai_summary\": response.document_summary,\n            \"ai_concepts\": response.key_concepts,\n            \"ai_context\": response.narrative_context\n        })\n\n        # Convert LogicalSections to SectionObjects\n        sections = cls._build_section_objects(\n            response.sections, \n            num_text.size,\n        )\n\n        text = cls(\n            num_text=num_text,\n            language=response.language,\n            sections=sections,\n            metadata=existing_metadata\n        )\n        text.merge_metadata(new_metadata)\n        text.merge_metadata(Metadata.from_yaml(ai_metadata))\n        return text\n\n    def merge_metadata(self, new_metadata: Metadata, override=False) -&gt; None:\n        \"\"\"\n        Merge new metadata with existing metadata.\n\n        For now, performs simple dict-like union (|=) but can be extended \n        to handle more complex merging logic in the future (e.g., merging \n        nested structures, handling conflicts, merging arrays).\n\n        Args:\n        new_metadata: Metadata to merge with existing metadata\n        override: If True, new_metadata values override existing values\n                            If False, existing values are preserved\n        \"\"\"\n        # Currently using simple dict union\n        # Future implementations might handle:\n        # - Deep merging of nested structures\n        # - Special handling of specific fields\n        # - Array/list merging strategies\n        # - Conflict resolution\n        # - Metadata versioning\n        if not new_metadata:\n            return\n\n        if override:\n            self._metadata |= new_metadata  # new overrides existing\n        else:\n            self._metadata = new_metadata | self._metadata # existing values preserved\n\n        logger.debug(\"Merging new metadata into TextObject\")\n\n    def update_metadata(self, **kwargs) -&gt; None:\n        \"\"\"Update metadata with new key-value pairs.\"\"\"\n        new_metadata = Metadata(kwargs)\n        self.merge_metadata(new_metadata)\n\n    def validate_sections(self) -&gt; None:\n        \"\"\"Basic validation of section integrity.\"\"\"\n        if not self._sections:\n            raise ValueError(\"No sections set.\")\n\n        # Check section ordering and bounds\n        for i, section in enumerate(self._sections):\n            if section.section_range.start &lt; 1:\n                logger.warning(f\"Section {i}: start line must be &gt;= 1\")\n            if section.section_range.start &gt; self.num_text.size:\n                logger.warning(f\"Section {i}: start line exceeds text length\")\n            if i &gt; 0 and \\\n                section.section_range.start &lt;= self._sections[i-1].section_range.start:\n                logger.warning(f\"Section {i}: non-sequential start line\")\n\n    def get_section_content(self, index: int) -&gt; str:     \n        if not self._sections:\n            raise ValueError(\"No Sections available.\")\n        \"\"\"Get content for a section.\"\"\"            \n        if index &lt; 0 or index &gt;= len(self._sections):\n            raise IndexError(\"Section index out of range\")\n\n        section = self._sections[index]\n        return self.num_text.get_segment(\n            section.section_range.start, \n            section.section_range.end\n        )\n\n    def export_info(self, source_file: Optional[Path] = None) -&gt; TextObjectInfo:\n        \"\"\"Export serializable state.\"\"\"\n        if source_file:\n            source_file = source_file.resolve() # use absolute path for info\n\n        return TextObjectInfo(\n            source_file=source_file,\n            language=self.language,\n            sections=self.sections,\n            metadata=self.metadata\n        )\n\n    @classmethod\n    def from_info(\n        cls, \n        info: TextObjectInfo, \n        metadata: Metadata, \n        num_text: 'NumberedText'\n        ) -&gt; 'TextObject':\n        \"\"\"Create TextObject from info and content.\"\"\"\n        text_obj = cls(\n            num_text=num_text, \n            language=info.language, \n            sections=info.sections, \n            metadata=info.metadata\n            )\n\n        text_obj.merge_metadata(metadata)\n        return text_obj\n\n    @classmethod\n    def from_text_file(\n        cls,\n        file: Path\n    ) -&gt; 'TextObject':\n        text_str = read_str_from_file(file)\n        return cls.from_str(text_str)\n\n    @classmethod\n    def from_section_file(\n        cls, \n        section_file: Path, \n        source: Optional[str] = None\n        ) -&gt; 'TextObject':\n        \"\"\"\n        Create TextObject from a section info file, loading content from source_file.\n        Metadata is extracted from the source_file or from content.\n\n        Args:\n            section_file: Path to JSON file containing TextObjectInfo\n            source: Optional source string in case no source file is found.\n\n        Returns:\n            TextObject instance\n\n        Raises:\n            ValueError: If source_file is missing from section info\n            FileNotFoundError: If either section_file or source_file not found\n        \"\"\"\n        # Check section file exists\n        if not section_file.exists():\n            raise FileNotFoundError(f\"Section file not found: {section_file}\")\n\n        # Load and parse section info\n        info = TextObjectInfo.model_validate_json(read_str_from_file(section_file))\n\n        if not source:  # passed content always takes precedence over source_file\n            # check if source file exists\n            if not info.source_file:\n                raise ValueError(f\"No content available: no source_file specified \"\n                                 f\"in section info: {section_file}\")\n\n            source_path = Path(info.source_file)\n            if not source_path.exists():\n                raise FileNotFoundError(\n                    f\"No content available: Source file not found: {source_path}\"\n                    )\n\n            # Load source from path\n            source = read_str_from_file(source_path)\n\n        metadata, content = Frontmatter.extract(source)\n\n        # Create TextObject\n        return cls.from_info(info=info, \n                             metadata=metadata, \n                             num_text=NumberedText(content)\n                             )\n\n    def save(\n        self,\n        path: Path,\n        output_format: StorageFormatType = StorageFormat.TEXT,\n        source_file: Optional[Path] = None,\n        pretty: bool = True\n        ) -&gt; None:\n        \"\"\"\n        Save TextObject to file in specified format.\n\n        Args:\n            path: Output file path\n            output_format: \"text\" for full content+metadata or \"json\" for serialized state\n            source_file: Optional source file to record in metadata\n            pretty: For JSON output, whether to pretty print\n        \"\"\"\n        if isinstance(output_format, str):\n            output_format = StorageFormat(output_format)\n\n        if output_format == StorageFormat.TEXT:\n            # Full text output with metadata as frontmatter\n            write_str_to_file(path, str(self))\n\n        elif output_format == StorageFormat.JSON:\n            # Export serializable state\n            info = self.export_info(source_file)\n            json_str = info.model_dump_json(indent=2 if pretty else None)\n            write_str_to_file(path, json_str)\n\n    @classmethod\n    def load(\n        cls,\n        path: Path,\n        config: Optional[LoadConfig] = None\n    ) -&gt; 'TextObject':\n        \"\"\"\n        Load TextObject from file with optional configuration.\n\n        Args:\n            path: Input file path\n            config: Optional loading configuration. If not provided,\n                loads directly from text file.\n\n        Returns:\n            TextObject instance\n\n        Usage:\n            # Load from text file with frontmatter\n            obj = TextObject.load(Path(\"content.txt\"))\n\n            # Load state from JSON with source content string\n            config = LoadConfig(\n                format=StorageFormat.JSON,\n                source_content=\"Text content...\"\n            )\n            obj = TextObject.load(Path(\"state.json\"), config)\n\n            # Load state from JSON with source content file\n            config = LoadConfig(\n                format=StorageFormat.JSON,\n                source_content=Path(\"content.txt\")\n            )\n            obj = TextObject.load(Path(\"state.json\"), config)\n        \"\"\"\n        # Use default config if none provided\n        config = config or LoadConfig()\n\n        if config.format == StorageFormat.TEXT:\n            return cls.from_text_file(path)\n\n        elif config.format == StorageFormat.JSON:\n            return cls.from_section_file(path, source=config.get_source_text())\n\n        else:\n            raise ValueError(\"Unknown load configuration format.\")\n\n    def transform(\n        self,\n        data_str: Optional[str] = None,\n        language: Optional[str] = None, \n        metadata: Optional[Metadata] = None,\n        process_metadata: Optional[ProcessMetadata] = None,\n        sections: Optional[List[SectionObject]] = None\n    ) -&gt; Self:\n        \"\"\"Update TextObject content and metadata in place.\n\n        Optionally modifies the object's content, language, and adds process tracking.\n        Process history is maintained in metadata.\n\n        Args:\n            data_str: New text content\n            language: New language code  \n            metadata: Metadata to merge into the object\n            process_metadata: Identifier and details for the process performed\n            sections: Optional replacement list of sections\n        \"\"\"\n        # Update potentially changed elements\n        if data_str:\n            self.num_text = NumberedText(data_str)\n        if language:\n            self.language = language\n        if metadata:\n            self.merge_metadata(metadata)\n        if process_metadata:    \n            self._metadata.add_process_info(process_metadata)\n        if sections:\n            self._sections = sections\n\n        return self\n\n    @property\n    def metadata(self) -&gt; Metadata:\n        \"\"\"Access to metadata dictionary.\"\"\"\n        return self._metadata  \n\n    @property\n    def section_count(self) -&gt; int:\n        return len(self._sections) if self._sections else 0\n\n    @property\n    def last_line_num(self) -&gt; int:\n        return self.num_text.size\n\n    @property\n    def sections(self) -&gt; List[SectionObject]:\n        \"\"\"Access to sections list.\"\"\"\n        return self._sections or []\n\n    @property\n    def content(self) -&gt; str:\n        return self.num_text.content\n\n    @property\n    def metadata_str(self) -&gt; str:\n        return self.metadata.to_yaml()\n\n    @property\n    def numbered_content(self) -&gt; str:\n        return self.num_text.numbered_content\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.content","title":"<code>content</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.language","title":"<code>language = language or get_language_code_from_text(num_text.content)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.last_line_num","title":"<code>last_line_num</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Access to metadata dictionary.</p>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.metadata_str","title":"<code>metadata_str</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.num_text","title":"<code>num_text = num_text</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.numbered_content","title":"<code>numbered_content</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.section_count","title":"<code>section_count</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.sections","title":"<code>sections</code>  <code>property</code>","text":"<p>Access to sections list.</p>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.__init__","title":"<code>__init__(num_text, language=None, sections=None, metadata=None)</code>","text":"<p>Initialize a TextObject with content and optional organizing components.</p> <p>Parameters:</p> Name Type Description Default <code>num_text</code> <code>NumberedText</code> <p>Text content with line numbering</p> required <code>language</code> <code>Optional[str]</code> <p>ISO 639-1 language code. If None, auto-detected from content</p> <code>None</code> <code>sections</code> <code>Optional[List[SectionObject]]</code> <p>Initial sections defining text organization. If None,        text is considered un-sectioned.</p> <code>None</code> <code>metadata</code> <code>Optional[Metadata]</code> <p>Initial metadata. If None, creates empty metadata container</p> <code>None</code> Note <p>Until sections are established, section-based methods will raise a value error if called.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def __init__(self, \n    num_text: NumberedText, \n    language: Optional[str] = None, \n    sections: Optional[List[SectionObject]] = None,\n    metadata: Optional[Metadata] = None):\n    \"\"\"\n    Initialize a TextObject with content and optional organizing components.\n\n    Args:\n        num_text: Text content with line numbering\n        language: ISO 639-1 language code. If None, auto-detected from content\n        sections: Initial sections defining text organization. If None, \n                  text is considered un-sectioned.\n        metadata: Initial metadata. If None, creates empty metadata container\n\n    Note:\n        Until sections are established, section-based methods will raise a value\n        error if called.\n    \"\"\"\n    self.num_text = num_text\n    self.language = language or get_language_code_from_text(num_text.content)\n    self._sections = sections or []\n    self._metadata = metadata or Metadata()\n\n    if sections:\n        self.validate_sections()\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate through sections, yielding full section information.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def __iter__(self) -&gt; Iterator[SectionEntry]:\n    \"\"\"Iterate through sections, yielding full section information.\"\"\"\n    if not self._sections:\n        raise ValueError(\"No Sections available.\")\n\n    for i, section in enumerate(self._sections):\n        content = self.num_text.get_segment(\n            section.section_range.start, \n            section.section_range.end\n        )\n        yield SectionEntry(\n            number=i+1,\n            title=section.title,\n            range=section.section_range,\n            content=content\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.__str__","title":"<code>__str__()</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def __str__(self) -&gt; str:\n    return Frontmatter.embed(self.metadata, self.content)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.export_info","title":"<code>export_info(source_file=None)</code>","text":"<p>Export serializable state.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def export_info(self, source_file: Optional[Path] = None) -&gt; TextObjectInfo:\n    \"\"\"Export serializable state.\"\"\"\n    if source_file:\n        source_file = source_file.resolve() # use absolute path for info\n\n    return TextObjectInfo(\n        source_file=source_file,\n        language=self.language,\n        sections=self.sections,\n        metadata=self.metadata\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.from_info","title":"<code>from_info(info, metadata, num_text)</code>  <code>classmethod</code>","text":"<p>Create TextObject from info and content.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_info(\n    cls, \n    info: TextObjectInfo, \n    metadata: Metadata, \n    num_text: 'NumberedText'\n    ) -&gt; 'TextObject':\n    \"\"\"Create TextObject from info and content.\"\"\"\n    text_obj = cls(\n        num_text=num_text, \n        language=info.language, \n        sections=info.sections, \n        metadata=info.metadata\n        )\n\n    text_obj.merge_metadata(metadata)\n    return text_obj\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.from_response","title":"<code>from_response(response, existing_metadata, num_text)</code>  <code>classmethod</code>","text":"<p>Create TextObject from AI response format.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_response(\n    cls, \n    response: AIResponse,\n    existing_metadata: Metadata,\n    num_text: 'NumberedText'\n) -&gt; 'TextObject':\n    \"\"\"Create TextObject from AI response format.\"\"\"\n    # Create metadata from response\n    ai_metadata = response.document_metadata\n    new_metadata = Metadata({\n        \"ai_summary\": response.document_summary,\n        \"ai_concepts\": response.key_concepts,\n        \"ai_context\": response.narrative_context\n    })\n\n    # Convert LogicalSections to SectionObjects\n    sections = cls._build_section_objects(\n        response.sections, \n        num_text.size,\n    )\n\n    text = cls(\n        num_text=num_text,\n        language=response.language,\n        sections=sections,\n        metadata=existing_metadata\n    )\n    text.merge_metadata(new_metadata)\n    text.merge_metadata(Metadata.from_yaml(ai_metadata))\n    return text\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.from_section_file","title":"<code>from_section_file(section_file, source=None)</code>  <code>classmethod</code>","text":"<p>Create TextObject from a section info file, loading content from source_file. Metadata is extracted from the source_file or from content.</p> <p>Parameters:</p> Name Type Description Default <code>section_file</code> <code>Path</code> <p>Path to JSON file containing TextObjectInfo</p> required <code>source</code> <code>Optional[str]</code> <p>Optional source string in case no source file is found.</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source_file is missing from section info</p> <code>FileNotFoundError</code> <p>If either section_file or source_file not found</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_section_file(\n    cls, \n    section_file: Path, \n    source: Optional[str] = None\n    ) -&gt; 'TextObject':\n    \"\"\"\n    Create TextObject from a section info file, loading content from source_file.\n    Metadata is extracted from the source_file or from content.\n\n    Args:\n        section_file: Path to JSON file containing TextObjectInfo\n        source: Optional source string in case no source file is found.\n\n    Returns:\n        TextObject instance\n\n    Raises:\n        ValueError: If source_file is missing from section info\n        FileNotFoundError: If either section_file or source_file not found\n    \"\"\"\n    # Check section file exists\n    if not section_file.exists():\n        raise FileNotFoundError(f\"Section file not found: {section_file}\")\n\n    # Load and parse section info\n    info = TextObjectInfo.model_validate_json(read_str_from_file(section_file))\n\n    if not source:  # passed content always takes precedence over source_file\n        # check if source file exists\n        if not info.source_file:\n            raise ValueError(f\"No content available: no source_file specified \"\n                             f\"in section info: {section_file}\")\n\n        source_path = Path(info.source_file)\n        if not source_path.exists():\n            raise FileNotFoundError(\n                f\"No content available: Source file not found: {source_path}\"\n                )\n\n        # Load source from path\n        source = read_str_from_file(source_path)\n\n    metadata, content = Frontmatter.extract(source)\n\n    # Create TextObject\n    return cls.from_info(info=info, \n                         metadata=metadata, \n                         num_text=NumberedText(content)\n                         )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.from_str","title":"<code>from_str(text, language=None, sections=None, metadata=None)</code>  <code>classmethod</code>","text":"<p>Create a TextObject from a string, extracting any frontmatter.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text string, potentially containing frontmatter</p> required <code>language</code> <code>Optional[str]</code> <p>ISO language code</p> <code>None</code> <code>sections</code> <code>Optional[List[SectionObject]]</code> <p>List of section objects</p> <code>None</code> <code>metadata</code> <code>Optional[Metadata]</code> <p>Optional base metadata to merge with frontmatter</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject instance with combined metadata</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_str(\n    cls,\n    text: str,\n    language: Optional[str] = None,\n    sections: Optional[List[SectionObject]] = None,\n    metadata: Optional[Metadata] = None\n) -&gt; 'TextObject':\n    \"\"\"\n    Create a TextObject from a string, extracting any frontmatter.\n\n    Args:\n        text: Input text string, potentially containing frontmatter\n        language: ISO language code\n        sections: List of section objects\n        metadata: Optional base metadata to merge with frontmatter\n\n    Returns:\n        TextObject instance with combined metadata\n    \"\"\"\n    # Extract any frontmatter and merge with provided metadata\n    frontmatter_metadata, content = Frontmatter.extract(text)\n\n    # Create NumberedText from content without frontmatter\n    numbered_text = NumberedText(content)\n\n    obj = cls(\n        num_text=numbered_text,\n        language=language,\n        sections=sections,\n        metadata=frontmatter_metadata\n    )\n    if metadata:\n        obj.merge_metadata(metadata)\n\n    return obj\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.from_text_file","title":"<code>from_text_file(file)</code>  <code>classmethod</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef from_text_file(\n    cls,\n    file: Path\n) -&gt; 'TextObject':\n    text_str = read_str_from_file(file)\n    return cls.from_str(text_str)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.get_section_content","title":"<code>get_section_content(index)</code>","text":"Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def get_section_content(self, index: int) -&gt; str:     \n    if not self._sections:\n        raise ValueError(\"No Sections available.\")\n    \"\"\"Get content for a section.\"\"\"            \n    if index &lt; 0 or index &gt;= len(self._sections):\n        raise IndexError(\"Section index out of range\")\n\n    section = self._sections[index]\n    return self.num_text.get_segment(\n        section.section_range.start, \n        section.section_range.end\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.load","title":"<code>load(path, config=None)</code>  <code>classmethod</code>","text":"<p>Load TextObject from file with optional configuration.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Input file path</p> required <code>config</code> <code>Optional[LoadConfig]</code> <p>Optional loading configuration. If not provided, loads directly from text file.</p> <code>None</code> <p>Returns:</p> Type Description <code>TextObject</code> <p>TextObject instance</p> Usage Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    path: Path,\n    config: Optional[LoadConfig] = None\n) -&gt; 'TextObject':\n    \"\"\"\n    Load TextObject from file with optional configuration.\n\n    Args:\n        path: Input file path\n        config: Optional loading configuration. If not provided,\n            loads directly from text file.\n\n    Returns:\n        TextObject instance\n\n    Usage:\n        # Load from text file with frontmatter\n        obj = TextObject.load(Path(\"content.txt\"))\n\n        # Load state from JSON with source content string\n        config = LoadConfig(\n            format=StorageFormat.JSON,\n            source_content=\"Text content...\"\n        )\n        obj = TextObject.load(Path(\"state.json\"), config)\n\n        # Load state from JSON with source content file\n        config = LoadConfig(\n            format=StorageFormat.JSON,\n            source_content=Path(\"content.txt\")\n        )\n        obj = TextObject.load(Path(\"state.json\"), config)\n    \"\"\"\n    # Use default config if none provided\n    config = config or LoadConfig()\n\n    if config.format == StorageFormat.TEXT:\n        return cls.from_text_file(path)\n\n    elif config.format == StorageFormat.JSON:\n        return cls.from_section_file(path, source=config.get_source_text())\n\n    else:\n        raise ValueError(\"Unknown load configuration format.\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.load--load-from-text-file-with-frontmatter","title":"Load from text file with frontmatter","text":"<p>obj = TextObject.load(Path(\"content.txt\"))</p>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.load--load-state-from-json-with-source-content-string","title":"Load state from JSON with source content string","text":"<p>config = LoadConfig(     format=StorageFormat.JSON,     source_content=\"Text content...\" ) obj = TextObject.load(Path(\"state.json\"), config)</p>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.load--load-state-from-json-with-source-content-file","title":"Load state from JSON with source content file","text":"<p>config = LoadConfig(     format=StorageFormat.JSON,     source_content=Path(\"content.txt\") ) obj = TextObject.load(Path(\"state.json\"), config)</p>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.merge_metadata","title":"<code>merge_metadata(new_metadata, override=False)</code>","text":"<p>Merge new metadata with existing metadata.</p> <p>For now, performs simple dict-like union (|=) but can be extended  to handle more complex merging logic in the future (e.g., merging  nested structures, handling conflicts, merging arrays).</p> <p>Args: new_metadata: Metadata to merge with existing metadata override: If True, new_metadata values override existing values                     If False, existing values are preserved</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def merge_metadata(self, new_metadata: Metadata, override=False) -&gt; None:\n    \"\"\"\n    Merge new metadata with existing metadata.\n\n    For now, performs simple dict-like union (|=) but can be extended \n    to handle more complex merging logic in the future (e.g., merging \n    nested structures, handling conflicts, merging arrays).\n\n    Args:\n    new_metadata: Metadata to merge with existing metadata\n    override: If True, new_metadata values override existing values\n                        If False, existing values are preserved\n    \"\"\"\n    # Currently using simple dict union\n    # Future implementations might handle:\n    # - Deep merging of nested structures\n    # - Special handling of specific fields\n    # - Array/list merging strategies\n    # - Conflict resolution\n    # - Metadata versioning\n    if not new_metadata:\n        return\n\n    if override:\n        self._metadata |= new_metadata  # new overrides existing\n    else:\n        self._metadata = new_metadata | self._metadata # existing values preserved\n\n    logger.debug(\"Merging new metadata into TextObject\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.save","title":"<code>save(path, output_format=StorageFormat.TEXT, source_file=None, pretty=True)</code>","text":"<p>Save TextObject to file in specified format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output file path</p> required <code>output_format</code> <code>StorageFormatType</code> <p>\"text\" for full content+metadata or \"json\" for serialized state</p> <code>TEXT</code> <code>source_file</code> <code>Optional[Path]</code> <p>Optional source file to record in metadata</p> <code>None</code> <code>pretty</code> <code>bool</code> <p>For JSON output, whether to pretty print</p> <code>True</code> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def save(\n    self,\n    path: Path,\n    output_format: StorageFormatType = StorageFormat.TEXT,\n    source_file: Optional[Path] = None,\n    pretty: bool = True\n    ) -&gt; None:\n    \"\"\"\n    Save TextObject to file in specified format.\n\n    Args:\n        path: Output file path\n        output_format: \"text\" for full content+metadata or \"json\" for serialized state\n        source_file: Optional source file to record in metadata\n        pretty: For JSON output, whether to pretty print\n    \"\"\"\n    if isinstance(output_format, str):\n        output_format = StorageFormat(output_format)\n\n    if output_format == StorageFormat.TEXT:\n        # Full text output with metadata as frontmatter\n        write_str_to_file(path, str(self))\n\n    elif output_format == StorageFormat.JSON:\n        # Export serializable state\n        info = self.export_info(source_file)\n        json_str = info.model_dump_json(indent=2 if pretty else None)\n        write_str_to_file(path, json_str)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.transform","title":"<code>transform(data_str=None, language=None, metadata=None, process_metadata=None, sections=None)</code>","text":"<p>Update TextObject content and metadata in place.</p> <p>Optionally modifies the object's content, language, and adds process tracking. Process history is maintained in metadata.</p> <p>Parameters:</p> Name Type Description Default <code>data_str</code> <code>Optional[str]</code> <p>New text content</p> <code>None</code> <code>language</code> <code>Optional[str]</code> <p>New language code  </p> <code>None</code> <code>metadata</code> <code>Optional[Metadata]</code> <p>Metadata to merge into the object</p> <code>None</code> <code>process_metadata</code> <code>Optional[ProcessMetadata]</code> <p>Identifier and details for the process performed</p> <code>None</code> <code>sections</code> <code>Optional[List[SectionObject]]</code> <p>Optional replacement list of sections</p> <code>None</code> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def transform(\n    self,\n    data_str: Optional[str] = None,\n    language: Optional[str] = None, \n    metadata: Optional[Metadata] = None,\n    process_metadata: Optional[ProcessMetadata] = None,\n    sections: Optional[List[SectionObject]] = None\n) -&gt; Self:\n    \"\"\"Update TextObject content and metadata in place.\n\n    Optionally modifies the object's content, language, and adds process tracking.\n    Process history is maintained in metadata.\n\n    Args:\n        data_str: New text content\n        language: New language code  \n        metadata: Metadata to merge into the object\n        process_metadata: Identifier and details for the process performed\n        sections: Optional replacement list of sections\n    \"\"\"\n    # Update potentially changed elements\n    if data_str:\n        self.num_text = NumberedText(data_str)\n    if language:\n        self.language = language\n    if metadata:\n        self.merge_metadata(metadata)\n    if process_metadata:    \n        self._metadata.add_process_info(process_metadata)\n    if sections:\n        self._sections = sections\n\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.update_metadata","title":"<code>update_metadata(**kwargs)</code>","text":"<p>Update metadata with new key-value pairs.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def update_metadata(self, **kwargs) -&gt; None:\n    \"\"\"Update metadata with new key-value pairs.\"\"\"\n    new_metadata = Metadata(kwargs)\n    self.merge_metadata(new_metadata)\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObject.validate_sections","title":"<code>validate_sections()</code>","text":"<p>Basic validation of section integrity.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def validate_sections(self) -&gt; None:\n    \"\"\"Basic validation of section integrity.\"\"\"\n    if not self._sections:\n        raise ValueError(\"No sections set.\")\n\n    # Check section ordering and bounds\n    for i, section in enumerate(self._sections):\n        if section.section_range.start &lt; 1:\n            logger.warning(f\"Section {i}: start line must be &gt;= 1\")\n        if section.section_range.start &gt; self.num_text.size:\n            logger.warning(f\"Section {i}: start line exceeds text length\")\n        if i &gt; 0 and \\\n            section.section_range.start &lt;= self._sections[i-1].section_range.start:\n            logger.warning(f\"Section {i}: non-sequential start line\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObjectInfo","title":"<code>TextObjectInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Serializable information about a text and its sections.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>class TextObjectInfo(BaseModel):\n    \"\"\"Serializable information about a text and its sections.\"\"\"\n    source_file: Optional[Path] = None  # Original text file path\n    language: str\n    sections: List[SectionObject]\n    metadata: Metadata\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Ensure metadata is always a Metadata instance after initialization.\"\"\"\n        if isinstance(self.metadata, dict):\n            self.metadata = Metadata(self.metadata)\n        elif not isinstance(self.metadata, Metadata):\n            raise ValueError(f\"Unexpected type for metadata: {type(self.metadata)}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObjectInfo.language","title":"<code>language</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObjectInfo.metadata","title":"<code>metadata</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObjectInfo.sections","title":"<code>sections</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObjectInfo.source_file","title":"<code>source_file = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.text_object.TextObjectInfo.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Ensure metadata is always a Metadata instance after initialization.</p> Source code in <code>src/tnh_scholar/ai_text_processing/text_object.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Ensure metadata is always a Metadata instance after initialization.\"\"\"\n    if isinstance(self.metadata, dict):\n        self.metadata = Metadata(self.metadata)\n    elif not isinstance(self.metadata, Metadata):\n        raise ValueError(f\"Unexpected type for metadata: {type(self.metadata)}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ai_text_processing.typing","title":"<code>typing</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.typing.ProcessorResult","title":"<code>ProcessorResult = Union[str, ResponseFormat]</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ai_text_processing.typing.ResponseFormat","title":"<code>ResponseFormat = TypeVar('ResponseFormat', bound=BaseModel)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing","title":"<code>audio_processing</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.__all__","title":"<code>__all__ = ['DiarizationConfig', 'detect_nonsilent', 'detect_whisper_boundaries', 'split_audio', 'split_audio_at_boundaries']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.DiarizationConfig","title":"<code>DiarizationConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/config.py</code> <pre><code>class DiarizationConfig(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive = False,\n        env_prefix = \"DIARIZATION_\",\n        extra=\"ignore\",\n    )\n    speaker: SpeakerConfig = SpeakerConfig()\n    chunk: ChunkConfig = ChunkConfig()\n    language: LanguageConfig = LanguageConfig()\n    mapping: MappingPolicy = MappingPolicy()\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.DiarizationConfig.chunk","title":"<code>chunk = ChunkConfig()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.DiarizationConfig.language","title":"<code>language = LanguageConfig()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.DiarizationConfig.mapping","title":"<code>mapping = MappingPolicy()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.DiarizationConfig.model_config","title":"<code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=False, env_prefix='DIARIZATION_', extra='ignore')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.DiarizationConfig.speaker","title":"<code>speaker = SpeakerConfig()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.detect_whisper_boundaries","title":"<code>detect_whisper_boundaries(audio_file, model_size='tiny', language=None)</code>","text":"<p>Detect sentence boundaries using a Whisper model.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>Path to the audio file.</p> required <code>model_size</code> <code>str</code> <p>Whisper model size.</p> <code>'tiny'</code> <code>language</code> <code>str</code> <p>Language to force for transcription (e.g. 'en', 'vi'), or None for auto.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Boundary]</code> <p>List[Boundary]: A list of sentence boundaries with text.</p> Example <p>boundaries = detect_whisper_boundaries(Path(\"my_audio.mp3\"), model_size=\"tiny\") for b in boundaries: ...     print(b.start, b.end, b.text)</p> Source code in <code>src/tnh_scholar/audio_processing/audio_legacy.py</code> <pre><code>def detect_whisper_boundaries(\n    audio_file: Path, model_size: str = \"tiny\", language: str = None\n) -&gt; List[Boundary]:\n    \"\"\"\n    Detect sentence boundaries using a Whisper model.\n\n    Args:\n        audio_file (Path): Path to the audio file.\n        model_size (str): Whisper model size.\n        language (str): Language to force for transcription (e.g. 'en', 'vi'), or None for auto.\n\n    Returns:\n        List[Boundary]: A list of sentence boundaries with text.\n\n    Example:\n        &gt;&gt;&gt; boundaries = detect_whisper_boundaries(Path(\"my_audio.mp3\"), model_size=\"tiny\")\n        &gt;&gt;&gt; for b in boundaries:\n        ...     print(b.start, b.end, b.text)\n    \"\"\"\n\n    os.environ[\"KMP_WARNINGS\"] = \"0\"  # Turn of OMP warning message\n\n    # Load model\n    logger.info(\"Loading Whisper model...\")\n    model = load_whisper_model(model_size)\n    logger.info(f\"Model '{model_size}' loaded.\")\n\n    if language:\n        logger.info(f\"Language for boundaries set to '{language}'\")\n    else:\n        logger.info(\"Language not set. Autodetect will be used in Whisper model.\")\n\n    # with TimeProgress(expected_time=expected_time, desc=\"Generating transcription boundaries\"):\n    boundary_transcription = whisper_model_transcribe(\n        model,\n        str(audio_file),\n        task=\"transcribe\",\n        word_timestamps=True,\n        language=language,\n        verbose=False,\n    )\n\n    sentence_boundaries = [\n        Boundary(start=segment[\"start\"], end=segment[\"end\"], text=segment[\"text\"])\n        for segment in boundary_transcription[\"segments\"]\n    ]\n    return sentence_boundaries, boundary_transcription\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.split_audio","title":"<code>split_audio(audio_file, method='whisper', output_dir=None, model_size='tiny', language=None, min_silence_len=MIN_SILENCE_LENGTH, silence_thresh=SILENCE_DBFS_THRESHOLD, max_duration=MAX_DURATION)</code>","text":"<p>High-level function to split an audio file into chunks based on a chosen method.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>The input audio file.</p> required <code>method</code> <code>str</code> <p>Splitting method, \"silence\" or \"whisper\".</p> <code>'whisper'</code> <code>output_dir</code> <code>Path</code> <p>Directory to store output.</p> <code>None</code> <code>model_size</code> <code>str</code> <p>Whisper model size if method='whisper'.</p> <code>'tiny'</code> <code>language</code> <code>str</code> <p>Language for whisper transcription if method='whisper'.</p> <code>None</code> <code>min_silence_len</code> <code>int</code> <p>For silence-based detection, min silence length in ms.</p> <code>MIN_SILENCE_LENGTH</code> <code>silence_thresh</code> <code>int</code> <p>Silence threshold in dBFS.</p> <code>SILENCE_DBFS_THRESHOLD</code> <code>max_duration</code> <code>int</code> <p>Max chunk length in seconds (also used to derive ms threshold).</p> <code>MAX_DURATION</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Directory containing the resulting chunks.</p> Example Source code in <code>src/tnh_scholar/audio_processing/audio_legacy.py</code> <pre><code>def split_audio(\n    audio_file: Path,\n    method: str = \"whisper\",\n    output_dir: Optional[Path] = None,\n    model_size: str = \"tiny\",\n    language: str = None,\n    min_silence_len: int = MIN_SILENCE_LENGTH,\n    silence_thresh: int = SILENCE_DBFS_THRESHOLD,\n    max_duration: int = MAX_DURATION,\n) -&gt; Path:\n    \"\"\"\n    High-level function to split an audio file into chunks based on a chosen method.\n\n    Args:\n        audio_file (Path): The input audio file.\n        method (str): Splitting method, \"silence\" or \"whisper\".\n        output_dir (Path): Directory to store output.\n        model_size (str): Whisper model size if method='whisper'.\n        language (str): Language for whisper transcription if method='whisper'.\n        min_silence_len (int): For silence-based detection, min silence length in ms.\n        silence_thresh (int): Silence threshold in dBFS.\n        max_duration (int): Max chunk length in seconds (also used to derive ms threshold).\n\n    Returns:\n        Path: Directory containing the resulting chunks.\n\n    Example:\n        &gt;&gt;&gt; # Split using silence detection\n        &gt;&gt;&gt; split_audio(Path(\"my_audio.mp3\"), method=\"silence\")\n\n        &gt;&gt;&gt; # Split using whisper-based sentence boundaries\n        &gt;&gt;&gt; split_audio(Path(\"my_audio.mp3\"), method=\"whisper\", model_size=\"base\", language=\"en\")\n    \"\"\"\n\n    logger.info(f\"Splitting audio with max_duration={max_duration} seconds\")\n\n    if method == \"whisper\":\n        boundaries, _ = detect_whisper_boundaries(\n            audio_file, model_size=model_size, language=language\n        )\n\n    elif method == \"silence\":\n        max_duration_ms = (\n            max_duration * 1000\n        )  # convert duration in seconds to milliseconds\n        boundaries = detect_silence_boundaries(\n            audio_file,\n            min_silence_len=min_silence_len,\n            silence_thresh=silence_thresh,\n            max_duration=max_duration_ms,\n        )\n    else:\n        raise ValueError(f\"Unknown method: {method}. Must be 'silence' or 'whisper'.\")\n\n    # delete all files in the output_dir (this is useful for reprocessing)\n\n    return split_audio_at_boundaries(\n        audio_file, boundaries, output_dir=output_dir, max_duration=max_duration\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.split_audio--split-using-silence-detection","title":"Split using silence detection","text":"<p>split_audio(Path(\"my_audio.mp3\"), method=\"silence\")</p>"},{"location":"api/#tnh_scholar.audio_processing.split_audio--split-using-whisper-based-sentence-boundaries","title":"Split using whisper-based sentence boundaries","text":"<p>split_audio(Path(\"my_audio.mp3\"), method=\"whisper\", model_size=\"base\", language=\"en\")</p>"},{"location":"api/#tnh_scholar.audio_processing.split_audio_at_boundaries","title":"<code>split_audio_at_boundaries(audio_file, boundaries, output_dir=None, max_duration=MAX_DURATION)</code>","text":"<p>Split the audio file into chunks based on provided boundaries, ensuring all audio is included and boundaries align with the start of Whisper segments.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>The input audio file.</p> required <code>boundaries</code> <code>List[Boundary]</code> <p>Detected boundaries.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to store the resulting chunks.</p> <code>None</code> <code>max_duration</code> <code>int</code> <p>Maximum chunk length in seconds.</p> <code>MAX_DURATION</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Directory containing the chunked audio files.</p> Example <p>boundaries = [Boundary(34.02, 37.26, \"...\"), Boundary(38.0, 41.18, \"...\")] out_dir = split_audio_at_boundaries(Path(\"my_audio.mp3\"), boundaries)</p> Source code in <code>src/tnh_scholar/audio_processing/audio_legacy.py</code> <pre><code>def split_audio_at_boundaries(\n    audio_file: Path,\n    boundaries: List[Boundary],\n    output_dir: Path = None,\n    max_duration: int = MAX_DURATION,\n) -&gt; Path:\n    \"\"\"\n    Split the audio file into chunks based on provided boundaries, ensuring all audio is included\n    and boundaries align with the start of Whisper segments.\n\n    Args:\n        audio_file (Path): The input audio file.\n        boundaries (List[Boundary]): Detected boundaries.\n        output_dir (Path): Directory to store the resulting chunks.\n        max_duration (int): Maximum chunk length in seconds.\n\n    Returns:\n        Path: Directory containing the chunked audio files.\n\n    Example:\n        &gt;&gt;&gt; boundaries = [Boundary(34.02, 37.26, \"...\"), Boundary(38.0, 41.18, \"...\")]\n        &gt;&gt;&gt; out_dir = split_audio_at_boundaries(Path(\"my_audio.mp3\"), boundaries)\n    \"\"\"\n    logger.info(f\"Splitting audio with max_duration={max_duration} seconds\")\n\n    # Load the audio file\n    audio = AudioSegment.from_file(audio_file)\n\n    # Create output directory based on filename\n    if output_dir is None:\n        output_dir = audio_file.parent / f\"{audio_file.stem}_chunks\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Clean up the output directory\n    for file in output_dir.iterdir():\n        if file.is_file():\n            logger.info(f\"Deleting existing file: {file}\")\n            file.unlink()\n\n    chunk_start = 0  # Start time for the first chunk in ms\n    chunk_count = 1\n    current_chunk = AudioSegment.empty()\n\n    for idx, boundary in enumerate(boundaries):\n        segment_start_ms = int(boundary.start * 1000)\n        if idx + 1 &lt; len(boundaries):\n            segment_end_ms = int(\n                boundaries[idx + 1].start * 1000\n            )  # Next boundary's start\n        else:\n            segment_end_ms = len(audio)  # End of the audio for the last boundary\n\n        # Adjust for the first segment starting at 0\n        if idx == 0 and segment_start_ms &gt; 0:\n            segment_start_ms = 0  # Ensure we include the very beginning of the audio\n\n        segment = audio[segment_start_ms:segment_end_ms]\n\n        logger.debug(\n            f\"Boundary index: {idx}, segment_start: {segment_start_ms / 1000}, segment_end: {segment_end_ms / 1000}, duration: {segment.duration_seconds}\"\n        )\n        logger.debug(f\"Current chunk Duration (s): {current_chunk.duration_seconds}\")\n\n        if len(current_chunk) + len(segment) &lt;= max_duration * 1000:\n            # Add segment to the current chunk\n            current_chunk += segment\n        else:\n            # Export current chunk\n            chunk_path = output_dir / f\"chunk_{chunk_count}.mp3\"\n            current_chunk.export(chunk_path, format=\"mp3\")\n            logger.info(f\"Exported: {chunk_path}\")\n            chunk_count += 1\n\n            # Start a new chunk with the current segment\n            current_chunk = segment\n\n    # Export the final chunk if any audio remains\n    if len(current_chunk) &gt; 0:\n        chunk_path = output_dir / f\"chunk_{chunk_count}.mp3\"\n        current_chunk.export(chunk_path, format=\"mp3\")\n        logger.info(f\"Exported: {chunk_path}\")\n\n    return output_dir\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy","title":"<code>audio_legacy</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.EXPECTED_TIME_FACTOR","title":"<code>EXPECTED_TIME_FACTOR = 0.45</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.MAX_DURATION","title":"<code>MAX_DURATION = 10 * 60</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.MAX_DURATION_MS","title":"<code>MAX_DURATION_MS = 10 * 60 * 1000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.MAX_INT16","title":"<code>MAX_INT16 = 32768.0</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.MIN_SILENCE_LENGTH","title":"<code>MIN_SILENCE_LENGTH = 1000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.SEEK_LENGTH","title":"<code>SEEK_LENGTH = 50</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.SILENCE_DBFS_THRESHOLD","title":"<code>SILENCE_DBFS_THRESHOLD = -30</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.logger","title":"<code>logger = get_child_logger('audio_processing')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.Boundary","title":"<code>Boundary</code>  <code>dataclass</code>","text":"<p>A data structure representing a detected audio boundary.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>float</code> <p>Start time of the segment in seconds.</p> <code>end</code> <code>float</code> <p>End time of the segment in seconds.</p> <code>text</code> <code>str</code> <p>Associated text (empty if silence-based).</p> Example <p>b = Boundary(start=0.0, end=30.0, text=\"Hello world\") b.start, b.end, b.text (0.0, 30.0, 'Hello world')</p> Source code in <code>src/tnh_scholar/audio_processing/audio_legacy.py</code> <pre><code>@dataclass\nclass Boundary:\n    \"\"\"A data structure representing a detected audio boundary.\n\n    Attributes:\n        start (float): Start time of the segment in seconds.\n        end (float): End time of the segment in seconds.\n        text (str): Associated text (empty if silence-based).\n\n    Example:\n        &gt;&gt;&gt; b = Boundary(start=0.0, end=30.0, text=\"Hello world\")\n        &gt;&gt;&gt; b.start, b.end, b.text\n        (0.0, 30.0, 'Hello world')\n    \"\"\"\n\n    start: float\n    end: float\n    text: str = \"\"\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.Boundary.end","title":"<code>end</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.Boundary.start","title":"<code>start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.Boundary.text","title":"<code>text = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.Boundary.__init__","title":"<code>__init__(start, end, text='')</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.audio_to_numpy","title":"<code>audio_to_numpy(audio_segment)</code>","text":"<p>Convert an AudioSegment object to a NumPy array suitable for Whisper.</p> <p>Parameters:</p> Name Type Description Default <code>audio_segment</code> <code>AudioSegment</code> <p>The input audio segment to convert.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A mono-channel NumPy array normalized to the range [-1, 1].</p> Example <p>audio = AudioSegment.from_file(\"example.mp3\") audio_numpy = audio_to_numpy(audio)</p> Source code in <code>src/tnh_scholar/audio_processing/audio_legacy.py</code> <pre><code>def audio_to_numpy(audio_segment: AudioSegment) -&gt; np.ndarray:\n    \"\"\"\n    Convert an AudioSegment object to a NumPy array suitable for Whisper.\n\n    Args:\n        audio_segment (AudioSegment): The input audio segment to convert.\n\n    Returns:\n        np.ndarray: A mono-channel NumPy array normalized to the range [-1, 1].\n\n    Example:\n        &gt;&gt;&gt; audio = AudioSegment.from_file(\"example.mp3\")\n        &gt;&gt;&gt; audio_numpy = audio_to_numpy(audio)\n    \"\"\"\n    # Convert the audio segment to raw sample data\n    raw_data = np.array(audio_segment.get_array_of_samples()).astype(np.float32)\n\n    # Normalize data to the range [-1, 1]\n    raw_data /= MAX_INT16\n\n    # Ensure mono-channel (use first channel if stereo)\n    if audio_segment.channels &gt; 1:\n        raw_data = raw_data.reshape(-1, audio_segment.channels)[:, 0]\n\n    return raw_data\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.detect_silence_boundaries","title":"<code>detect_silence_boundaries(audio_file, min_silence_len=MIN_SILENCE_LENGTH, silence_thresh=SILENCE_DBFS_THRESHOLD, max_duration=MAX_DURATION_MS)</code>","text":"<p>Detect boundaries (start/end times) based on silence detection.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>Path to the audio file.</p> required <code>min_silence_len</code> <code>int</code> <p>Minimum silence length to consider for splitting (ms).</p> <code>MIN_SILENCE_LENGTH</code> <code>silence_thresh</code> <code>int</code> <p>Silence threshold in dBFS.</p> <code>SILENCE_DBFS_THRESHOLD</code> <code>max_duration</code> <code>int</code> <p>Maximum duration of any segment (ms).</p> <code>MAX_DURATION_MS</code> <p>Returns:</p> Type Description <code>Tuple[List[Boundary], Dict]</code> <p>List[Boundary]: A list of boundaries with empty text.</p> Example <p>boundaries = detect_silence_boundaries(Path(\"my_audio.mp3\")) for b in boundaries: ...     print(b.start, b.end)</p> Source code in <code>src/tnh_scholar/audio_processing/audio_legacy.py</code> <pre><code>def detect_silence_boundaries(\n    audio_file: Path,\n    min_silence_len: int = MIN_SILENCE_LENGTH,\n    silence_thresh: int = SILENCE_DBFS_THRESHOLD,\n    max_duration: int = MAX_DURATION_MS,\n) -&gt; Tuple[List[Boundary], Dict]:\n    \"\"\"\n    Detect boundaries (start/end times) based on silence detection.\n\n    Args:\n        audio_file (Path): Path to the audio file.\n        min_silence_len (int): Minimum silence length to consider for splitting (ms).\n        silence_thresh (int): Silence threshold in dBFS.\n        max_duration (int): Maximum duration of any segment (ms).\n\n    Returns:\n        List[Boundary]: A list of boundaries with empty text.\n\n    Example:\n        &gt;&gt;&gt; boundaries = detect_silence_boundaries(Path(\"my_audio.mp3\"))\n        &gt;&gt;&gt; for b in boundaries:\n        ...     print(b.start, b.end)\n    \"\"\"\n    logger.debug(\n        f\"Detecting silence boundaries with min_silence={min_silence_len}, silence_thresh={silence_thresh}\"\n    )\n\n    audio = AudioSegment.from_file(audio_file)\n    nonsilent_ranges = detect_nonsilent(\n        audio,\n        min_silence_len=min_silence_len,\n        silence_thresh=silence_thresh,\n        seek_step=SEEK_LENGTH,\n    )\n\n    # Combine ranges to enforce max_duration\n    if not nonsilent_ranges:\n        # If no nonsilent segments found, return entire file as one boundary\n        duration_s = len(audio) / 1000.0\n        return [Boundary(start=0.0, end=duration_s, text=\"\")]\n\n    combined_ranges = []\n    current_start, current_end = nonsilent_ranges[0]\n    for start, end in nonsilent_ranges[1:]:\n        if (current_end - current_start) + (end - start) &lt;= max_duration:\n            # Extend the current segment\n            current_end = end\n        else:\n            combined_ranges.append((current_start, current_end))\n            current_start, current_end = start, end\n    combined_ranges.append((current_start, current_end))\n\n    return [\n        Boundary(start=start_ms / 1000.0, end=end_ms / 1000.0, text=\"\")\n        for start_ms, end_ms in combined_ranges\n    ]\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.detect_whisper_boundaries","title":"<code>detect_whisper_boundaries(audio_file, model_size='tiny', language=None)</code>","text":"<p>Detect sentence boundaries using a Whisper model.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>Path to the audio file.</p> required <code>model_size</code> <code>str</code> <p>Whisper model size.</p> <code>'tiny'</code> <code>language</code> <code>str</code> <p>Language to force for transcription (e.g. 'en', 'vi'), or None for auto.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Boundary]</code> <p>List[Boundary]: A list of sentence boundaries with text.</p> Example <p>boundaries = detect_whisper_boundaries(Path(\"my_audio.mp3\"), model_size=\"tiny\") for b in boundaries: ...     print(b.start, b.end, b.text)</p> Source code in <code>src/tnh_scholar/audio_processing/audio_legacy.py</code> <pre><code>def detect_whisper_boundaries(\n    audio_file: Path, model_size: str = \"tiny\", language: str = None\n) -&gt; List[Boundary]:\n    \"\"\"\n    Detect sentence boundaries using a Whisper model.\n\n    Args:\n        audio_file (Path): Path to the audio file.\n        model_size (str): Whisper model size.\n        language (str): Language to force for transcription (e.g. 'en', 'vi'), or None for auto.\n\n    Returns:\n        List[Boundary]: A list of sentence boundaries with text.\n\n    Example:\n        &gt;&gt;&gt; boundaries = detect_whisper_boundaries(Path(\"my_audio.mp3\"), model_size=\"tiny\")\n        &gt;&gt;&gt; for b in boundaries:\n        ...     print(b.start, b.end, b.text)\n    \"\"\"\n\n    os.environ[\"KMP_WARNINGS\"] = \"0\"  # Turn of OMP warning message\n\n    # Load model\n    logger.info(\"Loading Whisper model...\")\n    model = load_whisper_model(model_size)\n    logger.info(f\"Model '{model_size}' loaded.\")\n\n    if language:\n        logger.info(f\"Language for boundaries set to '{language}'\")\n    else:\n        logger.info(\"Language not set. Autodetect will be used in Whisper model.\")\n\n    # with TimeProgress(expected_time=expected_time, desc=\"Generating transcription boundaries\"):\n    boundary_transcription = whisper_model_transcribe(\n        model,\n        str(audio_file),\n        task=\"transcribe\",\n        word_timestamps=True,\n        language=language,\n        verbose=False,\n    )\n\n    sentence_boundaries = [\n        Boundary(start=segment[\"start\"], end=segment[\"end\"], text=segment[\"text\"])\n        for segment in boundary_transcription[\"segments\"]\n    ]\n    return sentence_boundaries, boundary_transcription\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.split_audio","title":"<code>split_audio(audio_file, method='whisper', output_dir=None, model_size='tiny', language=None, min_silence_len=MIN_SILENCE_LENGTH, silence_thresh=SILENCE_DBFS_THRESHOLD, max_duration=MAX_DURATION)</code>","text":"<p>High-level function to split an audio file into chunks based on a chosen method.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>The input audio file.</p> required <code>method</code> <code>str</code> <p>Splitting method, \"silence\" or \"whisper\".</p> <code>'whisper'</code> <code>output_dir</code> <code>Path</code> <p>Directory to store output.</p> <code>None</code> <code>model_size</code> <code>str</code> <p>Whisper model size if method='whisper'.</p> <code>'tiny'</code> <code>language</code> <code>str</code> <p>Language for whisper transcription if method='whisper'.</p> <code>None</code> <code>min_silence_len</code> <code>int</code> <p>For silence-based detection, min silence length in ms.</p> <code>MIN_SILENCE_LENGTH</code> <code>silence_thresh</code> <code>int</code> <p>Silence threshold in dBFS.</p> <code>SILENCE_DBFS_THRESHOLD</code> <code>max_duration</code> <code>int</code> <p>Max chunk length in seconds (also used to derive ms threshold).</p> <code>MAX_DURATION</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Directory containing the resulting chunks.</p> Example Source code in <code>src/tnh_scholar/audio_processing/audio_legacy.py</code> <pre><code>def split_audio(\n    audio_file: Path,\n    method: str = \"whisper\",\n    output_dir: Optional[Path] = None,\n    model_size: str = \"tiny\",\n    language: str = None,\n    min_silence_len: int = MIN_SILENCE_LENGTH,\n    silence_thresh: int = SILENCE_DBFS_THRESHOLD,\n    max_duration: int = MAX_DURATION,\n) -&gt; Path:\n    \"\"\"\n    High-level function to split an audio file into chunks based on a chosen method.\n\n    Args:\n        audio_file (Path): The input audio file.\n        method (str): Splitting method, \"silence\" or \"whisper\".\n        output_dir (Path): Directory to store output.\n        model_size (str): Whisper model size if method='whisper'.\n        language (str): Language for whisper transcription if method='whisper'.\n        min_silence_len (int): For silence-based detection, min silence length in ms.\n        silence_thresh (int): Silence threshold in dBFS.\n        max_duration (int): Max chunk length in seconds (also used to derive ms threshold).\n\n    Returns:\n        Path: Directory containing the resulting chunks.\n\n    Example:\n        &gt;&gt;&gt; # Split using silence detection\n        &gt;&gt;&gt; split_audio(Path(\"my_audio.mp3\"), method=\"silence\")\n\n        &gt;&gt;&gt; # Split using whisper-based sentence boundaries\n        &gt;&gt;&gt; split_audio(Path(\"my_audio.mp3\"), method=\"whisper\", model_size=\"base\", language=\"en\")\n    \"\"\"\n\n    logger.info(f\"Splitting audio with max_duration={max_duration} seconds\")\n\n    if method == \"whisper\":\n        boundaries, _ = detect_whisper_boundaries(\n            audio_file, model_size=model_size, language=language\n        )\n\n    elif method == \"silence\":\n        max_duration_ms = (\n            max_duration * 1000\n        )  # convert duration in seconds to milliseconds\n        boundaries = detect_silence_boundaries(\n            audio_file,\n            min_silence_len=min_silence_len,\n            silence_thresh=silence_thresh,\n            max_duration=max_duration_ms,\n        )\n    else:\n        raise ValueError(f\"Unknown method: {method}. Must be 'silence' or 'whisper'.\")\n\n    # delete all files in the output_dir (this is useful for reprocessing)\n\n    return split_audio_at_boundaries(\n        audio_file, boundaries, output_dir=output_dir, max_duration=max_duration\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.split_audio--split-using-silence-detection","title":"Split using silence detection","text":"<p>split_audio(Path(\"my_audio.mp3\"), method=\"silence\")</p>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.split_audio--split-using-whisper-based-sentence-boundaries","title":"Split using whisper-based sentence boundaries","text":"<p>split_audio(Path(\"my_audio.mp3\"), method=\"whisper\", model_size=\"base\", language=\"en\")</p>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.split_audio_at_boundaries","title":"<code>split_audio_at_boundaries(audio_file, boundaries, output_dir=None, max_duration=MAX_DURATION)</code>","text":"<p>Split the audio file into chunks based on provided boundaries, ensuring all audio is included and boundaries align with the start of Whisper segments.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>The input audio file.</p> required <code>boundaries</code> <code>List[Boundary]</code> <p>Detected boundaries.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to store the resulting chunks.</p> <code>None</code> <code>max_duration</code> <code>int</code> <p>Maximum chunk length in seconds.</p> <code>MAX_DURATION</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Directory containing the chunked audio files.</p> Example <p>boundaries = [Boundary(34.02, 37.26, \"...\"), Boundary(38.0, 41.18, \"...\")] out_dir = split_audio_at_boundaries(Path(\"my_audio.mp3\"), boundaries)</p> Source code in <code>src/tnh_scholar/audio_processing/audio_legacy.py</code> <pre><code>def split_audio_at_boundaries(\n    audio_file: Path,\n    boundaries: List[Boundary],\n    output_dir: Path = None,\n    max_duration: int = MAX_DURATION,\n) -&gt; Path:\n    \"\"\"\n    Split the audio file into chunks based on provided boundaries, ensuring all audio is included\n    and boundaries align with the start of Whisper segments.\n\n    Args:\n        audio_file (Path): The input audio file.\n        boundaries (List[Boundary]): Detected boundaries.\n        output_dir (Path): Directory to store the resulting chunks.\n        max_duration (int): Maximum chunk length in seconds.\n\n    Returns:\n        Path: Directory containing the chunked audio files.\n\n    Example:\n        &gt;&gt;&gt; boundaries = [Boundary(34.02, 37.26, \"...\"), Boundary(38.0, 41.18, \"...\")]\n        &gt;&gt;&gt; out_dir = split_audio_at_boundaries(Path(\"my_audio.mp3\"), boundaries)\n    \"\"\"\n    logger.info(f\"Splitting audio with max_duration={max_duration} seconds\")\n\n    # Load the audio file\n    audio = AudioSegment.from_file(audio_file)\n\n    # Create output directory based on filename\n    if output_dir is None:\n        output_dir = audio_file.parent / f\"{audio_file.stem}_chunks\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Clean up the output directory\n    for file in output_dir.iterdir():\n        if file.is_file():\n            logger.info(f\"Deleting existing file: {file}\")\n            file.unlink()\n\n    chunk_start = 0  # Start time for the first chunk in ms\n    chunk_count = 1\n    current_chunk = AudioSegment.empty()\n\n    for idx, boundary in enumerate(boundaries):\n        segment_start_ms = int(boundary.start * 1000)\n        if idx + 1 &lt; len(boundaries):\n            segment_end_ms = int(\n                boundaries[idx + 1].start * 1000\n            )  # Next boundary's start\n        else:\n            segment_end_ms = len(audio)  # End of the audio for the last boundary\n\n        # Adjust for the first segment starting at 0\n        if idx == 0 and segment_start_ms &gt; 0:\n            segment_start_ms = 0  # Ensure we include the very beginning of the audio\n\n        segment = audio[segment_start_ms:segment_end_ms]\n\n        logger.debug(\n            f\"Boundary index: {idx}, segment_start: {segment_start_ms / 1000}, segment_end: {segment_end_ms / 1000}, duration: {segment.duration_seconds}\"\n        )\n        logger.debug(f\"Current chunk Duration (s): {current_chunk.duration_seconds}\")\n\n        if len(current_chunk) + len(segment) &lt;= max_duration * 1000:\n            # Add segment to the current chunk\n            current_chunk += segment\n        else:\n            # Export current chunk\n            chunk_path = output_dir / f\"chunk_{chunk_count}.mp3\"\n            current_chunk.export(chunk_path, format=\"mp3\")\n            logger.info(f\"Exported: {chunk_path}\")\n            chunk_count += 1\n\n            # Start a new chunk with the current segment\n            current_chunk = segment\n\n    # Export the final chunk if any audio remains\n    if len(current_chunk) &gt; 0:\n        chunk_path = output_dir / f\"chunk_{chunk_count}.mp3\"\n        current_chunk.export(chunk_path, format=\"mp3\")\n        logger.info(f\"Exported: {chunk_path}\")\n\n    return output_dir\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.whisper_model_transcribe","title":"<code>whisper_model_transcribe(model, input_source, *args, **kwargs)</code>","text":"<p>Wrapper around model.transcribe that suppresses the known 'FP16 is not supported on CPU; using FP32 instead' UserWarning and redirects unwanted 'OMP' messages to prevent interference.</p> <p>This function accepts all args and kwargs that model.transcribe normally does, and supports input sources as file paths (str or Path) or in-memory audio arrays.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The Whisper model instance.</p> required <code>input_source</code> <code>Union[str, Path, ndarray]</code> <p>Input audio file path, URL, or in-memory audio array.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments for model.transcribe.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for model.transcribe.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Transcription result from model.transcribe.</p> Example Source code in <code>src/tnh_scholar/audio_processing/audio_legacy.py</code> <pre><code>def whisper_model_transcribe(\n    model: Any,\n    input_source: Any,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Wrapper around model.transcribe that suppresses the known\n    'FP16 is not supported on CPU; using FP32 instead' UserWarning\n    and redirects unwanted 'OMP' messages to prevent interference.\n\n    This function accepts all args and kwargs that model.transcribe normally does,\n    and supports input sources as file paths (str or Path) or in-memory audio arrays.\n\n    Parameters:\n        model (Any): The Whisper model instance.\n        input_source (Union[str, Path, np.ndarray]): Input audio file path, URL, or in-memory audio array.\n        *args: Additional positional arguments for model.transcribe.\n        **kwargs: Additional keyword arguments for model.transcribe.\n\n    Returns:\n        Dict[str, Any]: Transcription result from model.transcribe.\n\n    Example:\n        # Using a file path\n        result = whisper_model_transcribe(my_model, \"sample_audio.mp3\", verbose=True)\n\n        # Using an audio array\n        result = whisper_model_transcribe(my_model, audio_array, language=\"en\")\n    \"\"\"\n\n    # class StdoutFilter(io.StringIO):\n    #     def __init__(self, original_stdout):\n    #         super().__init__()\n    #         self.original_stdout = original_stdout\n\n    #     def write(self, message):\n    #         # Suppress specific messages like 'OMP:' while allowing others\n    #         if \"OMP:\" not in message:\n    #             self.original_stdout.write(message)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            message=\"FP16 is not supported on CPU; using FP32 instead\",\n            category=UserWarning,\n        )\n\n        # Redirect stdout to suppress OMP messages\n        # original_stdout = sys.stdout\n        # sys.stdout = filtered_stdout\n\n        try:\n            # Convert Path to str if needed\n            if isinstance(input_source, Path):\n                input_source = str(input_source)\n\n            # Call the original transcribe function\n            return model.transcribe(input_source, *args, **kwargs)\n        finally:\n            # Restore original stdout\n            # sys.stdout = original_stdout\n            pass\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.whisper_model_transcribe--using-a-file-path","title":"Using a file path","text":"<p>result = whisper_model_transcribe(my_model, \"sample_audio.mp3\", verbose=True)</p>"},{"location":"api/#tnh_scholar.audio_processing.audio_legacy.whisper_model_transcribe--using-an-audio-array","title":"Using an audio array","text":"<p>result = whisper_model_transcribe(my_model, audio_array, language=\"en\")</p>"},{"location":"api/#tnh_scholar.audio_processing.diarization","title":"<code>diarization</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.__all__","title":"<code>__all__ = ['DiarizationProcessor', 'diarize', 'diarize_to_file', 'DiarizationParams', 'PyannoteClient', 'PyannoteConfig']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationParams","title":"<code>DiarizationParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Per-request diarization options; maps to pyannote API payload. Use .to_api_dict() to emit API field names.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class DiarizationParams(BaseModel):\n    \"\"\"\n    Per-request diarization options; maps to pyannote API payload.\n    Use .to_api_dict() to emit API field names.\n    \"\"\"\n\n    model_config = ConfigDict(\n        frozen=True,            # make instances immutable\n        populate_by_name=True,  # allow using pythonic field names with aliases\n        extra=\"forbid\",         # catch accidental fields at construction\n    )\n\n    # Pythonic attribute -&gt; API alias on dump\n    num_speakers: int | Literal[\"auto\"] | None = Field(\n        default=None,\n        alias=\"numSpeakers\",\n        description=\"Fixed number of speakers or 'auto' for detection.\",\n    )\n    confidence: float | None = Field(\n        default=None,\n        ge=0.0,\n        le=1.0,\n        description=\"Confidence threshold for segments.\",\n    )\n    webhook: AnyUrl | None = Field(\n        default=None,\n        description=\"Webhook URL for job status callbacks.\",\n    )\n\n    def to_api_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return payload dict using API field names (camelCase) and excluding Nones.\"\"\"\n        return self.model_dump(by_alias=True, exclude_none=True)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationParams.confidence","title":"<code>confidence = Field(default=None, ge=0.0, le=1.0, description='Confidence threshold for segments.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationParams.model_config","title":"<code>model_config = ConfigDict(frozen=True, populate_by_name=True, extra='forbid')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationParams.num_speakers","title":"<code>num_speakers = Field(default=None, alias='numSpeakers', description=\"Fixed number of speakers or 'auto' for detection.\")</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationParams.webhook","title":"<code>webhook = Field(default=None, description='Webhook URL for job status callbacks.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationParams.to_api_dict","title":"<code>to_api_dict()</code>","text":"<p>Return payload dict using API field names (camelCase) and excluding Nones.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>def to_api_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return payload dict using API field names (camelCase) and excluding Nones.\"\"\"\n    return self.model_dump(by_alias=True, exclude_none=True)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor","title":"<code>DiarizationProcessor</code>","text":"<p>Orchestrator over a DiarizationService.</p> <p>This layer delegates to the service for generation and handles persistence.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>class DiarizationProcessor:\n    \"\"\"Orchestrator over a DiarizationService.\n\n    This layer delegates to the service for generation and handles persistence.\n    \"\"\"\n\n    def __init__(\n        self,\n        audio_file_path: Path,\n        output_path: Optional[Path] = None,\n        *,\n        service: Optional[DiarizationService] = None,\n        params: Optional[DiarizationParams] = None,\n        api_key: Optional[str] = None,\n        writer: Optional[ResultWriter] = None,\n    ) -&gt; None:\n        self.audio_file_path: Path = audio_file_path.resolve()\n        if not self.audio_file_path.exists():\n            raise FileNotFoundError(f\"Audio file not found: {audio_file_path}\")\n\n        # Default output path\n        self.output_path: Path = (\n            output_path.resolve()\n            if output_path is not None\n            else self.audio_file_path.parent / f\"{self.audio_file_path.stem}{PYANNOTE_FILE_STR}.json\"\n        )\n\n        # Service &amp; config\n        # If a concrete service is not provided, default to PyannoteService.\n        # Only pass api_key to PyannoteClient if it is not None.\n        default_client = PyannoteClient(api_key) if api_key is not None else PyannoteClient()\n        self.service: DiarizationService = service or PyannoteService(default_client)\n        self.params: Optional[DiarizationParams] = params\n        self.writer: ResultWriter = writer or FileResultWriter()\n\n        # Cached state\n        self._last_response: Optional[DiarizationResponse] = None\n        self._last_job_id: Optional[str] = None\n\n    # ---- Two-phase job control (nice for UIs) --------------------------------\n\n    def start(self) -&gt; JobHandle:\n        \"\"\"Start a job and cache its job_id.\"\"\"\n        job_id = self.service.start(self.audio_file_path, params=self.params)\n        if not job_id:\n            raise RuntimeError(\"Diarization service returned empty job_id\")\n        self._last_job_id = job_id\n        return JobHandle(job_id=job_id)\n\n    def get_response(\n        self, job: Optional[Union[JobHandle, str]] = None, *, wait_until_complete: bool = False\n        ) -&gt; DiarizationResponse:\n        \"\"\"Fetch current/final response for a job, caching the last response.\"\"\"\n        target_id: Optional[str]\n        if isinstance(job, JobHandle):\n            target_id = job.job_id\n        else:\n            target_id = job or self._last_job_id\n        if target_id is None:\n            raise ValueError(\n                \"No job_id provided and no previous job has been started. Call start() or pass a job_id.\"\n            )\n        resp = self.service.get_response(target_id, wait_until_complete=wait_until_complete)\n        self._last_response = resp\n        return resp\n\n    # ---- One-shot path --------------------------------------------------------\n\n    def generate(self, *, wait_until_complete: bool = True) -&gt; DiarizationResponse:\n        \"\"\"One-shot convenience: delegate to the service and cache the response.\"\"\"\n        resp = self.service.generate(\n            self.audio_file_path, \n            params=self.params, \n            wait_until_complete=wait_until_complete\n            )\n        self._last_response = resp\n        # If the service exposes a job_id in the envelope, cache it for UIs\n        # Do not fail on metadata issues; response is primary.\n        try:\n            job_id = getattr(resp, \"job_id\", None)\n            if isinstance(job_id, str):\n                self._last_job_id = job_id\n        except (AttributeError, TypeError) as e:\n            logger.warning(f\"Could not extract job_id from response: {e}\")\n        return resp\n\n    # ---- Persistence ----------------------------------------------------------\n\n    def export(self, response: Optional[DiarizationResponse] = None) -&gt; Path:\n        \"\"\"Write the provided or last response to `self.output_path`.\"\"\"\n        result = response or self._last_response\n        if result is None:\n            raise ValueError(\n                \"No DiarizationResponse available; call generate()/get_response() first or pass response=\"\n                )\n        return self.writer.write(self.output_path, result)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor.audio_file_path","title":"<code>audio_file_path = audio_file_path.resolve()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor.output_path","title":"<code>output_path = output_path.resolve() if output_path is not None else self.audio_file_path.parent / f'{self.audio_file_path.stem}{PYANNOTE_FILE_STR}.json'</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor.service","title":"<code>service = service or PyannoteService(default_client)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor.writer","title":"<code>writer = writer or FileResultWriter()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor.__init__","title":"<code>__init__(audio_file_path, output_path=None, *, service=None, params=None, api_key=None, writer=None)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def __init__(\n    self,\n    audio_file_path: Path,\n    output_path: Optional[Path] = None,\n    *,\n    service: Optional[DiarizationService] = None,\n    params: Optional[DiarizationParams] = None,\n    api_key: Optional[str] = None,\n    writer: Optional[ResultWriter] = None,\n) -&gt; None:\n    self.audio_file_path: Path = audio_file_path.resolve()\n    if not self.audio_file_path.exists():\n        raise FileNotFoundError(f\"Audio file not found: {audio_file_path}\")\n\n    # Default output path\n    self.output_path: Path = (\n        output_path.resolve()\n        if output_path is not None\n        else self.audio_file_path.parent / f\"{self.audio_file_path.stem}{PYANNOTE_FILE_STR}.json\"\n    )\n\n    # Service &amp; config\n    # If a concrete service is not provided, default to PyannoteService.\n    # Only pass api_key to PyannoteClient if it is not None.\n    default_client = PyannoteClient(api_key) if api_key is not None else PyannoteClient()\n    self.service: DiarizationService = service or PyannoteService(default_client)\n    self.params: Optional[DiarizationParams] = params\n    self.writer: ResultWriter = writer or FileResultWriter()\n\n    # Cached state\n    self._last_response: Optional[DiarizationResponse] = None\n    self._last_job_id: Optional[str] = None\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor.export","title":"<code>export(response=None)</code>","text":"<p>Write the provided or last response to <code>self.output_path</code>.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def export(self, response: Optional[DiarizationResponse] = None) -&gt; Path:\n    \"\"\"Write the provided or last response to `self.output_path`.\"\"\"\n    result = response or self._last_response\n    if result is None:\n        raise ValueError(\n            \"No DiarizationResponse available; call generate()/get_response() first or pass response=\"\n            )\n    return self.writer.write(self.output_path, result)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor.generate","title":"<code>generate(*, wait_until_complete=True)</code>","text":"<p>One-shot convenience: delegate to the service and cache the response.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def generate(self, *, wait_until_complete: bool = True) -&gt; DiarizationResponse:\n    \"\"\"One-shot convenience: delegate to the service and cache the response.\"\"\"\n    resp = self.service.generate(\n        self.audio_file_path, \n        params=self.params, \n        wait_until_complete=wait_until_complete\n        )\n    self._last_response = resp\n    # If the service exposes a job_id in the envelope, cache it for UIs\n    # Do not fail on metadata issues; response is primary.\n    try:\n        job_id = getattr(resp, \"job_id\", None)\n        if isinstance(job_id, str):\n            self._last_job_id = job_id\n    except (AttributeError, TypeError) as e:\n        logger.warning(f\"Could not extract job_id from response: {e}\")\n    return resp\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor.get_response","title":"<code>get_response(job=None, *, wait_until_complete=False)</code>","text":"<p>Fetch current/final response for a job, caching the last response.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def get_response(\n    self, job: Optional[Union[JobHandle, str]] = None, *, wait_until_complete: bool = False\n    ) -&gt; DiarizationResponse:\n    \"\"\"Fetch current/final response for a job, caching the last response.\"\"\"\n    target_id: Optional[str]\n    if isinstance(job, JobHandle):\n        target_id = job.job_id\n    else:\n        target_id = job or self._last_job_id\n    if target_id is None:\n        raise ValueError(\n            \"No job_id provided and no previous job has been started. Call start() or pass a job_id.\"\n        )\n    resp = self.service.get_response(target_id, wait_until_complete=wait_until_complete)\n    self._last_response = resp\n    return resp\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.DiarizationProcessor.start","title":"<code>start()</code>","text":"<p>Start a job and cache its job_id.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def start(self) -&gt; JobHandle:\n    \"\"\"Start a job and cache its job_id.\"\"\"\n    job_id = self.service.start(self.audio_file_path, params=self.params)\n    if not job_id:\n        raise RuntimeError(\"Diarization service returned empty job_id\")\n    self._last_job_id = job_id\n    return JobHandle(job_id=job_id)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient","title":"<code>PyannoteClient</code>","text":"<p>Client for interacting with the pyannote.ai speaker diarization API.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>class PyannoteClient:\n    \"\"\"Client for interacting with the pyannote.ai speaker diarization API.\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, config: Optional[PyannoteConfig] = None):\n        \"\"\"\n        Initialize with API key.\n\n        Args:\n            api_key: Pyannote.ai API key (defaults to environment variable)\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"PYANNOTEAI_API_TOKEN\")\n        if not self.api_key:\n            raise APIKeyError(\n                \"API key is required. Set PYANNOTEAI_API_TOKEN environment \"\n                \"variable or pass as parameter\"\n            )\n\n        self.config = config or PyannoteConfig()\n        self.polling_config = self.config.polling_config\n\n        # Upload-specific timeouts (longer than general calls)\n        self.upload_timeout = self.config.upload_timeout\n        self.upload_max_retries = self.config.upload_max_retries\n        self.network_timeout = self.config.network_timeout\n\n        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n    # -----------------------\n    # Upload helpers\n    # -----------------------\n    def _create_media_id(self) -&gt; str:\n        \"\"\"Generate a unique media ID.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n        return f\"{self.config.media_prefix}{timestamp}\"\n\n    def _upload_file(self, file_path: Path, upload_url: str) -&gt; bool:\n        \"\"\"\n        Upload file to the provided URL.\n\n        Args:\n            file_path: Path to the file to upload\n            upload_url: URL to upload to\n\n        Returns:\n            bool: True if upload successful, False otherwise\n        \"\"\"\n        try:\n            logger.info(f\"Uploading file to Pyannote.ai: {file_path}\")\n            with open(file_path, \"rb\") as file_data:\n                upload_response = requests.put(\n                    upload_url,\n                    data=file_data,\n                    headers={\"Content-Type\": self.config.media_content_type},\n                    timeout=self.upload_timeout,\n                )\n\n            upload_response.raise_for_status()\n            logger.info(\"File uploaded successfully\")\n            return True\n\n        except requests.RequestException as e:\n            logger.error(f\"Failed to upload file: {e}\")\n            return False\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential_jitter(exp_base=2, initial=3, max=30),\n        retry=retry_if_exception_type(\n            (requests.RequestException, requests.Timeout, requests.ConnectionError)\n            ),\n    )\n    def upload_audio(self, file_path: Path) -&gt; Optional[str]:\n        \"\"\"\n        Upload audio file with retry logic for network robustness.\n\n        Retries on network errors with exponential backoff.\n        Fails fast on permanent errors (auth, file not found, etc.).\n        \"\"\"\n        try:\n            if not file_path.exists() or not file_path.is_file():\n                logger.error(f\"Audio file not found or is not a file: {file_path}\")\n                return None\n        except OSError as e:\n            logger.error(f\"Error accessing audio file '{file_path}': {e}\")\n            return None\n\n        try:\n            file_size_mb = file_path.stat().st_size / (1024 * 1024)\n        except OSError as e:\n            logger.error(f\"Error reading file size for '{file_path}': {e}\")\n            return None\n\n        logger.info(f\"Starting upload of {file_path.name} ({file_size_mb:.1f}MB)\")\n\n        try:\n            # Create media ID\n            media_id = self._create_media_id()\n            logger.debug(f\"Created media ID: {media_id}\")\n\n            # Get upload URL (this is fast, use normal timeout)\n            upload_url = self._data_upload_url(media_id)\n            if not upload_url:\n                return None\n\n            # Upload file (this is slow, use extended timeout)\n            if self._upload_file(file_path, upload_url):\n                logger.info(f\"Upload completed successfully: {media_id}\")\n                return media_id\n            else:\n                logger.error(f\"Upload failed for {file_path.name}\")\n                return None\n\n        except Exception as e:\n            # Log but don't retry - let tenacity handle retries\n            logger.error(f\"Upload attempt failed: {e}\")\n            raise  # Re-raise for tenacity to handle\n\n    def _data_upload_url(self, media_id: str) -&gt; Optional[str]:\n        response = requests.post(\n            self.config.media_input_endpoint,\n            headers=self.headers,\n            json={\"url\": media_id},\n            timeout=self.network_timeout,\n        )\n        upload_url = self._extract_response_info(\n            response, \"url\", \"No upload URL in API response\"\n        )\n        logger.debug(f\"Got upload URL for media ID: {media_id}\")\n        return upload_url\n\n    def _extract_response_info(self, response, response_type, error_msg):\n        response.raise_for_status()\n        info = response.json()\n        if result := info.get(response_type):\n            return result\n        else:\n            raise ValueError(error_msg)\n\n    # -----------------------\n    # Start job\n    # -----------------------\n    def start_diarization(self, media_id: str, params: Optional[DiarizationParams] = None) -&gt; Optional[str]:\n        \"\"\"\n        Start diarization job with pyannote.ai API.\n\n        Args:\n            media_id: The media ID from upload_audio\n            params: Optional parameters for diarization\n\n        Returns:\n            Optional[str]: The job ID if started successfully, None otherwise\n        \"\"\"\n        try:\n            return self._send_payload(media_id, params)\n        except requests.RequestException as e:\n            logger.error(f\"API request failed: {e}\")\n            return None\n        except ValueError as e:\n            logger.error(f\"Invalid API response: {e}\")\n            return None\n\n    def _send_payload(self, media_id, params):\n        payload: Dict[str, Any] = {\"url\": media_id}\n        if params:\n            payload |= params.to_api_dict()\n            logger.info(f\"Starting diarization with params: {params}\")\n        logger.debug(f\"Full payload: {payload}\")\n\n        response = requests.post(self.config.diarize_endpoint, headers=self.headers, json=payload)\n        job_id = self._extract_response_info(\n            response, JOB_ID_FIELD, \"API response missing job ID\"\n        )\n        logger.info(f\"Diarization job {job_id} started successfully\")\n        return job_id\n\n    # -----------------------\n    # Status / Polling\n    # -----------------------\n    def check_job_status(self, job_id: str) -&gt; Optional[JobStatusResponse]:\n        \"\"\"\n        Check the status of a diarization job.\n\n        Returns a typed transport model (JobStatusResponse) or None on failure.\n        \"\"\"\n        return self._check_status_with_retry(job_id)\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential_jitter(exp_base=2, initial=1, max=10),\n        retry=retry_if_exception_type(\n            (requests.RequestException, requests.Timeout, requests.ConnectionError)\n            ),\n    )\n    def _check_status_with_retry(self, job_id: str) -&gt; Optional[JobStatusResponse]:\n        \"\"\"\n        Check job status with network error retry logic.\n\n        Retries network failures without killing the polling loop.\n        Fails fast on API errors (auth, malformed response, etc.).\n\n        Used as the status function in the JobPoller helper class.\n        \"\"\"\n        try:\n            endpoint = f\"{self.config.job_status_endpoint}/{job_id}\"\n            response = requests.get(endpoint, headers=self.headers)\n            response.raise_for_status()\n            result = response.json()\n\n            try:\n                jsr = JobStatusResponse.model_validate(result)\n            except Exception as ve:\n                logger.error(f\"Invalid status response for job {job_id}: {result} ({ve})\")\n                return None\n\n            return jsr\n\n        except requests.RequestException as e:\n            logger.warning(f\"Status check network error for job {job_id}: {e}\")\n            raise  # Let tenacity retry\n        except Exception as e:\n            logger.error(f\"Unexpected status check error for job {job_id}: {e}\")\n            return None  # Don't retry on unexpected errors\n\n    class JobPoller:\n        \"\"\"\n        Generic job polling helper for long-running async jobs.\n        \"\"\"\n\n        def __init__(self, status_fn, job_id: str, polling_config: PollingConfig):\n            self.status_fn = status_fn\n            self.job_id = job_id\n            self.polling_config = polling_config\n            self.poll_count = 0\n            self.start_time = time.time()\n            self.last_status: Optional[JobStatusResponse] = None\n            self._last_error_reason: Optional[str] = None\n\n        def _poll(self) -&gt; JobStatusResponse | _PollSignal | None:\n            self.poll_count += 1\n            try:\n                status_response = self.status_fn(self.job_id)\n            except RetryError as e:\n                self._last_error_reason = f\"status check retry exhausted: {e}\"\n                logger.error(f\"Status check retries exhausted for job {self.job_id}: {e}\")\n                return _PollSignal.STATUS_RETRY_EXHAUSTED\n\n            if status_response is None:\n                logger.error(f\"Failed to get status for job {self.job_id} after retries\")\n                self._last_error_reason = \"status response None\"\n                return None\n\n            # track last known status for timeout / errors\n            self.last_status = status_response\n\n            status = status_response.status\n            elapsed = time.time() - self.start_time\n\n            if status == JobStatus.SUCCEEDED:\n                logger.info(\n                    f\"Job {self.job_id} completed successfully after {elapsed:.1f}s ({self.poll_count} polls)\"\n                )\n                return status_response\n\n            if status == JobStatus.FAILED:\n                logger.error(f\"Job {self.job_id} failed: {status_response.server_error_msg}\")\n                return status_response\n\n            # Job still running - calculate next poll interval\n            logger.info(f\"Job {self.job_id} status: {status} (elapsed: {elapsed:.1f}s)\")\n            return _PollSignal.CONTINUE\n\n        # --- Internal builders to attach polling context and craft JSRs ---\n        def _attach_context(\n            self, \n            base: Optional[JobStatusResponse], \n            *, \n            outcome: PollOutcome, \n            elapsed: float, \n            msg: Optional[str] = None\n            ) -&gt; JobStatusResponse:\n            \"\"\"Return a JSR carrying outcome + poll context. If `base` exists, preserve its\n            status/payload/server_error_msg unless `msg` overrides it. Otherwise, synthesize a minimal JSR.\"\"\"\n            if base is None:\n                return JobStatusResponse(\n                    job_id=self.job_id,\n                    outcome=outcome,\n                    status=None,\n                    server_error_msg=msg,\n                    payload=None,\n                    polls=self.poll_count,\n                    elapsed_s=elapsed,\n                )\n            return JobStatusResponse(\n                job_id=self.job_id,\n                outcome=outcome,\n                status=base.status,\n                server_error_msg=msg if msg is not None else base.server_error_msg,\n                payload=base.payload,\n                polls=self.poll_count,\n                elapsed_s=elapsed,\n            )\n\n        def _on_terminal(self, jsr: JobStatusResponse, *, elapsed: float) -&gt; JobStatusResponse:\n            \"\"\"Attach poll context to a terminal server response (SUCCEEDED/FAILED).\"\"\"\n            return JobStatusResponse(\n                job_id=self.job_id,\n                outcome=PollOutcome.SUCCEEDED if jsr.status == JobStatus.SUCCEEDED else PollOutcome.FAILED,\n                status=jsr.status,\n                server_error_msg=jsr.server_error_msg,\n                payload=jsr.payload,\n                polls=self.poll_count,\n                elapsed_s=elapsed,\n            )\n\n        def _on_status_retry_exhausted(self, *, elapsed: float) -&gt; JobStatusResponse:\n            return self._attach_context(\n                self.last_status, \n                outcome=PollOutcome.NETWORK_ERROR, \n                elapsed=elapsed, \n                msg=self._last_error_reason\n                )\n\n        def _on_invalid_payload(self, *, elapsed: float) -&gt; JobStatusResponse:\n            return self._attach_context(\n                self.last_status, \n                outcome=PollOutcome.ERROR, \n                elapsed=elapsed, \n                msg=\"invalid status payload\"\n                )\n\n        def _on_timeout(self, err: RetryError, *, elapsed: float) -&gt; JobStatusResponse:\n            return self._attach_context(\n                self.last_status, \n                outcome=PollOutcome.TIMEOUT, \n                elapsed=elapsed, \n                msg=str(err)\n                )\n\n        def _on_interrupt(self, *, elapsed: float) -&gt; JobStatusResponse:\n            return self._attach_context(\n                self.last_status, \n                outcome=PollOutcome.INTERRUPTED, \n                elapsed=elapsed, \n                msg=\"KeyboardInterrupt\"\n                )\n\n        def _on_exception(self, err: Exception, *, elapsed: float) -&gt; JobStatusResponse:\n            return self._attach_context(\n                self.last_status, \n                outcome=PollOutcome.ERROR, \n                elapsed=elapsed, \n                msg=str(err)\n                )\n\n        def run(self) -&gt; JobStatusResponse:\n            try:\n                result = self._setup_and_run_poll()\n                elapsed = time.time() - self.start_time\n\n                if isinstance(result, JobStatusResponse):\n                    # Terminal SUCCEEDED/FAILED (or unexpected non-terminal delivered): attach context\n                    return self._on_terminal(result, elapsed=elapsed)\n\n                if result is _PollSignal.STATUS_RETRY_EXHAUSTED:\n                    return self._on_status_retry_exhausted(elapsed=elapsed)\n\n                # None indicates invalid status payload or unexpected branch\n                return self._on_invalid_payload(elapsed=elapsed)\n\n            except RetryError as e:\n                # Outer polling timeout\n                elapsed = time.time() - self.start_time\n                logger.info(f\"Polling timed out for job {self.job_id} after {elapsed:.1f}s\")\n                return self._on_timeout(e, elapsed=elapsed)\n            except KeyboardInterrupt:\n                elapsed = time.time() - self.start_time\n                logger.info(f\"Polling for job {self.job_id} interrupted by user. Exiting.\")\n                return self._on_interrupt(elapsed=elapsed)\n            except Exception as e:\n                elapsed = time.time() - self.start_time\n                logger.error(f\"Polling failed for job {self.job_id}: {e}\")\n                return self._on_exception(e, elapsed=elapsed)\n\n        def _setup_and_run_poll(self) -&gt; Optional[JobStatusResponse | _PollSignal]:\n            cfg = self.polling_config\n            stop_policy = stop_never if cfg.polling_timeout is None else stop_after_delay(cfg.polling_timeout)\n            retrying = Retrying(\n                retry=retry_if_result(lambda result: result is _PollSignal.CONTINUE),\n                stop=stop_policy,\n                wait=wait_exponential_jitter(\n                    exp_base=cfg.exp_base,\n                    initial=cfg.initial_poll_time,\n                    max=cfg.max_interval,\n                ),\n                reraise=True,\n            )\n            result = retrying(self._poll)\n            if isinstance(result, JobStatusResponse):\n                return result\n            # could be STATUS_RETRY_EXHAUSTED sentinel or None\n            logger.info(f\"Polling ended with result: {result}\")\n            return result\n\n    def poll_job_until_complete(\n        self,\n        job_id: str,\n        estimated_duration: Optional[float] = None,\n        timeout: Optional[float] = None,\n        wait_until_complete: Optional[bool] = False,\n    ) -&gt; JobStatusResponse:\n        \"\"\"\n        Poll until the job reaches a terminal state or a client-side stop condition, and\n        return a unified JobStatusResponse (JSR) that includes both the server payload\n        and polling context via `outcome`, `polls`, and `elapsed_s`.\n\n        Args:\n            job_id: Remote job identifier to poll.\n            estimated_duration: Optional hint; currently unused (reserved for adaptive backoff).\n            timeout: Optional hard timeout in seconds for this poll call. If provided, it overrides\n                     the client's default polling timeout. Ignored if `wait_until_complete` is True.\n            wait_until_complete: If True, ignore timeout and poll indefinitely (subject to process lifetime).\n\n        Returns:\n            JobStatusResponse: unified transport + polling-context result.\n        \"\"\"\n        if timeout is not None and wait_until_complete:\n            raise ConfigurationError(\"Timeout cannot be set with wait_until_complete\")\n\n        # Derive an effective timeout for this call, without mutating client defaults\n        effective_timeout = None if wait_until_complete else (\n            timeout if timeout is not None else self.polling_config.polling_timeout\n            )\n\n        cfg = PollingConfig(\n            polling_timeout=effective_timeout,\n            initial_poll_time=self.polling_config.initial_poll_time,\n            exp_base=self.polling_config.exp_base,\n            max_interval=self.polling_config.max_interval,\n        )\n\n        poller = self.JobPoller(\n            status_fn=self._check_status_with_retry,\n            job_id=job_id,\n            polling_config=cfg,\n        )\n        return poller.run()\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.api_key","title":"<code>api_key = api_key or os.getenv('PYANNOTEAI_API_TOKEN')</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.config","title":"<code>config = config or PyannoteConfig()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.headers","title":"<code>headers = {'Authorization': f'Bearer {self.api_key}'}</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.network_timeout","title":"<code>network_timeout = self.config.network_timeout</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.polling_config","title":"<code>polling_config = self.config.polling_config</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.upload_max_retries","title":"<code>upload_max_retries = self.config.upload_max_retries</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.upload_timeout","title":"<code>upload_timeout = self.config.upload_timeout</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.JobPoller","title":"<code>JobPoller</code>","text":"<p>Generic job polling helper for long-running async jobs.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>class JobPoller:\n    \"\"\"\n    Generic job polling helper for long-running async jobs.\n    \"\"\"\n\n    def __init__(self, status_fn, job_id: str, polling_config: PollingConfig):\n        self.status_fn = status_fn\n        self.job_id = job_id\n        self.polling_config = polling_config\n        self.poll_count = 0\n        self.start_time = time.time()\n        self.last_status: Optional[JobStatusResponse] = None\n        self._last_error_reason: Optional[str] = None\n\n    def _poll(self) -&gt; JobStatusResponse | _PollSignal | None:\n        self.poll_count += 1\n        try:\n            status_response = self.status_fn(self.job_id)\n        except RetryError as e:\n            self._last_error_reason = f\"status check retry exhausted: {e}\"\n            logger.error(f\"Status check retries exhausted for job {self.job_id}: {e}\")\n            return _PollSignal.STATUS_RETRY_EXHAUSTED\n\n        if status_response is None:\n            logger.error(f\"Failed to get status for job {self.job_id} after retries\")\n            self._last_error_reason = \"status response None\"\n            return None\n\n        # track last known status for timeout / errors\n        self.last_status = status_response\n\n        status = status_response.status\n        elapsed = time.time() - self.start_time\n\n        if status == JobStatus.SUCCEEDED:\n            logger.info(\n                f\"Job {self.job_id} completed successfully after {elapsed:.1f}s ({self.poll_count} polls)\"\n            )\n            return status_response\n\n        if status == JobStatus.FAILED:\n            logger.error(f\"Job {self.job_id} failed: {status_response.server_error_msg}\")\n            return status_response\n\n        # Job still running - calculate next poll interval\n        logger.info(f\"Job {self.job_id} status: {status} (elapsed: {elapsed:.1f}s)\")\n        return _PollSignal.CONTINUE\n\n    # --- Internal builders to attach polling context and craft JSRs ---\n    def _attach_context(\n        self, \n        base: Optional[JobStatusResponse], \n        *, \n        outcome: PollOutcome, \n        elapsed: float, \n        msg: Optional[str] = None\n        ) -&gt; JobStatusResponse:\n        \"\"\"Return a JSR carrying outcome + poll context. If `base` exists, preserve its\n        status/payload/server_error_msg unless `msg` overrides it. Otherwise, synthesize a minimal JSR.\"\"\"\n        if base is None:\n            return JobStatusResponse(\n                job_id=self.job_id,\n                outcome=outcome,\n                status=None,\n                server_error_msg=msg,\n                payload=None,\n                polls=self.poll_count,\n                elapsed_s=elapsed,\n            )\n        return JobStatusResponse(\n            job_id=self.job_id,\n            outcome=outcome,\n            status=base.status,\n            server_error_msg=msg if msg is not None else base.server_error_msg,\n            payload=base.payload,\n            polls=self.poll_count,\n            elapsed_s=elapsed,\n        )\n\n    def _on_terminal(self, jsr: JobStatusResponse, *, elapsed: float) -&gt; JobStatusResponse:\n        \"\"\"Attach poll context to a terminal server response (SUCCEEDED/FAILED).\"\"\"\n        return JobStatusResponse(\n            job_id=self.job_id,\n            outcome=PollOutcome.SUCCEEDED if jsr.status == JobStatus.SUCCEEDED else PollOutcome.FAILED,\n            status=jsr.status,\n            server_error_msg=jsr.server_error_msg,\n            payload=jsr.payload,\n            polls=self.poll_count,\n            elapsed_s=elapsed,\n        )\n\n    def _on_status_retry_exhausted(self, *, elapsed: float) -&gt; JobStatusResponse:\n        return self._attach_context(\n            self.last_status, \n            outcome=PollOutcome.NETWORK_ERROR, \n            elapsed=elapsed, \n            msg=self._last_error_reason\n            )\n\n    def _on_invalid_payload(self, *, elapsed: float) -&gt; JobStatusResponse:\n        return self._attach_context(\n            self.last_status, \n            outcome=PollOutcome.ERROR, \n            elapsed=elapsed, \n            msg=\"invalid status payload\"\n            )\n\n    def _on_timeout(self, err: RetryError, *, elapsed: float) -&gt; JobStatusResponse:\n        return self._attach_context(\n            self.last_status, \n            outcome=PollOutcome.TIMEOUT, \n            elapsed=elapsed, \n            msg=str(err)\n            )\n\n    def _on_interrupt(self, *, elapsed: float) -&gt; JobStatusResponse:\n        return self._attach_context(\n            self.last_status, \n            outcome=PollOutcome.INTERRUPTED, \n            elapsed=elapsed, \n            msg=\"KeyboardInterrupt\"\n            )\n\n    def _on_exception(self, err: Exception, *, elapsed: float) -&gt; JobStatusResponse:\n        return self._attach_context(\n            self.last_status, \n            outcome=PollOutcome.ERROR, \n            elapsed=elapsed, \n            msg=str(err)\n            )\n\n    def run(self) -&gt; JobStatusResponse:\n        try:\n            result = self._setup_and_run_poll()\n            elapsed = time.time() - self.start_time\n\n            if isinstance(result, JobStatusResponse):\n                # Terminal SUCCEEDED/FAILED (or unexpected non-terminal delivered): attach context\n                return self._on_terminal(result, elapsed=elapsed)\n\n            if result is _PollSignal.STATUS_RETRY_EXHAUSTED:\n                return self._on_status_retry_exhausted(elapsed=elapsed)\n\n            # None indicates invalid status payload or unexpected branch\n            return self._on_invalid_payload(elapsed=elapsed)\n\n        except RetryError as e:\n            # Outer polling timeout\n            elapsed = time.time() - self.start_time\n            logger.info(f\"Polling timed out for job {self.job_id} after {elapsed:.1f}s\")\n            return self._on_timeout(e, elapsed=elapsed)\n        except KeyboardInterrupt:\n            elapsed = time.time() - self.start_time\n            logger.info(f\"Polling for job {self.job_id} interrupted by user. Exiting.\")\n            return self._on_interrupt(elapsed=elapsed)\n        except Exception as e:\n            elapsed = time.time() - self.start_time\n            logger.error(f\"Polling failed for job {self.job_id}: {e}\")\n            return self._on_exception(e, elapsed=elapsed)\n\n    def _setup_and_run_poll(self) -&gt; Optional[JobStatusResponse | _PollSignal]:\n        cfg = self.polling_config\n        stop_policy = stop_never if cfg.polling_timeout is None else stop_after_delay(cfg.polling_timeout)\n        retrying = Retrying(\n            retry=retry_if_result(lambda result: result is _PollSignal.CONTINUE),\n            stop=stop_policy,\n            wait=wait_exponential_jitter(\n                exp_base=cfg.exp_base,\n                initial=cfg.initial_poll_time,\n                max=cfg.max_interval,\n            ),\n            reraise=True,\n        )\n        result = retrying(self._poll)\n        if isinstance(result, JobStatusResponse):\n            return result\n        # could be STATUS_RETRY_EXHAUSTED sentinel or None\n        logger.info(f\"Polling ended with result: {result}\")\n        return result\n</code></pre> <code>job_id = job_id</code> <code>instance-attribute</code> \u00b6 <code>last_status = None</code> <code>instance-attribute</code> \u00b6 <code>poll_count = 0</code> <code>instance-attribute</code> \u00b6 <code>polling_config = polling_config</code> <code>instance-attribute</code> \u00b6 <code>start_time = time.time()</code> <code>instance-attribute</code> \u00b6 <code>status_fn = status_fn</code> <code>instance-attribute</code> \u00b6 <code>__init__(status_fn, job_id, polling_config)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def __init__(self, status_fn, job_id: str, polling_config: PollingConfig):\n    self.status_fn = status_fn\n    self.job_id = job_id\n    self.polling_config = polling_config\n    self.poll_count = 0\n    self.start_time = time.time()\n    self.last_status: Optional[JobStatusResponse] = None\n    self._last_error_reason: Optional[str] = None\n</code></pre> <code>run()</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def run(self) -&gt; JobStatusResponse:\n    try:\n        result = self._setup_and_run_poll()\n        elapsed = time.time() - self.start_time\n\n        if isinstance(result, JobStatusResponse):\n            # Terminal SUCCEEDED/FAILED (or unexpected non-terminal delivered): attach context\n            return self._on_terminal(result, elapsed=elapsed)\n\n        if result is _PollSignal.STATUS_RETRY_EXHAUSTED:\n            return self._on_status_retry_exhausted(elapsed=elapsed)\n\n        # None indicates invalid status payload or unexpected branch\n        return self._on_invalid_payload(elapsed=elapsed)\n\n    except RetryError as e:\n        # Outer polling timeout\n        elapsed = time.time() - self.start_time\n        logger.info(f\"Polling timed out for job {self.job_id} after {elapsed:.1f}s\")\n        return self._on_timeout(e, elapsed=elapsed)\n    except KeyboardInterrupt:\n        elapsed = time.time() - self.start_time\n        logger.info(f\"Polling for job {self.job_id} interrupted by user. Exiting.\")\n        return self._on_interrupt(elapsed=elapsed)\n    except Exception as e:\n        elapsed = time.time() - self.start_time\n        logger.error(f\"Polling failed for job {self.job_id}: {e}\")\n        return self._on_exception(e, elapsed=elapsed)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.__init__","title":"<code>__init__(api_key=None, config=None)</code>","text":"<p>Initialize with API key.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>Pyannote.ai API key (defaults to environment variable)</p> <code>None</code> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def __init__(self, api_key: Optional[str] = None, config: Optional[PyannoteConfig] = None):\n    \"\"\"\n    Initialize with API key.\n\n    Args:\n        api_key: Pyannote.ai API key (defaults to environment variable)\n    \"\"\"\n    self.api_key = api_key or os.getenv(\"PYANNOTEAI_API_TOKEN\")\n    if not self.api_key:\n        raise APIKeyError(\n            \"API key is required. Set PYANNOTEAI_API_TOKEN environment \"\n            \"variable or pass as parameter\"\n        )\n\n    self.config = config or PyannoteConfig()\n    self.polling_config = self.config.polling_config\n\n    # Upload-specific timeouts (longer than general calls)\n    self.upload_timeout = self.config.upload_timeout\n    self.upload_max_retries = self.config.upload_max_retries\n    self.network_timeout = self.config.network_timeout\n\n    self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.check_job_status","title":"<code>check_job_status(job_id)</code>","text":"<p>Check the status of a diarization job.</p> <p>Returns a typed transport model (JobStatusResponse) or None on failure.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def check_job_status(self, job_id: str) -&gt; Optional[JobStatusResponse]:\n    \"\"\"\n    Check the status of a diarization job.\n\n    Returns a typed transport model (JobStatusResponse) or None on failure.\n    \"\"\"\n    return self._check_status_with_retry(job_id)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.poll_job_until_complete","title":"<code>poll_job_until_complete(job_id, estimated_duration=None, timeout=None, wait_until_complete=False)</code>","text":"<p>Poll until the job reaches a terminal state or a client-side stop condition, and return a unified JobStatusResponse (JSR) that includes both the server payload and polling context via <code>outcome</code>, <code>polls</code>, and <code>elapsed_s</code>.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Remote job identifier to poll.</p> required <code>estimated_duration</code> <code>Optional[float]</code> <p>Optional hint; currently unused (reserved for adaptive backoff).</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>Optional hard timeout in seconds for this poll call. If provided, it overrides      the client's default polling timeout. Ignored if <code>wait_until_complete</code> is True.</p> <code>None</code> <code>wait_until_complete</code> <code>Optional[bool]</code> <p>If True, ignore timeout and poll indefinitely (subject to process lifetime).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>JobStatusResponse</code> <code>JobStatusResponse</code> <p>unified transport + polling-context result.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def poll_job_until_complete(\n    self,\n    job_id: str,\n    estimated_duration: Optional[float] = None,\n    timeout: Optional[float] = None,\n    wait_until_complete: Optional[bool] = False,\n) -&gt; JobStatusResponse:\n    \"\"\"\n    Poll until the job reaches a terminal state or a client-side stop condition, and\n    return a unified JobStatusResponse (JSR) that includes both the server payload\n    and polling context via `outcome`, `polls`, and `elapsed_s`.\n\n    Args:\n        job_id: Remote job identifier to poll.\n        estimated_duration: Optional hint; currently unused (reserved for adaptive backoff).\n        timeout: Optional hard timeout in seconds for this poll call. If provided, it overrides\n                 the client's default polling timeout. Ignored if `wait_until_complete` is True.\n        wait_until_complete: If True, ignore timeout and poll indefinitely (subject to process lifetime).\n\n    Returns:\n        JobStatusResponse: unified transport + polling-context result.\n    \"\"\"\n    if timeout is not None and wait_until_complete:\n        raise ConfigurationError(\"Timeout cannot be set with wait_until_complete\")\n\n    # Derive an effective timeout for this call, without mutating client defaults\n    effective_timeout = None if wait_until_complete else (\n        timeout if timeout is not None else self.polling_config.polling_timeout\n        )\n\n    cfg = PollingConfig(\n        polling_timeout=effective_timeout,\n        initial_poll_time=self.polling_config.initial_poll_time,\n        exp_base=self.polling_config.exp_base,\n        max_interval=self.polling_config.max_interval,\n    )\n\n    poller = self.JobPoller(\n        status_fn=self._check_status_with_retry,\n        job_id=job_id,\n        polling_config=cfg,\n    )\n    return poller.run()\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.start_diarization","title":"<code>start_diarization(media_id, params=None)</code>","text":"<p>Start diarization job with pyannote.ai API.</p> <p>Parameters:</p> Name Type Description Default <code>media_id</code> <code>str</code> <p>The media ID from upload_audio</p> required <code>params</code> <code>Optional[DiarizationParams]</code> <p>Optional parameters for diarization</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The job ID if started successfully, None otherwise</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def start_diarization(self, media_id: str, params: Optional[DiarizationParams] = None) -&gt; Optional[str]:\n    \"\"\"\n    Start diarization job with pyannote.ai API.\n\n    Args:\n        media_id: The media ID from upload_audio\n        params: Optional parameters for diarization\n\n    Returns:\n        Optional[str]: The job ID if started successfully, None otherwise\n    \"\"\"\n    try:\n        return self._send_payload(media_id, params)\n    except requests.RequestException as e:\n        logger.error(f\"API request failed: {e}\")\n        return None\n    except ValueError as e:\n        logger.error(f\"Invalid API response: {e}\")\n        return None\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteClient.upload_audio","title":"<code>upload_audio(file_path)</code>","text":"<p>Upload audio file with retry logic for network robustness.</p> <p>Retries on network errors with exponential backoff. Fails fast on permanent errors (auth, file not found, etc.).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential_jitter(exp_base=2, initial=3, max=30),\n    retry=retry_if_exception_type(\n        (requests.RequestException, requests.Timeout, requests.ConnectionError)\n        ),\n)\ndef upload_audio(self, file_path: Path) -&gt; Optional[str]:\n    \"\"\"\n    Upload audio file with retry logic for network robustness.\n\n    Retries on network errors with exponential backoff.\n    Fails fast on permanent errors (auth, file not found, etc.).\n    \"\"\"\n    try:\n        if not file_path.exists() or not file_path.is_file():\n            logger.error(f\"Audio file not found or is not a file: {file_path}\")\n            return None\n    except OSError as e:\n        logger.error(f\"Error accessing audio file '{file_path}': {e}\")\n        return None\n\n    try:\n        file_size_mb = file_path.stat().st_size / (1024 * 1024)\n    except OSError as e:\n        logger.error(f\"Error reading file size for '{file_path}': {e}\")\n        return None\n\n    logger.info(f\"Starting upload of {file_path.name} ({file_size_mb:.1f}MB)\")\n\n    try:\n        # Create media ID\n        media_id = self._create_media_id()\n        logger.debug(f\"Created media ID: {media_id}\")\n\n        # Get upload URL (this is fast, use normal timeout)\n        upload_url = self._data_upload_url(media_id)\n        if not upload_url:\n            return None\n\n        # Upload file (this is slow, use extended timeout)\n        if self._upload_file(file_path, upload_url):\n            logger.info(f\"Upload completed successfully: {media_id}\")\n            return media_id\n        else:\n            logger.error(f\"Upload failed for {file_path.name}\")\n            return None\n\n    except Exception as e:\n        # Log but don't retry - let tenacity handle retries\n        logger.error(f\"Upload attempt failed: {e}\")\n        raise  # Re-raise for tenacity to handle\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig","title":"<code>PyannoteConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration constants for Pyannote API.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/config.py</code> <pre><code>class PyannoteConfig(BaseSettings):\n    \"\"\"Configuration constants for Pyannote API.\"\"\"\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive = False,\n        env_prefix = \"PYANNOTE_\",\n        extra=\"ignore\",\n    )\n\n    # API Endpoints\n    base_url: str = \"https://api.pyannote.ai/v1\"\n\n    @property\n    def media_input_endpoint(self) -&gt; str:\n        return f\"{self.base_url}/media/input\"\n\n    @property\n    def diarize_endpoint(self) -&gt; str:\n        return f\"{self.base_url}/diarize\"\n\n    @property\n    def job_status_endpoint(self) -&gt; str:\n        return f\"{self.base_url}/jobs\"\n\n    # Media\n    media_prefix: str = \"media://diarization-\"\n    media_content_type: str = \"audio/mpeg\"\n\n    # Upload-specific settings\n    upload_timeout: int = 300  # 5 minutes for large files\n    upload_max_retries: int = 3\n\n    # Network specific settings\n    network_timeout: int = 3 # seconds\n\n    # Polling\n    polling_config: PollingConfig = PollingConfig()\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.base_url","title":"<code>base_url = 'https://api.pyannote.ai/v1'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.diarize_endpoint","title":"<code>diarize_endpoint</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.job_status_endpoint","title":"<code>job_status_endpoint</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.media_content_type","title":"<code>media_content_type = 'audio/mpeg'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.media_input_endpoint","title":"<code>media_input_endpoint</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.media_prefix","title":"<code>media_prefix = 'media://diarization-'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.model_config","title":"<code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=False, env_prefix='PYANNOTE_', extra='ignore')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.network_timeout","title":"<code>network_timeout = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.polling_config","title":"<code>polling_config = PollingConfig()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.upload_max_retries","title":"<code>upload_max_retries = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.PyannoteConfig.upload_timeout","title":"<code>upload_timeout = 300</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.diarize","title":"<code>diarize(audio_file_path, output_path=None, *, params=None, service=None, api_key=None, wait_until_complete=True)</code>","text":"<p>One-shot convenience to generate a result and (optionally) write it.</p> <p>This returns the <code>DiarizationResponse</code>. Writing is left to callers or <code>diarize_to_file</code> below.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def diarize(\n    audio_file_path: Path,\n    output_path: Optional[Path] = None,\n    *,\n    params: Optional[DiarizationParams] = None,\n    service: Optional[DiarizationService] = None,\n    api_key: Optional[str] = None,\n    wait_until_complete: bool = True,\n) -&gt; DiarizationResponse:\n    \"\"\"One-shot convenience to generate a result and (optionally) write it.\n\n    This returns the `DiarizationResponse`. Writing is left to callers or\n    `diarize_to_file` below.\n    \"\"\"\n    processor = DiarizationProcessor(\n        audio_file_path,\n        output_path=output_path,\n        service=service,\n        params=params,\n        api_key=api_key,\n    )\n    return processor.generate(wait_until_complete=wait_until_complete)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.diarize_to_file","title":"<code>diarize_to_file(audio_file_path, output_path=None, *, params=None, service=None, api_key=None, wait_until_complete=True)</code>","text":"<p>Convenience helper: generate then export to JSON if successful; returns response</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def diarize_to_file(\n    audio_file_path: Path,\n    output_path: Optional[Path] = None,\n    *,\n    params: Optional[DiarizationParams] = None,\n    service: Optional[DiarizationService] = None,\n    api_key: Optional[str] = None,\n    wait_until_complete: bool = True,\n) -&gt; DiarizationResponse:\n    \"\"\"Convenience helper: generate then export to JSON if successful; returns response\"\"\"\n    processor = DiarizationProcessor(\n        audio_file_path,\n        output_path=output_path,\n        service=service,\n        params=params,\n        api_key=api_key,\n    )\n    response = processor.generate(wait_until_complete=wait_until_complete)\n    if isinstance(response, DiarizationSucceeded):\n        processor.export()\n    return response\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.audio","title":"<code>audio</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.audio.__all__","title":"<code>__all__ = ['AudioHandler', 'AudioHandlerConfig']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.audio.AudioHandler","title":"<code>AudioHandler</code>","text":"<p>Isolates audio operations and external dependencies (pydub, ffmpeg).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/handler.py</code> <pre><code>class AudioHandler:\n    \"\"\"Isolates audio operations and external dependencies (pydub, ffmpeg).\"\"\"\n\n    def __init__(\n        self, \n        config: AudioHandlerConfig = AudioHandlerConfig()\n        ):\n        self.config = config\n        # Sensible fall\u2011backs for optional config values\n        self.base_audio: AudioSegment\n        self.output_format: Optional[str] = config.output_format\n        self.input_format: Optional[str] = None\n\n    def build_audio_chunk(self, chunk: DiarizationChunk, audio_file: Path) -&gt; AudioChunk:\n        \"\"\"builds and sets the internal chunk.audio to be the new AudioChunk\"\"\"\n\n        self._set_io_format(audio_file)\n        base_audio = self._load_audio(audio_file)\n        self._validate_segments(chunk)\n\n        audio_segment = self._assemble_segments(chunk, base_audio)\n        audio_chunk = AudioChunk(\n            data=self._export_audio(audio_segment),\n            start_ms=chunk.start_time,\n            end_ms=chunk.end_time,\n            format=self.output_format,\n        )\n        chunk.audio = audio_chunk\n        return audio_chunk\n\n    def export_audio_bytes(self, audio_segment: AudioSegment, format_str: Optional[str] = None) -&gt; BytesIO:\n        \"\"\"Export AudioSegment to BytesIO for services/modules that require file-like objects.\"\"\"\n        return self._export_audio(audio_segment, format_str)\n\n    def _set_io_format(self, audio_file: Path):\n        formats = self.config.SUPPORTED_FORMATS\n        suffix = audio_file.suffix.lstrip(\".\").lower()\n        if not suffix or suffix not in formats:\n            raise ValueError(\n                f\"Unsupported or missing audio file format: '{audio_file.suffix}'. \"\n                f\"Supported formats are: {', '.join(sorted(formats))}\"\n            )\n        self.input_format = suffix\n\n        # Use input format if output format not specified\n        self.output_format = self.output_format or self.input_format\n\n    def _load_audio(self, audio_file: Path) -&gt; AudioSegment:\n        \"\"\"Load the audio file and validate format.\"\"\"\n        return AudioSegment.from_file(audio_file, format=self.input_format)\n\n    def _validate_segments(self, chunk: DiarizationChunk):\n        \"\"\"Ensure all segments have gap_before and spacing_time attributes set.\"\"\"\n        for i, segment in enumerate(chunk.segments):\n            if not hasattr(segment, \"gap_before\") or not hasattr(segment, \"spacing_time\"):\n                raise ValueError(\n                    f\"Segment at index {i} missing required gap annotations: \"\n                    f\"gap_before={getattr(segment, 'gap_before', None)}, \"\n                    f\"spacing_time={getattr(segment, 'spacing_time', None)}\"\n                )\n\n    def _assemble_segments(self, chunk: DiarizationChunk, base_audio: AudioSegment) -&gt; AudioSegment:\n        \"\"\"Assemble audio for the given diarization chunk using gap information.\"\"\"\n        assembled: AudioSegment = AudioSegment.empty()\n        offset = 0\n        prev_end: Optional[int] = None\n        audio_length = len(base_audio)\n\n        def _clamp(val, min_val, max_val):\n            return max(min_val, min(val, max_val))\n\n        def _add_silence(duration):\n            nonlocal assembled, offset\n            if duration &gt; 0:\n                assembled += AudioSegment.silent(duration=duration)\n                offset += duration\n\n        def _add_interval_audio(start, end):\n            nonlocal assembled, offset\n            start = _clamp(start, 0, audio_length)\n            end = _clamp(end, 0, audio_length)\n            if end &gt; start:\n                interval_audio = base_audio[start:end]\n                assembled += interval_audio\n                offset += len(interval_audio)\n\n        def _add_segment_audio(start, end):\n            nonlocal assembled, offset\n            start = _clamp(start, 0, audio_length)\n            end = _clamp(end, 0, audio_length)\n            if end &gt; start:\n                seg_audio: AudioSegment = base_audio[start:end]\n                assembled += seg_audio\n                offset += len(seg_audio)\n                return len(seg_audio)\n            return 0\n\n        for segment in chunk.segments:\n            seg_start = int(segment.start)\n            seg_end = int(segment.end)\n\n            # Handle gap before segment\n            if prev_end is not None:\n                if self.config.silence_all_intervals or getattr(segment, \"gap_before\", False):\n                    spacing_time = getattr(segment, \"spacing_time\", 0)\n                    _add_silence(spacing_time)\n                elif seg_start &gt; prev_end:\n                    _add_interval_audio(prev_end, seg_start)\n\n            # Append current segment audio (clamped)\n            segment.audio_map_start = offset\n            _add_segment_audio(seg_start, seg_end)\n\n            prev_end = seg_end\n\n        return assembled\n\n    # TODO: in _export_audio:\n    # handle needed parameters for various export formats (can use kwargs for options)    \n    def _export_audio(\n        self, \n        audio_segment: AudioSegment,  \n        format_str: Optional[str] = None\n        ) -&gt; BytesIO:\n        \"\"\"Export *audio segment* in the configured format and return raw bytes.\"\"\"\n\n        export_format = format_str or self.output_format\n        supported_formats = self.config.SUPPORTED_FORMATS\n\n        if not export_format:\n            raise ConfigurationError(\"Cannot export. Output format not specified.\")\n\n        if export_format not in supported_formats:\n            raise ValueError(\n                f\"Unsupported export format: '{export_format}'. \"\n                f\"Supported formats are: {', '.join(sorted(supported_formats))}\"\n            )\n\n        file_obj = BytesIO()\n        try:\n            audio_segment.export(file_obj, format=export_format)\n            file_obj.seek(0)\n        except Exception as e:\n            logger.error(f\"Failed to export audio segment: {e}\")\n            raise RuntimeError(f\"Audio export failed: {e}\") from e\n        return file_obj\n</code></pre> <code>base_audio</code> <code>instance-attribute</code> \u00b6 <code>config = config</code> <code>instance-attribute</code> \u00b6 <code>input_format = None</code> <code>instance-attribute</code> \u00b6 <code>output_format = config.output_format</code> <code>instance-attribute</code> \u00b6 <code>__init__(config=AudioHandlerConfig())</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/handler.py</code> <pre><code>def __init__(\n    self, \n    config: AudioHandlerConfig = AudioHandlerConfig()\n    ):\n    self.config = config\n    # Sensible fall\u2011backs for optional config values\n    self.base_audio: AudioSegment\n    self.output_format: Optional[str] = config.output_format\n    self.input_format: Optional[str] = None\n</code></pre> <code>build_audio_chunk(chunk, audio_file)</code> \u00b6 <p>builds and sets the internal chunk.audio to be the new AudioChunk</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/handler.py</code> <pre><code>def build_audio_chunk(self, chunk: DiarizationChunk, audio_file: Path) -&gt; AudioChunk:\n    \"\"\"builds and sets the internal chunk.audio to be the new AudioChunk\"\"\"\n\n    self._set_io_format(audio_file)\n    base_audio = self._load_audio(audio_file)\n    self._validate_segments(chunk)\n\n    audio_segment = self._assemble_segments(chunk, base_audio)\n    audio_chunk = AudioChunk(\n        data=self._export_audio(audio_segment),\n        start_ms=chunk.start_time,\n        end_ms=chunk.end_time,\n        format=self.output_format,\n    )\n    chunk.audio = audio_chunk\n    return audio_chunk\n</code></pre> <code>export_audio_bytes(audio_segment, format_str=None)</code> \u00b6 <p>Export AudioSegment to BytesIO for services/modules that require file-like objects.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/handler.py</code> <pre><code>def export_audio_bytes(self, audio_segment: AudioSegment, format_str: Optional[str] = None) -&gt; BytesIO:\n    \"\"\"Export AudioSegment to BytesIO for services/modules that require file-like objects.\"\"\"\n    return self._export_audio(audio_segment, format_str)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.audio.AudioHandlerConfig","title":"<code>AudioHandlerConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration settings for the AudioHandler. All audio time units are milliseconds (int)</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/config.py</code> <pre><code>class AudioHandlerConfig(BaseSettings):\n    \"\"\"\n    Configuration settings for the AudioHandler.\n    All audio time units are milliseconds (int)\n    \"\"\"\n\n    output_format: Optional[str] = Field(\n        default=None,\n        description=\n        \"Audio output format used when exporting segments (e.g., 'wav', 'mp3').\"\n    )\n    temp_storage_dir: Optional[Path] = Field(\n        default=None,\n        description=\n        \"Optional directory path for storing temporary audio files (currently unused).\"\n    )\n    max_segment_length: Optional[int] = Field(\n        default=None,\n        description=\"Maximum allowed segment length (in milliseconds).\"\n    )\n    silence_all_intervals: bool = Field(\n        default=False,\n        description=\"If True, replace every non-zero interval between consecutive diarization segments \" \n        \"with silence of length spacing_time.\"\n    )\n    SUPPORTED_FORMATS: frozenset = frozenset({\"mp3\", \"wav\", \"flac\", \"ogg\", \"m4a\", \"mp4\"})\n    class Config:\n        env_prefix = \"AUDIO_HANDLER_\"  # Optional: allow env vars like AUDIO_HANDLER_OUTPUT_FORMAT\n</code></pre> <code>SUPPORTED_FORMATS = frozenset({'mp3', 'wav', 'flac', 'ogg', 'm4a', 'mp4'})</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>max_segment_length = Field(default=None, description='Maximum allowed segment length (in milliseconds).')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>output_format = Field(default=None, description=\"Audio output format used when exporting segments (e.g., 'wav', 'mp3').\")</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>silence_all_intervals = Field(default=False, description='If True, replace every non-zero interval between consecutive diarization segments with silence of length spacing_time.')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>temp_storage_dir = Field(default=None, description='Optional directory path for storing temporary audio files (currently unused).')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>Config</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/config.py</code> <pre><code>class Config:\n    env_prefix = \"AUDIO_HANDLER_\"  # Optional: allow env vars like AUDIO_HANDLER_OUTPUT_FORMAT\n</code></pre> <code>env_prefix = 'AUDIO_HANDLER_'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.audio.config","title":"<code>config</code>","text":"<code>AudioHandlerConfig</code> \u00b6 <p>               Bases: <code>BaseSettings</code></p> <p>Configuration settings for the AudioHandler. All audio time units are milliseconds (int)</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/config.py</code> <pre><code>class AudioHandlerConfig(BaseSettings):\n    \"\"\"\n    Configuration settings for the AudioHandler.\n    All audio time units are milliseconds (int)\n    \"\"\"\n\n    output_format: Optional[str] = Field(\n        default=None,\n        description=\n        \"Audio output format used when exporting segments (e.g., 'wav', 'mp3').\"\n    )\n    temp_storage_dir: Optional[Path] = Field(\n        default=None,\n        description=\n        \"Optional directory path for storing temporary audio files (currently unused).\"\n    )\n    max_segment_length: Optional[int] = Field(\n        default=None,\n        description=\"Maximum allowed segment length (in milliseconds).\"\n    )\n    silence_all_intervals: bool = Field(\n        default=False,\n        description=\"If True, replace every non-zero interval between consecutive diarization segments \" \n        \"with silence of length spacing_time.\"\n    )\n    SUPPORTED_FORMATS: frozenset = frozenset({\"mp3\", \"wav\", \"flac\", \"ogg\", \"m4a\", \"mp4\"})\n    class Config:\n        env_prefix = \"AUDIO_HANDLER_\"  # Optional: allow env vars like AUDIO_HANDLER_OUTPUT_FORMAT\n</code></pre> <code>SUPPORTED_FORMATS = frozenset({'mp3', 'wav', 'flac', 'ogg', 'm4a', 'mp4'})</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>max_segment_length = Field(default=None, description='Maximum allowed segment length (in milliseconds).')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>output_format = Field(default=None, description=\"Audio output format used when exporting segments (e.g., 'wav', 'mp3').\")</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>silence_all_intervals = Field(default=False, description='If True, replace every non-zero interval between consecutive diarization segments with silence of length spacing_time.')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>temp_storage_dir = Field(default=None, description='Optional directory path for storing temporary audio files (currently unused).')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>Config</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/config.py</code> <pre><code>class Config:\n    env_prefix = \"AUDIO_HANDLER_\"  # Optional: allow env vars like AUDIO_HANDLER_OUTPUT_FORMAT\n</code></pre> <code>env_prefix = 'AUDIO_HANDLER_'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.audio.handler","title":"<code>handler</code>","text":"<p>Audio handler utilities for slicing and assembling audio around diarization chunks.  Designed for pipeline-friendly, single-responsibility methods so that higher-level services can remain agnostic of the underlying audio library.</p> <p>This implementation purposely keeps logic minimal for testing.</p> <code>logger = get_child_logger(__name__)</code> <code>module-attribute</code> \u00b6 <code>AudioHandler</code> \u00b6 <p>Isolates audio operations and external dependencies (pydub, ffmpeg).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/handler.py</code> <pre><code>class AudioHandler:\n    \"\"\"Isolates audio operations and external dependencies (pydub, ffmpeg).\"\"\"\n\n    def __init__(\n        self, \n        config: AudioHandlerConfig = AudioHandlerConfig()\n        ):\n        self.config = config\n        # Sensible fall\u2011backs for optional config values\n        self.base_audio: AudioSegment\n        self.output_format: Optional[str] = config.output_format\n        self.input_format: Optional[str] = None\n\n    def build_audio_chunk(self, chunk: DiarizationChunk, audio_file: Path) -&gt; AudioChunk:\n        \"\"\"builds and sets the internal chunk.audio to be the new AudioChunk\"\"\"\n\n        self._set_io_format(audio_file)\n        base_audio = self._load_audio(audio_file)\n        self._validate_segments(chunk)\n\n        audio_segment = self._assemble_segments(chunk, base_audio)\n        audio_chunk = AudioChunk(\n            data=self._export_audio(audio_segment),\n            start_ms=chunk.start_time,\n            end_ms=chunk.end_time,\n            format=self.output_format,\n        )\n        chunk.audio = audio_chunk\n        return audio_chunk\n\n    def export_audio_bytes(self, audio_segment: AudioSegment, format_str: Optional[str] = None) -&gt; BytesIO:\n        \"\"\"Export AudioSegment to BytesIO for services/modules that require file-like objects.\"\"\"\n        return self._export_audio(audio_segment, format_str)\n\n    def _set_io_format(self, audio_file: Path):\n        formats = self.config.SUPPORTED_FORMATS\n        suffix = audio_file.suffix.lstrip(\".\").lower()\n        if not suffix or suffix not in formats:\n            raise ValueError(\n                f\"Unsupported or missing audio file format: '{audio_file.suffix}'. \"\n                f\"Supported formats are: {', '.join(sorted(formats))}\"\n            )\n        self.input_format = suffix\n\n        # Use input format if output format not specified\n        self.output_format = self.output_format or self.input_format\n\n    def _load_audio(self, audio_file: Path) -&gt; AudioSegment:\n        \"\"\"Load the audio file and validate format.\"\"\"\n        return AudioSegment.from_file(audio_file, format=self.input_format)\n\n    def _validate_segments(self, chunk: DiarizationChunk):\n        \"\"\"Ensure all segments have gap_before and spacing_time attributes set.\"\"\"\n        for i, segment in enumerate(chunk.segments):\n            if not hasattr(segment, \"gap_before\") or not hasattr(segment, \"spacing_time\"):\n                raise ValueError(\n                    f\"Segment at index {i} missing required gap annotations: \"\n                    f\"gap_before={getattr(segment, 'gap_before', None)}, \"\n                    f\"spacing_time={getattr(segment, 'spacing_time', None)}\"\n                )\n\n    def _assemble_segments(self, chunk: DiarizationChunk, base_audio: AudioSegment) -&gt; AudioSegment:\n        \"\"\"Assemble audio for the given diarization chunk using gap information.\"\"\"\n        assembled: AudioSegment = AudioSegment.empty()\n        offset = 0\n        prev_end: Optional[int] = None\n        audio_length = len(base_audio)\n\n        def _clamp(val, min_val, max_val):\n            return max(min_val, min(val, max_val))\n\n        def _add_silence(duration):\n            nonlocal assembled, offset\n            if duration &gt; 0:\n                assembled += AudioSegment.silent(duration=duration)\n                offset += duration\n\n        def _add_interval_audio(start, end):\n            nonlocal assembled, offset\n            start = _clamp(start, 0, audio_length)\n            end = _clamp(end, 0, audio_length)\n            if end &gt; start:\n                interval_audio = base_audio[start:end]\n                assembled += interval_audio\n                offset += len(interval_audio)\n\n        def _add_segment_audio(start, end):\n            nonlocal assembled, offset\n            start = _clamp(start, 0, audio_length)\n            end = _clamp(end, 0, audio_length)\n            if end &gt; start:\n                seg_audio: AudioSegment = base_audio[start:end]\n                assembled += seg_audio\n                offset += len(seg_audio)\n                return len(seg_audio)\n            return 0\n\n        for segment in chunk.segments:\n            seg_start = int(segment.start)\n            seg_end = int(segment.end)\n\n            # Handle gap before segment\n            if prev_end is not None:\n                if self.config.silence_all_intervals or getattr(segment, \"gap_before\", False):\n                    spacing_time = getattr(segment, \"spacing_time\", 0)\n                    _add_silence(spacing_time)\n                elif seg_start &gt; prev_end:\n                    _add_interval_audio(prev_end, seg_start)\n\n            # Append current segment audio (clamped)\n            segment.audio_map_start = offset\n            _add_segment_audio(seg_start, seg_end)\n\n            prev_end = seg_end\n\n        return assembled\n\n    # TODO: in _export_audio:\n    # handle needed parameters for various export formats (can use kwargs for options)    \n    def _export_audio(\n        self, \n        audio_segment: AudioSegment,  \n        format_str: Optional[str] = None\n        ) -&gt; BytesIO:\n        \"\"\"Export *audio segment* in the configured format and return raw bytes.\"\"\"\n\n        export_format = format_str or self.output_format\n        supported_formats = self.config.SUPPORTED_FORMATS\n\n        if not export_format:\n            raise ConfigurationError(\"Cannot export. Output format not specified.\")\n\n        if export_format not in supported_formats:\n            raise ValueError(\n                f\"Unsupported export format: '{export_format}'. \"\n                f\"Supported formats are: {', '.join(sorted(supported_formats))}\"\n            )\n\n        file_obj = BytesIO()\n        try:\n            audio_segment.export(file_obj, format=export_format)\n            file_obj.seek(0)\n        except Exception as e:\n            logger.error(f\"Failed to export audio segment: {e}\")\n            raise RuntimeError(f\"Audio export failed: {e}\") from e\n        return file_obj\n</code></pre> <code>base_audio</code> <code>instance-attribute</code> \u00b6 <code>config = config</code> <code>instance-attribute</code> \u00b6 <code>input_format = None</code> <code>instance-attribute</code> \u00b6 <code>output_format = config.output_format</code> <code>instance-attribute</code> \u00b6 <code>__init__(config=AudioHandlerConfig())</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/handler.py</code> <pre><code>def __init__(\n    self, \n    config: AudioHandlerConfig = AudioHandlerConfig()\n    ):\n    self.config = config\n    # Sensible fall\u2011backs for optional config values\n    self.base_audio: AudioSegment\n    self.output_format: Optional[str] = config.output_format\n    self.input_format: Optional[str] = None\n</code></pre> <code>build_audio_chunk(chunk, audio_file)</code> \u00b6 <p>builds and sets the internal chunk.audio to be the new AudioChunk</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/handler.py</code> <pre><code>def build_audio_chunk(self, chunk: DiarizationChunk, audio_file: Path) -&gt; AudioChunk:\n    \"\"\"builds and sets the internal chunk.audio to be the new AudioChunk\"\"\"\n\n    self._set_io_format(audio_file)\n    base_audio = self._load_audio(audio_file)\n    self._validate_segments(chunk)\n\n    audio_segment = self._assemble_segments(chunk, base_audio)\n    audio_chunk = AudioChunk(\n        data=self._export_audio(audio_segment),\n        start_ms=chunk.start_time,\n        end_ms=chunk.end_time,\n        format=self.output_format,\n    )\n    chunk.audio = audio_chunk\n    return audio_chunk\n</code></pre> <code>export_audio_bytes(audio_segment, format_str=None)</code> \u00b6 <p>Export AudioSegment to BytesIO for services/modules that require file-like objects.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/audio/handler.py</code> <pre><code>def export_audio_bytes(self, audio_segment: AudioSegment, format_str: Optional[str] = None) -&gt; BytesIO:\n    \"\"\"Export AudioSegment to BytesIO for services/modules that require file-like objects.\"\"\"\n    return self._export_audio(audio_segment, format_str)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.chunker","title":"<code>chunker</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.chunker.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.chunker.DiarizationChunker","title":"<code>DiarizationChunker</code>","text":"<p>Class for chunking diarization results into processing units based on configurable duration targets.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/chunker.py</code> <pre><code>class DiarizationChunker:\n    \"\"\"\n    Class for chunking diarization results into processing units\n    based on configurable duration targets.\n    \"\"\"\n\n    def __init__(self, **config_options):\n        \"\"\"Initialize chunker with additional config_options.\"\"\"\n        self.config = ChunkConfig()\n\n        self._handle_config_options(config_options)\n\n\n    def extract_contiguous_chunks(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n        \"\"\"\n        Split diarization segments into contiguous chunks of\n        approximately target_duration, without splitting on speaker changes.\n\n        Args:\n            segments: List of speaker segments from diarization\n\n        Returns:\n            List[Chunk]: Flat list of contiguous chunks\n        \"\"\"\n        if not segments:\n            return []\n\n        extractor = self._ChunkExtractor(self.config, split_on_speaker_change=False)\n        return extractor.extract(segments)\n\n    class _ChunkExtractor:\n        def __init__(self, config: ChunkConfig, split_on_speaker_change: bool = True):\n            self.config = config\n            self.split_on_speaker_change = split_on_speaker_change\n            self.gap_threshold = self.config.gap_threshold\n            self.spacing = self.config.gap_spacing_time\n            self.chunks: List[DiarizationChunk] = []\n            self.current_chunk_segments: List[DiarizedSegment] = []\n            self.chunk_start: int = 0\n            self.current_speaker = \"\"\n            self.accumulated_time: int = 0\n\n        @property\n        def last_segment(self):\n            return self.current_chunk_segments[-1] if self.current_chunk_segments else None\n\n        def extract(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n            if not segments:\n                return []\n\n            self.chunk_start = int(segments[0].start)\n            self.current_speaker = segments[0].speaker\n            for segment in segments:\n                self._check_segment_duration(segment)  \n                self._process_segment(segment)\n\n            self._finalize_last_chunk()\n            return self.chunks\n\n        def _process_segment(self, segment: DiarizedSegment):\n            if self._should_split(segment):\n                self._finalize_current_chunk(segment)\n                self.chunk_start = int(segment.start)                \n            self._add_segment(segment)\n\n        def _add_segment(self, segment: DiarizedSegment):\n            gap_time =  self._gap_time(segment)\n            if gap_time &gt; self.gap_threshold:\n                segment.gap_before = True\n                segment.spacing_time = self.spacing\n                self.accumulated_time += int(segment.duration) + self.spacing\n            else:\n                segment.gap_before = False\n                segment.spacing_time = max(gap_time, 0)\n                self.accumulated_time += int(segment.duration) + gap_time\n            self.current_chunk_segments.append(segment)\n            self.current_speaker = segment.speaker\n\n        def _gap_time(self, segment) -&gt; int:\n            if self.last_segment is None:\n                # If no last_segment, this is first segment, so no gap.\n                return 0 \n            else:\n                return segment.start - self.last_segment.end\n\n\n        def _should_split(self, segment: DiarizedSegment) -&gt; bool:\n            gap_time = self._gap_time(segment)\n            interval_time = gap_time if gap_time &lt; self.gap_threshold else self.spacing\n            accumulated_time = self.accumulated_time + interval_time + segment.duration\n            return accumulated_time &gt;= self.config.target_duration \n\n        def _finalize_current_chunk(self, next_segment: Optional[DiarizedSegment]):\n            if self.current_chunk_segments:\n                assert self.last_segment is not None\n                self.chunks.append(\n                    DiarizationChunk(\n                        start_time=int(self.chunk_start),\n                        end_time=int(self.last_segment.end), \n                        segments=self.current_chunk_segments.copy(),\n                        audio=None,\n                        accumulated_time=self.accumulated_time\n                    )\n                )\n                self._reset_chunk_state(next_segment)             \n\n        def _reset_chunk_state(self, next_segment):\n            self.current_chunk_segments = []\n            self.accumulated_time = 0\n            if self.split_on_speaker_change and next_segment:\n                    self.current_speaker = next_segment.speaker\n\n        def _finalize_last_chunk(self):\n            if self.current_chunk_segments:\n                self._handle_final_segments()\n\n        def _check_segment_duration(self, segment: DiarizedSegment) -&gt; None:\n            \"\"\"Check if segment exceeds target duration and issue warning if needed.\"\"\"\n            if segment.duration &gt; self.config.target_duration:\n                logger.warning(f\"Found segment longer than \"\n                            f\"target duration: {segment.duration_sec:.0f}s\")\n\n        def _handle_final_segments(self) -&gt; None:\n            \"\"\"Append final segments to last chunk if below min duration.\"\"\"\n            approx_remaining_time = sum(segment.duration for segment in self.current_chunk_segments)\n            final_time = self.accumulated_time + approx_remaining_time\n            min_time = self.config.min_duration\n\n            if final_time &lt; min_time and self.chunks:\n               self._merge_to_last_chunk()\n            else:\n                # Create standalone chunk\n                self._finalize_current_chunk(next_segment=None)\n\n        def _merge_to_last_chunk(self):\n            \"\"\"Merge segments to the last chunk processed. self.chunks cannot be empty.\"\"\"\n            assert self.chunks\n            self.chunks[-1].segments.extend(self.current_chunk_segments)\n            self.chunks[-1].end_time = int(self.current_chunk_segments[-1].end)\n            self.chunks[-1].accumulated_time += self.accumulated_time\n\n\n\n    def _handle_config_options(self, config_options: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles additional configuration options, \n        logging a warning for unrecognized keys.\n        \"\"\"\n        for key, value in config_options.items():\n            if hasattr(self.config, key):\n                setattr(self.config, key, value)\n            else:\n                logger.warning(f\"Unrecognized configuration option: {key}\")\n</code></pre> <code>config = ChunkConfig()</code> <code>instance-attribute</code> \u00b6 <code>__init__(**config_options)</code> \u00b6 <p>Initialize chunker with additional config_options.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/chunker.py</code> <pre><code>def __init__(self, **config_options):\n    \"\"\"Initialize chunker with additional config_options.\"\"\"\n    self.config = ChunkConfig()\n\n    self._handle_config_options(config_options)\n</code></pre> <code>extract_contiguous_chunks(segments)</code> \u00b6 <p>Split diarization segments into contiguous chunks of approximately target_duration, without splitting on speaker changes.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>List[DiarizedSegment]</code> <p>List of speaker segments from diarization</p> required <p>Returns:</p> Type Description <code>List[DiarizationChunk]</code> <p>List[Chunk]: Flat list of contiguous chunks</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/chunker.py</code> <pre><code>def extract_contiguous_chunks(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n    \"\"\"\n    Split diarization segments into contiguous chunks of\n    approximately target_duration, without splitting on speaker changes.\n\n    Args:\n        segments: List of speaker segments from diarization\n\n    Returns:\n        List[Chunk]: Flat list of contiguous chunks\n    \"\"\"\n    if not segments:\n        return []\n\n    extractor = self._ChunkExtractor(self.config, split_on_speaker_change=False)\n    return extractor.extract(segments)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.config","title":"<code>config</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.config.ChunkConfig","title":"<code>ChunkConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for chunking</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/config.py</code> <pre><code>class ChunkConfig(BaseSettings):\n    \"\"\"Configuration for chunking\"\"\"\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive = False,\n        env_prefix = \"CHUNK_\",\n        extra=\"ignore\",\n    )\n\n    # Target duration for each chunk in milliseconds (default: 5 minutes = 300,000ms)\n    target_duration: int = 300_000\n\n    # Minimum duration for final chunk (in ms); shorter chunks are merged\n    min_duration: int = 30_000 # 30 seconds\n\n    # Maximum allowed gap between segments for audio processing\n    gap_threshold: int = 4000\n\n    # Spacing used between segments that are greater than gap threshold ms apart\n    gap_spacing_time: int = 1000 \n</code></pre> <code>gap_spacing_time = 1000</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>gap_threshold = 4000</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>min_duration = 30000</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=False, env_prefix='CHUNK_', extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>target_duration = 300000</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.config.DiarizationConfig","title":"<code>DiarizationConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/config.py</code> <pre><code>class DiarizationConfig(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive = False,\n        env_prefix = \"DIARIZATION_\",\n        extra=\"ignore\",\n    )\n    speaker: SpeakerConfig = SpeakerConfig()\n    chunk: ChunkConfig = ChunkConfig()\n    language: LanguageConfig = LanguageConfig()\n    mapping: MappingPolicy = MappingPolicy()\n</code></pre> <code>chunk = ChunkConfig()</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>language = LanguageConfig()</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>mapping = MappingPolicy()</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=False, env_prefix='DIARIZATION_', extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>speaker = SpeakerConfig()</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.config.LanguageConfig","title":"<code>LanguageConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/config.py</code> <pre><code>class LanguageConfig(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive = False,\n        env_prefix = \"LANGUAGE_\",\n        extra=\"ignore\",\n    )\n    # Duration for language probe sampling in milliseconds (default: 2 seconds)\n    probe_time: int = 10_000\n\n    # The file format used for language probe file-like objects\n    export_format: str = \"wav\"\n\n    # Default language\n    default_language: str = \"en\"\n</code></pre> <code>default_language = 'en'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>export_format = 'wav'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=False, env_prefix='LANGUAGE_', extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>probe_time = 10000</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.config.MappingPolicy","title":"<code>MappingPolicy</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Mapping policy for transport\u2192domain shaping.</p> <p>TODO (future parameters to consider): - min_segment_ms: int                # drop micro-segments below threshold - merge_gap_ms: int                  # merge adjacent same-speaker if gap \u2264 this - round_ms_to: int                   # quantize boundaries (e.g., 10ms) - confidence_floor: float | None     # filter out low-confidence segments - suppress_unlabeled: bool           # drop segments missing speaker id - attach_raw_payload: bool           # persist raw API payload in metadata - version: int                       # policy versioning for reproducibility</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/config.py</code> <pre><code>class MappingPolicy(BaseSettings):\n    \"\"\"Mapping policy for transport\u2192domain shaping.\n\n    TODO (future parameters to consider):\n    - min_segment_ms: int                # drop micro-segments below threshold\n    - merge_gap_ms: int                  # merge adjacent same-speaker if gap \u2264 this\n    - round_ms_to: int                   # quantize boundaries (e.g., 10ms)\n    - confidence_floor: float | None     # filter out low-confidence segments\n    - suppress_unlabeled: bool           # drop segments missing speaker id\n    - attach_raw_payload: bool           # persist raw API payload in metadata\n    - version: int                       # policy versioning for reproducibility\n    \"\"\"\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive = False,\n        env_prefix = \"MAPPING_\",\n        extra=\"ignore\",\n    )\n\n    # Current, minimal policy (kept in sync with existing flags in use)\n    default_speaker_label: str = \"SPEAKER_00\"\n    single_speaker: bool = False\n</code></pre> <code>default_speaker_label = 'SPEAKER_00'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=False, env_prefix='MAPPING_', extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>single_speaker = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.config.PollingConfig","title":"<code>PollingConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration constants for a generic polling class used to for Pyannote API polling.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/config.py</code> <pre><code>class PollingConfig(BaseSettings):\n    \"\"\"Configuration constants for a generic polling class used to for Pyannote API polling.\"\"\"\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive = False,\n        env_prefix = \"PYANNOTE_POLL_\",\n        extra=\"ignore\",\n    )\n\n    polling_interval: int = 15\n    polling_timeout: float | None = 300.0  # seconds. set to None for time unlimited\n    initial_poll_time: int = 7\n    exp_base: int = 2\n    max_interval: int = 30\n</code></pre> <code>exp_base = 2</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>initial_poll_time = 7</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>max_interval = 30</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=False, env_prefix='PYANNOTE_POLL_', extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>polling_interval = 15</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>polling_timeout = 300.0</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.config.PyannoteConfig","title":"<code>PyannoteConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration constants for Pyannote API.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/config.py</code> <pre><code>class PyannoteConfig(BaseSettings):\n    \"\"\"Configuration constants for Pyannote API.\"\"\"\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive = False,\n        env_prefix = \"PYANNOTE_\",\n        extra=\"ignore\",\n    )\n\n    # API Endpoints\n    base_url: str = \"https://api.pyannote.ai/v1\"\n\n    @property\n    def media_input_endpoint(self) -&gt; str:\n        return f\"{self.base_url}/media/input\"\n\n    @property\n    def diarize_endpoint(self) -&gt; str:\n        return f\"{self.base_url}/diarize\"\n\n    @property\n    def job_status_endpoint(self) -&gt; str:\n        return f\"{self.base_url}/jobs\"\n\n    # Media\n    media_prefix: str = \"media://diarization-\"\n    media_content_type: str = \"audio/mpeg\"\n\n    # Upload-specific settings\n    upload_timeout: int = 300  # 5 minutes for large files\n    upload_max_retries: int = 3\n\n    # Network specific settings\n    network_timeout: int = 3 # seconds\n\n    # Polling\n    polling_config: PollingConfig = PollingConfig()\n</code></pre> <code>base_url = 'https://api.pyannote.ai/v1'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>diarize_endpoint</code> <code>property</code> \u00b6 <code>job_status_endpoint</code> <code>property</code> \u00b6 <code>media_content_type = 'audio/mpeg'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>media_input_endpoint</code> <code>property</code> \u00b6 <code>media_prefix = 'media://diarization-'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=False, env_prefix='PYANNOTE_', extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>network_timeout = 3</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>polling_config = PollingConfig()</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>upload_max_retries = 3</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>upload_timeout = 300</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.config.SpeakerConfig","title":"<code>SpeakerConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration settings for speaker block generation.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/config.py</code> <pre><code>class SpeakerConfig(BaseSettings):\n    \"\"\"Configuration settings for speaker block generation.\"\"\"\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive = False,\n        env_prefix = \"SPEAKER_\",\n        extra=\"ignore\",\n    )\n\n    # Set the default gap allowed between segments that will allow grouping of\n    # consecutive same-speaker segments\n    same_speaker_gap_threshold: TimeMs = TimeMs.from_seconds(2)\n\n    default_speaker_label: str = \"SPEAKER_00\"\n\n    # If set to true, all speakers are set to default speaker label\n    single_speaker: bool = False\n</code></pre> <code>default_speaker_label = 'SPEAKER_00'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=False, env_prefix='SPEAKER_', extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>same_speaker_gap_threshold = TimeMs.from_seconds(2)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>single_speaker = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.models","title":"<code>models</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.models.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.models.AudioChunk","title":"<code>AudioChunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>class AudioChunk(BaseModel):\n    data: BytesIO\n    start_ms: int\n    end_ms: int\n    sample_rate: Optional[int] = None\n    channels: Optional[int] = None\n    format: Optional[str] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre> <code>channels = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>data</code> <code>instance-attribute</code> \u00b6 <code>end_ms</code> <code>instance-attribute</code> \u00b6 <code>format = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>sample_rate = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>start_ms</code> <code>instance-attribute</code> \u00b6 <code>Config</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>class Config:\n    arbitrary_types_allowed = True\n</code></pre> <code>arbitrary_types_allowed = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.models.AugDiarizedSegment","title":"<code>AugDiarizedSegment</code>","text":"<p>               Bases: <code>DiarizedSegment</code></p> <p>DiarizedSegment with additional chunking/processing metadata.</p> <p>This class extends <code>DiarizationSegment</code> and adds fields that are only set during chunk accumulation or downstream processing.</p> <p>Attributes:</p> Name Type Description <code>gap_before</code> <code>bool</code> <p>Indicates if there is a gap greater than the configured threshold before this segment. Set only during chunk accumulation.</p> <code>spacing_time</code> <code>TimeMs</code> <p>The spacing (in ms) between this and the previous segment, possibly adjusted if there is a gap before. Set only during chunk accumulation.</p> <code>audio</code> <code>TNHAudioSegment</code> <p>The audio data for this segment, sliced from the original audio.</p> Notes <ul> <li>The <code>audio</code> field is a slice of the original audio corresponding to this segment.</li> <li>All time values (start, end, duration) are relative to the original audio.</li> <li>When slicing or probing the <code>audio</code> field, use times relative to 0 (i.e., 0 to duration).</li> <li>For language probing or any operation on <code>audio</code>,    always use 0 as the start and <code>duration</code> as the end.</li> </ul> Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>class AugDiarizedSegment(DiarizedSegment):\n    \"\"\"\n    DiarizedSegment with additional chunking/processing metadata.\n\n    This class extends `DiarizationSegment` and adds fields that are only set during\n    chunk accumulation or downstream processing.\n\n    Attributes:\n        gap_before (bool): Indicates if there is a gap greater than the configured threshold\n            before this segment. Set only during chunk accumulation.\n        spacing_time (TimeMs): The spacing (in ms) between this and the previous segment,\n            possibly adjusted if there is a gap before. Set only during chunk accumulation.\n        audio (AudioSegment): The audio data for this segment, sliced from the original audio.\n\n    Notes:\n        - The `audio` field is a slice of the original audio corresponding to this segment.\n        - All time values (start, end, duration) are relative to the original audio.\n        - When slicing or probing the `audio` field, use times relative to 0 (i.e., 0 to duration).\n        - For language probing or any operation on `audio`, \n          always use 0 as the start and `duration` as the end.\n    \"\"\"\n\n    @property\n    def relative_start(self) -&gt; TimeMs:\n        \"\"\"Start time relative to the segment audio (always 0).\"\"\"\n        return TimeMs(0)\n\n    @property\n    def relative_end(self) -&gt; TimeMs:\n        \"\"\"End time relative to the segment audio (duration of segment).\"\"\"\n        return self.duration\n\n    gap_before_new: bool  # rename when ready to move over to using this class \n    spacing_time_new: TimeMs  # rename when ready to move over to using this class \n    audio: Optional[AudioSegment]\n\n    @classmethod\n    def from_segment(\n        cls,\n        segment: DiarizedSegment,\n        gap_before: Optional[bool] = None,\n        spacing_time_new: Optional[TimeMs] = None,\n        audio: Optional[AudioSegment] = None,\n        **kwargs\n    ) -&gt; \"AugDiarizedSegment\":\n        \"\"\"\n        Create an AugDiarizedSegment from a DiarizedSegment, with optional new fields.\n        Args:\n            segment (DiarizedSegment): The base segment to copy fields from.\n            gap_before_new (bool, optional): Value for gap_before_new. Defaults to False.\n            spacing_time_new (TimeMs, optional): Value for spacing_time_new. Defaults to None.\n            audio (AudioSegment, optional): Audio data for this segment. Defaults to None.\n            **kwargs: Any additional fields to override.\n        Returns:\n            AugDiarizedSegment: The new augmented segment.\n        \"\"\"\n        return cls(\n            speaker=segment.speaker,\n            start=segment.start,\n            end=segment.end,\n            audio_map_start=segment.audio_map_start,\n            gap_before=segment.gap_before,\n            spacing_time=segment.spacing_time,\n            gap_before_new=segment.gap_before if segment.gap_before is not None else False,\n            spacing_time_new=spacing_time_new if spacing_time_new is not None else TimeMs(0),\n            audio=audio,\n            **kwargs\n        )\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre> <code>audio</code> <code>instance-attribute</code> \u00b6 <code>gap_before_new</code> <code>instance-attribute</code> \u00b6 <code>relative_end</code> <code>property</code> \u00b6 <p>End time relative to the segment audio (duration of segment).</p> <code>relative_start</code> <code>property</code> \u00b6 <p>Start time relative to the segment audio (always 0).</p> <code>spacing_time_new</code> <code>instance-attribute</code> \u00b6 <code>Config</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>class Config:\n    arbitrary_types_allowed = True\n</code></pre> <code>arbitrary_types_allowed = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>from_segment(segment, gap_before=None, spacing_time_new=None, audio=None, **kwargs)</code> <code>classmethod</code> \u00b6 <p>Create an AugDiarizedSegment from a DiarizedSegment, with optional new fields. Args:     segment (DiarizedSegment): The base segment to copy fields from.     gap_before_new (bool, optional): Value for gap_before_new. Defaults to False.     spacing_time_new (TimeMs, optional): Value for spacing_time_new. Defaults to None.     audio (AudioSegment, optional): Audio data for this segment. Defaults to None.     **kwargs: Any additional fields to override. Returns:     AugDiarizedSegment: The new augmented segment.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>@classmethod\ndef from_segment(\n    cls,\n    segment: DiarizedSegment,\n    gap_before: Optional[bool] = None,\n    spacing_time_new: Optional[TimeMs] = None,\n    audio: Optional[AudioSegment] = None,\n    **kwargs\n) -&gt; \"AugDiarizedSegment\":\n    \"\"\"\n    Create an AugDiarizedSegment from a DiarizedSegment, with optional new fields.\n    Args:\n        segment (DiarizedSegment): The base segment to copy fields from.\n        gap_before_new (bool, optional): Value for gap_before_new. Defaults to False.\n        spacing_time_new (TimeMs, optional): Value for spacing_time_new. Defaults to None.\n        audio (AudioSegment, optional): Audio data for this segment. Defaults to None.\n        **kwargs: Any additional fields to override.\n    Returns:\n        AugDiarizedSegment: The new augmented segment.\n    \"\"\"\n    return cls(\n        speaker=segment.speaker,\n        start=segment.start,\n        end=segment.end,\n        audio_map_start=segment.audio_map_start,\n        gap_before=segment.gap_before,\n        spacing_time=segment.spacing_time,\n        gap_before_new=segment.gap_before if segment.gap_before is not None else False,\n        spacing_time_new=spacing_time_new if spacing_time_new is not None else TimeMs(0),\n        audio=audio,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.models.DiarizationChunk","title":"<code>DiarizationChunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a chunk of segments to be processed together.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>class DiarizationChunk(BaseModel):\n    \"\"\"Represents a chunk of segments to be processed together.\"\"\"\n    start_time: int  # Start time in milliseconds\n    end_time: int    # End time in milliseconds\n    audio: Optional[AudioChunk] = None\n    segments: List[DiarizedSegment]\n    accumulated_time: int = 0\n    class Config:\n        arbitrary_types_allowed = True\n\n    @property\n    def total_duration(self) -&gt; int:\n        \"\"\"Get chunk duration in milliseconds.\"\"\"\n        return self.end_time - self.start_time\n\n    @property\n    def total_duration_sec(self) -&gt; float:\n        return convert_ms_to_sec(self.total_duration)\n\n    @property\n    def total_duration_time(self) -&gt; \"TimeMs\":\n        return TimeMs(self.total_duration)\n</code></pre> <code>accumulated_time = 0</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>audio = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>end_time</code> <code>instance-attribute</code> \u00b6 <code>segments</code> <code>instance-attribute</code> \u00b6 <code>start_time</code> <code>instance-attribute</code> \u00b6 <code>total_duration</code> <code>property</code> \u00b6 <p>Get chunk duration in milliseconds.</p> <code>total_duration_sec</code> <code>property</code> \u00b6 <code>total_duration_time</code> <code>property</code> \u00b6 <code>Config</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>class Config:\n    arbitrary_types_allowed = True\n</code></pre> <code>arbitrary_types_allowed = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.models.DiarizedSegment","title":"<code>DiarizedSegment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a diarized audio segment for a single speaker.</p> <p>Attributes:</p> Name Type Description <code>speaker</code> <code>str</code> <p>The speaker label for this segment.</p> <code>start</code> <code>TimeMs</code> <p>Start time in milliseconds.</p> <code>end</code> <code>TimeMs</code> <p>End time in milliseconds.</p> <code>audio_map_start</code> <code>Optional[int]</code> <p>Location in the audio output file, if mapped.</p> <code>gap_before</code> <code>Optional[bool]</code> <p>Indicates if there is a gap greater than the configured threshold before this segment. This attribute is set exclusively by <code>ChunkAccumulator.add_segment()</code> and should be None until that point.</p> <code>spacing_time</code> <code>Optional[int]</code> <p>The spacing (in ms) between this and the previous segment, possibly adjusted if there is a gap before. This attribute is also set exclusively by <code>ChunkAccumulator.add_segment()</code> and should be None until that point.</p> Notes <ul> <li><code>gap_before</code> and <code>spacing_time</code> are not set during initial diarization, but are assigned   only when the segment is accumulated into a chunk for downstream audio handling.</li> <li>These fields should be considered write-once and must not be mutated elsewhere.</li> </ul> Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>class DiarizedSegment(BaseModel):\n    \"\"\"\n    Represents a diarized audio segment for a single speaker.\n\n    Attributes:\n        speaker (str): The speaker label for this segment.\n        start (TimeMs): Start time in milliseconds.\n        end (TimeMs): End time in milliseconds.\n        audio_map_start (Optional[int]): Location in the audio output file, if mapped.\n        gap_before (Optional[bool]): Indicates if there is a gap greater than the configured threshold\n            before this segment. This attribute is set exclusively by `ChunkAccumulator.add_segment()`\n            and should be None until that point.\n        spacing_time (Optional[int]): The spacing (in ms) between this and the previous segment,\n            possibly adjusted if there is a gap before. This attribute is also set exclusively by\n            `ChunkAccumulator.add_segment()` and should be None until that point.\n\n    Notes:\n        - `gap_before` and `spacing_time` are not set during initial diarization, but are assigned\n          only when the segment is accumulated into a chunk for downstream audio handling.\n        - These fields should be considered write-once and must not be mutated elsewhere.\n    \"\"\"\n    speaker: str\n    start: TimeMs  # Start time in milliseconds\n    end: TimeMs    # End time in milliseconds\n    audio_map_start: Optional[int] # location in the audio output file\n    gap_before: Optional[bool] # indicates a gap &gt; gap_threshold before this segment\n    spacing_time: Optional[int] # spacing between this and previous segment; adjusted spacing if gap before\n\n    @property\n    def duration(self) -&gt; \"TimeMs\":\n        \"\"\"Get segment duration in milliseconds.\"\"\"\n        return TimeMs(self.end - self.start)\n\n    @property\n    def duration_sec(self) -&gt; float:\n        return self.duration.to_seconds()\n\n    # ------------------------------------------------------------------- #\n    # IMPLEMENTATION NOTE\n    # Convenience wrappers returning the new Time abstraction so can\n    # start migrating call\u2011sites incrementally without touching the int\u2011ms\n    # fields just yet.\n    # ------------------------------------------------------------------- #\n    @property\n    def start_time(self) -&gt; \"TimeMs\":\n        return self.start\n\n    @property\n    def end_time(self) -&gt; \"TimeMs\":\n        return self.end\n\n    @property\n    def mapped_start(self):\n        \"\"\"Downstream registry field set by the audio handler\"\"\"\n        return self.start if self.audio_map_start is None else self.audio_map_start\n\n    @property\n    def mapped_end(self):\n        if self.audio_map_start is None:\n            return self.end \n        else:\n            return self.audio_map_start + int(self.duration) \n\n    def normalize(self) -&gt; None:\n        \"\"\"Normalize the duration of the segment to be nonzero and validate start/end values.\"\"\"\n        # Validate that start and end are non-negative integers\n        if not isinstance(self.start, int) or not isinstance(self.end, int):\n            raise ValueError(\"Segment start and end must be integers, \"\n                             f\"got start={self.start}, end={self.end}\")\n        if self.start &lt; 0 or self.end &lt; 0:\n            raise ValueError(f\"Segment start and end must be non-negative, \"\n                             f\"got start={self.start}, end={self.end}\")\n\n        # Explicitly handle negative durations\n        if self.end &lt; self.start:\n            logger.warning(\n                f\"Invalid segment duration detected: start ({self.start}) &gt; end ({self.end}). \"\n                \"Adjusting end to ensure minimum duration of 1.\"\n            )\n            self.end = TimeMs(self.start + 1)  # set minimum nonzero duration\n\n        # Ensure minimum nonzero duration\n        if self.start == self.end:\n            logger.warning(\n                f\"Zero segment duration detected: start ({self.start}) == end ({self.end}). \"\n                \"Adjusting end to ensure minimum duration of 1.\"\n            )\n            self.end = TimeMs(self.start + 1)  # set minimum nonzero duration\n</code></pre> <code>audio_map_start</code> <code>instance-attribute</code> \u00b6 <code>duration</code> <code>property</code> \u00b6 <p>Get segment duration in milliseconds.</p> <code>duration_sec</code> <code>property</code> \u00b6 <code>end</code> <code>instance-attribute</code> \u00b6 <code>end_time</code> <code>property</code> \u00b6 <code>gap_before</code> <code>instance-attribute</code> \u00b6 <code>mapped_end</code> <code>property</code> \u00b6 <code>mapped_start</code> <code>property</code> \u00b6 <p>Downstream registry field set by the audio handler</p> <code>spacing_time</code> <code>instance-attribute</code> \u00b6 <code>speaker</code> <code>instance-attribute</code> \u00b6 <code>start</code> <code>instance-attribute</code> \u00b6 <code>start_time</code> <code>property</code> \u00b6 <code>normalize()</code> \u00b6 <p>Normalize the duration of the segment to be nonzero and validate start/end values.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>def normalize(self) -&gt; None:\n    \"\"\"Normalize the duration of the segment to be nonzero and validate start/end values.\"\"\"\n    # Validate that start and end are non-negative integers\n    if not isinstance(self.start, int) or not isinstance(self.end, int):\n        raise ValueError(\"Segment start and end must be integers, \"\n                         f\"got start={self.start}, end={self.end}\")\n    if self.start &lt; 0 or self.end &lt; 0:\n        raise ValueError(f\"Segment start and end must be non-negative, \"\n                         f\"got start={self.start}, end={self.end}\")\n\n    # Explicitly handle negative durations\n    if self.end &lt; self.start:\n        logger.warning(\n            f\"Invalid segment duration detected: start ({self.start}) &gt; end ({self.end}). \"\n            \"Adjusting end to ensure minimum duration of 1.\"\n        )\n        self.end = TimeMs(self.start + 1)  # set minimum nonzero duration\n\n    # Ensure minimum nonzero duration\n    if self.start == self.end:\n        logger.warning(\n            f\"Zero segment duration detected: start ({self.start}) == end ({self.end}). \"\n            \"Adjusting end to ensure minimum duration of 1.\"\n        )\n        self.end = TimeMs(self.start + 1)  # set minimum nonzero duration\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.models.SpeakerBlock","title":"<code>SpeakerBlock</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A block of contiguous or near-contiguous segments spoken by the same speaker.</p> <p>Used as a higher-level abstraction over diarization segments to simplify chunking strategies (e.g., language-aware sampling, re-segmentation).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>class SpeakerBlock(BaseModel):\n    \"\"\"A block of contiguous or near-contiguous segments spoken by the same speaker.\n\n    Used as a higher-level abstraction over diarization segments to simplify\n    chunking strategies (e.g., language-aware sampling, re-segmentation).\n    \"\"\"\n\n    speaker: str\n    segments: list[\"DiarizedSegment\"]\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    @property\n    def start(self) -&gt; \"TimeMs\":\n        return TimeMs(self.segments[0].start)\n\n    @property\n    def end(self) -&gt; \"TimeMs\":\n        return TimeMs(self.segments[-1].end)\n\n    @property\n    def duration(self) -&gt; \"TimeMs\":\n        return TimeMs(self.end - self.start)\n\n    @property\n    def duration_sec(self) -&gt; float:\n        return self.duration.to_seconds()\n\n    @property\n    def segment_count(self) -&gt; int:\n        return len(self.segments)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"custom serializer for SpeakerBlock with validation.\"\"\"\n        # Validate speaker\n        if not isinstance(self.speaker, str) or not self.speaker:\n            logger.error(\"SpeakerBlock.to_dict: 'speaker' must be a non-empty string.\")\n            raise ValueError(\"'speaker' must be a non-empty string.\")\n\n        # Validate segments\n        if not isinstance(self.segments, list) or not self.segments:\n            logger.error(\"SpeakerBlock.to_dict: 'segments' must be a non-empty list.\")\n            raise ValueError(\"'segments' must be a non-empty list of DiarizedSegment.\")\n\n        for idx, segment in enumerate(self.segments):\n            if not isinstance(segment, DiarizedSegment):\n                logger.error(f\"SpeakerBlock.to_dict: Segment at index {idx} is not a DiarizedSegment.\")\n                raise TypeError(f\"Segment at index {idx} is not a DiarizedSegment.\")\n\n        # Validate start/end/duration\n        try:\n            start = int(self.start)\n            end = int(self.end)\n            duration = int(self.duration)\n            duration_sec = float(self.duration_sec)\n            segment_count = int(self.segment_count)\n        except Exception as e:\n            logger.error(f\"SpeakerBlock.to_dict: Error computing time fields: {e}\")\n            raise\n\n        return {\n            \"speaker\": self.speaker,\n            \"segments\": [segment.model_dump() for segment in self.segments],\n            \"start\": start,\n            \"end\": end,\n            \"duration\": duration,\n            \"duration_sec\": duration_sec,\n            \"segment_count\": segment_count,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"SpeakerBlock\":\n        \"\"\"\n        Create a SpeakerBlock from a dictionary (output of to_dict).\n        Args:\n            data (dict): Dictionary with keys matching SpeakerBlock fields.\n        Returns:\n            SpeakerBlock: Deserialized SpeakerBlock instance.\n        Raises:\n            ValueError, TypeError: If validation fails.\n        \"\"\"\n        if not isinstance(data, dict):\n            logger.error(\"SpeakerBlock.from_dict: Input data must be a dictionary.\")\n            raise TypeError(\"Input data must be a dictionary.\")\n\n        if \"speaker\" not in data or not isinstance(data[\"speaker\"], str) or not data[\"speaker\"]:\n            logger.error(\"SpeakerBlock.from_dict: 'speaker' must be a non-empty string.\")\n            raise ValueError(\"'speaker' must be a non-empty string.\")\n\n        if \"segments\" not in data or not isinstance(data[\"segments\"], list) or not data[\"segments\"]:\n            logger.error(\"SpeakerBlock.from_dict: 'segments' must be a non-empty list.\")\n            raise ValueError(\"'segments' must be a non-empty list.\")\n\n        segments = []\n        for idx, seg in enumerate(data[\"segments\"]):\n            if not isinstance(seg, dict):\n                logger.error(f\"SpeakerBlock.from_dict: Segment at index {idx} is not a dict.\")\n                raise TypeError(f\"Segment at index {idx} is not a dict.\")\n            try:\n                segment = DiarizedSegment(**seg)\n            except Exception as e:\n                logger.error(\n                    f\"SpeakerBlock.from_dict: Failed to construct DiarizedSegment at index {idx}: {e}\"\n                    )\n                raise\n            segments.append(segment)\n\n        return cls(speaker=data[\"speaker\"], segments=segments)\n</code></pre> <code>duration</code> <code>property</code> \u00b6 <code>duration_sec</code> <code>property</code> \u00b6 <code>end</code> <code>property</code> \u00b6 <code>segment_count</code> <code>property</code> \u00b6 <code>segments</code> <code>instance-attribute</code> \u00b6 <code>speaker</code> <code>instance-attribute</code> \u00b6 <code>start</code> <code>property</code> \u00b6 <code>Config</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>class Config:\n    arbitrary_types_allowed = True\n</code></pre> <code>arbitrary_types_allowed = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>from_dict(data)</code> <code>classmethod</code> \u00b6 <p>Create a SpeakerBlock from a dictionary (output of to_dict). Args:     data (dict): Dictionary with keys matching SpeakerBlock fields. Returns:     SpeakerBlock: Deserialized SpeakerBlock instance. Raises:     ValueError, TypeError: If validation fails.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; \"SpeakerBlock\":\n    \"\"\"\n    Create a SpeakerBlock from a dictionary (output of to_dict).\n    Args:\n        data (dict): Dictionary with keys matching SpeakerBlock fields.\n    Returns:\n        SpeakerBlock: Deserialized SpeakerBlock instance.\n    Raises:\n        ValueError, TypeError: If validation fails.\n    \"\"\"\n    if not isinstance(data, dict):\n        logger.error(\"SpeakerBlock.from_dict: Input data must be a dictionary.\")\n        raise TypeError(\"Input data must be a dictionary.\")\n\n    if \"speaker\" not in data or not isinstance(data[\"speaker\"], str) or not data[\"speaker\"]:\n        logger.error(\"SpeakerBlock.from_dict: 'speaker' must be a non-empty string.\")\n        raise ValueError(\"'speaker' must be a non-empty string.\")\n\n    if \"segments\" not in data or not isinstance(data[\"segments\"], list) or not data[\"segments\"]:\n        logger.error(\"SpeakerBlock.from_dict: 'segments' must be a non-empty list.\")\n        raise ValueError(\"'segments' must be a non-empty list.\")\n\n    segments = []\n    for idx, seg in enumerate(data[\"segments\"]):\n        if not isinstance(seg, dict):\n            logger.error(f\"SpeakerBlock.from_dict: Segment at index {idx} is not a dict.\")\n            raise TypeError(f\"Segment at index {idx} is not a dict.\")\n        try:\n            segment = DiarizedSegment(**seg)\n        except Exception as e:\n            logger.error(\n                f\"SpeakerBlock.from_dict: Failed to construct DiarizedSegment at index {idx}: {e}\"\n                )\n            raise\n        segments.append(segment)\n\n    return cls(speaker=data[\"speaker\"], segments=segments)\n</code></pre> <code>to_dict()</code> \u00b6 <p>custom serializer for SpeakerBlock with validation.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/models.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"custom serializer for SpeakerBlock with validation.\"\"\"\n    # Validate speaker\n    if not isinstance(self.speaker, str) or not self.speaker:\n        logger.error(\"SpeakerBlock.to_dict: 'speaker' must be a non-empty string.\")\n        raise ValueError(\"'speaker' must be a non-empty string.\")\n\n    # Validate segments\n    if not isinstance(self.segments, list) or not self.segments:\n        logger.error(\"SpeakerBlock.to_dict: 'segments' must be a non-empty list.\")\n        raise ValueError(\"'segments' must be a non-empty list of DiarizedSegment.\")\n\n    for idx, segment in enumerate(self.segments):\n        if not isinstance(segment, DiarizedSegment):\n            logger.error(f\"SpeakerBlock.to_dict: Segment at index {idx} is not a DiarizedSegment.\")\n            raise TypeError(f\"Segment at index {idx} is not a DiarizedSegment.\")\n\n    # Validate start/end/duration\n    try:\n        start = int(self.start)\n        end = int(self.end)\n        duration = int(self.duration)\n        duration_sec = float(self.duration_sec)\n        segment_count = int(self.segment_count)\n    except Exception as e:\n        logger.error(f\"SpeakerBlock.to_dict: Error computing time fields: {e}\")\n        raise\n\n    return {\n        \"speaker\": self.speaker,\n        \"segments\": [segment.model_dump() for segment in self.segments],\n        \"start\": start,\n        \"end\": end,\n        \"duration\": duration,\n        \"duration_sec\": duration_sec,\n        \"segment_count\": segment_count,\n    }\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.protocols","title":"<code>protocols</code>","text":"<p>Interfaces shared by diarization strategy classes.</p>"},{"location":"api/#tnh_scholar.audio_processing.diarization.protocols.AudioFetcher","title":"<code>AudioFetcher</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Abstract audio provider for probing a segment.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>class AudioFetcher(Protocol):\n    \"\"\"Abstract audio provider for probing a segment.\"\"\"\n\n    def extract_audio(self, start_ms: int, end_ms: int) -&gt; Path: ...\n</code></pre> <code>extract_audio(start_ms, end_ms)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>def extract_audio(self, start_ms: int, end_ms: int) -&gt; Path: ...\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.protocols.ChunkingStrategy","title":"<code>ChunkingStrategy</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol every chunking strategy must satisfy.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>class ChunkingStrategy(Protocol):\n    \"\"\"\n    Protocol every chunking strategy must satisfy.\n    \"\"\"\n\n    def extract(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]: ...\n</code></pre> <code>extract(segments)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>def extract(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]: ...\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.protocols.DiarizationService","title":"<code>DiarizationService</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for any diarization service.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>@runtime_checkable\nclass DiarizationService(Protocol):\n    \"\"\"Protocol for any diarization service.\"\"\"\n\n    def start(self, audio_path: Path, params: Optional[DiarizationParams] = None) -&gt; str:\n        \"\"\"Start a diarization job and return an opaque job_id.\"\"\" \n        ...\n\n\n    def get_response(self, job_id: str, *, wait_until_complete: bool = False) -&gt; DiarizationResponse: \n        \"\"\"Return the current state or final result as a DiarizationResponse.\n\n            When `wait_until_complete` is True, the service blocks until a terminal\n            state (succeeded/failed/timeout) and returns that envelope.\n            \"\"\"\n        ...\n\n    def generate(\n        self,\n        audio_path: Path,\n        params: Optional[DiarizationParams] = None,\n        *,\n        wait_until_complete: bool = True,\n    ) -&gt; DiarizationResponse: \n        ...\n        \"\"\"One-shot convenience: start + (optionally) wait + fetch + map.\n\n        Implementations may optimize this path; default behavior can be\n        start() followed by get_response().\n        \"\"\"\n</code></pre> <code>generate(audio_path, params=None, *, wait_until_complete=True)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>def generate(\n    self,\n    audio_path: Path,\n    params: Optional[DiarizationParams] = None,\n    *,\n    wait_until_complete: bool = True,\n) -&gt; DiarizationResponse: \n    ...\n    \"\"\"One-shot convenience: start + (optionally) wait + fetch + map.\n\n    Implementations may optimize this path; default behavior can be\n    start() followed by get_response().\n    \"\"\"\n</code></pre> <code>get_response(job_id, *, wait_until_complete=False)</code> \u00b6 <p>Return the current state or final result as a DiarizationResponse.</p> <p>When <code>wait_until_complete</code> is True, the service blocks until a terminal state (succeeded/failed/timeout) and returns that envelope.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>def get_response(self, job_id: str, *, wait_until_complete: bool = False) -&gt; DiarizationResponse: \n    \"\"\"Return the current state or final result as a DiarizationResponse.\n\n        When `wait_until_complete` is True, the service blocks until a terminal\n        state (succeeded/failed/timeout) and returns that envelope.\n        \"\"\"\n    ...\n</code></pre> <code>start(audio_path, params=None)</code> \u00b6 <p>Start a diarization job and return an opaque job_id.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>def start(self, audio_path: Path, params: Optional[DiarizationParams] = None) -&gt; str:\n    \"\"\"Start a diarization job and return an opaque job_id.\"\"\" \n    ...\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.protocols.LanguageDetector","title":"<code>LanguageDetector</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Abstract language detector (e.g., fastText, Whisper-lang).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>class LanguageDetector(Protocol):\n    \"\"\"Abstract language detector (e.g., fastText, Whisper-lang).\"\"\"\n\n    def detect(self, audio: AudioSegment, format_str: str) -&gt; Optional[str]: ...\n</code></pre> <code>detect(audio, format_str)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>def detect(self, audio: AudioSegment, format_str: str) -&gt; Optional[str]: ...\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.protocols.ResultWriter","title":"<code>ResultWriter</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Port for persisting diarization results.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>class ResultWriter(Protocol):\n    \"\"\"Port for persisting diarization results.\"\"\"\n\n    def write(self, path: Path, response: DiarizationResponse) -&gt; Path:\n        ...\n</code></pre> <code>write(path, response)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>def write(self, path: Path, response: DiarizationResponse) -&gt; Path:\n    ...\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.protocols.SegmentAdapter","title":"<code>SegmentAdapter</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>class SegmentAdapter(Protocol):\n    def to_segments(\n        self, \n        data: Any\n        ) -&gt; List[DiarizedSegment]: ...\n</code></pre> <code>to_segments(data)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>def to_segments(\n    self, \n    data: Any\n    ) -&gt; List[DiarizedSegment]: ...\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_adapter","title":"<code>pyannote_adapter</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_adapter.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_adapter.PyannoteAdapter","title":"<code>PyannoteAdapter</code>","text":"<p>               Bases: <code>SegmentAdapter</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_adapter.py</code> <pre><code>class PyannoteAdapter(SegmentAdapter):\n    def __init__(self, config: DiarizationConfig = DiarizationConfig()):\n        self.config = config\n\n    def to_segments(self, data: Dict[str, List['PyannoteEntry']]) -&gt; List[DiarizedSegment]:\n        \"\"\"\n        Convert a pyannoteai diarization result dict to list of DiarizationSegment objects.\n        \"\"\"\n        entries = self._extract_entries(data)\n        valid_entries = self._validate_pyannote_entries(entries)\n        segments: List[DiarizedSegment] = []\n        for e in valid_entries:\n            segment = DiarizedSegment(\n                speaker=str(e.get(\"speaker\", \"SPEAKER_00\")),\n                start=TimeMs.from_seconds(float(e[\"start\"])),\n                end=TimeMs.from_seconds(float(e[\"end\"])),\n                audio_map_start=None,\n                gap_before=None,\n                spacing_time=None,\n            )\n            segments.append(segment)\n        return self._sort_and_normalize_segments(segments)\n\n    def to_response(\n        self, jsr: JobStatusResponse\n    ) -&gt; DiarizationResponse:\n        \"\"\"\n        Convert a JobStatusResponse to a DiarizationResponse (domain layer).\n        \"\"\"\n        if self._is_successful(jsr):\n            return self._build_succeeded(jsr)\n        if self._is_outcome_failure(jsr):\n            return self._build_outcome_failure(jsr)\n        if self._is_api_failure(jsr):\n            return self._build_api_failed(jsr)\n        if self._is_pending(jsr):\n            return self._build_pending(jsr)\n        if self._is_running(jsr):\n            return self._build_running(jsr)\n        return self._build_fallback(jsr)\n\n    def _extract_entries(self, payload: Dict[str, Any] | None) -&gt; List[dict[str, Any]]:\n        raw = payload or {}\n        if isinstance(raw.get(\"diarization\"), list):\n            return list(raw[\"diarization\"])\n        segments = raw.get(\"segments\")\n        if isinstance(segments, list):\n            return list(segments)\n        ann = raw.get(\"annotation\")\n        if isinstance(ann, dict) and isinstance(ann.get(\"segments\"), list):\n            return list(ann[\"segments\"])\n        logger.warning(\n            \"Unexpected payload shape in _extract_entries: %r\", payload\n        )\n        return []\n\n    def _validate_pyannote_entries(self, entries: List[dict[str, Any]]) -&gt; List[dict[str, Any]]:\n        valid = []\n        for e in entries:\n            if not isinstance(e, dict):\n                logger.warning(\"Entry is not a dict: %r\", e)\n                continue\n            if any(k not in e for k in (\"start\", \"end\")):\n                logger.warning(\"Missing 'start' or 'end' in entry: %r\", e)\n                continue\n            try:\n                float(e[\"start\"])\n                float(e[\"end\"])\n            except (ValueError, TypeError):\n                logger.warning(\"Non-numeric 'start' or 'end' in entry: %r\", e)\n                continue\n            valid.append(e)\n        return valid\n\n    def _sort_and_normalize_segments(\n        self, segments: List[DiarizedSegment]\n        ) -&gt; List[DiarizedSegment]:\n        self._sort_by_start(segments)\n        for segment in segments:\n            segment.normalize()\n        return segments\n\n    def _sort_by_start(self, segments: List[DiarizedSegment]) -&gt; None:\n        segments.sort(key=lambda segment: segment.start)\n\n    def _map_outcome_to_error(self, outcome: PollOutcome, status: Optional[JobStatus]) -&gt; ErrorCode:\n        if outcome == PollOutcome.SUCCEEDED:\n            logger.warning(\n                \"PollOutcome.SUCCEEDED was mapped to ErrorCode.UNKNOWN in map_outcome_to_error. \"\n                \"This indicates a logic error.\"\n            )\n            return ErrorCode.UNKNOWN\n        if outcome == PollOutcome.FAILED:\n            return ErrorCode.API_ERROR\n        if outcome == PollOutcome.TIMEOUT:\n            return ErrorCode.TIMEOUT\n        if outcome == PollOutcome.NETWORK_ERROR:\n            return ErrorCode.TRANSIENT\n        if outcome == PollOutcome.INTERRUPTED:\n            return ErrorCode.CANCELLED\n        if outcome == PollOutcome.ERROR:\n            if status in (JobStatus.PENDING, JobStatus.RUNNING):\n                return ErrorCode.TRANSIENT\n            return ErrorCode.UNKNOWN\n        return ErrorCode.UNKNOWN\n\n    def _is_successful(self, jsr: JobStatusResponse) -&gt; bool:\n        return jsr.outcome == PollOutcome.SUCCEEDED and jsr.status == JobStatus.SUCCEEDED\n\n    def _is_outcome_failure(self, jsr: JobStatusResponse) -&gt; bool:\n        return jsr.outcome in (\n            PollOutcome.TIMEOUT,\n            PollOutcome.NETWORK_ERROR,\n            PollOutcome.INTERRUPTED,\n            PollOutcome.ERROR,\n        )\n\n    def _is_api_failure(self, jsr: JobStatusResponse) -&gt; bool:\n        return jsr.status == JobStatus.FAILED\n\n    def _is_pending(self, jsr: JobStatusResponse) -&gt; bool:\n        return jsr.status == JobStatus.PENDING\n\n    def _is_running(self, jsr: JobStatusResponse) -&gt; bool:\n        return jsr.status == JobStatus.RUNNING\n\n    def _build_succeeded(self, jsr: JobStatusResponse) -&gt; DiarizationSucceeded:\n        payload = jsr.payload or {}\n        segments = self.to_segments(payload)\n        num_speakers = payload.get(\"numSpeakers\", payload.get(\"num_speakers\"))\n        return DiarizationSucceeded(\n            status=\"succeeded\",\n            job_id=jsr.job_id,\n            result=DiarizationResult(segments=segments, num_speakers=num_speakers, metadata=None),\n            raw=jsr.model_dump(mode=\"json\"),\n        )\n\n    def _build_outcome_failure(self, jsr: JobStatusResponse) -&gt; DiarizationFailed:\n        code = self._map_outcome_to_error(jsr.outcome, jsr.status)\n        message = jsr.server_error_msg or \"Null Message\"\n        return DiarizationFailed(\n            status=\"failed\",\n            job_id=jsr.job_id,\n            error=ErrorInfo(\n                code=code,\n                message=message,\n                details={\n                    \"outcome\": jsr.outcome.value,\n                    \"status\": jsr.status.value if jsr.status else None,\n                    \"polls\": jsr.polls,\n                    \"elapsed_s\": jsr.elapsed_s,\n                },\n            ),\n            raw=jsr.model_dump(mode=\"json\"),\n        )\n\n    def _build_api_failed(self, jsr: JobStatusResponse) -&gt; DiarizationFailed:\n        return DiarizationFailed(\n            status=\"failed\",\n            job_id=jsr.job_id,\n            error=ErrorInfo(\n                code=ErrorCode.API_ERROR,\n                message=jsr.server_error_msg or \"Remote job failed\",\n                details={\"status\": jsr.status.value if jsr.status else None},\n            ),\n            raw=jsr.model_dump(mode=\"json\"),\n        )\n\n    def _build_pending(self, jsr: JobStatusResponse) -&gt; DiarizationPending:\n        return DiarizationPending(status=\"pending\", job_id=jsr.job_id, raw=jsr.model_dump(mode=\"json\"))\n\n    def _build_running(self, jsr: JobStatusResponse) -&gt; DiarizationRunning:\n        return DiarizationRunning(status=\"running\", job_id=jsr.job_id, raw=jsr.model_dump(mode=\"json\"))\n\n    def _build_fallback(self, jsr: JobStatusResponse) -&gt; DiarizationFailed:\n        return DiarizationFailed(\n            status=\"failed\",\n            job_id=jsr.job_id,\n            error=ErrorInfo(\n                code=ErrorCode.UNKNOWN,\n                message=(\n                    f\"Unknown outcome/status combination: outcome={jsr.outcome.value}, \"\n                    f\"status={getattr(jsr.status, 'value', None)}\"\n                ),\n                details={\n                    \"outcome\": jsr.outcome.value,\n                    \"status\": jsr.status.value if jsr.status else None,\n                    \"polls\": jsr.polls,\n                    \"elapsed_s\": jsr.elapsed_s,\n                },\n            ),\n            raw=jsr.model_dump(mode=\"json\"),\n        )\n\n    def failed_start(self):\n        return DiarizationFailed(\n            status=\"failed\",\n            job_id=None,\n            error=ErrorInfo(\n                code=ErrorCode.TRANSIENT,\n                message=(\"Job failed to upload or start.\"),\n                details=None,\n            )\n\n        )\n</code></pre> <code>config = config</code> <code>instance-attribute</code> \u00b6 <code>__init__(config=DiarizationConfig())</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_adapter.py</code> <pre><code>def __init__(self, config: DiarizationConfig = DiarizationConfig()):\n    self.config = config\n</code></pre> <code>failed_start()</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_adapter.py</code> <pre><code>def failed_start(self):\n    return DiarizationFailed(\n        status=\"failed\",\n        job_id=None,\n        error=ErrorInfo(\n            code=ErrorCode.TRANSIENT,\n            message=(\"Job failed to upload or start.\"),\n            details=None,\n        )\n\n    )\n</code></pre> <code>to_response(jsr)</code> \u00b6 <p>Convert a JobStatusResponse to a DiarizationResponse (domain layer).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_adapter.py</code> <pre><code>def to_response(\n    self, jsr: JobStatusResponse\n) -&gt; DiarizationResponse:\n    \"\"\"\n    Convert a JobStatusResponse to a DiarizationResponse (domain layer).\n    \"\"\"\n    if self._is_successful(jsr):\n        return self._build_succeeded(jsr)\n    if self._is_outcome_failure(jsr):\n        return self._build_outcome_failure(jsr)\n    if self._is_api_failure(jsr):\n        return self._build_api_failed(jsr)\n    if self._is_pending(jsr):\n        return self._build_pending(jsr)\n    if self._is_running(jsr):\n        return self._build_running(jsr)\n    return self._build_fallback(jsr)\n</code></pre> <code>to_segments(data)</code> \u00b6 <p>Convert a pyannoteai diarization result dict to list of DiarizationSegment objects.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_adapter.py</code> <pre><code>def to_segments(self, data: Dict[str, List['PyannoteEntry']]) -&gt; List[DiarizedSegment]:\n    \"\"\"\n    Convert a pyannoteai diarization result dict to list of DiarizationSegment objects.\n    \"\"\"\n    entries = self._extract_entries(data)\n    valid_entries = self._validate_pyannote_entries(entries)\n    segments: List[DiarizedSegment] = []\n    for e in valid_entries:\n        segment = DiarizedSegment(\n            speaker=str(e.get(\"speaker\", \"SPEAKER_00\")),\n            start=TimeMs.from_seconds(float(e[\"start\"])),\n            end=TimeMs.from_seconds(float(e[\"end\"])),\n            audio_map_start=None,\n            gap_before=None,\n            spacing_time=None,\n        )\n        segments.append(segment)\n    return self._sort_and_normalize_segments(segments)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_client","title":"<code>pyannote_client</code>","text":"<p>pyannote_client.py</p> <p>Client interface for interacting with the pyannote.ai speaker diarization API.</p> <p>This module provides a robust, object-oriented client for uploading audio files, starting diarization jobs, polling for job completion, and retrieving results from the pyannote.ai API. It includes retry logic, configurable timeouts, and support for advanced diarization parameters.</p> Typical usage <p>client = PyannoteClient(api_key=\"your_api_key\") media_id = client.upload_audio(Path(\"audio.mp3\")) job_id = client.start_diarization(media_id) result = client.poll_job_until_complete(job_id)</p>"},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_client.JOB_ID_FIELD","title":"<code>JOB_ID_FIELD = 'jobId'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_client.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_client.APIKeyError","title":"<code>APIKeyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when API key is missing or invalid.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>class APIKeyError(Exception):\n    \"\"\"Raised when API key is missing or invalid.\"\"\"\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_client.PyannoteClient","title":"<code>PyannoteClient</code>","text":"<p>Client for interacting with the pyannote.ai speaker diarization API.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>class PyannoteClient:\n    \"\"\"Client for interacting with the pyannote.ai speaker diarization API.\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, config: Optional[PyannoteConfig] = None):\n        \"\"\"\n        Initialize with API key.\n\n        Args:\n            api_key: Pyannote.ai API key (defaults to environment variable)\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"PYANNOTEAI_API_TOKEN\")\n        if not self.api_key:\n            raise APIKeyError(\n                \"API key is required. Set PYANNOTEAI_API_TOKEN environment \"\n                \"variable or pass as parameter\"\n            )\n\n        self.config = config or PyannoteConfig()\n        self.polling_config = self.config.polling_config\n\n        # Upload-specific timeouts (longer than general calls)\n        self.upload_timeout = self.config.upload_timeout\n        self.upload_max_retries = self.config.upload_max_retries\n        self.network_timeout = self.config.network_timeout\n\n        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n    # -----------------------\n    # Upload helpers\n    # -----------------------\n    def _create_media_id(self) -&gt; str:\n        \"\"\"Generate a unique media ID.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n        return f\"{self.config.media_prefix}{timestamp}\"\n\n    def _upload_file(self, file_path: Path, upload_url: str) -&gt; bool:\n        \"\"\"\n        Upload file to the provided URL.\n\n        Args:\n            file_path: Path to the file to upload\n            upload_url: URL to upload to\n\n        Returns:\n            bool: True if upload successful, False otherwise\n        \"\"\"\n        try:\n            logger.info(f\"Uploading file to Pyannote.ai: {file_path}\")\n            with open(file_path, \"rb\") as file_data:\n                upload_response = requests.put(\n                    upload_url,\n                    data=file_data,\n                    headers={\"Content-Type\": self.config.media_content_type},\n                    timeout=self.upload_timeout,\n                )\n\n            upload_response.raise_for_status()\n            logger.info(\"File uploaded successfully\")\n            return True\n\n        except requests.RequestException as e:\n            logger.error(f\"Failed to upload file: {e}\")\n            return False\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential_jitter(exp_base=2, initial=3, max=30),\n        retry=retry_if_exception_type(\n            (requests.RequestException, requests.Timeout, requests.ConnectionError)\n            ),\n    )\n    def upload_audio(self, file_path: Path) -&gt; Optional[str]:\n        \"\"\"\n        Upload audio file with retry logic for network robustness.\n\n        Retries on network errors with exponential backoff.\n        Fails fast on permanent errors (auth, file not found, etc.).\n        \"\"\"\n        try:\n            if not file_path.exists() or not file_path.is_file():\n                logger.error(f\"Audio file not found or is not a file: {file_path}\")\n                return None\n        except OSError as e:\n            logger.error(f\"Error accessing audio file '{file_path}': {e}\")\n            return None\n\n        try:\n            file_size_mb = file_path.stat().st_size / (1024 * 1024)\n        except OSError as e:\n            logger.error(f\"Error reading file size for '{file_path}': {e}\")\n            return None\n\n        logger.info(f\"Starting upload of {file_path.name} ({file_size_mb:.1f}MB)\")\n\n        try:\n            # Create media ID\n            media_id = self._create_media_id()\n            logger.debug(f\"Created media ID: {media_id}\")\n\n            # Get upload URL (this is fast, use normal timeout)\n            upload_url = self._data_upload_url(media_id)\n            if not upload_url:\n                return None\n\n            # Upload file (this is slow, use extended timeout)\n            if self._upload_file(file_path, upload_url):\n                logger.info(f\"Upload completed successfully: {media_id}\")\n                return media_id\n            else:\n                logger.error(f\"Upload failed for {file_path.name}\")\n                return None\n\n        except Exception as e:\n            # Log but don't retry - let tenacity handle retries\n            logger.error(f\"Upload attempt failed: {e}\")\n            raise  # Re-raise for tenacity to handle\n\n    def _data_upload_url(self, media_id: str) -&gt; Optional[str]:\n        response = requests.post(\n            self.config.media_input_endpoint,\n            headers=self.headers,\n            json={\"url\": media_id},\n            timeout=self.network_timeout,\n        )\n        upload_url = self._extract_response_info(\n            response, \"url\", \"No upload URL in API response\"\n        )\n        logger.debug(f\"Got upload URL for media ID: {media_id}\")\n        return upload_url\n\n    def _extract_response_info(self, response, response_type, error_msg):\n        response.raise_for_status()\n        info = response.json()\n        if result := info.get(response_type):\n            return result\n        else:\n            raise ValueError(error_msg)\n\n    # -----------------------\n    # Start job\n    # -----------------------\n    def start_diarization(self, media_id: str, params: Optional[DiarizationParams] = None) -&gt; Optional[str]:\n        \"\"\"\n        Start diarization job with pyannote.ai API.\n\n        Args:\n            media_id: The media ID from upload_audio\n            params: Optional parameters for diarization\n\n        Returns:\n            Optional[str]: The job ID if started successfully, None otherwise\n        \"\"\"\n        try:\n            return self._send_payload(media_id, params)\n        except requests.RequestException as e:\n            logger.error(f\"API request failed: {e}\")\n            return None\n        except ValueError as e:\n            logger.error(f\"Invalid API response: {e}\")\n            return None\n\n    def _send_payload(self, media_id, params):\n        payload: Dict[str, Any] = {\"url\": media_id}\n        if params:\n            payload |= params.to_api_dict()\n            logger.info(f\"Starting diarization with params: {params}\")\n        logger.debug(f\"Full payload: {payload}\")\n\n        response = requests.post(self.config.diarize_endpoint, headers=self.headers, json=payload)\n        job_id = self._extract_response_info(\n            response, JOB_ID_FIELD, \"API response missing job ID\"\n        )\n        logger.info(f\"Diarization job {job_id} started successfully\")\n        return job_id\n\n    # -----------------------\n    # Status / Polling\n    # -----------------------\n    def check_job_status(self, job_id: str) -&gt; Optional[JobStatusResponse]:\n        \"\"\"\n        Check the status of a diarization job.\n\n        Returns a typed transport model (JobStatusResponse) or None on failure.\n        \"\"\"\n        return self._check_status_with_retry(job_id)\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential_jitter(exp_base=2, initial=1, max=10),\n        retry=retry_if_exception_type(\n            (requests.RequestException, requests.Timeout, requests.ConnectionError)\n            ),\n    )\n    def _check_status_with_retry(self, job_id: str) -&gt; Optional[JobStatusResponse]:\n        \"\"\"\n        Check job status with network error retry logic.\n\n        Retries network failures without killing the polling loop.\n        Fails fast on API errors (auth, malformed response, etc.).\n\n        Used as the status function in the JobPoller helper class.\n        \"\"\"\n        try:\n            endpoint = f\"{self.config.job_status_endpoint}/{job_id}\"\n            response = requests.get(endpoint, headers=self.headers)\n            response.raise_for_status()\n            result = response.json()\n\n            try:\n                jsr = JobStatusResponse.model_validate(result)\n            except Exception as ve:\n                logger.error(f\"Invalid status response for job {job_id}: {result} ({ve})\")\n                return None\n\n            return jsr\n\n        except requests.RequestException as e:\n            logger.warning(f\"Status check network error for job {job_id}: {e}\")\n            raise  # Let tenacity retry\n        except Exception as e:\n            logger.error(f\"Unexpected status check error for job {job_id}: {e}\")\n            return None  # Don't retry on unexpected errors\n\n    class JobPoller:\n        \"\"\"\n        Generic job polling helper for long-running async jobs.\n        \"\"\"\n\n        def __init__(self, status_fn, job_id: str, polling_config: PollingConfig):\n            self.status_fn = status_fn\n            self.job_id = job_id\n            self.polling_config = polling_config\n            self.poll_count = 0\n            self.start_time = time.time()\n            self.last_status: Optional[JobStatusResponse] = None\n            self._last_error_reason: Optional[str] = None\n\n        def _poll(self) -&gt; JobStatusResponse | _PollSignal | None:\n            self.poll_count += 1\n            try:\n                status_response = self.status_fn(self.job_id)\n            except RetryError as e:\n                self._last_error_reason = f\"status check retry exhausted: {e}\"\n                logger.error(f\"Status check retries exhausted for job {self.job_id}: {e}\")\n                return _PollSignal.STATUS_RETRY_EXHAUSTED\n\n            if status_response is None:\n                logger.error(f\"Failed to get status for job {self.job_id} after retries\")\n                self._last_error_reason = \"status response None\"\n                return None\n\n            # track last known status for timeout / errors\n            self.last_status = status_response\n\n            status = status_response.status\n            elapsed = time.time() - self.start_time\n\n            if status == JobStatus.SUCCEEDED:\n                logger.info(\n                    f\"Job {self.job_id} completed successfully after {elapsed:.1f}s ({self.poll_count} polls)\"\n                )\n                return status_response\n\n            if status == JobStatus.FAILED:\n                logger.error(f\"Job {self.job_id} failed: {status_response.server_error_msg}\")\n                return status_response\n\n            # Job still running - calculate next poll interval\n            logger.info(f\"Job {self.job_id} status: {status} (elapsed: {elapsed:.1f}s)\")\n            return _PollSignal.CONTINUE\n\n        # --- Internal builders to attach polling context and craft JSRs ---\n        def _attach_context(\n            self, \n            base: Optional[JobStatusResponse], \n            *, \n            outcome: PollOutcome, \n            elapsed: float, \n            msg: Optional[str] = None\n            ) -&gt; JobStatusResponse:\n            \"\"\"Return a JSR carrying outcome + poll context. If `base` exists, preserve its\n            status/payload/server_error_msg unless `msg` overrides it. Otherwise, synthesize a minimal JSR.\"\"\"\n            if base is None:\n                return JobStatusResponse(\n                    job_id=self.job_id,\n                    outcome=outcome,\n                    status=None,\n                    server_error_msg=msg,\n                    payload=None,\n                    polls=self.poll_count,\n                    elapsed_s=elapsed,\n                )\n            return JobStatusResponse(\n                job_id=self.job_id,\n                outcome=outcome,\n                status=base.status,\n                server_error_msg=msg if msg is not None else base.server_error_msg,\n                payload=base.payload,\n                polls=self.poll_count,\n                elapsed_s=elapsed,\n            )\n\n        def _on_terminal(self, jsr: JobStatusResponse, *, elapsed: float) -&gt; JobStatusResponse:\n            \"\"\"Attach poll context to a terminal server response (SUCCEEDED/FAILED).\"\"\"\n            return JobStatusResponse(\n                job_id=self.job_id,\n                outcome=PollOutcome.SUCCEEDED if jsr.status == JobStatus.SUCCEEDED else PollOutcome.FAILED,\n                status=jsr.status,\n                server_error_msg=jsr.server_error_msg,\n                payload=jsr.payload,\n                polls=self.poll_count,\n                elapsed_s=elapsed,\n            )\n\n        def _on_status_retry_exhausted(self, *, elapsed: float) -&gt; JobStatusResponse:\n            return self._attach_context(\n                self.last_status, \n                outcome=PollOutcome.NETWORK_ERROR, \n                elapsed=elapsed, \n                msg=self._last_error_reason\n                )\n\n        def _on_invalid_payload(self, *, elapsed: float) -&gt; JobStatusResponse:\n            return self._attach_context(\n                self.last_status, \n                outcome=PollOutcome.ERROR, \n                elapsed=elapsed, \n                msg=\"invalid status payload\"\n                )\n\n        def _on_timeout(self, err: RetryError, *, elapsed: float) -&gt; JobStatusResponse:\n            return self._attach_context(\n                self.last_status, \n                outcome=PollOutcome.TIMEOUT, \n                elapsed=elapsed, \n                msg=str(err)\n                )\n\n        def _on_interrupt(self, *, elapsed: float) -&gt; JobStatusResponse:\n            return self._attach_context(\n                self.last_status, \n                outcome=PollOutcome.INTERRUPTED, \n                elapsed=elapsed, \n                msg=\"KeyboardInterrupt\"\n                )\n\n        def _on_exception(self, err: Exception, *, elapsed: float) -&gt; JobStatusResponse:\n            return self._attach_context(\n                self.last_status, \n                outcome=PollOutcome.ERROR, \n                elapsed=elapsed, \n                msg=str(err)\n                )\n\n        def run(self) -&gt; JobStatusResponse:\n            try:\n                result = self._setup_and_run_poll()\n                elapsed = time.time() - self.start_time\n\n                if isinstance(result, JobStatusResponse):\n                    # Terminal SUCCEEDED/FAILED (or unexpected non-terminal delivered): attach context\n                    return self._on_terminal(result, elapsed=elapsed)\n\n                if result is _PollSignal.STATUS_RETRY_EXHAUSTED:\n                    return self._on_status_retry_exhausted(elapsed=elapsed)\n\n                # None indicates invalid status payload or unexpected branch\n                return self._on_invalid_payload(elapsed=elapsed)\n\n            except RetryError as e:\n                # Outer polling timeout\n                elapsed = time.time() - self.start_time\n                logger.info(f\"Polling timed out for job {self.job_id} after {elapsed:.1f}s\")\n                return self._on_timeout(e, elapsed=elapsed)\n            except KeyboardInterrupt:\n                elapsed = time.time() - self.start_time\n                logger.info(f\"Polling for job {self.job_id} interrupted by user. Exiting.\")\n                return self._on_interrupt(elapsed=elapsed)\n            except Exception as e:\n                elapsed = time.time() - self.start_time\n                logger.error(f\"Polling failed for job {self.job_id}: {e}\")\n                return self._on_exception(e, elapsed=elapsed)\n\n        def _setup_and_run_poll(self) -&gt; Optional[JobStatusResponse | _PollSignal]:\n            cfg = self.polling_config\n            stop_policy = stop_never if cfg.polling_timeout is None else stop_after_delay(cfg.polling_timeout)\n            retrying = Retrying(\n                retry=retry_if_result(lambda result: result is _PollSignal.CONTINUE),\n                stop=stop_policy,\n                wait=wait_exponential_jitter(\n                    exp_base=cfg.exp_base,\n                    initial=cfg.initial_poll_time,\n                    max=cfg.max_interval,\n                ),\n                reraise=True,\n            )\n            result = retrying(self._poll)\n            if isinstance(result, JobStatusResponse):\n                return result\n            # could be STATUS_RETRY_EXHAUSTED sentinel or None\n            logger.info(f\"Polling ended with result: {result}\")\n            return result\n\n    def poll_job_until_complete(\n        self,\n        job_id: str,\n        estimated_duration: Optional[float] = None,\n        timeout: Optional[float] = None,\n        wait_until_complete: Optional[bool] = False,\n    ) -&gt; JobStatusResponse:\n        \"\"\"\n        Poll until the job reaches a terminal state or a client-side stop condition, and\n        return a unified JobStatusResponse (JSR) that includes both the server payload\n        and polling context via `outcome`, `polls`, and `elapsed_s`.\n\n        Args:\n            job_id: Remote job identifier to poll.\n            estimated_duration: Optional hint; currently unused (reserved for adaptive backoff).\n            timeout: Optional hard timeout in seconds for this poll call. If provided, it overrides\n                     the client's default polling timeout. Ignored if `wait_until_complete` is True.\n            wait_until_complete: If True, ignore timeout and poll indefinitely (subject to process lifetime).\n\n        Returns:\n            JobStatusResponse: unified transport + polling-context result.\n        \"\"\"\n        if timeout is not None and wait_until_complete:\n            raise ConfigurationError(\"Timeout cannot be set with wait_until_complete\")\n\n        # Derive an effective timeout for this call, without mutating client defaults\n        effective_timeout = None if wait_until_complete else (\n            timeout if timeout is not None else self.polling_config.polling_timeout\n            )\n\n        cfg = PollingConfig(\n            polling_timeout=effective_timeout,\n            initial_poll_time=self.polling_config.initial_poll_time,\n            exp_base=self.polling_config.exp_base,\n            max_interval=self.polling_config.max_interval,\n        )\n\n        poller = self.JobPoller(\n            status_fn=self._check_status_with_retry,\n            job_id=job_id,\n            polling_config=cfg,\n        )\n        return poller.run()\n</code></pre> <code>api_key = api_key or os.getenv('PYANNOTEAI_API_TOKEN')</code> <code>instance-attribute</code> \u00b6 <code>config = config or PyannoteConfig()</code> <code>instance-attribute</code> \u00b6 <code>headers = {'Authorization': f'Bearer {self.api_key}'}</code> <code>instance-attribute</code> \u00b6 <code>network_timeout = self.config.network_timeout</code> <code>instance-attribute</code> \u00b6 <code>polling_config = self.config.polling_config</code> <code>instance-attribute</code> \u00b6 <code>upload_max_retries = self.config.upload_max_retries</code> <code>instance-attribute</code> \u00b6 <code>upload_timeout = self.config.upload_timeout</code> <code>instance-attribute</code> \u00b6 <code>JobPoller</code> \u00b6 <p>Generic job polling helper for long-running async jobs.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>class JobPoller:\n    \"\"\"\n    Generic job polling helper for long-running async jobs.\n    \"\"\"\n\n    def __init__(self, status_fn, job_id: str, polling_config: PollingConfig):\n        self.status_fn = status_fn\n        self.job_id = job_id\n        self.polling_config = polling_config\n        self.poll_count = 0\n        self.start_time = time.time()\n        self.last_status: Optional[JobStatusResponse] = None\n        self._last_error_reason: Optional[str] = None\n\n    def _poll(self) -&gt; JobStatusResponse | _PollSignal | None:\n        self.poll_count += 1\n        try:\n            status_response = self.status_fn(self.job_id)\n        except RetryError as e:\n            self._last_error_reason = f\"status check retry exhausted: {e}\"\n            logger.error(f\"Status check retries exhausted for job {self.job_id}: {e}\")\n            return _PollSignal.STATUS_RETRY_EXHAUSTED\n\n        if status_response is None:\n            logger.error(f\"Failed to get status for job {self.job_id} after retries\")\n            self._last_error_reason = \"status response None\"\n            return None\n\n        # track last known status for timeout / errors\n        self.last_status = status_response\n\n        status = status_response.status\n        elapsed = time.time() - self.start_time\n\n        if status == JobStatus.SUCCEEDED:\n            logger.info(\n                f\"Job {self.job_id} completed successfully after {elapsed:.1f}s ({self.poll_count} polls)\"\n            )\n            return status_response\n\n        if status == JobStatus.FAILED:\n            logger.error(f\"Job {self.job_id} failed: {status_response.server_error_msg}\")\n            return status_response\n\n        # Job still running - calculate next poll interval\n        logger.info(f\"Job {self.job_id} status: {status} (elapsed: {elapsed:.1f}s)\")\n        return _PollSignal.CONTINUE\n\n    # --- Internal builders to attach polling context and craft JSRs ---\n    def _attach_context(\n        self, \n        base: Optional[JobStatusResponse], \n        *, \n        outcome: PollOutcome, \n        elapsed: float, \n        msg: Optional[str] = None\n        ) -&gt; JobStatusResponse:\n        \"\"\"Return a JSR carrying outcome + poll context. If `base` exists, preserve its\n        status/payload/server_error_msg unless `msg` overrides it. Otherwise, synthesize a minimal JSR.\"\"\"\n        if base is None:\n            return JobStatusResponse(\n                job_id=self.job_id,\n                outcome=outcome,\n                status=None,\n                server_error_msg=msg,\n                payload=None,\n                polls=self.poll_count,\n                elapsed_s=elapsed,\n            )\n        return JobStatusResponse(\n            job_id=self.job_id,\n            outcome=outcome,\n            status=base.status,\n            server_error_msg=msg if msg is not None else base.server_error_msg,\n            payload=base.payload,\n            polls=self.poll_count,\n            elapsed_s=elapsed,\n        )\n\n    def _on_terminal(self, jsr: JobStatusResponse, *, elapsed: float) -&gt; JobStatusResponse:\n        \"\"\"Attach poll context to a terminal server response (SUCCEEDED/FAILED).\"\"\"\n        return JobStatusResponse(\n            job_id=self.job_id,\n            outcome=PollOutcome.SUCCEEDED if jsr.status == JobStatus.SUCCEEDED else PollOutcome.FAILED,\n            status=jsr.status,\n            server_error_msg=jsr.server_error_msg,\n            payload=jsr.payload,\n            polls=self.poll_count,\n            elapsed_s=elapsed,\n        )\n\n    def _on_status_retry_exhausted(self, *, elapsed: float) -&gt; JobStatusResponse:\n        return self._attach_context(\n            self.last_status, \n            outcome=PollOutcome.NETWORK_ERROR, \n            elapsed=elapsed, \n            msg=self._last_error_reason\n            )\n\n    def _on_invalid_payload(self, *, elapsed: float) -&gt; JobStatusResponse:\n        return self._attach_context(\n            self.last_status, \n            outcome=PollOutcome.ERROR, \n            elapsed=elapsed, \n            msg=\"invalid status payload\"\n            )\n\n    def _on_timeout(self, err: RetryError, *, elapsed: float) -&gt; JobStatusResponse:\n        return self._attach_context(\n            self.last_status, \n            outcome=PollOutcome.TIMEOUT, \n            elapsed=elapsed, \n            msg=str(err)\n            )\n\n    def _on_interrupt(self, *, elapsed: float) -&gt; JobStatusResponse:\n        return self._attach_context(\n            self.last_status, \n            outcome=PollOutcome.INTERRUPTED, \n            elapsed=elapsed, \n            msg=\"KeyboardInterrupt\"\n            )\n\n    def _on_exception(self, err: Exception, *, elapsed: float) -&gt; JobStatusResponse:\n        return self._attach_context(\n            self.last_status, \n            outcome=PollOutcome.ERROR, \n            elapsed=elapsed, \n            msg=str(err)\n            )\n\n    def run(self) -&gt; JobStatusResponse:\n        try:\n            result = self._setup_and_run_poll()\n            elapsed = time.time() - self.start_time\n\n            if isinstance(result, JobStatusResponse):\n                # Terminal SUCCEEDED/FAILED (or unexpected non-terminal delivered): attach context\n                return self._on_terminal(result, elapsed=elapsed)\n\n            if result is _PollSignal.STATUS_RETRY_EXHAUSTED:\n                return self._on_status_retry_exhausted(elapsed=elapsed)\n\n            # None indicates invalid status payload or unexpected branch\n            return self._on_invalid_payload(elapsed=elapsed)\n\n        except RetryError as e:\n            # Outer polling timeout\n            elapsed = time.time() - self.start_time\n            logger.info(f\"Polling timed out for job {self.job_id} after {elapsed:.1f}s\")\n            return self._on_timeout(e, elapsed=elapsed)\n        except KeyboardInterrupt:\n            elapsed = time.time() - self.start_time\n            logger.info(f\"Polling for job {self.job_id} interrupted by user. Exiting.\")\n            return self._on_interrupt(elapsed=elapsed)\n        except Exception as e:\n            elapsed = time.time() - self.start_time\n            logger.error(f\"Polling failed for job {self.job_id}: {e}\")\n            return self._on_exception(e, elapsed=elapsed)\n\n    def _setup_and_run_poll(self) -&gt; Optional[JobStatusResponse | _PollSignal]:\n        cfg = self.polling_config\n        stop_policy = stop_never if cfg.polling_timeout is None else stop_after_delay(cfg.polling_timeout)\n        retrying = Retrying(\n            retry=retry_if_result(lambda result: result is _PollSignal.CONTINUE),\n            stop=stop_policy,\n            wait=wait_exponential_jitter(\n                exp_base=cfg.exp_base,\n                initial=cfg.initial_poll_time,\n                max=cfg.max_interval,\n            ),\n            reraise=True,\n        )\n        result = retrying(self._poll)\n        if isinstance(result, JobStatusResponse):\n            return result\n        # could be STATUS_RETRY_EXHAUSTED sentinel or None\n        logger.info(f\"Polling ended with result: {result}\")\n        return result\n</code></pre> <code>job_id = job_id</code> <code>instance-attribute</code> \u00b6 <code>last_status = None</code> <code>instance-attribute</code> \u00b6 <code>poll_count = 0</code> <code>instance-attribute</code> \u00b6 <code>polling_config = polling_config</code> <code>instance-attribute</code> \u00b6 <code>start_time = time.time()</code> <code>instance-attribute</code> \u00b6 <code>status_fn = status_fn</code> <code>instance-attribute</code> \u00b6 <code>__init__(status_fn, job_id, polling_config)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def __init__(self, status_fn, job_id: str, polling_config: PollingConfig):\n    self.status_fn = status_fn\n    self.job_id = job_id\n    self.polling_config = polling_config\n    self.poll_count = 0\n    self.start_time = time.time()\n    self.last_status: Optional[JobStatusResponse] = None\n    self._last_error_reason: Optional[str] = None\n</code></pre> <code>run()</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def run(self) -&gt; JobStatusResponse:\n    try:\n        result = self._setup_and_run_poll()\n        elapsed = time.time() - self.start_time\n\n        if isinstance(result, JobStatusResponse):\n            # Terminal SUCCEEDED/FAILED (or unexpected non-terminal delivered): attach context\n            return self._on_terminal(result, elapsed=elapsed)\n\n        if result is _PollSignal.STATUS_RETRY_EXHAUSTED:\n            return self._on_status_retry_exhausted(elapsed=elapsed)\n\n        # None indicates invalid status payload or unexpected branch\n        return self._on_invalid_payload(elapsed=elapsed)\n\n    except RetryError as e:\n        # Outer polling timeout\n        elapsed = time.time() - self.start_time\n        logger.info(f\"Polling timed out for job {self.job_id} after {elapsed:.1f}s\")\n        return self._on_timeout(e, elapsed=elapsed)\n    except KeyboardInterrupt:\n        elapsed = time.time() - self.start_time\n        logger.info(f\"Polling for job {self.job_id} interrupted by user. Exiting.\")\n        return self._on_interrupt(elapsed=elapsed)\n    except Exception as e:\n        elapsed = time.time() - self.start_time\n        logger.error(f\"Polling failed for job {self.job_id}: {e}\")\n        return self._on_exception(e, elapsed=elapsed)\n</code></pre> <code>__init__(api_key=None, config=None)</code> \u00b6 <p>Initialize with API key.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>Pyannote.ai API key (defaults to environment variable)</p> <code>None</code> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def __init__(self, api_key: Optional[str] = None, config: Optional[PyannoteConfig] = None):\n    \"\"\"\n    Initialize with API key.\n\n    Args:\n        api_key: Pyannote.ai API key (defaults to environment variable)\n    \"\"\"\n    self.api_key = api_key or os.getenv(\"PYANNOTEAI_API_TOKEN\")\n    if not self.api_key:\n        raise APIKeyError(\n            \"API key is required. Set PYANNOTEAI_API_TOKEN environment \"\n            \"variable or pass as parameter\"\n        )\n\n    self.config = config or PyannoteConfig()\n    self.polling_config = self.config.polling_config\n\n    # Upload-specific timeouts (longer than general calls)\n    self.upload_timeout = self.config.upload_timeout\n    self.upload_max_retries = self.config.upload_max_retries\n    self.network_timeout = self.config.network_timeout\n\n    self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n</code></pre> <code>check_job_status(job_id)</code> \u00b6 <p>Check the status of a diarization job.</p> <p>Returns a typed transport model (JobStatusResponse) or None on failure.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def check_job_status(self, job_id: str) -&gt; Optional[JobStatusResponse]:\n    \"\"\"\n    Check the status of a diarization job.\n\n    Returns a typed transport model (JobStatusResponse) or None on failure.\n    \"\"\"\n    return self._check_status_with_retry(job_id)\n</code></pre> <code>poll_job_until_complete(job_id, estimated_duration=None, timeout=None, wait_until_complete=False)</code> \u00b6 <p>Poll until the job reaches a terminal state or a client-side stop condition, and return a unified JobStatusResponse (JSR) that includes both the server payload and polling context via <code>outcome</code>, <code>polls</code>, and <code>elapsed_s</code>.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Remote job identifier to poll.</p> required <code>estimated_duration</code> <code>Optional[float]</code> <p>Optional hint; currently unused (reserved for adaptive backoff).</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>Optional hard timeout in seconds for this poll call. If provided, it overrides      the client's default polling timeout. Ignored if <code>wait_until_complete</code> is True.</p> <code>None</code> <code>wait_until_complete</code> <code>Optional[bool]</code> <p>If True, ignore timeout and poll indefinitely (subject to process lifetime).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>JobStatusResponse</code> <code>JobStatusResponse</code> <p>unified transport + polling-context result.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def poll_job_until_complete(\n    self,\n    job_id: str,\n    estimated_duration: Optional[float] = None,\n    timeout: Optional[float] = None,\n    wait_until_complete: Optional[bool] = False,\n) -&gt; JobStatusResponse:\n    \"\"\"\n    Poll until the job reaches a terminal state or a client-side stop condition, and\n    return a unified JobStatusResponse (JSR) that includes both the server payload\n    and polling context via `outcome`, `polls`, and `elapsed_s`.\n\n    Args:\n        job_id: Remote job identifier to poll.\n        estimated_duration: Optional hint; currently unused (reserved for adaptive backoff).\n        timeout: Optional hard timeout in seconds for this poll call. If provided, it overrides\n                 the client's default polling timeout. Ignored if `wait_until_complete` is True.\n        wait_until_complete: If True, ignore timeout and poll indefinitely (subject to process lifetime).\n\n    Returns:\n        JobStatusResponse: unified transport + polling-context result.\n    \"\"\"\n    if timeout is not None and wait_until_complete:\n        raise ConfigurationError(\"Timeout cannot be set with wait_until_complete\")\n\n    # Derive an effective timeout for this call, without mutating client defaults\n    effective_timeout = None if wait_until_complete else (\n        timeout if timeout is not None else self.polling_config.polling_timeout\n        )\n\n    cfg = PollingConfig(\n        polling_timeout=effective_timeout,\n        initial_poll_time=self.polling_config.initial_poll_time,\n        exp_base=self.polling_config.exp_base,\n        max_interval=self.polling_config.max_interval,\n    )\n\n    poller = self.JobPoller(\n        status_fn=self._check_status_with_retry,\n        job_id=job_id,\n        polling_config=cfg,\n    )\n    return poller.run()\n</code></pre> <code>start_diarization(media_id, params=None)</code> \u00b6 <p>Start diarization job with pyannote.ai API.</p> <p>Parameters:</p> Name Type Description Default <code>media_id</code> <code>str</code> <p>The media ID from upload_audio</p> required <code>params</code> <code>Optional[DiarizationParams]</code> <p>Optional parameters for diarization</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The job ID if started successfully, None otherwise</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>def start_diarization(self, media_id: str, params: Optional[DiarizationParams] = None) -&gt; Optional[str]:\n    \"\"\"\n    Start diarization job with pyannote.ai API.\n\n    Args:\n        media_id: The media ID from upload_audio\n        params: Optional parameters for diarization\n\n    Returns:\n        Optional[str]: The job ID if started successfully, None otherwise\n    \"\"\"\n    try:\n        return self._send_payload(media_id, params)\n    except requests.RequestException as e:\n        logger.error(f\"API request failed: {e}\")\n        return None\n    except ValueError as e:\n        logger.error(f\"Invalid API response: {e}\")\n        return None\n</code></pre> <code>upload_audio(file_path)</code> \u00b6 <p>Upload audio file with retry logic for network robustness.</p> <p>Retries on network errors with exponential backoff. Fails fast on permanent errors (auth, file not found, etc.).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_client.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential_jitter(exp_base=2, initial=3, max=30),\n    retry=retry_if_exception_type(\n        (requests.RequestException, requests.Timeout, requests.ConnectionError)\n        ),\n)\ndef upload_audio(self, file_path: Path) -&gt; Optional[str]:\n    \"\"\"\n    Upload audio file with retry logic for network robustness.\n\n    Retries on network errors with exponential backoff.\n    Fails fast on permanent errors (auth, file not found, etc.).\n    \"\"\"\n    try:\n        if not file_path.exists() or not file_path.is_file():\n            logger.error(f\"Audio file not found or is not a file: {file_path}\")\n            return None\n    except OSError as e:\n        logger.error(f\"Error accessing audio file '{file_path}': {e}\")\n        return None\n\n    try:\n        file_size_mb = file_path.stat().st_size / (1024 * 1024)\n    except OSError as e:\n        logger.error(f\"Error reading file size for '{file_path}': {e}\")\n        return None\n\n    logger.info(f\"Starting upload of {file_path.name} ({file_size_mb:.1f}MB)\")\n\n    try:\n        # Create media ID\n        media_id = self._create_media_id()\n        logger.debug(f\"Created media ID: {media_id}\")\n\n        # Get upload URL (this is fast, use normal timeout)\n        upload_url = self._data_upload_url(media_id)\n        if not upload_url:\n            return None\n\n        # Upload file (this is slow, use extended timeout)\n        if self._upload_file(file_path, upload_url):\n            logger.info(f\"Upload completed successfully: {media_id}\")\n            return media_id\n        else:\n            logger.error(f\"Upload failed for {file_path.name}\")\n            return None\n\n    except Exception as e:\n        # Log but don't retry - let tenacity handle retries\n        logger.error(f\"Upload attempt failed: {e}\")\n        raise  # Re-raise for tenacity to handle\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_diarize","title":"<code>pyannote_diarize</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_diarize.PYANNOTE_FILE_STR","title":"<code>PYANNOTE_FILE_STR = '_pyannote_diarization'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_diarize.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_diarize.DiarizationProcessor","title":"<code>DiarizationProcessor</code>","text":"<p>Orchestrator over a DiarizationService.</p> <p>This layer delegates to the service for generation and handles persistence.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>class DiarizationProcessor:\n    \"\"\"Orchestrator over a DiarizationService.\n\n    This layer delegates to the service for generation and handles persistence.\n    \"\"\"\n\n    def __init__(\n        self,\n        audio_file_path: Path,\n        output_path: Optional[Path] = None,\n        *,\n        service: Optional[DiarizationService] = None,\n        params: Optional[DiarizationParams] = None,\n        api_key: Optional[str] = None,\n        writer: Optional[ResultWriter] = None,\n    ) -&gt; None:\n        self.audio_file_path: Path = audio_file_path.resolve()\n        if not self.audio_file_path.exists():\n            raise FileNotFoundError(f\"Audio file not found: {audio_file_path}\")\n\n        # Default output path\n        self.output_path: Path = (\n            output_path.resolve()\n            if output_path is not None\n            else self.audio_file_path.parent / f\"{self.audio_file_path.stem}{PYANNOTE_FILE_STR}.json\"\n        )\n\n        # Service &amp; config\n        # If a concrete service is not provided, default to PyannoteService.\n        # Only pass api_key to PyannoteClient if it is not None.\n        default_client = PyannoteClient(api_key) if api_key is not None else PyannoteClient()\n        self.service: DiarizationService = service or PyannoteService(default_client)\n        self.params: Optional[DiarizationParams] = params\n        self.writer: ResultWriter = writer or FileResultWriter()\n\n        # Cached state\n        self._last_response: Optional[DiarizationResponse] = None\n        self._last_job_id: Optional[str] = None\n\n    # ---- Two-phase job control (nice for UIs) --------------------------------\n\n    def start(self) -&gt; JobHandle:\n        \"\"\"Start a job and cache its job_id.\"\"\"\n        job_id = self.service.start(self.audio_file_path, params=self.params)\n        if not job_id:\n            raise RuntimeError(\"Diarization service returned empty job_id\")\n        self._last_job_id = job_id\n        return JobHandle(job_id=job_id)\n\n    def get_response(\n        self, job: Optional[Union[JobHandle, str]] = None, *, wait_until_complete: bool = False\n        ) -&gt; DiarizationResponse:\n        \"\"\"Fetch current/final response for a job, caching the last response.\"\"\"\n        target_id: Optional[str]\n        if isinstance(job, JobHandle):\n            target_id = job.job_id\n        else:\n            target_id = job or self._last_job_id\n        if target_id is None:\n            raise ValueError(\n                \"No job_id provided and no previous job has been started. Call start() or pass a job_id.\"\n            )\n        resp = self.service.get_response(target_id, wait_until_complete=wait_until_complete)\n        self._last_response = resp\n        return resp\n\n    # ---- One-shot path --------------------------------------------------------\n\n    def generate(self, *, wait_until_complete: bool = True) -&gt; DiarizationResponse:\n        \"\"\"One-shot convenience: delegate to the service and cache the response.\"\"\"\n        resp = self.service.generate(\n            self.audio_file_path, \n            params=self.params, \n            wait_until_complete=wait_until_complete\n            )\n        self._last_response = resp\n        # If the service exposes a job_id in the envelope, cache it for UIs\n        # Do not fail on metadata issues; response is primary.\n        try:\n            job_id = getattr(resp, \"job_id\", None)\n            if isinstance(job_id, str):\n                self._last_job_id = job_id\n        except (AttributeError, TypeError) as e:\n            logger.warning(f\"Could not extract job_id from response: {e}\")\n        return resp\n\n    # ---- Persistence ----------------------------------------------------------\n\n    def export(self, response: Optional[DiarizationResponse] = None) -&gt; Path:\n        \"\"\"Write the provided or last response to `self.output_path`.\"\"\"\n        result = response or self._last_response\n        if result is None:\n            raise ValueError(\n                \"No DiarizationResponse available; call generate()/get_response() first or pass response=\"\n                )\n        return self.writer.write(self.output_path, result)\n</code></pre> <code>audio_file_path = audio_file_path.resolve()</code> <code>instance-attribute</code> \u00b6 <code>output_path = output_path.resolve() if output_path is not None else self.audio_file_path.parent / f'{self.audio_file_path.stem}{PYANNOTE_FILE_STR}.json'</code> <code>instance-attribute</code> \u00b6 <code>params = params</code> <code>instance-attribute</code> \u00b6 <code>service = service or PyannoteService(default_client)</code> <code>instance-attribute</code> \u00b6 <code>writer = writer or FileResultWriter()</code> <code>instance-attribute</code> \u00b6 <code>__init__(audio_file_path, output_path=None, *, service=None, params=None, api_key=None, writer=None)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def __init__(\n    self,\n    audio_file_path: Path,\n    output_path: Optional[Path] = None,\n    *,\n    service: Optional[DiarizationService] = None,\n    params: Optional[DiarizationParams] = None,\n    api_key: Optional[str] = None,\n    writer: Optional[ResultWriter] = None,\n) -&gt; None:\n    self.audio_file_path: Path = audio_file_path.resolve()\n    if not self.audio_file_path.exists():\n        raise FileNotFoundError(f\"Audio file not found: {audio_file_path}\")\n\n    # Default output path\n    self.output_path: Path = (\n        output_path.resolve()\n        if output_path is not None\n        else self.audio_file_path.parent / f\"{self.audio_file_path.stem}{PYANNOTE_FILE_STR}.json\"\n    )\n\n    # Service &amp; config\n    # If a concrete service is not provided, default to PyannoteService.\n    # Only pass api_key to PyannoteClient if it is not None.\n    default_client = PyannoteClient(api_key) if api_key is not None else PyannoteClient()\n    self.service: DiarizationService = service or PyannoteService(default_client)\n    self.params: Optional[DiarizationParams] = params\n    self.writer: ResultWriter = writer or FileResultWriter()\n\n    # Cached state\n    self._last_response: Optional[DiarizationResponse] = None\n    self._last_job_id: Optional[str] = None\n</code></pre> <code>export(response=None)</code> \u00b6 <p>Write the provided or last response to <code>self.output_path</code>.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def export(self, response: Optional[DiarizationResponse] = None) -&gt; Path:\n    \"\"\"Write the provided or last response to `self.output_path`.\"\"\"\n    result = response or self._last_response\n    if result is None:\n        raise ValueError(\n            \"No DiarizationResponse available; call generate()/get_response() first or pass response=\"\n            )\n    return self.writer.write(self.output_path, result)\n</code></pre> <code>generate(*, wait_until_complete=True)</code> \u00b6 <p>One-shot convenience: delegate to the service and cache the response.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def generate(self, *, wait_until_complete: bool = True) -&gt; DiarizationResponse:\n    \"\"\"One-shot convenience: delegate to the service and cache the response.\"\"\"\n    resp = self.service.generate(\n        self.audio_file_path, \n        params=self.params, \n        wait_until_complete=wait_until_complete\n        )\n    self._last_response = resp\n    # If the service exposes a job_id in the envelope, cache it for UIs\n    # Do not fail on metadata issues; response is primary.\n    try:\n        job_id = getattr(resp, \"job_id\", None)\n        if isinstance(job_id, str):\n            self._last_job_id = job_id\n    except (AttributeError, TypeError) as e:\n        logger.warning(f\"Could not extract job_id from response: {e}\")\n    return resp\n</code></pre> <code>get_response(job=None, *, wait_until_complete=False)</code> \u00b6 <p>Fetch current/final response for a job, caching the last response.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def get_response(\n    self, job: Optional[Union[JobHandle, str]] = None, *, wait_until_complete: bool = False\n    ) -&gt; DiarizationResponse:\n    \"\"\"Fetch current/final response for a job, caching the last response.\"\"\"\n    target_id: Optional[str]\n    if isinstance(job, JobHandle):\n        target_id = job.job_id\n    else:\n        target_id = job or self._last_job_id\n    if target_id is None:\n        raise ValueError(\n            \"No job_id provided and no previous job has been started. Call start() or pass a job_id.\"\n        )\n    resp = self.service.get_response(target_id, wait_until_complete=wait_until_complete)\n    self._last_response = resp\n    return resp\n</code></pre> <code>start()</code> \u00b6 <p>Start a job and cache its job_id.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def start(self) -&gt; JobHandle:\n    \"\"\"Start a job and cache its job_id.\"\"\"\n    job_id = self.service.start(self.audio_file_path, params=self.params)\n    if not job_id:\n        raise RuntimeError(\"Diarization service returned empty job_id\")\n    self._last_job_id = job_id\n    return JobHandle(job_id=job_id)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_diarize.FileResultWriter","title":"<code>FileResultWriter</code>","text":"<p>Default file-system writer to JSON.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>class FileResultWriter:\n    \"\"\"Default file-system writer to JSON.\"\"\"\n\n    def write(self, path: Path, response: DiarizationResponse) -&gt; Path:\n        ensure_directory_exists(path.parent)\n        write_str_to_file(path, response.model_dump_json(indent=2), overwrite=True)\n        logger.debug(f\"DiarizationResponse saved to {path}\")\n        return path\n</code></pre> <code>write(path, response)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def write(self, path: Path, response: DiarizationResponse) -&gt; Path:\n    ensure_directory_exists(path.parent)\n    write_str_to_file(path, response.model_dump_json(indent=2), overwrite=True)\n    logger.debug(f\"DiarizationResponse saved to {path}\")\n    return path\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_diarize.PyannoteService","title":"<code>PyannoteService</code>","text":"<p>               Bases: <code>DiarizationService</code></p> <p>Concrete implementation of DiarizationService for pyannote.ai.</p> <p>Bridges transport (PyannoteClient) and mapping (PyannoteAdapter) while exposing a clean domain-facing API.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>class PyannoteService(DiarizationService):\n    \"\"\"Concrete implementation of DiarizationService for pyannote.ai.\n\n    Bridges transport (PyannoteClient) and mapping (PyannoteAdapter) while\n    exposing a clean domain-facing API.\n    \"\"\"\n\n    def __init__(self, client: Optional[PyannoteClient] = None, adapter: Optional[PyannoteAdapter] = None):\n        self.client = client or PyannoteClient()\n        self.adapter = adapter or PyannoteAdapter()\n\n    # --- DiarizationService protocol ---\n    def start(self, audio_path: Path, params: Optional[DiarizationParams] = None) -&gt; str:\n        media_id = self.client.upload_audio(audio_path)\n        if not media_id:\n            return \"\"\n        job_id = self.client.start_diarization(media_id, params=params)\n        return job_id or \"\"\n\n    def get_response(self, job_id: str, *, wait_until_complete: bool = False) -&gt; DiarizationResponse:\n        jsr: Optional[JobStatusResponse]\n\n        jsr = self.client.poll_job_until_complete(job_id, wait_until_complete)\n\n        return self.adapter.to_response(jsr)\n\n    def generate(\n        self, \n        audio_path: Path, \n        params: Optional[DiarizationParams] = None, \n        *, \n        wait_until_complete: bool = True\n        ) -&gt; DiarizationResponse:\n        if job_id := self.start(audio_path, params=params):\n            return self.get_response(job_id, wait_until_complete=wait_until_complete)\n        return self.adapter.failed_start()\n</code></pre> <code>adapter = adapter or PyannoteAdapter()</code> <code>instance-attribute</code> \u00b6 <code>client = client or PyannoteClient()</code> <code>instance-attribute</code> \u00b6 <code>__init__(client=None, adapter=None)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def __init__(self, client: Optional[PyannoteClient] = None, adapter: Optional[PyannoteAdapter] = None):\n    self.client = client or PyannoteClient()\n    self.adapter = adapter or PyannoteAdapter()\n</code></pre> <code>generate(audio_path, params=None, *, wait_until_complete=True)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def generate(\n    self, \n    audio_path: Path, \n    params: Optional[DiarizationParams] = None, \n    *, \n    wait_until_complete: bool = True\n    ) -&gt; DiarizationResponse:\n    if job_id := self.start(audio_path, params=params):\n        return self.get_response(job_id, wait_until_complete=wait_until_complete)\n    return self.adapter.failed_start()\n</code></pre> <code>get_response(job_id, *, wait_until_complete=False)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def get_response(self, job_id: str, *, wait_until_complete: bool = False) -&gt; DiarizationResponse:\n    jsr: Optional[JobStatusResponse]\n\n    jsr = self.client.poll_job_until_complete(job_id, wait_until_complete)\n\n    return self.adapter.to_response(jsr)\n</code></pre> <code>start(audio_path, params=None)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def start(self, audio_path: Path, params: Optional[DiarizationParams] = None) -&gt; str:\n    media_id = self.client.upload_audio(audio_path)\n    if not media_id:\n        return \"\"\n    job_id = self.client.start_diarization(media_id, params=params)\n    return job_id or \"\"\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_diarize.diarize","title":"<code>diarize(audio_file_path, output_path=None, *, params=None, service=None, api_key=None, wait_until_complete=True)</code>","text":"<p>One-shot convenience to generate a result and (optionally) write it.</p> <p>This returns the <code>DiarizationResponse</code>. Writing is left to callers or <code>diarize_to_file</code> below.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def diarize(\n    audio_file_path: Path,\n    output_path: Optional[Path] = None,\n    *,\n    params: Optional[DiarizationParams] = None,\n    service: Optional[DiarizationService] = None,\n    api_key: Optional[str] = None,\n    wait_until_complete: bool = True,\n) -&gt; DiarizationResponse:\n    \"\"\"One-shot convenience to generate a result and (optionally) write it.\n\n    This returns the `DiarizationResponse`. Writing is left to callers or\n    `diarize_to_file` below.\n    \"\"\"\n    processor = DiarizationProcessor(\n        audio_file_path,\n        output_path=output_path,\n        service=service,\n        params=params,\n        api_key=api_key,\n    )\n    return processor.generate(wait_until_complete=wait_until_complete)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.pyannote_diarize.diarize_to_file","title":"<code>diarize_to_file(audio_file_path, output_path=None, *, params=None, service=None, api_key=None, wait_until_complete=True)</code>","text":"<p>Convenience helper: generate then export to JSON if successful; returns response</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/pyannote_diarize.py</code> <pre><code>def diarize_to_file(\n    audio_file_path: Path,\n    output_path: Optional[Path] = None,\n    *,\n    params: Optional[DiarizationParams] = None,\n    service: Optional[DiarizationService] = None,\n    api_key: Optional[str] = None,\n    wait_until_complete: bool = True,\n) -&gt; DiarizationResponse:\n    \"\"\"Convenience helper: generate then export to JSON if successful; returns response\"\"\"\n    processor = DiarizationProcessor(\n        audio_file_path,\n        output_path=output_path,\n        service=service,\n        params=params,\n        api_key=api_key,\n    )\n    response = processor.generate(wait_until_complete=wait_until_complete)\n    if isinstance(response, DiarizationSucceeded):\n        processor.export()\n    return response\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas","title":"<code>schemas</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.DiarizationResponse","title":"<code>DiarizationResponse = Annotated[Union[DiarizationSucceeded, DiarizationFailed, DiarizationPending, DiarizationRunning], Field(discriminator='status')]</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.__all__","title":"<code>__all__ = ['PollOutcome', 'DiarizationParams', 'StartDiarizationResponse', 'JobStatus', 'JobStatusResponse', 'ErrorCode', 'ErrorInfo', 'DiarizationResult', 'DiarizationSucceeded', 'DiarizationFailed', 'DiarizationPending', 'DiarizationRunning', 'DiarizationResponse']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.DiarizationFailed","title":"<code>DiarizationFailed</code>","text":"<p>               Bases: <code>_BaseResponse</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class DiarizationFailed(_BaseResponse):\n    status: Literal[\"failed\"]\n    error: ErrorInfo\n</code></pre> <code>error</code> <code>instance-attribute</code> \u00b6 <code>status</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.DiarizationParams","title":"<code>DiarizationParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Per-request diarization options; maps to pyannote API payload. Use .to_api_dict() to emit API field names.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class DiarizationParams(BaseModel):\n    \"\"\"\n    Per-request diarization options; maps to pyannote API payload.\n    Use .to_api_dict() to emit API field names.\n    \"\"\"\n\n    model_config = ConfigDict(\n        frozen=True,            # make instances immutable\n        populate_by_name=True,  # allow using pythonic field names with aliases\n        extra=\"forbid\",         # catch accidental fields at construction\n    )\n\n    # Pythonic attribute -&gt; API alias on dump\n    num_speakers: int | Literal[\"auto\"] | None = Field(\n        default=None,\n        alias=\"numSpeakers\",\n        description=\"Fixed number of speakers or 'auto' for detection.\",\n    )\n    confidence: float | None = Field(\n        default=None,\n        ge=0.0,\n        le=1.0,\n        description=\"Confidence threshold for segments.\",\n    )\n    webhook: AnyUrl | None = Field(\n        default=None,\n        description=\"Webhook URL for job status callbacks.\",\n    )\n\n    def to_api_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return payload dict using API field names (camelCase) and excluding Nones.\"\"\"\n        return self.model_dump(by_alias=True, exclude_none=True)\n</code></pre> <code>confidence = Field(default=None, ge=0.0, le=1.0, description='Confidence threshold for segments.')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = ConfigDict(frozen=True, populate_by_name=True, extra='forbid')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>num_speakers = Field(default=None, alias='numSpeakers', description=\"Fixed number of speakers or 'auto' for detection.\")</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>webhook = Field(default=None, description='Webhook URL for job status callbacks.')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>to_api_dict()</code> \u00b6 <p>Return payload dict using API field names (camelCase) and excluding Nones.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>def to_api_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return payload dict using API field names (camelCase) and excluding Nones.\"\"\"\n    return self.model_dump(by_alias=True, exclude_none=True)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.DiarizationPending","title":"<code>DiarizationPending</code>","text":"<p>               Bases: <code>_BaseResponse</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class DiarizationPending(_BaseResponse):\n    status: Literal[\"pending\"]\n</code></pre> <code>status</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.DiarizationResult","title":"<code>DiarizationResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Domain-level diarization payload used by the rest of the system. NOTE: <code>segments</code> is intentionally typed as <code>list[Any]</code> so that it can hold your project\u2019s <code>DiarizedSegment</code> instances from <code>models.py</code> without creating an import cycle. You can tighten this typing later to <code>list[DiarizedSegment]</code> and import under TYPE_CHECKING if desired.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class DiarizationResult(BaseModel):\n    \"\"\"\n    Domain-level diarization payload used by the rest of the system.\n    NOTE: `segments` is intentionally typed as `list[Any]` so that it can\n    hold your project\u2019s `DiarizedSegment` instances from `models.py` without\n    creating an import cycle. You can tighten this typing later to\n    `list[DiarizedSegment]` and import under TYPE_CHECKING if desired.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True, extra=\"ignore\")\n\n    segments: list[Any]\n    num_speakers: int | None = None\n    metadata: dict[str, Any] | None = None\n</code></pre> <code>metadata = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = ConfigDict(frozen=True, extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>num_speakers = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>segments</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.DiarizationRunning","title":"<code>DiarizationRunning</code>","text":"<p>               Bases: <code>_BaseResponse</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class DiarizationRunning(_BaseResponse):\n    status: Literal[\"running\"]\n</code></pre> <code>status</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.DiarizationSucceeded","title":"<code>DiarizationSucceeded</code>","text":"<p>               Bases: <code>_BaseResponse</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class DiarizationSucceeded(_BaseResponse):\n    status: Literal[\"succeeded\"]\n    result: DiarizationResult\n</code></pre> <code>result</code> <code>instance-attribute</code> \u00b6 <code>status</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.ErrorCode","title":"<code>ErrorCode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Client- and adapter-level error taxonomy (not server statuses).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class ErrorCode(str, Enum):\n    \"\"\"Client- and adapter-level error taxonomy (not server statuses).\"\"\"\n\n    TIMEOUT = \"timeout\"        # client-side polling exceeded\n    CANCELLED = \"cancelled\"      # user/client initiated cancellation\n    TRANSIENT = \"transient\"      # retryable infra/network issue\n    BAD_REQUEST = \"bad_request\"  # invalid params before hitting API\n    API_ERROR = \"api_error\"      # remote API responded with error\n    PARSE_ERROR = \"parse_error\"  # unexpected/invalid payload shape\n    UNKNOWN = \"unknown\"\n</code></pre> <code>API_ERROR = 'api_error'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>BAD_REQUEST = 'bad_request'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>CANCELLED = 'cancelled'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>PARSE_ERROR = 'parse_error'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>TIMEOUT = 'timeout'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>TRANSIENT = 'transient'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>UNKNOWN = 'unknown'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.ErrorInfo","title":"<code>ErrorInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class ErrorInfo(BaseModel):\n    model_config = ConfigDict(frozen=True, extra=\"allow\")\n\n    code: ErrorCode\n    message: str\n    details: dict[str, Any] | None = None\n</code></pre> <code>code</code> <code>instance-attribute</code> \u00b6 <code>details = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>message</code> <code>instance-attribute</code> \u00b6 <code>model_config = ConfigDict(frozen=True, extra='allow')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.JobHandle","title":"<code>JobHandle</code>  <code>dataclass</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>@dataclass(frozen=True)\nclass JobHandle:\n    job_id: str\n    backend: Literal[\"pyannote\"] = \"pyannote\"\n</code></pre> <code>backend = 'pyannote'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>job_id</code> <code>instance-attribute</code> \u00b6 <code>__init__(job_id, backend='pyannote')</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.JobStatus","title":"<code>JobStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class JobStatus(str, Enum):\n    SUCCEEDED = \"succeeded\"\n    FAILED = \"failed\"\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n</code></pre> <code>FAILED = 'failed'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>PENDING = 'pending'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>RUNNING = 'running'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>SUCCEEDED = 'succeeded'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.JobStatusResponse","title":"<code>JobStatusResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Job Status Result (JSR): unified transport payload + client polling context. Combines transport-level fields with client-side polling metadata.</p> <p>Semantics: - <code>outcome</code> describes how polling concluded (terminal success/failure, timeout, network error, etc.). - <code>status</code> is the last known server job status (<code>SUCCEEDED</code>, <code>FAILED</code>, <code>RUNNING</code>, <code>PENDING</code>) - <code>server_error_msg</code> and <code>payload</code> mirror the remote payload when present. - <code>polls</code> and <code>elapsed_s</code> report client polling metrics.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class JobStatusResponse(BaseModel):\n    \"\"\"\n    Job Status Result (JSR): unified transport payload + client polling context.\n    Combines transport-level fields with client-side polling metadata.\n\n    Semantics:\n    - `outcome` describes how polling concluded (terminal success/failure, timeout, network error, etc.).\n    - `status` is the last known *server* job status (`SUCCEEDED`, `FAILED`, `RUNNING`, `PENDING`)\n    - `server_error_msg` and `payload` mirror the remote payload when present.\n    - `polls` and `elapsed_s` report client polling metrics.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True, extra=\"ignore\")\n\n    # The job id for connected to this response\n    job_id: str\n\n    # How the client-side polling finished\n    outcome: PollOutcome\n\n    # Last known server-side status (may be None if never retrieved)\n    status: Optional[JobStatus] = None\n\n    # Transport-mirrored fields (when server responded with them)\n    server_error_msg: Optional[str] = None\n    payload: Optional[dict[str, Any]] = None\n\n    # Client-side polling metadata\n    polls: int = 0\n    elapsed_s: float = 0.0\n</code></pre> <code>elapsed_s = 0.0</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>job_id</code> <code>instance-attribute</code> \u00b6 <code>model_config = ConfigDict(frozen=True, extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>outcome</code> <code>instance-attribute</code> \u00b6 <code>payload = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>polls = 0</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>server_error_msg = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>status = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.PollOutcome","title":"<code>PollOutcome</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class PollOutcome(str, Enum):\n    SUCCEEDED = \"succeeded\"\n    FAILED = \"failed\"\n    TIMEOUT = \"timeout\"\n    NETWORK_ERROR = \"network_error\"\n    INTERRUPTED = \"interrupted\"\n    ERROR = \"error\"\n</code></pre> <code>ERROR = 'error'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>FAILED = 'failed'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>INTERRUPTED = 'interrupted'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>NETWORK_ERROR = 'network_error'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>SUCCEEDED = 'succeeded'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>TIMEOUT = 'timeout'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.schemas.StartDiarizationResponse","title":"<code>StartDiarizationResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Minimal typed view of the start-diarization response.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/schemas.py</code> <pre><code>class StartDiarizationResponse(BaseModel):\n    \"\"\"\n    Minimal typed view of the start-diarization response.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True, extra=\"ignore\")\n\n    job_id: str = Field(alias=\"jobId\")\n</code></pre> <code>job_id = Field(alias='jobId')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = ConfigDict(frozen=True, extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies","title":"<code>strategies</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies.__all__","title":"<code>__all__ = ['LanguageDetector', 'LanguageProbe', 'WhisperLanguageDetector', 'group_speaker_blocks', 'TimeGapChunker']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies.LanguageDetector","title":"<code>LanguageDetector</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Abstract language detector (e.g., fastText, Whisper-lang).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>class LanguageDetector(Protocol):\n    \"\"\"Abstract language detector (e.g., fastText, Whisper-lang).\"\"\"\n\n    def detect(self, audio: AudioSegment, format_str: str) -&gt; Optional[str]: ...\n</code></pre> <code>detect(audio, format_str)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/protocols.py</code> <pre><code>def detect(self, audio: AudioSegment, format_str: str) -&gt; Optional[str]: ...\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies.LanguageProbe","title":"<code>LanguageProbe</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>class LanguageProbe:\n    def __init__(self, config: DiarizationConfig, detector: LanguageDetector):\n        self.probe_time = config.language.probe_time\n        self.export_format = config.language.export_format\n        self.detector = detector\n\n    def segment_language(\n        self,\n        aug_segment: AugDiarizedSegment,\n    ) -&gt; str:\n        \"\"\"\n        Get segment ISO-639 language code from an Augmented Diarize Segment which contains audio.\n\n        The probe window is always relative to the segment audio (0=start, duration=end).\n        \"\"\"\n        probe_start, probe_end = self._calculate_probe_window(aug_segment)\n\n        if aug_segment.audio is None:\n            raise ValueError(f\"Segment Audio has not been set: {aug_segment}\")\n\n        # All slicing is relative to the segment audio (0 to duration)\n        audio_segment = aug_segment.audio[probe_start:probe_end]\n        language = self.detector.detect(audio_segment, self.export_format)\n\n        if language is not None:\n            return language\n        logger.warning(f\"No language detected in language probe for segment {aug_segment}.\")\n        return \"unknown\"\n\n    def _calculate_probe_window(\n        self,\n        aug_segment: AugDiarizedSegment,\n    ) -&gt; tuple[TimeMs, TimeMs]:\n        \"\"\"\n        Calculate start/end times for language probe sampling, \n        always relative to the segment audio (0 to duration).\n        \"\"\"\n        duration = aug_segment.duration\n        if duration &lt;= self.probe_time:\n            return TimeMs(0), duration\n        return self._extract_center_window(duration)\n\n    def _extract_center_window(\n        self,\n        duration: TimeMs,\n    ) -&gt; tuple[TimeMs, TimeMs]:\n        \"\"\"\n        Extract probe window from center of segment audio (relative time).\n        \"\"\"\n        center_time = duration // 2\n        half_probe = self.probe_time // 2\n\n        probe_start = TimeMs(max(0, center_time - half_probe))\n        probe_end = TimeMs(min(duration, center_time + half_probe))\n\n        return probe_start, probe_end\n</code></pre> <code>detector = detector</code> <code>instance-attribute</code> \u00b6 <code>export_format = config.language.export_format</code> <code>instance-attribute</code> \u00b6 <code>probe_time = config.language.probe_time</code> <code>instance-attribute</code> \u00b6 <code>__init__(config, detector)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>def __init__(self, config: DiarizationConfig, detector: LanguageDetector):\n    self.probe_time = config.language.probe_time\n    self.export_format = config.language.export_format\n    self.detector = detector\n</code></pre> <code>segment_language(aug_segment)</code> \u00b6 <p>Get segment ISO-639 language code from an Augmented Diarize Segment which contains audio.</p> <p>The probe window is always relative to the segment audio (0=start, duration=end).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>def segment_language(\n    self,\n    aug_segment: AugDiarizedSegment,\n) -&gt; str:\n    \"\"\"\n    Get segment ISO-639 language code from an Augmented Diarize Segment which contains audio.\n\n    The probe window is always relative to the segment audio (0=start, duration=end).\n    \"\"\"\n    probe_start, probe_end = self._calculate_probe_window(aug_segment)\n\n    if aug_segment.audio is None:\n        raise ValueError(f\"Segment Audio has not been set: {aug_segment}\")\n\n    # All slicing is relative to the segment audio (0 to duration)\n    audio_segment = aug_segment.audio[probe_start:probe_end]\n    language = self.detector.detect(audio_segment, self.export_format)\n\n    if language is not None:\n        return language\n    logger.warning(f\"No language detected in language probe for segment {aug_segment}.\")\n    return \"unknown\"\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies.TimeGapChunker","title":"<code>TimeGapChunker</code>","text":"<p>               Bases: <code>ChunkingStrategy</code></p> <p>Chunker that ignores speaker/language and uses only time-gap logic.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/time_gap.py</code> <pre><code>class TimeGapChunker(ChunkingStrategy):\n    \"\"\"Chunker that ignores speaker/language and uses only time-gap logic.\"\"\"\n\n    def __init__(self, config: DiarizationConfig = DiarizationConfig()):\n        self.cfg = config\n\n    def extract(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n        \"\"\"Extract time-based chunks from diarization segments.\"\"\"\n        if not segments:\n            return []\n\n        walker = SegmentWalker(segments)\n        accumulator = ChunkAccumulator(self.cfg)\n\n        for context in walker.walk():\n            if self._should_finalize_chunk(context, accumulator):\n                accumulator.finalize_chunk()\n\n            gap_time, gap_before = self._calculate_gap_info(context)\n            accumulator.add_segment(context.segment, gap_time, gap_before)\n\n        return accumulator.finalize_and_get_chunks()\n\n    def _should_finalize_chunk(self, context, accumulator: ChunkAccumulator) -&gt; bool:\n        \"\"\"Determine if current chunk should be finalized before adding segment.\"\"\"\n        if not accumulator.current_segments:\n            return False\n\n        gap_time, _ = self._calculate_gap_info(context)\n        projected_time = accumulator.accumulated_time + context.segment.duration + gap_time\n\n        # Don't split if remaining time would create small final chunk\n        if context.remaining_time &lt; self.cfg.chunk.min_duration:\n            return False\n\n        return projected_time &gt;= self.cfg.chunk.target_duration\n\n    def _calculate_gap_info(self, context) -&gt; tuple[TimeMs, bool]:\n        \"\"\"Calculate gap time and gap_before flag for current segment.\"\"\"\n        if context.is_first:\n            return TimeMs(0), False\n\n        gap_time = context.time_interval_prev or TimeMs(0)\n        gap_before = gap_time &gt; self.cfg.chunk.gap_threshold\n\n        # Use configured spacing for large gaps, actual gap time for small gaps\n        spacing_time = TimeMs(self.cfg.chunk.gap_spacing_time) if gap_before else gap_time\n\n        return spacing_time, gap_before\n</code></pre> <code>cfg = config</code> <code>instance-attribute</code> \u00b6 <code>__init__(config=DiarizationConfig())</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/time_gap.py</code> <pre><code>def __init__(self, config: DiarizationConfig = DiarizationConfig()):\n    self.cfg = config\n</code></pre> <code>extract(segments)</code> \u00b6 <p>Extract time-based chunks from diarization segments.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/time_gap.py</code> <pre><code>def extract(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n    \"\"\"Extract time-based chunks from diarization segments.\"\"\"\n    if not segments:\n        return []\n\n    walker = SegmentWalker(segments)\n    accumulator = ChunkAccumulator(self.cfg)\n\n    for context in walker.walk():\n        if self._should_finalize_chunk(context, accumulator):\n            accumulator.finalize_chunk()\n\n        gap_time, gap_before = self._calculate_gap_info(context)\n        accumulator.add_segment(context.segment, gap_time, gap_before)\n\n    return accumulator.finalize_and_get_chunks()\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies.WhisperLanguageDetector","title":"<code>WhisperLanguageDetector</code>","text":"<p>Language detector using Whisper service.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>class WhisperLanguageDetector:\n    \"\"\"Language detector using Whisper service.\"\"\"\n\n    def __init__(self, model: str = \"whisper-1\", audio_handler: Optional[AudioHandler] = None):\n        self.model = model\n        self.audio_handler = audio_handler or AudioHandler()\n\n    def detect(self, audio: AudioSegment, format_str: str) -&gt; Optional[str]:\n        from tnh_scholar.audio_processing.transcription.whisper_service import WhisperTranscriptionService\n        whisper = WhisperTranscriptionService(model=self.model, language=None, response_format=\"verbose_json\")\n        try:\n            audio_bytes = self.audio_handler.export_audio_bytes(audio, format_str=format_str)\n            options = patch_whisper_options(options = None, file_extension=format_str)\n            result = whisper.transcribe(audio_bytes, options=options)\n            logger.debug(f\"full transcription result: {result}\")\n            return self._extract_language_from_result(result)\n        except Exception as e:\n            logger.warning(f\"Language detection failed: {e}\")\n            return None\n\n    def _extract_language_from_result(self, result) -&gt; Optional[str]:\n        \"\"\"Extract language code from transcription result.\"\"\"\n        return getattr(result, 'language', None)\n</code></pre> <code>audio_handler = audio_handler or AudioHandler()</code> <code>instance-attribute</code> \u00b6 <code>model = model</code> <code>instance-attribute</code> \u00b6 <code>__init__(model='whisper-1', audio_handler=None)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>def __init__(self, model: str = \"whisper-1\", audio_handler: Optional[AudioHandler] = None):\n    self.model = model\n    self.audio_handler = audio_handler or AudioHandler()\n</code></pre> <code>detect(audio, format_str)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>def detect(self, audio: AudioSegment, format_str: str) -&gt; Optional[str]:\n    from tnh_scholar.audio_processing.transcription.whisper_service import WhisperTranscriptionService\n    whisper = WhisperTranscriptionService(model=self.model, language=None, response_format=\"verbose_json\")\n    try:\n        audio_bytes = self.audio_handler.export_audio_bytes(audio, format_str=format_str)\n        options = patch_whisper_options(options = None, file_extension=format_str)\n        result = whisper.transcribe(audio_bytes, options=options)\n        logger.debug(f\"full transcription result: {result}\")\n        return self._extract_language_from_result(result)\n    except Exception as e:\n        logger.warning(f\"Language detection failed: {e}\")\n        return None\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies.group_speaker_blocks","title":"<code>group_speaker_blocks(segments, config=DiarizationConfig())</code>","text":"<p>Group contiguous or near-contiguous segments by speaker identity.</p> <p>Segments are grouped into <code>SpeakerBlock</code>s when the speaker remains the same and the gap between consecutive segments is less than the configured threshold.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>List[DiarizedSegment]</code> <p>A list of diarization segments (must be sorted by start time).</p> required <code>config</code> <code>DiarizationConfig</code> <p>Configuration containing the allowed gap between segments.</p> <code>DiarizationConfig()</code> <p>Returns:</p> Type Description <code>List[SpeakerBlock]</code> <p>A list of SpeakerBlock objects representing grouped speaker runs.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/speaker_blocker.py</code> <pre><code>def group_speaker_blocks(\n    segments: List[DiarizedSegment],\n    config: DiarizationConfig = DiarizationConfig()\n) -&gt; List[SpeakerBlock]:\n    \"\"\"Group contiguous or near-contiguous segments by speaker identity.\n\n    Segments are grouped into `SpeakerBlock`s when the speaker remains the same\n    and the gap between consecutive segments is less than the configured threshold.\n\n    Parameters:\n        segments: A list of diarization segments (must be sorted by start time).\n        config: Configuration containing the allowed gap between segments.\n\n    Returns:\n        A list of SpeakerBlock objects representing grouped speaker runs.\n    \"\"\"\n    if not segments:\n        return []\n\n    blocks: List[SpeakerBlock] = []\n    buffer: List[DiarizedSegment] = [segments[0]]\n\n    gap_threshold = config.speaker.same_speaker_gap_threshold\n\n    for current in segments[1:]:\n        previous = buffer[-1]\n        same_speaker = current.speaker == previous.speaker\n        gap = current.start - previous.end\n\n        if same_speaker and gap &lt;= gap_threshold:\n            buffer.append(current)\n        else:\n            blocks.append(SpeakerBlock(speaker=buffer[0].speaker, segments=buffer))\n            buffer = [current]\n\n    if buffer:\n        blocks.append(SpeakerBlock(speaker=buffer[0].speaker, segments=buffer))\n\n    return blocks\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies.language_based","title":"<code>language_based</code>","text":"<p>LanguageChunker \u2013 chunking informed by speaker blocks + language probing.</p> <code>logger = get_child_logger(__name__)</code> <code>module-attribute</code> \u00b6 <code>LanguageChunker</code> \u00b6 <p>               Bases: <code>ChunkingStrategy</code></p> <p>Strategy:</p> <ol> <li>Group contiguous segments into SpeakerBlock objects.</li> <li>For each block longer than <code>language_probe_threshold</code> probe language    at configurable offsets; if mismatch, split on language change.</li> <li>Build chunks respecting <code>target_time</code> similar to TimeGapChunker.</li> </ol> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_based.py</code> <pre><code>class LanguageChunker(ChunkingStrategy):\n    \"\"\"\n    Strategy:\n\n    1. Group contiguous segments into SpeakerBlock objects.\n    2. For each block longer than ``language_probe_threshold`` probe language\n       at configurable offsets; if mismatch, split on language change.\n    3. Build chunks respecting ``target_time`` similar to TimeGapChunker.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: ChunkConfig = ChunkConfig(),\n        fetcher: AudioFetcher | None = None,\n        detector: LanguageDetector | None = None,\n        language_probe_threshold: TimeMs = TimeMs(90_000),\n    ):\n        self.cfg = cfg\n        self.fetcher = fetcher\n        self.detector = detector\n        self.lang_thresh = language_probe_threshold\n\n    def extract(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n        if not segments:\n            return []\n\n        blocks = group_speaker_blocks(\n            segments, config=self.cfg.speaker_block\n        )  # attribute from DiarizationConfig\n        # Optionally split blocks on language change\n        enriched_segments: List[DiarizedSegment] = []\n        for block in blocks:\n            if block.duration &gt;= self.lang_thresh and self.fetcher and self.detector:\n                enriched_segments.extend(self._split_block_on_language(block))\n            else:\n                enriched_segments.extend(block.segments)\n\n        # Now fall back to pure time-gap chunking\n        from .time_gap import TimeGapChunker\n\n        return TimeGapChunker(self.cfg).extract(enriched_segments)\n\n\n    def _split_block_on_language(self, block):\n        \"\"\"\n        Probe language at 25% and 75% of block; if mismatch, split.\n        Very naive \u2013 replace with richer algorithm later.\n        \"\"\"\n        assert self.fetcher and self.detector  # guaranteed by caller\n        first_seg = block.segments[0]\n        last_seg = block.segments[-1]\n        quarter_point = first_seg.start + (block.duration // 4)\n        three_quarter = first_seg.start + (block.duration * 3 // 4)\n\n        probe_segs = [self._segment_at(block, quarter_point),\n                      self._segment_at(block, three_quarter)]\n\n        langs = {probe_segment_language(s, self.fetcher, self.detector) for s in probe_segs}\n\n        if len(langs) &lt;= 1:\n            return block.segments  # All one language\n\n        # Language split \u2192 naively split at midpoint\n        midpoint_ms = block.start + (block.duration // 2)\n        left, right = [], []\n        for seg in block.segments:\n            (left if seg.end &lt;= midpoint_ms else right).append(seg)\n\n        return left + right\n\n    def _segment_at(self, block, ms):\n        \"\"\"Return the first segment covering the given ms offset.\"\"\"\n        for seg in block.segments:\n            if seg.start &lt;= ms &lt; seg.end:\n                return seg\n        return block.segments[0]  # fallback\n</code></pre> <code>cfg = cfg</code> <code>instance-attribute</code> \u00b6 <code>detector = detector</code> <code>instance-attribute</code> \u00b6 <code>fetcher = fetcher</code> <code>instance-attribute</code> \u00b6 <code>lang_thresh = language_probe_threshold</code> <code>instance-attribute</code> \u00b6 <code>__init__(cfg=ChunkConfig(), fetcher=None, detector=None, language_probe_threshold=TimeMs(90000))</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_based.py</code> <pre><code>def __init__(\n    self,\n    cfg: ChunkConfig = ChunkConfig(),\n    fetcher: AudioFetcher | None = None,\n    detector: LanguageDetector | None = None,\n    language_probe_threshold: TimeMs = TimeMs(90_000),\n):\n    self.cfg = cfg\n    self.fetcher = fetcher\n    self.detector = detector\n    self.lang_thresh = language_probe_threshold\n</code></pre> <code>extract(segments)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_based.py</code> <pre><code>def extract(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n    if not segments:\n        return []\n\n    blocks = group_speaker_blocks(\n        segments, config=self.cfg.speaker_block\n    )  # attribute from DiarizationConfig\n    # Optionally split blocks on language change\n    enriched_segments: List[DiarizedSegment] = []\n    for block in blocks:\n        if block.duration &gt;= self.lang_thresh and self.fetcher and self.detector:\n            enriched_segments.extend(self._split_block_on_language(block))\n        else:\n            enriched_segments.extend(block.segments)\n\n    # Now fall back to pure time-gap chunking\n    from .time_gap import TimeGapChunker\n\n    return TimeGapChunker(self.cfg).extract(enriched_segments)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies.language_probe","title":"<code>language_probe</code>","text":"<p>Lightweight language-detection helpers pluggable into chunkers.</p> <code>logger = get_child_logger(__name__)</code> <code>module-attribute</code> \u00b6 <code>LanguageProbe</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>class LanguageProbe:\n    def __init__(self, config: DiarizationConfig, detector: LanguageDetector):\n        self.probe_time = config.language.probe_time\n        self.export_format = config.language.export_format\n        self.detector = detector\n\n    def segment_language(\n        self,\n        aug_segment: AugDiarizedSegment,\n    ) -&gt; str:\n        \"\"\"\n        Get segment ISO-639 language code from an Augmented Diarize Segment which contains audio.\n\n        The probe window is always relative to the segment audio (0=start, duration=end).\n        \"\"\"\n        probe_start, probe_end = self._calculate_probe_window(aug_segment)\n\n        if aug_segment.audio is None:\n            raise ValueError(f\"Segment Audio has not been set: {aug_segment}\")\n\n        # All slicing is relative to the segment audio (0 to duration)\n        audio_segment = aug_segment.audio[probe_start:probe_end]\n        language = self.detector.detect(audio_segment, self.export_format)\n\n        if language is not None:\n            return language\n        logger.warning(f\"No language detected in language probe for segment {aug_segment}.\")\n        return \"unknown\"\n\n    def _calculate_probe_window(\n        self,\n        aug_segment: AugDiarizedSegment,\n    ) -&gt; tuple[TimeMs, TimeMs]:\n        \"\"\"\n        Calculate start/end times for language probe sampling, \n        always relative to the segment audio (0 to duration).\n        \"\"\"\n        duration = aug_segment.duration\n        if duration &lt;= self.probe_time:\n            return TimeMs(0), duration\n        return self._extract_center_window(duration)\n\n    def _extract_center_window(\n        self,\n        duration: TimeMs,\n    ) -&gt; tuple[TimeMs, TimeMs]:\n        \"\"\"\n        Extract probe window from center of segment audio (relative time).\n        \"\"\"\n        center_time = duration // 2\n        half_probe = self.probe_time // 2\n\n        probe_start = TimeMs(max(0, center_time - half_probe))\n        probe_end = TimeMs(min(duration, center_time + half_probe))\n\n        return probe_start, probe_end\n</code></pre> <code>detector = detector</code> <code>instance-attribute</code> \u00b6 <code>export_format = config.language.export_format</code> <code>instance-attribute</code> \u00b6 <code>probe_time = config.language.probe_time</code> <code>instance-attribute</code> \u00b6 <code>__init__(config, detector)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>def __init__(self, config: DiarizationConfig, detector: LanguageDetector):\n    self.probe_time = config.language.probe_time\n    self.export_format = config.language.export_format\n    self.detector = detector\n</code></pre> <code>segment_language(aug_segment)</code> \u00b6 <p>Get segment ISO-639 language code from an Augmented Diarize Segment which contains audio.</p> <p>The probe window is always relative to the segment audio (0=start, duration=end).</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>def segment_language(\n    self,\n    aug_segment: AugDiarizedSegment,\n) -&gt; str:\n    \"\"\"\n    Get segment ISO-639 language code from an Augmented Diarize Segment which contains audio.\n\n    The probe window is always relative to the segment audio (0=start, duration=end).\n    \"\"\"\n    probe_start, probe_end = self._calculate_probe_window(aug_segment)\n\n    if aug_segment.audio is None:\n        raise ValueError(f\"Segment Audio has not been set: {aug_segment}\")\n\n    # All slicing is relative to the segment audio (0 to duration)\n    audio_segment = aug_segment.audio[probe_start:probe_end]\n    language = self.detector.detect(audio_segment, self.export_format)\n\n    if language is not None:\n        return language\n    logger.warning(f\"No language detected in language probe for segment {aug_segment}.\")\n    return \"unknown\"\n</code></pre> <code>WhisperLanguageDetector</code> \u00b6 <p>Language detector using Whisper service.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>class WhisperLanguageDetector:\n    \"\"\"Language detector using Whisper service.\"\"\"\n\n    def __init__(self, model: str = \"whisper-1\", audio_handler: Optional[AudioHandler] = None):\n        self.model = model\n        self.audio_handler = audio_handler or AudioHandler()\n\n    def detect(self, audio: AudioSegment, format_str: str) -&gt; Optional[str]:\n        from tnh_scholar.audio_processing.transcription.whisper_service import WhisperTranscriptionService\n        whisper = WhisperTranscriptionService(model=self.model, language=None, response_format=\"verbose_json\")\n        try:\n            audio_bytes = self.audio_handler.export_audio_bytes(audio, format_str=format_str)\n            options = patch_whisper_options(options = None, file_extension=format_str)\n            result = whisper.transcribe(audio_bytes, options=options)\n            logger.debug(f\"full transcription result: {result}\")\n            return self._extract_language_from_result(result)\n        except Exception as e:\n            logger.warning(f\"Language detection failed: {e}\")\n            return None\n\n    def _extract_language_from_result(self, result) -&gt; Optional[str]:\n        \"\"\"Extract language code from transcription result.\"\"\"\n        return getattr(result, 'language', None)\n</code></pre> <code>audio_handler = audio_handler or AudioHandler()</code> <code>instance-attribute</code> \u00b6 <code>model = model</code> <code>instance-attribute</code> \u00b6 <code>__init__(model='whisper-1', audio_handler=None)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>def __init__(self, model: str = \"whisper-1\", audio_handler: Optional[AudioHandler] = None):\n    self.model = model\n    self.audio_handler = audio_handler or AudioHandler()\n</code></pre> <code>detect(audio, format_str)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/language_probe.py</code> <pre><code>def detect(self, audio: AudioSegment, format_str: str) -&gt; Optional[str]:\n    from tnh_scholar.audio_processing.transcription.whisper_service import WhisperTranscriptionService\n    whisper = WhisperTranscriptionService(model=self.model, language=None, response_format=\"verbose_json\")\n    try:\n        audio_bytes = self.audio_handler.export_audio_bytes(audio, format_str=format_str)\n        options = patch_whisper_options(options = None, file_extension=format_str)\n        result = whisper.transcribe(audio_bytes, options=options)\n        logger.debug(f\"full transcription result: {result}\")\n        return self._extract_language_from_result(result)\n    except Exception as e:\n        logger.warning(f\"Language detection failed: {e}\")\n        return None\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies.speaker_blocker","title":"<code>speaker_blocker</code>","text":"<code>group_speaker_blocks(segments, config=DiarizationConfig())</code> \u00b6 <p>Group contiguous or near-contiguous segments by speaker identity.</p> <p>Segments are grouped into <code>SpeakerBlock</code>s when the speaker remains the same and the gap between consecutive segments is less than the configured threshold.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>List[DiarizedSegment]</code> <p>A list of diarization segments (must be sorted by start time).</p> required <code>config</code> <code>DiarizationConfig</code> <p>Configuration containing the allowed gap between segments.</p> <code>DiarizationConfig()</code> <p>Returns:</p> Type Description <code>List[SpeakerBlock]</code> <p>A list of SpeakerBlock objects representing grouped speaker runs.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/speaker_blocker.py</code> <pre><code>def group_speaker_blocks(\n    segments: List[DiarizedSegment],\n    config: DiarizationConfig = DiarizationConfig()\n) -&gt; List[SpeakerBlock]:\n    \"\"\"Group contiguous or near-contiguous segments by speaker identity.\n\n    Segments are grouped into `SpeakerBlock`s when the speaker remains the same\n    and the gap between consecutive segments is less than the configured threshold.\n\n    Parameters:\n        segments: A list of diarization segments (must be sorted by start time).\n        config: Configuration containing the allowed gap between segments.\n\n    Returns:\n        A list of SpeakerBlock objects representing grouped speaker runs.\n    \"\"\"\n    if not segments:\n        return []\n\n    blocks: List[SpeakerBlock] = []\n    buffer: List[DiarizedSegment] = [segments[0]]\n\n    gap_threshold = config.speaker.same_speaker_gap_threshold\n\n    for current in segments[1:]:\n        previous = buffer[-1]\n        same_speaker = current.speaker == previous.speaker\n        gap = current.start - previous.end\n\n        if same_speaker and gap &lt;= gap_threshold:\n            buffer.append(current)\n        else:\n            blocks.append(SpeakerBlock(speaker=buffer[0].speaker, segments=buffer))\n            buffer = [current]\n\n    if buffer:\n        blocks.append(SpeakerBlock(speaker=buffer[0].speaker, segments=buffer))\n\n    return blocks\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.strategies.time_gap","title":"<code>time_gap</code>","text":"<p>TimeGapChunker \u2013 baseline strategy: split purely on accumulated time.</p> <code>logger = get_child_logger(__name__)</code> <code>module-attribute</code> \u00b6 <code>TimeGapChunker</code> \u00b6 <p>               Bases: <code>ChunkingStrategy</code></p> <p>Chunker that ignores speaker/language and uses only time-gap logic.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/time_gap.py</code> <pre><code>class TimeGapChunker(ChunkingStrategy):\n    \"\"\"Chunker that ignores speaker/language and uses only time-gap logic.\"\"\"\n\n    def __init__(self, config: DiarizationConfig = DiarizationConfig()):\n        self.cfg = config\n\n    def extract(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n        \"\"\"Extract time-based chunks from diarization segments.\"\"\"\n        if not segments:\n            return []\n\n        walker = SegmentWalker(segments)\n        accumulator = ChunkAccumulator(self.cfg)\n\n        for context in walker.walk():\n            if self._should_finalize_chunk(context, accumulator):\n                accumulator.finalize_chunk()\n\n            gap_time, gap_before = self._calculate_gap_info(context)\n            accumulator.add_segment(context.segment, gap_time, gap_before)\n\n        return accumulator.finalize_and_get_chunks()\n\n    def _should_finalize_chunk(self, context, accumulator: ChunkAccumulator) -&gt; bool:\n        \"\"\"Determine if current chunk should be finalized before adding segment.\"\"\"\n        if not accumulator.current_segments:\n            return False\n\n        gap_time, _ = self._calculate_gap_info(context)\n        projected_time = accumulator.accumulated_time + context.segment.duration + gap_time\n\n        # Don't split if remaining time would create small final chunk\n        if context.remaining_time &lt; self.cfg.chunk.min_duration:\n            return False\n\n        return projected_time &gt;= self.cfg.chunk.target_duration\n\n    def _calculate_gap_info(self, context) -&gt; tuple[TimeMs, bool]:\n        \"\"\"Calculate gap time and gap_before flag for current segment.\"\"\"\n        if context.is_first:\n            return TimeMs(0), False\n\n        gap_time = context.time_interval_prev or TimeMs(0)\n        gap_before = gap_time &gt; self.cfg.chunk.gap_threshold\n\n        # Use configured spacing for large gaps, actual gap time for small gaps\n        spacing_time = TimeMs(self.cfg.chunk.gap_spacing_time) if gap_before else gap_time\n\n        return spacing_time, gap_before\n</code></pre> <code>cfg = config</code> <code>instance-attribute</code> \u00b6 <code>__init__(config=DiarizationConfig())</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/time_gap.py</code> <pre><code>def __init__(self, config: DiarizationConfig = DiarizationConfig()):\n    self.cfg = config\n</code></pre> <code>extract(segments)</code> \u00b6 <p>Extract time-based chunks from diarization segments.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/strategies/time_gap.py</code> <pre><code>def extract(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n    \"\"\"Extract time-based chunks from diarization segments.\"\"\"\n    if not segments:\n        return []\n\n    walker = SegmentWalker(segments)\n    accumulator = ChunkAccumulator(self.cfg)\n\n    for context in walker.walk():\n        if self._should_finalize_chunk(context, accumulator):\n            accumulator.finalize_chunk()\n\n        gap_time, gap_before = self._calculate_gap_info(context)\n        accumulator.add_segment(context.segment, gap_time, gap_before)\n\n    return accumulator.finalize_and_get_chunks()\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.timeline_mapper","title":"<code>timeline_mapper</code>","text":"<p>Timeline mapping utilities for transforming timestamps from chunk-relative coordinates to original audio coordinates.</p> <p>This module enables mapping transcript segments back to their original positions in the source audio after processing chunked audio.</p>"},{"location":"api/#tnh_scholar.audio_processing.diarization.timeline_mapper.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.timeline_mapper.TimelineMapper","title":"<code>TimelineMapper</code>","text":"<p>Maps timestamps from chunk-relative coordinates to original audio coordinates.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/timeline_mapper.py</code> <pre><code>class TimelineMapper:\n    \"\"\"Maps timestamps from chunk-relative coordinates to original audio coordinates.\"\"\"\n\n    def __init__(self, config: Optional[TimelineMapperConfig] = None):\n        \"\"\"Initialize with optional configuration.\"\"\"\n        self.config = config or TimelineMapperConfig()\n\n    def remap(self, timed_text: TimedText, chunk: DiarizationChunk) -&gt; TimedText:\n        \"\"\"\n        Remap all timestamps in a TimedText object from chunk-relative to original audio coordinates.\n\n        Args:\n            timed_text: TimedText with chunk-relative timestamps\n            chunk: DiarizationChunk containing mapping information\n\n        Returns:\n            New TimedText object with remapped timestamps\n        \"\"\"\n\n        self._validate_diarize_segments(chunk)\n        mapper = self._TimeUnitMapper(chunk.segments, self.config)\n        self._validate_timed_text(timed_text)    \n\n        if timed_text.segments:    \n            timed_text = mapper.map_timed_text(timed_text)\n\n        if timed_text.words:\n            timed_text = mapper.map_timed_text(timed_text)\n\n        return timed_text\n\n    def _validate_diarize_segments(self, chunk: DiarizationChunk):\n        if not (segments:=chunk.segments):\n            logger.error(\"Empty segments.\")\n            raise ValueError(\"Cannot remap with empty chunk segments.\")\n\n        # Validate segments\n        for segment in segments:\n            if segment.audio_map_start is None:\n                raise ValueError(f\"Remap not possible. Segment {segment} is missing audio_map_time.\")\n            segment.normalize() \n\n    def _validate_timed_text(self, timed_text: TimedText):\n        timed_text.sort_by_start()\n        for timed_unit in timed_text.iter():\n            timed_unit.normalize()\n\n    class _TimeUnitMapper:\n        \"\"\"Internal helper class for time-unit mapping.\"\"\"\n\n        def __init__(self, map_segments: List[DiarizedSegment], config: TimelineMapperConfig):\n            self.map_segments = map_segments\n            self.config = config            \n\n        def map_timed_text(self, tt: TimedText) -&gt; TimedText:\n            \"\"\"Map timestamps in all TimedTextUnit collections contained in the TimedText object.\"\"\"\n            new_tt = tt.model_copy(deep=True)\n\n            sources: List[Tuple[str, List[TimedTextUnit]]] = []\n            if tt.is_segment_granularity():\n                sources.append((\"segments\", tt.segments))\n            if tt.is_word_granularity():\n                sources.append((\"words\", tt.words))\n\n            for attr, units in sources:\n                mapped_units = [self._map_text_unit(u) for u in units]\n                setattr(new_tt, attr, mapped_units)\n\n            return new_tt\n\n        def _map_text_unit(self, unit: TimedTextUnit) -&gt; TimedTextUnit:\n            \"\"\"Map a single TimedTextUnit's timestamps.\"\"\"\n            # Find the best matching segment\n            best_segment = self._find_best_segment(unit)\n\n            # Debug logging for mapping decision\n            if self.config.debug_logging:\n                self._log_mapping_choice(unit, best_segment)\n\n            # Apply mapping transformation and return new unit\n            return self._apply_mapping(\n                unit,\n                best_segment\n            )\n\n\n        def _log_mapping_choice(self, unit, segment):\n            logger.info(\n                    f\"Mapping unit (start: {unit.start_ms}, end: {unit.end_ms}) \"\n                    f\"to segment (start: {segment.start}, end: {segment.end}, \"\n                    f\"mapped_start: {segment.mapped_start}, mapped_end: {segment.mapped_end})\"\n                )\n\n        def _find_best_segment(self, unit: TimedTextUnit) -&gt; DiarizedSegment:\n            \"\"\"\n            Find the best segment to use for mapping a TimedTextUnit.\n\n            First tries to find segments with direct overlap.\n            If none, finds proximal segments and chooses the closest.\n            \"\"\"\n            if overlapping := self._find_overlapping_segments(unit):\n                return self._choose_best_overlap(unit, overlapping)\n\n            # If no overlaps, find proximal segments\n            before, after = self._find_proximal_segments(unit)\n\n            if before is None and after is None:\n                raise ValueError(\"A before or after segment was not found.\")\n\n            if before is None:\n                return after # type: ignore\n            if after is None:\n                return before\n\n            # Choose closest proximal segment\n            return self._choose_closest_proximal(unit, before, after)\n\n        def _find_overlapping_segments(self, unit: TimedTextUnit) -&gt; List[DiarizedSegment]:\n            \"\"\"Find all segments that overlap with the given unit.\"\"\"\n            return [\n                segment for segment in self.map_segments\n                if (segment.mapped_start &lt;= unit.end_ms and\n                    segment.mapped_end &gt;= unit.start_ms)\n            ]\n\n        def _choose_best_overlap(\n            self, \n            unit: TimedTextUnit, \n            candidates: List[DiarizedSegment]\n        ) -&gt; DiarizedSegment:\n            \"\"\"Choose the segment with the largest overlap with the unit.\"\"\"\n            best_segment = candidates[0]\n            best_overlap = self._calculate_overlap(unit, best_segment)\n\n            for segment in candidates[1:]:\n                overlap = self._calculate_overlap(unit, segment)\n                if overlap &gt; best_overlap:\n                    best_overlap = overlap\n                    best_segment = segment\n\n            return best_segment\n\n        def _calculate_overlap(\n            self, \n            unit: TimedTextUnit, \n            segment: DiarizedSegment\n        ) -&gt; int:\n            \"\"\"Calculate the amount of overlap between a unit and a segment in milliseconds.\"\"\"     \n            overlap_start = max(unit.start_ms, segment.mapped_start)\n            overlap_end = min(unit.end_ms, segment.mapped_end)\n\n            return max(0, overlap_end - overlap_start)\n\n        def _find_proximal_segments(\n            self, \n            unit: TimedTextUnit\n        ) -&gt; Tuple[Optional[DiarizedSegment], Optional[DiarizedSegment]]:\n            \"\"\"Find the nearest segments before and after the unit.\"\"\"\n            before = None\n            before_end = float('-inf')\n            after = None\n            after_start = float('inf')\n\n            for segment in self.map_segments:\n                # Check if segment ends before unit starts\n                if segment.mapped_end &lt;= unit.start_ms and segment.mapped_end &gt; before_end:\n                    before = segment\n                    before_end = segment.mapped_end\n\n                # Check if segment starts after unit ends\n                if segment.mapped_start &gt;= unit.end_ms and segment.mapped_start &lt; after_start:\n                    after = segment\n                    after_start = segment.mapped_start\n\n            if not (before or after):\n                raise ValueError(\"Before or after segments not found.\")\n\n            return before, after\n\n        def _choose_closest_proximal(\n            self,\n            unit: TimedTextUnit,\n            before: DiarizedSegment,\n            after: DiarizedSegment\n        ) -&gt; DiarizedSegment:\n            \"\"\"\n            Choose the closest proximal segment based on gap distance.\n            Requires both before and after segments (cannot be None)\n            \"\"\"\n            before_gap = unit.start_ms - before.mapped_end\n            after_gap = after.mapped_start - unit.end_ms\n\n            # Choose segment with smaller gap\n            return before if before_gap &lt;= after_gap else after\n\n        def _apply_mapping(\n            self, \n            unit: TimedTextUnit,\n            segment: DiarizedSegment\n        ) -&gt; TimedTextUnit:\n            \"\"\"\n            Apply the timeline mapping transformation.\n\n            Maps unit timestamps from chunk-relative to original timeline.\n            Returns the mapped (start_ms, end_ms) tuple.\n            \"\"\"\n            # Calculate offset from segment's local start\n            offset = unit.start_ms - segment.mapped_start\n\n            # Apply offset to original timeline\n            new_unit_start = segment.start + offset\n\n            # Preserve duration\n            duration = unit.duration_ms\n            new_unit_end = new_unit_start + duration\n\n            unit = unit.model_copy(\n                update={\"start_ms\": new_unit_start, \"end_ms\": new_unit_end}\n            )\n\n            if self.config.map_speakers:\n                unit.set_speaker(segment.speaker)\n\n            return unit\n</code></pre> <code>config = config or TimelineMapperConfig()</code> <code>instance-attribute</code> \u00b6 <code>__init__(config=None)</code> \u00b6 <p>Initialize with optional configuration.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/timeline_mapper.py</code> <pre><code>def __init__(self, config: Optional[TimelineMapperConfig] = None):\n    \"\"\"Initialize with optional configuration.\"\"\"\n    self.config = config or TimelineMapperConfig()\n</code></pre> <code>remap(timed_text, chunk)</code> \u00b6 <p>Remap all timestamps in a TimedText object from chunk-relative to original audio coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>timed_text</code> <code>TimedText</code> <p>TimedText with chunk-relative timestamps</p> required <code>chunk</code> <code>DiarizationChunk</code> <p>DiarizationChunk containing mapping information</p> required <p>Returns:</p> Type Description <code>TimedText</code> <p>New TimedText object with remapped timestamps</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/timeline_mapper.py</code> <pre><code>def remap(self, timed_text: TimedText, chunk: DiarizationChunk) -&gt; TimedText:\n    \"\"\"\n    Remap all timestamps in a TimedText object from chunk-relative to original audio coordinates.\n\n    Args:\n        timed_text: TimedText with chunk-relative timestamps\n        chunk: DiarizationChunk containing mapping information\n\n    Returns:\n        New TimedText object with remapped timestamps\n    \"\"\"\n\n    self._validate_diarize_segments(chunk)\n    mapper = self._TimeUnitMapper(chunk.segments, self.config)\n    self._validate_timed_text(timed_text)    \n\n    if timed_text.segments:    \n        timed_text = mapper.map_timed_text(timed_text)\n\n    if timed_text.words:\n        timed_text = mapper.map_timed_text(timed_text)\n\n    return timed_text\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.timeline_mapper.TimelineMapperConfig","title":"<code>TimelineMapperConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration options for timeline mapping.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/timeline_mapper.py</code> <pre><code>class TimelineMapperConfig(BaseModel):\n    \"\"\"Configuration options for timeline mapping.\"\"\"\n\n    debug_logging: bool = Field(\n        default=False,\n        description=\"Enable detailed logging of mapping decisions\"\n    )\n    map_speakers: bool = Field(\n        default=True,\n        description=\"Assign speaker to mapped timings using diarization segment speaker.\"\n    )\n</code></pre> <code>debug_logging = Field(default=False, description='Enable detailed logging of mapping decisions')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>map_speakers = Field(default=True, description='Assign speaker to mapped timings using diarization segment speaker.')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.types","title":"<code>types</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.types.PyannoteEntry","title":"<code>PyannoteEntry</code>","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/tnh_scholar/audio_processing/diarization/types.py</code> <pre><code>class PyannoteEntry(TypedDict):\n    speaker: str\n    start: float  # seconds\n    end: float    # seconds\n</code></pre> <code>end</code> <code>instance-attribute</code> \u00b6 <code>speaker</code> <code>instance-attribute</code> \u00b6 <code>start</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.diarization.viewer","title":"<code>viewer</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.diarization.viewer.close_segment_viewer","title":"<code>close_segment_viewer(pid)</code>","text":"<p>Terminate the Streamlit viewer process by PID.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/viewer.py</code> <pre><code>def close_segment_viewer(pid: int):\n    \"\"\"Terminate the Streamlit viewer process by PID.\"\"\"\n    try:\n        os.kill(pid, signal.SIGTERM)\n        print(f\"Closed Streamlit viewer (PID {pid})\")\n    except Exception as e:\n        print(f\"Failed to close Streamlit viewer (PID {pid}): {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.viewer.launch_segment_viewer","title":"<code>launch_segment_viewer(segments, master_audio_file)</code>","text":"<p>Export segment data to a temporary JSON file and launch Streamlit viewer. Args:     segments: List of dicts with diarization info (start, end, speaker).     master_audio_file: Path to the master audio file.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/viewer.py</code> <pre><code>def launch_segment_viewer(segments: List[SpeakerBlock], master_audio_file: Path):\n    \"\"\"\n    Export segment data to a temporary JSON file and launch Streamlit viewer.\n    Args:\n        segments: List of dicts with diarization info (start, end, speaker).\n        master_audio_file: Path to the master audio file.\n    \"\"\"\n    # Attach master audio file path to metadata\n    meta = {\"master_audio\": str(master_audio_file)}\n    serial_segments = [segment.to_dict() for segment in segments]\n    payload = {\"segments\": serial_segments, \"meta\": meta}\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False) as f:\n        json.dump(payload, f)\n        temp_path = f.name\n    cmd = [sys.executable, \"-m\", \"streamlit\", \"run\", str(Path(__file__).resolve()), \"--\", temp_path]\n    print(f\"Launching Streamlit viewer with data: {temp_path}\")\n    proc = subprocess.Popen(cmd)\n    return proc.pid\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.viewer.load_segments_from_file","title":"<code>load_segments_from_file(path)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/diarization/viewer.py</code> <pre><code>def load_segments_from_file(path):\n    with open(path, \"r\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.diarization.viewer.main","title":"<code>main()</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/diarization/viewer.py</code> <pre><code>def main():\n    # If a data file is passed as argument, load it\n    segments = None\n    meta = None\n    error_msg = None\n    if len(sys.argv) &gt; 1 and os.path.exists(sys.argv[-1]):\n        try:\n            payload = load_segments_from_file(sys.argv[-1])\n            segments = payload.get(\"segments\")\n            meta = payload.get(\"meta\")\n        except Exception as e:\n            error_msg = f\"Failed to load segment data: {e}\"\n    else:\n        st.error(\"No segment data file provided. This viewer requires explicit segment and audio file input.\")\n        st.stop()\n\n    if error_msg:\n        st.error(error_msg)\n        st.stop()\n\n    if not segments or not meta or not meta.get(\"master_audio\"):\n        st.error(\"Segments and master audio file must be provided.\")\n        st.stop()\n\n    master_audio_path = meta[\"master_audio\"]\n\n    # --- Deserialize SpeakerBlocks from dicts ---\n    blocks = [SpeakerBlock.from_dict(seg) for seg in segments]\n\n    # Enable wide mode for Streamlit app\n    st.set_page_config(layout=\"wide\")\n    st.write(\"## Segment Timeline Plot (seconds)\")\n    if not blocks:\n        st.error(\"No segment blocks found.\")\n        st.stop()\n\n\n    # --- Timeline Plot: group by speaker, color by speaker, number blocks ---\n    try:\n        speakers = list({block.speaker for block in blocks})\n        color_map = {\n            spk: pc.qualitative.Plotly[i % len(pc.qualitative.Plotly)] for i, spk in enumerate(speakers)\n        }\n\n        fig = go.Figure()\n        speaker_blocks = defaultdict(list)\n        for idx, block in enumerate(blocks):\n            speaker_blocks[block.speaker].append((idx, block))\n\n        bar_thickness = 0.6\n        for speaker, items in speaker_blocks.items():\n            y_val = speaker\n            for idx, block in items:\n                start_sec = block.start.to_seconds()\n                duration_sec = block.duration.to_seconds()\n                fig.add_trace(go.Bar(\n                    x=[duration_sec],\n                    y=[y_val],\n                    base=[start_sec],\n                    orientation='h',\n                    marker_color=color_map[speaker],\n                    name=f\"{idx+1}: {speaker}\",\n                    hovertext=f\"{idx+1}: {speaker} ({start_sec:.2f}s-{start_sec+duration_sec:.2f}s)\",\n                    width=bar_thickness\n                ))\n        fig.update_layout(\n            title=\"All Segments (seconds)\",\n            xaxis_title=\"Time (seconds)\",\n            yaxis_title=\"Speaker\",\n            showlegend=False,\n            bargap=0.2,\n            barmode=\"overlay\"\n        )\n        st.plotly_chart(fig, use_container_width=True)\n    except Exception as e:\n        st.error(f\"Error generating timeline plot: {e}\")\n\n\n    # --- Segment selection via entry box ---\n    st.write(\"## Enter Segment Number to Play\")\n    max_segment = len(blocks)\n    segment_num = st.number_input(\n        \"Segment number (1-based)\",\n        min_value=1,\n        max_value=max_segment,\n        value=1,\n        step=1,\n        help=f\"Enter a segment number between 1 and {max_segment}\"\n    )\n    selected_idx = segment_num - 1\n\n    block = blocks[selected_idx]\n    start_ms = block.start.to_ms()\n    end_ms = block.end.to_ms()\n    st.write(f\"Selected Segment: {segment_num} | Speaker: {block.speaker}\")\n    st.write(\n        f\"Start: {block.start.to_seconds():.2f}s, \"\n        f\"End: {block.end.to_seconds():.2f}s, \"\n        f\"Duration: {block.duration.to_seconds():.2f}s\"\n    )\n\n    # --- Play audio for selected segment ---\n    try:\n        audio = AudioSegment.from_file(master_audio_path)\n        segment_audio = audio[start_ms:end_ms]\n        buf = io.BytesIO()\n        segment_audio.export(buf, format=\"wav\")\n        st.audio(buf.getvalue(), format=\"audio/wav\")\n    except Exception as e:\n        st.error(f\"Error extracting or playing audio segment: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object","title":"<code>timed_object</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.__all__","title":"<code>__all__ = ['Granularity', 'TimedText', 'TimedTextUnit']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.Granularity","title":"<code>Granularity</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>class Granularity(str, Enum):\n    SEGMENT = \"segment\"\n    WORD = \"word\"\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.Granularity.SEGMENT","title":"<code>SEGMENT = 'segment'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.Granularity.WORD","title":"<code>WORD = 'word'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText","title":"<code>TimedText</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a collection of timed text units of a single granularity.</p> <p>Only one of <code>segments</code> or <code>words</code> is populated, determined by <code>granularity</code>. All units must match the declared granularity.</p> Notes <ul> <li>Start times must be non-decreasing (overlaps allowed for multiple speakers).</li> <li>Negative start_ms or end_ms values are not allowed.</li> <li>Durations must be strictly positive (&gt;0 ms).</li> <li>Mixed granularity is strictly prohibited.</li> </ul> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>class TimedText(BaseModel):\n    \"\"\"\n    Represents a collection of timed text units of a single granularity.\n\n    Only one of `segments` or `words` is populated, determined by `granularity`.\n    All units must match the declared granularity.\n\n    Notes:\n        - Start times must be non-decreasing (overlaps allowed for multiple speakers).\n        - Negative start_ms or end_ms values are not allowed.\n        - Durations must be strictly positive (&gt;0 ms).\n        - Mixed granularity is strictly prohibited.\n    \"\"\"\n\n    granularity: Granularity = Field(..., description=\"Granularity type for all units.\")\n    segments: List[TimedTextUnit] = Field(default_factory=list, description=\"Phrase-level timed units\")\n    words: List[TimedTextUnit] = Field(default_factory=list, description=\"Word-level timed units\")\n\n    def __init__(\n        self,\n        *,\n        granularity: Optional[Granularity] = None,\n        segments: Optional[List[TimedTextUnit]] = None,\n        words: Optional[List[TimedTextUnit]] = None,\n        units: Optional[List[TimedTextUnit]] = None,\n        **kwargs\n    ):\n        \"\"\"\n        Custom initializer for TimedText.\n        If `units` is provided, granularity is inferred from the first unit unless explicitly set.\n        If only `segments` or `words` is provided, granularity is set accordingly.\n        If all are empty, granularity must be provided.\n        \"\"\"\n        segments = segments or []\n        words = words or []\n        if units is not None:\n            if units:\n                inferred_granularity = units[0].granularity\n                granularity = granularity or inferred_granularity\n                if granularity == Granularity.SEGMENT:\n                    segments = units\n                    words = []\n                elif granularity == Granularity.WORD:\n                    words = units\n                    segments = []\n                else:\n                    raise ValueError(\"Invalid granularity inferred from units.\")\n            else:\n                if granularity is None:\n                    raise ValueError(\"Must provide granularity for empty TimedText.\")\n        elif segments:\n            granularity = granularity or Granularity.SEGMENT\n            words = []\n        elif words:\n            granularity = granularity or Granularity.WORD\n            segments = []\n        elif granularity is None:\n            raise ValueError(\"Must provide granularity for empty TimedText.\")\n\n        super().__init__(granularity=granularity, segments=segments, words=words, **kwargs)\n\n    @model_validator(mode=\"after\")\n    def _validate_exclusive_granularity(self):\n        \"\"\"\n        Validate that TimedText contains only units matching its granularity.\n        Allows empty TimedText objects for prototyping and construction.\n        Modular logic for segments and words.\n        \"\"\"\n        granularity = self.granularity\n        segments = self.segments\n        words = self.words\n\n        if granularity == Granularity.SEGMENT:\n            if words:\n                raise ValueError(\"TimedText with SEGMENT granularity must not have word units.\")\n            for unit in segments:\n                if unit.granularity != Granularity.SEGMENT:\n                    raise ValueError(\"All segment units must have granularity SEGMENT.\")\n        elif granularity == Granularity.WORD:\n            if segments:\n                raise ValueError(\"TimedText with WORD granularity must not have segment units.\")\n            for unit in words:\n                if unit.granularity != Granularity.WORD:\n                    raise ValueError(\"All word units must have granularity WORD.\")\n        else:\n            raise ValueError(\"Invalid granularity type.\")\n        return self\n\n    def model_post_init(self, __context) -&gt; None:\n        \"\"\"\n        After initialization, sort units by start time and normalize durations.\n        \"\"\"\n        self.sort_by_start()\n        for unit in self.units:\n            unit.normalize()\n\n    @property\n    def units(self) -&gt; List[TimedTextUnit]:\n        \"\"\"Return the list of units matching the granularity.\"\"\"\n        return self.segments if self.granularity == Granularity.SEGMENT else self.words\n\n    def is_segment_granularity(self) -&gt; bool:\n        \"\"\"Return True if granularity is SEGMENT.\"\"\"\n        return self.granularity == Granularity.SEGMENT\n\n    def is_word_granularity(self) -&gt; bool:\n        \"\"\"Return True if granularity is WORD.\"\"\"\n        return self.granularity == Granularity.WORD\n\n    @property\n    def start_ms(self) -&gt; int:\n        \"\"\"Get the start time of the earliest unit.\"\"\"\n        return min(unit.start_ms for unit in self.units) if self.units else 0\n\n    @property\n    def end_ms(self) -&gt; int:\n        \"\"\"Get the end time of the latest unit.\"\"\"\n        return max(unit.end_ms for unit in self.units) if self.units else 0\n\n    @property\n    def duration(self) -&gt; int:\n        \"\"\"Get the total duration in milliseconds.\"\"\"\n        return self.end_ms - self.start_ms\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of units.\"\"\"\n        return len(self.units)\n\n    def append(self, unit: TimedTextUnit):\n        \"\"\"Add a unit to the end.\"\"\"\n        if unit.granularity != self.granularity:\n            raise ValueError(f\"Cannot append unit with granularity {unit.granularity} \"\n                             \"to TimedText of granularity {self.granularity}.\")\n        self.units.append(unit)\n\n    def extend(self, units: List[TimedTextUnit]):\n        \"\"\"Add multiple units to the end.\"\"\"\n        for unit in units:\n            self.append(unit)\n\n    def clear(self):\n        \"\"\"Remove all units.\"\"\"\n        self.units.clear()\n\n    def set_speaker(self, index: int, speaker: str) -&gt; None:\n        \"\"\"Set speaker for a specific unit by index.\"\"\"\n        if not (0 &lt;= index &lt; len(self.units)):\n            raise IndexError(f\"Index {index} out of range for units.\")\n        self.units[index].set_speaker(speaker)\n\n    def set_all_speakers(self, speaker: str) -&gt; None:\n        \"\"\"Set the same speaker for all units.\"\"\"\n        for unit in self.units:\n            unit.set_speaker(speaker)\n\n    def shift(self, offset_ms: int) -&gt; None:\n        \"\"\"Shift all units by a given offset in milliseconds.\"\"\"\n        for i, unit in enumerate(self.units):\n            self.units[i] = unit.shift_time(offset_ms)\n\n    def sort_by_start(self) -&gt; None:\n        \"\"\"Sort units by start time.\"\"\"\n        self.units.sort(key=lambda unit: unit.start_ms)\n\n\n    @classmethod\n    def _new_with_units(\n        cls, units: List[TimedTextUnit], granularity: Optional[Granularity] = None\n    ) -&gt; \"TimedText\":\n        \"\"\"\n        Helper to create a new TimedText object with the given granularity and units.\n        If granularity is not provided, it is inferred from the first unit.\n        \"\"\"\n        if units:\n            inferred_granularity = units[0].granularity\n            granularity = granularity or inferred_granularity\n            if granularity == Granularity.SEGMENT:\n                return cls(granularity=granularity, segments=units, words=[])\n            elif granularity == Granularity.WORD:\n                return cls(granularity=granularity, segments=[], words=units)\n            else:\n                raise ValueError(\"Invalid granularity inferred from units.\")\n        else:\n            if granularity is None:\n                raise ValueError(\"Must provide granularity for empty TimedText.\")\n            if granularity in [Granularity.SEGMENT, Granularity.WORD]:\n                return cls(granularity=granularity, segments=[], words=[])\n            else:\n                raise ValueError(\"Invalid granularity provided.\")\n\n    def slice(self, start_ms: int, end_ms: int) -&gt; \"TimedText\":\n        \"\"\"\n        Return a new TimedText object containing only units within [start_ms, end_ms].\n        Units must overlap with the interval to be included.\n        \"\"\"\n        sliced_units = [\n            unit for unit in self.units\n            if unit.end_ms &gt; start_ms and unit.start_ms &lt; end_ms\n        ]\n        return self._new_with_units(sliced_units, self.granularity)\n\n    def filter_by_min_duration(self, min_duration_ms: int) -&gt; \"TimedText\":\n        \"\"\"\n        Return a new TimedText object containing only units with a minimum duration.\n        \"\"\"\n        filtered_units = [\n            unit for unit in self.units\n            if unit.duration_ms &gt;= min_duration_ms\n        ]\n        return self._new_with_units(filtered_units, self.granularity)\n\n    @classmethod\n    def merge(cls, items: List[\"TimedText\"]) -&gt; \"TimedText\":\n        \"\"\"\n        Merge a list of TimedText objects of the same granularity into a single TimedText object.\n        \"\"\"\n        if not items:\n            raise ValueError(\"No TimedText objects to merge.\")\n        granularity = items[0].granularity\n        for item in items:\n            if item.granularity != granularity:\n                raise ValueError(\"Cannot merge TimedText objects of different granularities.\")\n        all_units: List[TimedTextUnit] = []\n        for item in items:\n            all_units.extend(item.units)\n\n        # Use the classmethod to generate with units\n        return cls._new_with_units(all_units, granularity)\n\n    def iter(self) -&gt; Iterator[TimedTextUnit]:\n        \"\"\"\n        Unified iterator over the units of the correct granularity.\n        \"\"\"\n        return iter(self.units)\n\n    def iter_segments(self) -&gt; Iterator[TimedTextUnit]:\n        \"\"\"\n        Iterate over segment-level units.\n\n        Raises:\n            ValueError: If granularity is not SEGMENT.\n        \"\"\"\n        if not self.is_segment_granularity():\n            raise ValueError(\"Cannot call iter_segments() on TimedText with WORD granularity.\")\n        return iter(self.segments)\n\n    def iter_words(self) -&gt; Iterator[TimedTextUnit]:\n        \"\"\"\n        Iterate over word-level units.\n\n        Raises:\n            ValueError: If granularity is not WORD.\n        \"\"\"\n        if not self.is_word_granularity():\n            raise ValueError(\"Cannot call iter_words() on TimedText with SEGMENT granularity.\")\n        return iter(self.words)\n\n    def export_text(self, separator: str = \"\\n\", skip_empty: bool = True, show_speaker: bool = True) -&gt; str:\n        \"\"\"\n        Export the text content of all units as a single string.\n\n        Args:\n            separator: String used to separate units (default: newline).\n            skip_empty: If True, skip units with empty or whitespace-only text.\n            show_speaker: If True, add speaker info.\n\n        Returns:\n            Concatenated text of all units, separated by `separator`.\n        \"\"\"\n        def _text_line(unit: TimedTextUnit) -&gt; str:\n            if show_speaker and unit.speaker:\n                return f\"[{unit.speaker}] {unit.text}\"\n            return unit.text\n\n        texts = [\n            _text_line(unit) for unit in self.units\n            if not skip_empty or unit.text.strip()\n        ]\n        return separator.join(texts)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.duration","title":"<code>duration</code>  <code>property</code>","text":"<p>Get the total duration in milliseconds.</p>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.end_ms","title":"<code>end_ms</code>  <code>property</code>","text":"<p>Get the end time of the latest unit.</p>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.granularity","title":"<code>granularity = Field(..., description='Granularity type for all units.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.segments","title":"<code>segments = Field(default_factory=list, description='Phrase-level timed units')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.start_ms","title":"<code>start_ms</code>  <code>property</code>","text":"<p>Get the start time of the earliest unit.</p>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.units","title":"<code>units</code>  <code>property</code>","text":"<p>Return the list of units matching the granularity.</p>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.words","title":"<code>words = Field(default_factory=list, description='Word-level timed units')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.__init__","title":"<code>__init__(*, granularity=None, segments=None, words=None, units=None, **kwargs)</code>","text":"<p>Custom initializer for TimedText. If <code>units</code> is provided, granularity is inferred from the first unit unless explicitly set. If only <code>segments</code> or <code>words</code> is provided, granularity is set accordingly. If all are empty, granularity must be provided.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def __init__(\n    self,\n    *,\n    granularity: Optional[Granularity] = None,\n    segments: Optional[List[TimedTextUnit]] = None,\n    words: Optional[List[TimedTextUnit]] = None,\n    units: Optional[List[TimedTextUnit]] = None,\n    **kwargs\n):\n    \"\"\"\n    Custom initializer for TimedText.\n    If `units` is provided, granularity is inferred from the first unit unless explicitly set.\n    If only `segments` or `words` is provided, granularity is set accordingly.\n    If all are empty, granularity must be provided.\n    \"\"\"\n    segments = segments or []\n    words = words or []\n    if units is not None:\n        if units:\n            inferred_granularity = units[0].granularity\n            granularity = granularity or inferred_granularity\n            if granularity == Granularity.SEGMENT:\n                segments = units\n                words = []\n            elif granularity == Granularity.WORD:\n                words = units\n                segments = []\n            else:\n                raise ValueError(\"Invalid granularity inferred from units.\")\n        else:\n            if granularity is None:\n                raise ValueError(\"Must provide granularity for empty TimedText.\")\n    elif segments:\n        granularity = granularity or Granularity.SEGMENT\n        words = []\n    elif words:\n        granularity = granularity or Granularity.WORD\n        segments = []\n    elif granularity is None:\n        raise ValueError(\"Must provide granularity for empty TimedText.\")\n\n    super().__init__(granularity=granularity, segments=segments, words=words, **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of units.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of units.\"\"\"\n    return len(self.units)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.append","title":"<code>append(unit)</code>","text":"<p>Add a unit to the end.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def append(self, unit: TimedTextUnit):\n    \"\"\"Add a unit to the end.\"\"\"\n    if unit.granularity != self.granularity:\n        raise ValueError(f\"Cannot append unit with granularity {unit.granularity} \"\n                         \"to TimedText of granularity {self.granularity}.\")\n    self.units.append(unit)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.clear","title":"<code>clear()</code>","text":"<p>Remove all units.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def clear(self):\n    \"\"\"Remove all units.\"\"\"\n    self.units.clear()\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.export_text","title":"<code>export_text(separator='\\n', skip_empty=True, show_speaker=True)</code>","text":"<p>Export the text content of all units as a single string.</p> <p>Parameters:</p> Name Type Description Default <code>separator</code> <code>str</code> <p>String used to separate units (default: newline).</p> <code>'\\n'</code> <code>skip_empty</code> <code>bool</code> <p>If True, skip units with empty or whitespace-only text.</p> <code>True</code> <code>show_speaker</code> <code>bool</code> <p>If True, add speaker info.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Concatenated text of all units, separated by <code>separator</code>.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def export_text(self, separator: str = \"\\n\", skip_empty: bool = True, show_speaker: bool = True) -&gt; str:\n    \"\"\"\n    Export the text content of all units as a single string.\n\n    Args:\n        separator: String used to separate units (default: newline).\n        skip_empty: If True, skip units with empty or whitespace-only text.\n        show_speaker: If True, add speaker info.\n\n    Returns:\n        Concatenated text of all units, separated by `separator`.\n    \"\"\"\n    def _text_line(unit: TimedTextUnit) -&gt; str:\n        if show_speaker and unit.speaker:\n            return f\"[{unit.speaker}] {unit.text}\"\n        return unit.text\n\n    texts = [\n        _text_line(unit) for unit in self.units\n        if not skip_empty or unit.text.strip()\n    ]\n    return separator.join(texts)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.extend","title":"<code>extend(units)</code>","text":"<p>Add multiple units to the end.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def extend(self, units: List[TimedTextUnit]):\n    \"\"\"Add multiple units to the end.\"\"\"\n    for unit in units:\n        self.append(unit)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.filter_by_min_duration","title":"<code>filter_by_min_duration(min_duration_ms)</code>","text":"<p>Return a new TimedText object containing only units with a minimum duration.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def filter_by_min_duration(self, min_duration_ms: int) -&gt; \"TimedText\":\n    \"\"\"\n    Return a new TimedText object containing only units with a minimum duration.\n    \"\"\"\n    filtered_units = [\n        unit for unit in self.units\n        if unit.duration_ms &gt;= min_duration_ms\n    ]\n    return self._new_with_units(filtered_units, self.granularity)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.is_segment_granularity","title":"<code>is_segment_granularity()</code>","text":"<p>Return True if granularity is SEGMENT.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def is_segment_granularity(self) -&gt; bool:\n    \"\"\"Return True if granularity is SEGMENT.\"\"\"\n    return self.granularity == Granularity.SEGMENT\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.is_word_granularity","title":"<code>is_word_granularity()</code>","text":"<p>Return True if granularity is WORD.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def is_word_granularity(self) -&gt; bool:\n    \"\"\"Return True if granularity is WORD.\"\"\"\n    return self.granularity == Granularity.WORD\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.iter","title":"<code>iter()</code>","text":"<p>Unified iterator over the units of the correct granularity.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def iter(self) -&gt; Iterator[TimedTextUnit]:\n    \"\"\"\n    Unified iterator over the units of the correct granularity.\n    \"\"\"\n    return iter(self.units)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.iter_segments","title":"<code>iter_segments()</code>","text":"<p>Iterate over segment-level units.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If granularity is not SEGMENT.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def iter_segments(self) -&gt; Iterator[TimedTextUnit]:\n    \"\"\"\n    Iterate over segment-level units.\n\n    Raises:\n        ValueError: If granularity is not SEGMENT.\n    \"\"\"\n    if not self.is_segment_granularity():\n        raise ValueError(\"Cannot call iter_segments() on TimedText with WORD granularity.\")\n    return iter(self.segments)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.iter_words","title":"<code>iter_words()</code>","text":"<p>Iterate over word-level units.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If granularity is not WORD.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def iter_words(self) -&gt; Iterator[TimedTextUnit]:\n    \"\"\"\n    Iterate over word-level units.\n\n    Raises:\n        ValueError: If granularity is not WORD.\n    \"\"\"\n    if not self.is_word_granularity():\n        raise ValueError(\"Cannot call iter_words() on TimedText with SEGMENT granularity.\")\n    return iter(self.words)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.merge","title":"<code>merge(items)</code>  <code>classmethod</code>","text":"<p>Merge a list of TimedText objects of the same granularity into a single TimedText object.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>@classmethod\ndef merge(cls, items: List[\"TimedText\"]) -&gt; \"TimedText\":\n    \"\"\"\n    Merge a list of TimedText objects of the same granularity into a single TimedText object.\n    \"\"\"\n    if not items:\n        raise ValueError(\"No TimedText objects to merge.\")\n    granularity = items[0].granularity\n    for item in items:\n        if item.granularity != granularity:\n            raise ValueError(\"Cannot merge TimedText objects of different granularities.\")\n    all_units: List[TimedTextUnit] = []\n    for item in items:\n        all_units.extend(item.units)\n\n    # Use the classmethod to generate with units\n    return cls._new_with_units(all_units, granularity)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>After initialization, sort units by start time and normalize durations.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"\n    After initialization, sort units by start time and normalize durations.\n    \"\"\"\n    self.sort_by_start()\n    for unit in self.units:\n        unit.normalize()\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.set_all_speakers","title":"<code>set_all_speakers(speaker)</code>","text":"<p>Set the same speaker for all units.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def set_all_speakers(self, speaker: str) -&gt; None:\n    \"\"\"Set the same speaker for all units.\"\"\"\n    for unit in self.units:\n        unit.set_speaker(speaker)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.set_speaker","title":"<code>set_speaker(index, speaker)</code>","text":"<p>Set speaker for a specific unit by index.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def set_speaker(self, index: int, speaker: str) -&gt; None:\n    \"\"\"Set speaker for a specific unit by index.\"\"\"\n    if not (0 &lt;= index &lt; len(self.units)):\n        raise IndexError(f\"Index {index} out of range for units.\")\n    self.units[index].set_speaker(speaker)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.shift","title":"<code>shift(offset_ms)</code>","text":"<p>Shift all units by a given offset in milliseconds.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def shift(self, offset_ms: int) -&gt; None:\n    \"\"\"Shift all units by a given offset in milliseconds.\"\"\"\n    for i, unit in enumerate(self.units):\n        self.units[i] = unit.shift_time(offset_ms)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.slice","title":"<code>slice(start_ms, end_ms)</code>","text":"<p>Return a new TimedText object containing only units within [start_ms, end_ms]. Units must overlap with the interval to be included.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def slice(self, start_ms: int, end_ms: int) -&gt; \"TimedText\":\n    \"\"\"\n    Return a new TimedText object containing only units within [start_ms, end_ms].\n    Units must overlap with the interval to be included.\n    \"\"\"\n    sliced_units = [\n        unit for unit in self.units\n        if unit.end_ms &gt; start_ms and unit.start_ms &lt; end_ms\n    ]\n    return self._new_with_units(sliced_units, self.granularity)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedText.sort_by_start","title":"<code>sort_by_start()</code>","text":"<p>Sort units by start time.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def sort_by_start(self) -&gt; None:\n    \"\"\"Sort units by start time.\"\"\"\n    self.units.sort(key=lambda unit: unit.start_ms)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit","title":"<code>TimedTextUnit</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a timed unit with timestamps.</p> <p>A fundamental building block for subtitle and transcript processing that associates text content with start/end times and optional metadata. Can represent either a segment (phrase/sentence) or a word.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>class TimedTextUnit(BaseModel):\n    \"\"\"\n    Represents a timed unit with timestamps.\n\n    A fundamental building block for subtitle and transcript processing that\n    associates text content with start/end times and optional metadata.\n    Can represent either a segment (phrase/sentence) or a word.\n    \"\"\"\n    text: str = Field(..., description=\"The text content\")\n    start_ms: int = Field(..., description=\"Start time in milliseconds\")\n    end_ms: int = Field(..., description=\"End time in milliseconds\")\n    speaker: Optional[str] = Field(None, description=\"Speaker identifier if available\")\n    index: Optional[int] = Field(None, description=\"Entry index or sequence number\")\n    granularity: Granularity\n    confidence: Optional[float] = Field(None, description=\"Optional confidence score\")\n\n    @property\n    def duration_ms(self) -&gt; int:\n        \"\"\"Get duration in milliseconds.\"\"\"\n        return self.end_ms - self.start_ms\n\n    @property\n    def start_sec(self) -&gt; float:\n        \"\"\"Get start time in seconds.\"\"\"\n        return self.start_ms / 1000\n\n    @property\n    def end_sec(self) -&gt; float:\n        \"\"\"Get end time in seconds.\"\"\"\n        return self.end_ms / 1000\n\n    @property\n    def duration_sec(self) -&gt; float:\n        \"\"\"Get duration in seconds.\"\"\"\n        return self.duration_ms / 1000\n\n    def shift_time(self, offset_ms: int) -&gt; \"TimedTextUnit\":\n        \"\"\"Create a new TimedUnit with timestamps shifted by offset.\"\"\"\n        return self.model_copy(\n            update={\n                \"start_ms\": self.start_ms + offset_ms,\n                \"end_ms\": self.end_ms + offset_ms\n            }\n        )\n\n    def overlaps_with(self, other: \"TimedTextUnit\") -&gt; bool:\n        \"\"\"Check if this unit overlaps with another.\"\"\"\n        return (self.start_ms &lt;= other.end_ms and \n                other.start_ms &lt;= self.end_ms)\n\n    def set_speaker(self, speaker: str) -&gt; None:\n        \"\"\"Set the speaker label.\"\"\"\n        self.speaker = speaker\n\n    def normalize(self) -&gt; None:\n        \"\"\"Normalize the duration of the segment to be nonzero\"\"\"\n        if self.start_ms == self.end_ms:\n            self.end_ms = self.start_ms + 1 # minimum duration \n\n    @field_validator(\"start_ms\", \"end_ms\")\n    @classmethod\n    def _validate_time_non_negative(cls, v: int) -&gt; int:\n        if v &lt; 0:\n            raise ValueError(\"start_ms and end_ms must be non-negative.\")\n        return v\n\n    @field_validator(\"end_ms\")\n    @classmethod\n    def _validate_positive_duration(cls, end_ms: int, info) -&gt; int:\n        start_ms = info.data.get(\"start_ms\")\n        if start_ms is not None and end_ms &lt; start_ms:\n            raise ValueError(\n                f\"end_ms ({end_ms}) must be greater than start_ms ({start_ms}).\"\n            )\n        return end_ms\n\n    @field_validator(\"text\")\n    @classmethod\n    def _validate_word_text(cls, v: str, info):\n        granularity = info.data.get(\"granularity\", \"segment\")\n        if granularity == \"word\" and (\" \" in v.strip()):\n            raise ValueError(\n                \"Text for a word-level TimedUnit cannot contain whitespace.\"\n            )\n        return v\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.confidence","title":"<code>confidence = Field(None, description='Optional confidence score')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.duration_ms","title":"<code>duration_ms</code>  <code>property</code>","text":"<p>Get duration in milliseconds.</p>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.duration_sec","title":"<code>duration_sec</code>  <code>property</code>","text":"<p>Get duration in seconds.</p>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.end_ms","title":"<code>end_ms = Field(..., description='End time in milliseconds')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.end_sec","title":"<code>end_sec</code>  <code>property</code>","text":"<p>Get end time in seconds.</p>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.granularity","title":"<code>granularity</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.index","title":"<code>index = Field(None, description='Entry index or sequence number')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.speaker","title":"<code>speaker = Field(None, description='Speaker identifier if available')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.start_ms","title":"<code>start_ms = Field(..., description='Start time in milliseconds')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.start_sec","title":"<code>start_sec</code>  <code>property</code>","text":"<p>Get start time in seconds.</p>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.text","title":"<code>text = Field(..., description='The text content')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.normalize","title":"<code>normalize()</code>","text":"<p>Normalize the duration of the segment to be nonzero</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def normalize(self) -&gt; None:\n    \"\"\"Normalize the duration of the segment to be nonzero\"\"\"\n    if self.start_ms == self.end_ms:\n        self.end_ms = self.start_ms + 1 # minimum duration \n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.overlaps_with","title":"<code>overlaps_with(other)</code>","text":"<p>Check if this unit overlaps with another.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def overlaps_with(self, other: \"TimedTextUnit\") -&gt; bool:\n    \"\"\"Check if this unit overlaps with another.\"\"\"\n    return (self.start_ms &lt;= other.end_ms and \n            other.start_ms &lt;= self.end_ms)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.set_speaker","title":"<code>set_speaker(speaker)</code>","text":"<p>Set the speaker label.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def set_speaker(self, speaker: str) -&gt; None:\n    \"\"\"Set the speaker label.\"\"\"\n    self.speaker = speaker\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.TimedTextUnit.shift_time","title":"<code>shift_time(offset_ms)</code>","text":"<p>Create a new TimedUnit with timestamps shifted by offset.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def shift_time(self, offset_ms: int) -&gt; \"TimedTextUnit\":\n    \"\"\"Create a new TimedUnit with timestamps shifted by offset.\"\"\"\n    return self.model_copy(\n        update={\n            \"start_ms\": self.start_ms + offset_ms,\n            \"end_ms\": self.end_ms + offset_ms\n        }\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.timed_text","title":"<code>timed_text</code>","text":"<p>Module for handling timed text objects. For example, can be used  subtitles like VTT and SRT.</p> <p>This module provides classes and utilities for parsing, manipulating, and generating timed text objects useful in subtitle and transcript processing. It uses Pydantic for robust data validation and type safety.</p>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.timed_text.Granularity","title":"<code>Granularity</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>class Granularity(str, Enum):\n    SEGMENT = \"segment\"\n    WORD = \"word\"\n</code></pre> <code>SEGMENT = 'segment'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>WORD = 'word'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.timed_object.timed_text.TimedText","title":"<code>TimedText</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a collection of timed text units of a single granularity.</p> <p>Only one of <code>segments</code> or <code>words</code> is populated, determined by <code>granularity</code>. All units must match the declared granularity.</p> Notes <ul> <li>Start times must be non-decreasing (overlaps allowed for multiple speakers).</li> <li>Negative start_ms or end_ms values are not allowed.</li> <li>Durations must be strictly positive (&gt;0 ms).</li> <li>Mixed granularity is strictly prohibited.</li> </ul> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>class TimedText(BaseModel):\n    \"\"\"\n    Represents a collection of timed text units of a single granularity.\n\n    Only one of `segments` or `words` is populated, determined by `granularity`.\n    All units must match the declared granularity.\n\n    Notes:\n        - Start times must be non-decreasing (overlaps allowed for multiple speakers).\n        - Negative start_ms or end_ms values are not allowed.\n        - Durations must be strictly positive (&gt;0 ms).\n        - Mixed granularity is strictly prohibited.\n    \"\"\"\n\n    granularity: Granularity = Field(..., description=\"Granularity type for all units.\")\n    segments: List[TimedTextUnit] = Field(default_factory=list, description=\"Phrase-level timed units\")\n    words: List[TimedTextUnit] = Field(default_factory=list, description=\"Word-level timed units\")\n\n    def __init__(\n        self,\n        *,\n        granularity: Optional[Granularity] = None,\n        segments: Optional[List[TimedTextUnit]] = None,\n        words: Optional[List[TimedTextUnit]] = None,\n        units: Optional[List[TimedTextUnit]] = None,\n        **kwargs\n    ):\n        \"\"\"\n        Custom initializer for TimedText.\n        If `units` is provided, granularity is inferred from the first unit unless explicitly set.\n        If only `segments` or `words` is provided, granularity is set accordingly.\n        If all are empty, granularity must be provided.\n        \"\"\"\n        segments = segments or []\n        words = words or []\n        if units is not None:\n            if units:\n                inferred_granularity = units[0].granularity\n                granularity = granularity or inferred_granularity\n                if granularity == Granularity.SEGMENT:\n                    segments = units\n                    words = []\n                elif granularity == Granularity.WORD:\n                    words = units\n                    segments = []\n                else:\n                    raise ValueError(\"Invalid granularity inferred from units.\")\n            else:\n                if granularity is None:\n                    raise ValueError(\"Must provide granularity for empty TimedText.\")\n        elif segments:\n            granularity = granularity or Granularity.SEGMENT\n            words = []\n        elif words:\n            granularity = granularity or Granularity.WORD\n            segments = []\n        elif granularity is None:\n            raise ValueError(\"Must provide granularity for empty TimedText.\")\n\n        super().__init__(granularity=granularity, segments=segments, words=words, **kwargs)\n\n    @model_validator(mode=\"after\")\n    def _validate_exclusive_granularity(self):\n        \"\"\"\n        Validate that TimedText contains only units matching its granularity.\n        Allows empty TimedText objects for prototyping and construction.\n        Modular logic for segments and words.\n        \"\"\"\n        granularity = self.granularity\n        segments = self.segments\n        words = self.words\n\n        if granularity == Granularity.SEGMENT:\n            if words:\n                raise ValueError(\"TimedText with SEGMENT granularity must not have word units.\")\n            for unit in segments:\n                if unit.granularity != Granularity.SEGMENT:\n                    raise ValueError(\"All segment units must have granularity SEGMENT.\")\n        elif granularity == Granularity.WORD:\n            if segments:\n                raise ValueError(\"TimedText with WORD granularity must not have segment units.\")\n            for unit in words:\n                if unit.granularity != Granularity.WORD:\n                    raise ValueError(\"All word units must have granularity WORD.\")\n        else:\n            raise ValueError(\"Invalid granularity type.\")\n        return self\n\n    def model_post_init(self, __context) -&gt; None:\n        \"\"\"\n        After initialization, sort units by start time and normalize durations.\n        \"\"\"\n        self.sort_by_start()\n        for unit in self.units:\n            unit.normalize()\n\n    @property\n    def units(self) -&gt; List[TimedTextUnit]:\n        \"\"\"Return the list of units matching the granularity.\"\"\"\n        return self.segments if self.granularity == Granularity.SEGMENT else self.words\n\n    def is_segment_granularity(self) -&gt; bool:\n        \"\"\"Return True if granularity is SEGMENT.\"\"\"\n        return self.granularity == Granularity.SEGMENT\n\n    def is_word_granularity(self) -&gt; bool:\n        \"\"\"Return True if granularity is WORD.\"\"\"\n        return self.granularity == Granularity.WORD\n\n    @property\n    def start_ms(self) -&gt; int:\n        \"\"\"Get the start time of the earliest unit.\"\"\"\n        return min(unit.start_ms for unit in self.units) if self.units else 0\n\n    @property\n    def end_ms(self) -&gt; int:\n        \"\"\"Get the end time of the latest unit.\"\"\"\n        return max(unit.end_ms for unit in self.units) if self.units else 0\n\n    @property\n    def duration(self) -&gt; int:\n        \"\"\"Get the total duration in milliseconds.\"\"\"\n        return self.end_ms - self.start_ms\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of units.\"\"\"\n        return len(self.units)\n\n    def append(self, unit: TimedTextUnit):\n        \"\"\"Add a unit to the end.\"\"\"\n        if unit.granularity != self.granularity:\n            raise ValueError(f\"Cannot append unit with granularity {unit.granularity} \"\n                             \"to TimedText of granularity {self.granularity}.\")\n        self.units.append(unit)\n\n    def extend(self, units: List[TimedTextUnit]):\n        \"\"\"Add multiple units to the end.\"\"\"\n        for unit in units:\n            self.append(unit)\n\n    def clear(self):\n        \"\"\"Remove all units.\"\"\"\n        self.units.clear()\n\n    def set_speaker(self, index: int, speaker: str) -&gt; None:\n        \"\"\"Set speaker for a specific unit by index.\"\"\"\n        if not (0 &lt;= index &lt; len(self.units)):\n            raise IndexError(f\"Index {index} out of range for units.\")\n        self.units[index].set_speaker(speaker)\n\n    def set_all_speakers(self, speaker: str) -&gt; None:\n        \"\"\"Set the same speaker for all units.\"\"\"\n        for unit in self.units:\n            unit.set_speaker(speaker)\n\n    def shift(self, offset_ms: int) -&gt; None:\n        \"\"\"Shift all units by a given offset in milliseconds.\"\"\"\n        for i, unit in enumerate(self.units):\n            self.units[i] = unit.shift_time(offset_ms)\n\n    def sort_by_start(self) -&gt; None:\n        \"\"\"Sort units by start time.\"\"\"\n        self.units.sort(key=lambda unit: unit.start_ms)\n\n\n    @classmethod\n    def _new_with_units(\n        cls, units: List[TimedTextUnit], granularity: Optional[Granularity] = None\n    ) -&gt; \"TimedText\":\n        \"\"\"\n        Helper to create a new TimedText object with the given granularity and units.\n        If granularity is not provided, it is inferred from the first unit.\n        \"\"\"\n        if units:\n            inferred_granularity = units[0].granularity\n            granularity = granularity or inferred_granularity\n            if granularity == Granularity.SEGMENT:\n                return cls(granularity=granularity, segments=units, words=[])\n            elif granularity == Granularity.WORD:\n                return cls(granularity=granularity, segments=[], words=units)\n            else:\n                raise ValueError(\"Invalid granularity inferred from units.\")\n        else:\n            if granularity is None:\n                raise ValueError(\"Must provide granularity for empty TimedText.\")\n            if granularity in [Granularity.SEGMENT, Granularity.WORD]:\n                return cls(granularity=granularity, segments=[], words=[])\n            else:\n                raise ValueError(\"Invalid granularity provided.\")\n\n    def slice(self, start_ms: int, end_ms: int) -&gt; \"TimedText\":\n        \"\"\"\n        Return a new TimedText object containing only units within [start_ms, end_ms].\n        Units must overlap with the interval to be included.\n        \"\"\"\n        sliced_units = [\n            unit for unit in self.units\n            if unit.end_ms &gt; start_ms and unit.start_ms &lt; end_ms\n        ]\n        return self._new_with_units(sliced_units, self.granularity)\n\n    def filter_by_min_duration(self, min_duration_ms: int) -&gt; \"TimedText\":\n        \"\"\"\n        Return a new TimedText object containing only units with a minimum duration.\n        \"\"\"\n        filtered_units = [\n            unit for unit in self.units\n            if unit.duration_ms &gt;= min_duration_ms\n        ]\n        return self._new_with_units(filtered_units, self.granularity)\n\n    @classmethod\n    def merge(cls, items: List[\"TimedText\"]) -&gt; \"TimedText\":\n        \"\"\"\n        Merge a list of TimedText objects of the same granularity into a single TimedText object.\n        \"\"\"\n        if not items:\n            raise ValueError(\"No TimedText objects to merge.\")\n        granularity = items[0].granularity\n        for item in items:\n            if item.granularity != granularity:\n                raise ValueError(\"Cannot merge TimedText objects of different granularities.\")\n        all_units: List[TimedTextUnit] = []\n        for item in items:\n            all_units.extend(item.units)\n\n        # Use the classmethod to generate with units\n        return cls._new_with_units(all_units, granularity)\n\n    def iter(self) -&gt; Iterator[TimedTextUnit]:\n        \"\"\"\n        Unified iterator over the units of the correct granularity.\n        \"\"\"\n        return iter(self.units)\n\n    def iter_segments(self) -&gt; Iterator[TimedTextUnit]:\n        \"\"\"\n        Iterate over segment-level units.\n\n        Raises:\n            ValueError: If granularity is not SEGMENT.\n        \"\"\"\n        if not self.is_segment_granularity():\n            raise ValueError(\"Cannot call iter_segments() on TimedText with WORD granularity.\")\n        return iter(self.segments)\n\n    def iter_words(self) -&gt; Iterator[TimedTextUnit]:\n        \"\"\"\n        Iterate over word-level units.\n\n        Raises:\n            ValueError: If granularity is not WORD.\n        \"\"\"\n        if not self.is_word_granularity():\n            raise ValueError(\"Cannot call iter_words() on TimedText with SEGMENT granularity.\")\n        return iter(self.words)\n\n    def export_text(self, separator: str = \"\\n\", skip_empty: bool = True, show_speaker: bool = True) -&gt; str:\n        \"\"\"\n        Export the text content of all units as a single string.\n\n        Args:\n            separator: String used to separate units (default: newline).\n            skip_empty: If True, skip units with empty or whitespace-only text.\n            show_speaker: If True, add speaker info.\n\n        Returns:\n            Concatenated text of all units, separated by `separator`.\n        \"\"\"\n        def _text_line(unit: TimedTextUnit) -&gt; str:\n            if show_speaker and unit.speaker:\n                return f\"[{unit.speaker}] {unit.text}\"\n            return unit.text\n\n        texts = [\n            _text_line(unit) for unit in self.units\n            if not skip_empty or unit.text.strip()\n        ]\n        return separator.join(texts)\n</code></pre> <code>duration</code> <code>property</code> \u00b6 <p>Get the total duration in milliseconds.</p> <code>end_ms</code> <code>property</code> \u00b6 <p>Get the end time of the latest unit.</p> <code>granularity = Field(..., description='Granularity type for all units.')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>segments = Field(default_factory=list, description='Phrase-level timed units')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>start_ms</code> <code>property</code> \u00b6 <p>Get the start time of the earliest unit.</p> <code>units</code> <code>property</code> \u00b6 <p>Return the list of units matching the granularity.</p> <code>words = Field(default_factory=list, description='Word-level timed units')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>__init__(*, granularity=None, segments=None, words=None, units=None, **kwargs)</code> \u00b6 <p>Custom initializer for TimedText. If <code>units</code> is provided, granularity is inferred from the first unit unless explicitly set. If only <code>segments</code> or <code>words</code> is provided, granularity is set accordingly. If all are empty, granularity must be provided.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def __init__(\n    self,\n    *,\n    granularity: Optional[Granularity] = None,\n    segments: Optional[List[TimedTextUnit]] = None,\n    words: Optional[List[TimedTextUnit]] = None,\n    units: Optional[List[TimedTextUnit]] = None,\n    **kwargs\n):\n    \"\"\"\n    Custom initializer for TimedText.\n    If `units` is provided, granularity is inferred from the first unit unless explicitly set.\n    If only `segments` or `words` is provided, granularity is set accordingly.\n    If all are empty, granularity must be provided.\n    \"\"\"\n    segments = segments or []\n    words = words or []\n    if units is not None:\n        if units:\n            inferred_granularity = units[0].granularity\n            granularity = granularity or inferred_granularity\n            if granularity == Granularity.SEGMENT:\n                segments = units\n                words = []\n            elif granularity == Granularity.WORD:\n                words = units\n                segments = []\n            else:\n                raise ValueError(\"Invalid granularity inferred from units.\")\n        else:\n            if granularity is None:\n                raise ValueError(\"Must provide granularity for empty TimedText.\")\n    elif segments:\n        granularity = granularity or Granularity.SEGMENT\n        words = []\n    elif words:\n        granularity = granularity or Granularity.WORD\n        segments = []\n    elif granularity is None:\n        raise ValueError(\"Must provide granularity for empty TimedText.\")\n\n    super().__init__(granularity=granularity, segments=segments, words=words, **kwargs)\n</code></pre> <code>__len__()</code> \u00b6 <p>Return the number of units.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of units.\"\"\"\n    return len(self.units)\n</code></pre> <code>append(unit)</code> \u00b6 <p>Add a unit to the end.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def append(self, unit: TimedTextUnit):\n    \"\"\"Add a unit to the end.\"\"\"\n    if unit.granularity != self.granularity:\n        raise ValueError(f\"Cannot append unit with granularity {unit.granularity} \"\n                         \"to TimedText of granularity {self.granularity}.\")\n    self.units.append(unit)\n</code></pre> <code>clear()</code> \u00b6 <p>Remove all units.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def clear(self):\n    \"\"\"Remove all units.\"\"\"\n    self.units.clear()\n</code></pre> <code>export_text(separator='\\n', skip_empty=True, show_speaker=True)</code> \u00b6 <p>Export the text content of all units as a single string.</p> <p>Parameters:</p> Name Type Description Default <code>separator</code> <code>str</code> <p>String used to separate units (default: newline).</p> <code>'\\n'</code> <code>skip_empty</code> <code>bool</code> <p>If True, skip units with empty or whitespace-only text.</p> <code>True</code> <code>show_speaker</code> <code>bool</code> <p>If True, add speaker info.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Concatenated text of all units, separated by <code>separator</code>.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def export_text(self, separator: str = \"\\n\", skip_empty: bool = True, show_speaker: bool = True) -&gt; str:\n    \"\"\"\n    Export the text content of all units as a single string.\n\n    Args:\n        separator: String used to separate units (default: newline).\n        skip_empty: If True, skip units with empty or whitespace-only text.\n        show_speaker: If True, add speaker info.\n\n    Returns:\n        Concatenated text of all units, separated by `separator`.\n    \"\"\"\n    def _text_line(unit: TimedTextUnit) -&gt; str:\n        if show_speaker and unit.speaker:\n            return f\"[{unit.speaker}] {unit.text}\"\n        return unit.text\n\n    texts = [\n        _text_line(unit) for unit in self.units\n        if not skip_empty or unit.text.strip()\n    ]\n    return separator.join(texts)\n</code></pre> <code>extend(units)</code> \u00b6 <p>Add multiple units to the end.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def extend(self, units: List[TimedTextUnit]):\n    \"\"\"Add multiple units to the end.\"\"\"\n    for unit in units:\n        self.append(unit)\n</code></pre> <code>filter_by_min_duration(min_duration_ms)</code> \u00b6 <p>Return a new TimedText object containing only units with a minimum duration.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def filter_by_min_duration(self, min_duration_ms: int) -&gt; \"TimedText\":\n    \"\"\"\n    Return a new TimedText object containing only units with a minimum duration.\n    \"\"\"\n    filtered_units = [\n        unit for unit in self.units\n        if unit.duration_ms &gt;= min_duration_ms\n    ]\n    return self._new_with_units(filtered_units, self.granularity)\n</code></pre> <code>is_segment_granularity()</code> \u00b6 <p>Return True if granularity is SEGMENT.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def is_segment_granularity(self) -&gt; bool:\n    \"\"\"Return True if granularity is SEGMENT.\"\"\"\n    return self.granularity == Granularity.SEGMENT\n</code></pre> <code>is_word_granularity()</code> \u00b6 <p>Return True if granularity is WORD.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def is_word_granularity(self) -&gt; bool:\n    \"\"\"Return True if granularity is WORD.\"\"\"\n    return self.granularity == Granularity.WORD\n</code></pre> <code>iter()</code> \u00b6 <p>Unified iterator over the units of the correct granularity.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def iter(self) -&gt; Iterator[TimedTextUnit]:\n    \"\"\"\n    Unified iterator over the units of the correct granularity.\n    \"\"\"\n    return iter(self.units)\n</code></pre> <code>iter_segments()</code> \u00b6 <p>Iterate over segment-level units.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If granularity is not SEGMENT.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def iter_segments(self) -&gt; Iterator[TimedTextUnit]:\n    \"\"\"\n    Iterate over segment-level units.\n\n    Raises:\n        ValueError: If granularity is not SEGMENT.\n    \"\"\"\n    if not self.is_segment_granularity():\n        raise ValueError(\"Cannot call iter_segments() on TimedText with WORD granularity.\")\n    return iter(self.segments)\n</code></pre> <code>iter_words()</code> \u00b6 <p>Iterate over word-level units.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If granularity is not WORD.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def iter_words(self) -&gt; Iterator[TimedTextUnit]:\n    \"\"\"\n    Iterate over word-level units.\n\n    Raises:\n        ValueError: If granularity is not WORD.\n    \"\"\"\n    if not self.is_word_granularity():\n        raise ValueError(\"Cannot call iter_words() on TimedText with SEGMENT granularity.\")\n    return iter(self.words)\n</code></pre> <code>merge(items)</code> <code>classmethod</code> \u00b6 <p>Merge a list of TimedText objects of the same granularity into a single TimedText object.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>@classmethod\ndef merge(cls, items: List[\"TimedText\"]) -&gt; \"TimedText\":\n    \"\"\"\n    Merge a list of TimedText objects of the same granularity into a single TimedText object.\n    \"\"\"\n    if not items:\n        raise ValueError(\"No TimedText objects to merge.\")\n    granularity = items[0].granularity\n    for item in items:\n        if item.granularity != granularity:\n            raise ValueError(\"Cannot merge TimedText objects of different granularities.\")\n    all_units: List[TimedTextUnit] = []\n    for item in items:\n        all_units.extend(item.units)\n\n    # Use the classmethod to generate with units\n    return cls._new_with_units(all_units, granularity)\n</code></pre> <code>model_post_init(__context)</code> \u00b6 <p>After initialization, sort units by start time and normalize durations.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"\n    After initialization, sort units by start time and normalize durations.\n    \"\"\"\n    self.sort_by_start()\n    for unit in self.units:\n        unit.normalize()\n</code></pre> <code>set_all_speakers(speaker)</code> \u00b6 <p>Set the same speaker for all units.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def set_all_speakers(self, speaker: str) -&gt; None:\n    \"\"\"Set the same speaker for all units.\"\"\"\n    for unit in self.units:\n        unit.set_speaker(speaker)\n</code></pre> <code>set_speaker(index, speaker)</code> \u00b6 <p>Set speaker for a specific unit by index.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def set_speaker(self, index: int, speaker: str) -&gt; None:\n    \"\"\"Set speaker for a specific unit by index.\"\"\"\n    if not (0 &lt;= index &lt; len(self.units)):\n        raise IndexError(f\"Index {index} out of range for units.\")\n    self.units[index].set_speaker(speaker)\n</code></pre> <code>shift(offset_ms)</code> \u00b6 <p>Shift all units by a given offset in milliseconds.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def shift(self, offset_ms: int) -&gt; None:\n    \"\"\"Shift all units by a given offset in milliseconds.\"\"\"\n    for i, unit in enumerate(self.units):\n        self.units[i] = unit.shift_time(offset_ms)\n</code></pre> <code>slice(start_ms, end_ms)</code> \u00b6 <p>Return a new TimedText object containing only units within [start_ms, end_ms]. Units must overlap with the interval to be included.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def slice(self, start_ms: int, end_ms: int) -&gt; \"TimedText\":\n    \"\"\"\n    Return a new TimedText object containing only units within [start_ms, end_ms].\n    Units must overlap with the interval to be included.\n    \"\"\"\n    sliced_units = [\n        unit for unit in self.units\n        if unit.end_ms &gt; start_ms and unit.start_ms &lt; end_ms\n    ]\n    return self._new_with_units(sliced_units, self.granularity)\n</code></pre> <code>sort_by_start()</code> \u00b6 <p>Sort units by start time.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def sort_by_start(self) -&gt; None:\n    \"\"\"Sort units by start time.\"\"\"\n    self.units.sort(key=lambda unit: unit.start_ms)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.timed_object.timed_text.TimedTextUnit","title":"<code>TimedTextUnit</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a timed unit with timestamps.</p> <p>A fundamental building block for subtitle and transcript processing that associates text content with start/end times and optional metadata. Can represent either a segment (phrase/sentence) or a word.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>class TimedTextUnit(BaseModel):\n    \"\"\"\n    Represents a timed unit with timestamps.\n\n    A fundamental building block for subtitle and transcript processing that\n    associates text content with start/end times and optional metadata.\n    Can represent either a segment (phrase/sentence) or a word.\n    \"\"\"\n    text: str = Field(..., description=\"The text content\")\n    start_ms: int = Field(..., description=\"Start time in milliseconds\")\n    end_ms: int = Field(..., description=\"End time in milliseconds\")\n    speaker: Optional[str] = Field(None, description=\"Speaker identifier if available\")\n    index: Optional[int] = Field(None, description=\"Entry index or sequence number\")\n    granularity: Granularity\n    confidence: Optional[float] = Field(None, description=\"Optional confidence score\")\n\n    @property\n    def duration_ms(self) -&gt; int:\n        \"\"\"Get duration in milliseconds.\"\"\"\n        return self.end_ms - self.start_ms\n\n    @property\n    def start_sec(self) -&gt; float:\n        \"\"\"Get start time in seconds.\"\"\"\n        return self.start_ms / 1000\n\n    @property\n    def end_sec(self) -&gt; float:\n        \"\"\"Get end time in seconds.\"\"\"\n        return self.end_ms / 1000\n\n    @property\n    def duration_sec(self) -&gt; float:\n        \"\"\"Get duration in seconds.\"\"\"\n        return self.duration_ms / 1000\n\n    def shift_time(self, offset_ms: int) -&gt; \"TimedTextUnit\":\n        \"\"\"Create a new TimedUnit with timestamps shifted by offset.\"\"\"\n        return self.model_copy(\n            update={\n                \"start_ms\": self.start_ms + offset_ms,\n                \"end_ms\": self.end_ms + offset_ms\n            }\n        )\n\n    def overlaps_with(self, other: \"TimedTextUnit\") -&gt; bool:\n        \"\"\"Check if this unit overlaps with another.\"\"\"\n        return (self.start_ms &lt;= other.end_ms and \n                other.start_ms &lt;= self.end_ms)\n\n    def set_speaker(self, speaker: str) -&gt; None:\n        \"\"\"Set the speaker label.\"\"\"\n        self.speaker = speaker\n\n    def normalize(self) -&gt; None:\n        \"\"\"Normalize the duration of the segment to be nonzero\"\"\"\n        if self.start_ms == self.end_ms:\n            self.end_ms = self.start_ms + 1 # minimum duration \n\n    @field_validator(\"start_ms\", \"end_ms\")\n    @classmethod\n    def _validate_time_non_negative(cls, v: int) -&gt; int:\n        if v &lt; 0:\n            raise ValueError(\"start_ms and end_ms must be non-negative.\")\n        return v\n\n    @field_validator(\"end_ms\")\n    @classmethod\n    def _validate_positive_duration(cls, end_ms: int, info) -&gt; int:\n        start_ms = info.data.get(\"start_ms\")\n        if start_ms is not None and end_ms &lt; start_ms:\n            raise ValueError(\n                f\"end_ms ({end_ms}) must be greater than start_ms ({start_ms}).\"\n            )\n        return end_ms\n\n    @field_validator(\"text\")\n    @classmethod\n    def _validate_word_text(cls, v: str, info):\n        granularity = info.data.get(\"granularity\", \"segment\")\n        if granularity == \"word\" and (\" \" in v.strip()):\n            raise ValueError(\n                \"Text for a word-level TimedUnit cannot contain whitespace.\"\n            )\n        return v\n</code></pre> <code>confidence = Field(None, description='Optional confidence score')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>duration_ms</code> <code>property</code> \u00b6 <p>Get duration in milliseconds.</p> <code>duration_sec</code> <code>property</code> \u00b6 <p>Get duration in seconds.</p> <code>end_ms = Field(..., description='End time in milliseconds')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>end_sec</code> <code>property</code> \u00b6 <p>Get end time in seconds.</p> <code>granularity</code> <code>instance-attribute</code> \u00b6 <code>index = Field(None, description='Entry index or sequence number')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>speaker = Field(None, description='Speaker identifier if available')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>start_ms = Field(..., description='Start time in milliseconds')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>start_sec</code> <code>property</code> \u00b6 <p>Get start time in seconds.</p> <code>text = Field(..., description='The text content')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>normalize()</code> \u00b6 <p>Normalize the duration of the segment to be nonzero</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def normalize(self) -&gt; None:\n    \"\"\"Normalize the duration of the segment to be nonzero\"\"\"\n    if self.start_ms == self.end_ms:\n        self.end_ms = self.start_ms + 1 # minimum duration \n</code></pre> <code>overlaps_with(other)</code> \u00b6 <p>Check if this unit overlaps with another.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def overlaps_with(self, other: \"TimedTextUnit\") -&gt; bool:\n    \"\"\"Check if this unit overlaps with another.\"\"\"\n    return (self.start_ms &lt;= other.end_ms and \n            other.start_ms &lt;= self.end_ms)\n</code></pre> <code>set_speaker(speaker)</code> \u00b6 <p>Set the speaker label.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def set_speaker(self, speaker: str) -&gt; None:\n    \"\"\"Set the speaker label.\"\"\"\n    self.speaker = speaker\n</code></pre> <code>shift_time(offset_ms)</code> \u00b6 <p>Create a new TimedUnit with timestamps shifted by offset.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def shift_time(self, offset_ms: int) -&gt; \"TimedTextUnit\":\n    \"\"\"Create a new TimedUnit with timestamps shifted by offset.\"\"\"\n    return self.model_copy(\n        update={\n            \"start_ms\": self.start_ms + offset_ms,\n            \"end_ms\": self.end_ms + offset_ms\n        }\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription","title":"<code>transcription</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.__all__","title":"<code>__all__ = ['patch_whisper_options', 'DiarizationChunker', 'TimedText', 'TextSegmentBuilder', 'TimedTextUnit', 'Granularity', 'TranscriptionService', 'TranscriptionServiceFactory']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.DiarizationChunker","title":"<code>DiarizationChunker</code>","text":"<p>Class for chunking diarization results into processing units based on configurable duration targets.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/chunker.py</code> <pre><code>class DiarizationChunker:\n    \"\"\"\n    Class for chunking diarization results into processing units\n    based on configurable duration targets.\n    \"\"\"\n\n    def __init__(self, **config_options):\n        \"\"\"Initialize chunker with additional config_options.\"\"\"\n        self.config = ChunkConfig()\n\n        self._handle_config_options(config_options)\n\n\n    def extract_contiguous_chunks(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n        \"\"\"\n        Split diarization segments into contiguous chunks of\n        approximately target_duration, without splitting on speaker changes.\n\n        Args:\n            segments: List of speaker segments from diarization\n\n        Returns:\n            List[Chunk]: Flat list of contiguous chunks\n        \"\"\"\n        if not segments:\n            return []\n\n        extractor = self._ChunkExtractor(self.config, split_on_speaker_change=False)\n        return extractor.extract(segments)\n\n    class _ChunkExtractor:\n        def __init__(self, config: ChunkConfig, split_on_speaker_change: bool = True):\n            self.config = config\n            self.split_on_speaker_change = split_on_speaker_change\n            self.gap_threshold = self.config.gap_threshold\n            self.spacing = self.config.gap_spacing_time\n            self.chunks: List[DiarizationChunk] = []\n            self.current_chunk_segments: List[DiarizedSegment] = []\n            self.chunk_start: int = 0\n            self.current_speaker = \"\"\n            self.accumulated_time: int = 0\n\n        @property\n        def last_segment(self):\n            return self.current_chunk_segments[-1] if self.current_chunk_segments else None\n\n        def extract(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n            if not segments:\n                return []\n\n            self.chunk_start = int(segments[0].start)\n            self.current_speaker = segments[0].speaker\n            for segment in segments:\n                self._check_segment_duration(segment)  \n                self._process_segment(segment)\n\n            self._finalize_last_chunk()\n            return self.chunks\n\n        def _process_segment(self, segment: DiarizedSegment):\n            if self._should_split(segment):\n                self._finalize_current_chunk(segment)\n                self.chunk_start = int(segment.start)                \n            self._add_segment(segment)\n\n        def _add_segment(self, segment: DiarizedSegment):\n            gap_time =  self._gap_time(segment)\n            if gap_time &gt; self.gap_threshold:\n                segment.gap_before = True\n                segment.spacing_time = self.spacing\n                self.accumulated_time += int(segment.duration) + self.spacing\n            else:\n                segment.gap_before = False\n                segment.spacing_time = max(gap_time, 0)\n                self.accumulated_time += int(segment.duration) + gap_time\n            self.current_chunk_segments.append(segment)\n            self.current_speaker = segment.speaker\n\n        def _gap_time(self, segment) -&gt; int:\n            if self.last_segment is None:\n                # If no last_segment, this is first segment, so no gap.\n                return 0 \n            else:\n                return segment.start - self.last_segment.end\n\n\n        def _should_split(self, segment: DiarizedSegment) -&gt; bool:\n            gap_time = self._gap_time(segment)\n            interval_time = gap_time if gap_time &lt; self.gap_threshold else self.spacing\n            accumulated_time = self.accumulated_time + interval_time + segment.duration\n            return accumulated_time &gt;= self.config.target_duration \n\n        def _finalize_current_chunk(self, next_segment: Optional[DiarizedSegment]):\n            if self.current_chunk_segments:\n                assert self.last_segment is not None\n                self.chunks.append(\n                    DiarizationChunk(\n                        start_time=int(self.chunk_start),\n                        end_time=int(self.last_segment.end), \n                        segments=self.current_chunk_segments.copy(),\n                        audio=None,\n                        accumulated_time=self.accumulated_time\n                    )\n                )\n                self._reset_chunk_state(next_segment)             \n\n        def _reset_chunk_state(self, next_segment):\n            self.current_chunk_segments = []\n            self.accumulated_time = 0\n            if self.split_on_speaker_change and next_segment:\n                    self.current_speaker = next_segment.speaker\n\n        def _finalize_last_chunk(self):\n            if self.current_chunk_segments:\n                self._handle_final_segments()\n\n        def _check_segment_duration(self, segment: DiarizedSegment) -&gt; None:\n            \"\"\"Check if segment exceeds target duration and issue warning if needed.\"\"\"\n            if segment.duration &gt; self.config.target_duration:\n                logger.warning(f\"Found segment longer than \"\n                            f\"target duration: {segment.duration_sec:.0f}s\")\n\n        def _handle_final_segments(self) -&gt; None:\n            \"\"\"Append final segments to last chunk if below min duration.\"\"\"\n            approx_remaining_time = sum(segment.duration for segment in self.current_chunk_segments)\n            final_time = self.accumulated_time + approx_remaining_time\n            min_time = self.config.min_duration\n\n            if final_time &lt; min_time and self.chunks:\n               self._merge_to_last_chunk()\n            else:\n                # Create standalone chunk\n                self._finalize_current_chunk(next_segment=None)\n\n        def _merge_to_last_chunk(self):\n            \"\"\"Merge segments to the last chunk processed. self.chunks cannot be empty.\"\"\"\n            assert self.chunks\n            self.chunks[-1].segments.extend(self.current_chunk_segments)\n            self.chunks[-1].end_time = int(self.current_chunk_segments[-1].end)\n            self.chunks[-1].accumulated_time += self.accumulated_time\n\n\n\n    def _handle_config_options(self, config_options: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles additional configuration options, \n        logging a warning for unrecognized keys.\n        \"\"\"\n        for key, value in config_options.items():\n            if hasattr(self.config, key):\n                setattr(self.config, key, value)\n            else:\n                logger.warning(f\"Unrecognized configuration option: {key}\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.DiarizationChunker.config","title":"<code>config = ChunkConfig()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.DiarizationChunker.__init__","title":"<code>__init__(**config_options)</code>","text":"<p>Initialize chunker with additional config_options.</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/chunker.py</code> <pre><code>def __init__(self, **config_options):\n    \"\"\"Initialize chunker with additional config_options.\"\"\"\n    self.config = ChunkConfig()\n\n    self._handle_config_options(config_options)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.DiarizationChunker.extract_contiguous_chunks","title":"<code>extract_contiguous_chunks(segments)</code>","text":"<p>Split diarization segments into contiguous chunks of approximately target_duration, without splitting on speaker changes.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>List[DiarizedSegment]</code> <p>List of speaker segments from diarization</p> required <p>Returns:</p> Type Description <code>List[DiarizationChunk]</code> <p>List[Chunk]: Flat list of contiguous chunks</p> Source code in <code>src/tnh_scholar/audio_processing/diarization/chunker.py</code> <pre><code>def extract_contiguous_chunks(self, segments: List[DiarizedSegment]) -&gt; List[DiarizationChunk]:\n    \"\"\"\n    Split diarization segments into contiguous chunks of\n    approximately target_duration, without splitting on speaker changes.\n\n    Args:\n        segments: List of speaker segments from diarization\n\n    Returns:\n        List[Chunk]: Flat list of contiguous chunks\n    \"\"\"\n    if not segments:\n        return []\n\n    extractor = self._ChunkExtractor(self.config, split_on_speaker_change=False)\n    return extractor.extract(segments)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.Granularity","title":"<code>Granularity</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>class Granularity(str, Enum):\n    SEGMENT = \"segment\"\n    WORD = \"word\"\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.Granularity.SEGMENT","title":"<code>SEGMENT = 'segment'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.Granularity.WORD","title":"<code>WORD = 'word'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder","title":"<code>TextSegmentBuilder</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/transcription/text_segment_builder.py</code> <pre><code>class TextSegmentBuilder:\n    def __init__(\n        self,\n        *,\n        max_duration_ms: Optional[int] = None, # milliseconds\n        target_characters: Optional[int] = None,\n        avoid_orphans: bool = True,\n        max_gap_duration_ms: Optional[int] = None, # milliseconds\n        ignore_speaker: bool = True,\n    ):\n        self.max_duration = max_duration_ms\n        self.target_characters = target_characters\n        self.avoid_orphans = avoid_orphans\n        self.max_gap_duration = max_gap_duration_ms\n        self.ignore_speaker = ignore_speaker\n\n        self.segments: List[TimedTextUnit] = []\n        self.current_words: List[TimedTextUnit] = []\n        self.current_characters = 0\n\n    def create_segments(self, timed_text: TimedText) -&gt; TimedText:\n        # Validate\n        if not timed_text.words:\n            raise ValueError(\n                \"TimedText object must have word-level units to build segments.\"\n                )\n\n        for unit in timed_text.words:\n            if unit.granularity != Granularity.WORD:\n                raise ValueError(f\"Expected WORD units, got {unit.granularity}\")\n\n        # Process\n        for word in timed_text.words:\n            if self._should_start_new_segment(word):\n                self._flush_current_words()\n            self._add_word(word)\n\n        self._flush_current_words()  # Final flush\n        return TimedText(segments=self.segments, granularity=Granularity.SEGMENT)\n\n    def _add_word(self, word: TimedTextUnit):\n        if self.current_words:\n            self.current_characters += 1  # space before the new word\n        self.current_characters += len(word.text)\n        self.current_words.append(word)\n\n\n    def _should_start_new_segment(self, word: TimedTextUnit) -&gt; bool:\n        if not self.current_words:\n            return False\n\n        # Speaker change\n        last_word = self.current_words[-1]\n        if not self.ignore_speaker and (word.speaker != last_word.speaker):\n            return True\n\n        # Significant pause\n        if self.max_gap_duration is not None:\n            pause = word.start_ms - last_word.end_ms\n            if pause &gt; self.max_gap_duration:\n                return True\n\n        # End punctuation\n        if last_word.text and self._is_punctuation_word(last_word.text):\n            return True\n\n        # Max duration\n        if self.max_duration is not None:\n            duration = word.end_ms - self.current_words[0].start_ms\n            if duration &gt; self.max_duration:\n                return True\n\n        # Target character count\n        if self.target_characters is not None:\n            total_chars = self.current_characters + len(word.text) + 1\n            if total_chars &gt; self.target_characters:\n                return True\n\n        return False\n\n    def _flush_current_words(self):\n        if not self.current_words:\n            return\n\n        segment_text = \" \".join(word.text for word in self.current_words)\n        segment = TimedTextUnit(\n            text=segment_text,\n            start_ms=self.current_words[0].start_ms,\n            end_ms=self.current_words[-1].end_ms,\n            granularity=Granularity.SEGMENT,\n            speaker=None if self.ignore_speaker else self._find_speaker(),\n            confidence=None,\n            index=None,\n        )\n        self.segments.append(segment)\n        self.current_words = []\n        self.current_characters = 0\n\n    def _find_speaker(self) -&gt; Optional[str]:\n        \"\"\"\n        Only called when ignore_speakers is False; \n        in this case we always split on speaker. \n        So only one speaker is expected. \n        \"\"\"\n        speakers = {word.speaker for word in self.current_words}\n        assert len(speakers) == 1, \"Inconsistent speakers in segment\"\n        return speakers.pop()\n\n    def _is_punctuation_word(self, word_text: str) -&gt; bool:\n        \"\"\"\n        Check if a word ending in punctuation should trigger a new segment,\n        excluding common abbreviations.\n        \"\"\"\n        if not word_text:\n            return False\n        return word_text[-1] in \".!?\" and word_text.lower() not in COMMON_ABBREVIATIONS\n\n\n    def build_segments(\n        self,\n        *,\n        target_duration: Optional[int] = None,\n        target_characters: Optional[int] = None,\n        avoid_orphans: Optional[bool] = True,\n        max_gap_duration: Optional[int] = None,\n        ignore_speaker: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Build or rebuild `segments` from the contents of `words`.\n\n        Args:\n            target_duration: Maximum desired segment duration in milliseconds.\n            target_characters: Maximum desired character length of a segment.\n            avoid_orphans: If True, prevent extremely short trailing segments.\n\n        Note:\n            This is a stub.  Concrete algorithms will be implemented later.\n\n        Raises:\n            NotImplementedError: Always, until implemented.\n        \"\"\"\n        raise NotImplementedError(\"build_segments is not yet implemented.\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.avoid_orphans","title":"<code>avoid_orphans = avoid_orphans</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.current_characters","title":"<code>current_characters = 0</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.current_words","title":"<code>current_words = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.ignore_speaker","title":"<code>ignore_speaker = ignore_speaker</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.max_duration","title":"<code>max_duration = max_duration_ms</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.max_gap_duration","title":"<code>max_gap_duration = max_gap_duration_ms</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.segments","title":"<code>segments = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.target_characters","title":"<code>target_characters = target_characters</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.__init__","title":"<code>__init__(*, max_duration_ms=None, target_characters=None, avoid_orphans=True, max_gap_duration_ms=None, ignore_speaker=True)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/transcription/text_segment_builder.py</code> <pre><code>def __init__(\n    self,\n    *,\n    max_duration_ms: Optional[int] = None, # milliseconds\n    target_characters: Optional[int] = None,\n    avoid_orphans: bool = True,\n    max_gap_duration_ms: Optional[int] = None, # milliseconds\n    ignore_speaker: bool = True,\n):\n    self.max_duration = max_duration_ms\n    self.target_characters = target_characters\n    self.avoid_orphans = avoid_orphans\n    self.max_gap_duration = max_gap_duration_ms\n    self.ignore_speaker = ignore_speaker\n\n    self.segments: List[TimedTextUnit] = []\n    self.current_words: List[TimedTextUnit] = []\n    self.current_characters = 0\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.build_segments","title":"<code>build_segments(*, target_duration=None, target_characters=None, avoid_orphans=True, max_gap_duration=None, ignore_speaker=False)</code>","text":"<p>Build or rebuild <code>segments</code> from the contents of <code>words</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target_duration</code> <code>Optional[int]</code> <p>Maximum desired segment duration in milliseconds.</p> <code>None</code> <code>target_characters</code> <code>Optional[int]</code> <p>Maximum desired character length of a segment.</p> <code>None</code> <code>avoid_orphans</code> <code>Optional[bool]</code> <p>If True, prevent extremely short trailing segments.</p> <code>True</code> Note <p>This is a stub.  Concrete algorithms will be implemented later.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always, until implemented.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/text_segment_builder.py</code> <pre><code>def build_segments(\n    self,\n    *,\n    target_duration: Optional[int] = None,\n    target_characters: Optional[int] = None,\n    avoid_orphans: Optional[bool] = True,\n    max_gap_duration: Optional[int] = None,\n    ignore_speaker: bool = False,\n) -&gt; None:\n    \"\"\"\n    Build or rebuild `segments` from the contents of `words`.\n\n    Args:\n        target_duration: Maximum desired segment duration in milliseconds.\n        target_characters: Maximum desired character length of a segment.\n        avoid_orphans: If True, prevent extremely short trailing segments.\n\n    Note:\n        This is a stub.  Concrete algorithms will be implemented later.\n\n    Raises:\n        NotImplementedError: Always, until implemented.\n    \"\"\"\n    raise NotImplementedError(\"build_segments is not yet implemented.\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TextSegmentBuilder.create_segments","title":"<code>create_segments(timed_text)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/transcription/text_segment_builder.py</code> <pre><code>def create_segments(self, timed_text: TimedText) -&gt; TimedText:\n    # Validate\n    if not timed_text.words:\n        raise ValueError(\n            \"TimedText object must have word-level units to build segments.\"\n            )\n\n    for unit in timed_text.words:\n        if unit.granularity != Granularity.WORD:\n            raise ValueError(f\"Expected WORD units, got {unit.granularity}\")\n\n    # Process\n    for word in timed_text.words:\n        if self._should_start_new_segment(word):\n            self._flush_current_words()\n        self._add_word(word)\n\n    self._flush_current_words()  # Final flush\n    return TimedText(segments=self.segments, granularity=Granularity.SEGMENT)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText","title":"<code>TimedText</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a collection of timed text units of a single granularity.</p> <p>Only one of <code>segments</code> or <code>words</code> is populated, determined by <code>granularity</code>. All units must match the declared granularity.</p> Notes <ul> <li>Start times must be non-decreasing (overlaps allowed for multiple speakers).</li> <li>Negative start_ms or end_ms values are not allowed.</li> <li>Durations must be strictly positive (&gt;0 ms).</li> <li>Mixed granularity is strictly prohibited.</li> </ul> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>class TimedText(BaseModel):\n    \"\"\"\n    Represents a collection of timed text units of a single granularity.\n\n    Only one of `segments` or `words` is populated, determined by `granularity`.\n    All units must match the declared granularity.\n\n    Notes:\n        - Start times must be non-decreasing (overlaps allowed for multiple speakers).\n        - Negative start_ms or end_ms values are not allowed.\n        - Durations must be strictly positive (&gt;0 ms).\n        - Mixed granularity is strictly prohibited.\n    \"\"\"\n\n    granularity: Granularity = Field(..., description=\"Granularity type for all units.\")\n    segments: List[TimedTextUnit] = Field(default_factory=list, description=\"Phrase-level timed units\")\n    words: List[TimedTextUnit] = Field(default_factory=list, description=\"Word-level timed units\")\n\n    def __init__(\n        self,\n        *,\n        granularity: Optional[Granularity] = None,\n        segments: Optional[List[TimedTextUnit]] = None,\n        words: Optional[List[TimedTextUnit]] = None,\n        units: Optional[List[TimedTextUnit]] = None,\n        **kwargs\n    ):\n        \"\"\"\n        Custom initializer for TimedText.\n        If `units` is provided, granularity is inferred from the first unit unless explicitly set.\n        If only `segments` or `words` is provided, granularity is set accordingly.\n        If all are empty, granularity must be provided.\n        \"\"\"\n        segments = segments or []\n        words = words or []\n        if units is not None:\n            if units:\n                inferred_granularity = units[0].granularity\n                granularity = granularity or inferred_granularity\n                if granularity == Granularity.SEGMENT:\n                    segments = units\n                    words = []\n                elif granularity == Granularity.WORD:\n                    words = units\n                    segments = []\n                else:\n                    raise ValueError(\"Invalid granularity inferred from units.\")\n            else:\n                if granularity is None:\n                    raise ValueError(\"Must provide granularity for empty TimedText.\")\n        elif segments:\n            granularity = granularity or Granularity.SEGMENT\n            words = []\n        elif words:\n            granularity = granularity or Granularity.WORD\n            segments = []\n        elif granularity is None:\n            raise ValueError(\"Must provide granularity for empty TimedText.\")\n\n        super().__init__(granularity=granularity, segments=segments, words=words, **kwargs)\n\n    @model_validator(mode=\"after\")\n    def _validate_exclusive_granularity(self):\n        \"\"\"\n        Validate that TimedText contains only units matching its granularity.\n        Allows empty TimedText objects for prototyping and construction.\n        Modular logic for segments and words.\n        \"\"\"\n        granularity = self.granularity\n        segments = self.segments\n        words = self.words\n\n        if granularity == Granularity.SEGMENT:\n            if words:\n                raise ValueError(\"TimedText with SEGMENT granularity must not have word units.\")\n            for unit in segments:\n                if unit.granularity != Granularity.SEGMENT:\n                    raise ValueError(\"All segment units must have granularity SEGMENT.\")\n        elif granularity == Granularity.WORD:\n            if segments:\n                raise ValueError(\"TimedText with WORD granularity must not have segment units.\")\n            for unit in words:\n                if unit.granularity != Granularity.WORD:\n                    raise ValueError(\"All word units must have granularity WORD.\")\n        else:\n            raise ValueError(\"Invalid granularity type.\")\n        return self\n\n    def model_post_init(self, __context) -&gt; None:\n        \"\"\"\n        After initialization, sort units by start time and normalize durations.\n        \"\"\"\n        self.sort_by_start()\n        for unit in self.units:\n            unit.normalize()\n\n    @property\n    def units(self) -&gt; List[TimedTextUnit]:\n        \"\"\"Return the list of units matching the granularity.\"\"\"\n        return self.segments if self.granularity == Granularity.SEGMENT else self.words\n\n    def is_segment_granularity(self) -&gt; bool:\n        \"\"\"Return True if granularity is SEGMENT.\"\"\"\n        return self.granularity == Granularity.SEGMENT\n\n    def is_word_granularity(self) -&gt; bool:\n        \"\"\"Return True if granularity is WORD.\"\"\"\n        return self.granularity == Granularity.WORD\n\n    @property\n    def start_ms(self) -&gt; int:\n        \"\"\"Get the start time of the earliest unit.\"\"\"\n        return min(unit.start_ms for unit in self.units) if self.units else 0\n\n    @property\n    def end_ms(self) -&gt; int:\n        \"\"\"Get the end time of the latest unit.\"\"\"\n        return max(unit.end_ms for unit in self.units) if self.units else 0\n\n    @property\n    def duration(self) -&gt; int:\n        \"\"\"Get the total duration in milliseconds.\"\"\"\n        return self.end_ms - self.start_ms\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of units.\"\"\"\n        return len(self.units)\n\n    def append(self, unit: TimedTextUnit):\n        \"\"\"Add a unit to the end.\"\"\"\n        if unit.granularity != self.granularity:\n            raise ValueError(f\"Cannot append unit with granularity {unit.granularity} \"\n                             \"to TimedText of granularity {self.granularity}.\")\n        self.units.append(unit)\n\n    def extend(self, units: List[TimedTextUnit]):\n        \"\"\"Add multiple units to the end.\"\"\"\n        for unit in units:\n            self.append(unit)\n\n    def clear(self):\n        \"\"\"Remove all units.\"\"\"\n        self.units.clear()\n\n    def set_speaker(self, index: int, speaker: str) -&gt; None:\n        \"\"\"Set speaker for a specific unit by index.\"\"\"\n        if not (0 &lt;= index &lt; len(self.units)):\n            raise IndexError(f\"Index {index} out of range for units.\")\n        self.units[index].set_speaker(speaker)\n\n    def set_all_speakers(self, speaker: str) -&gt; None:\n        \"\"\"Set the same speaker for all units.\"\"\"\n        for unit in self.units:\n            unit.set_speaker(speaker)\n\n    def shift(self, offset_ms: int) -&gt; None:\n        \"\"\"Shift all units by a given offset in milliseconds.\"\"\"\n        for i, unit in enumerate(self.units):\n            self.units[i] = unit.shift_time(offset_ms)\n\n    def sort_by_start(self) -&gt; None:\n        \"\"\"Sort units by start time.\"\"\"\n        self.units.sort(key=lambda unit: unit.start_ms)\n\n\n    @classmethod\n    def _new_with_units(\n        cls, units: List[TimedTextUnit], granularity: Optional[Granularity] = None\n    ) -&gt; \"TimedText\":\n        \"\"\"\n        Helper to create a new TimedText object with the given granularity and units.\n        If granularity is not provided, it is inferred from the first unit.\n        \"\"\"\n        if units:\n            inferred_granularity = units[0].granularity\n            granularity = granularity or inferred_granularity\n            if granularity == Granularity.SEGMENT:\n                return cls(granularity=granularity, segments=units, words=[])\n            elif granularity == Granularity.WORD:\n                return cls(granularity=granularity, segments=[], words=units)\n            else:\n                raise ValueError(\"Invalid granularity inferred from units.\")\n        else:\n            if granularity is None:\n                raise ValueError(\"Must provide granularity for empty TimedText.\")\n            if granularity in [Granularity.SEGMENT, Granularity.WORD]:\n                return cls(granularity=granularity, segments=[], words=[])\n            else:\n                raise ValueError(\"Invalid granularity provided.\")\n\n    def slice(self, start_ms: int, end_ms: int) -&gt; \"TimedText\":\n        \"\"\"\n        Return a new TimedText object containing only units within [start_ms, end_ms].\n        Units must overlap with the interval to be included.\n        \"\"\"\n        sliced_units = [\n            unit for unit in self.units\n            if unit.end_ms &gt; start_ms and unit.start_ms &lt; end_ms\n        ]\n        return self._new_with_units(sliced_units, self.granularity)\n\n    def filter_by_min_duration(self, min_duration_ms: int) -&gt; \"TimedText\":\n        \"\"\"\n        Return a new TimedText object containing only units with a minimum duration.\n        \"\"\"\n        filtered_units = [\n            unit for unit in self.units\n            if unit.duration_ms &gt;= min_duration_ms\n        ]\n        return self._new_with_units(filtered_units, self.granularity)\n\n    @classmethod\n    def merge(cls, items: List[\"TimedText\"]) -&gt; \"TimedText\":\n        \"\"\"\n        Merge a list of TimedText objects of the same granularity into a single TimedText object.\n        \"\"\"\n        if not items:\n            raise ValueError(\"No TimedText objects to merge.\")\n        granularity = items[0].granularity\n        for item in items:\n            if item.granularity != granularity:\n                raise ValueError(\"Cannot merge TimedText objects of different granularities.\")\n        all_units: List[TimedTextUnit] = []\n        for item in items:\n            all_units.extend(item.units)\n\n        # Use the classmethod to generate with units\n        return cls._new_with_units(all_units, granularity)\n\n    def iter(self) -&gt; Iterator[TimedTextUnit]:\n        \"\"\"\n        Unified iterator over the units of the correct granularity.\n        \"\"\"\n        return iter(self.units)\n\n    def iter_segments(self) -&gt; Iterator[TimedTextUnit]:\n        \"\"\"\n        Iterate over segment-level units.\n\n        Raises:\n            ValueError: If granularity is not SEGMENT.\n        \"\"\"\n        if not self.is_segment_granularity():\n            raise ValueError(\"Cannot call iter_segments() on TimedText with WORD granularity.\")\n        return iter(self.segments)\n\n    def iter_words(self) -&gt; Iterator[TimedTextUnit]:\n        \"\"\"\n        Iterate over word-level units.\n\n        Raises:\n            ValueError: If granularity is not WORD.\n        \"\"\"\n        if not self.is_word_granularity():\n            raise ValueError(\"Cannot call iter_words() on TimedText with SEGMENT granularity.\")\n        return iter(self.words)\n\n    def export_text(self, separator: str = \"\\n\", skip_empty: bool = True, show_speaker: bool = True) -&gt; str:\n        \"\"\"\n        Export the text content of all units as a single string.\n\n        Args:\n            separator: String used to separate units (default: newline).\n            skip_empty: If True, skip units with empty or whitespace-only text.\n            show_speaker: If True, add speaker info.\n\n        Returns:\n            Concatenated text of all units, separated by `separator`.\n        \"\"\"\n        def _text_line(unit: TimedTextUnit) -&gt; str:\n            if show_speaker and unit.speaker:\n                return f\"[{unit.speaker}] {unit.text}\"\n            return unit.text\n\n        texts = [\n            _text_line(unit) for unit in self.units\n            if not skip_empty or unit.text.strip()\n        ]\n        return separator.join(texts)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.duration","title":"<code>duration</code>  <code>property</code>","text":"<p>Get the total duration in milliseconds.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.end_ms","title":"<code>end_ms</code>  <code>property</code>","text":"<p>Get the end time of the latest unit.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.granularity","title":"<code>granularity = Field(..., description='Granularity type for all units.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.segments","title":"<code>segments = Field(default_factory=list, description='Phrase-level timed units')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.start_ms","title":"<code>start_ms</code>  <code>property</code>","text":"<p>Get the start time of the earliest unit.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.units","title":"<code>units</code>  <code>property</code>","text":"<p>Return the list of units matching the granularity.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.words","title":"<code>words = Field(default_factory=list, description='Word-level timed units')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.__init__","title":"<code>__init__(*, granularity=None, segments=None, words=None, units=None, **kwargs)</code>","text":"<p>Custom initializer for TimedText. If <code>units</code> is provided, granularity is inferred from the first unit unless explicitly set. If only <code>segments</code> or <code>words</code> is provided, granularity is set accordingly. If all are empty, granularity must be provided.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def __init__(\n    self,\n    *,\n    granularity: Optional[Granularity] = None,\n    segments: Optional[List[TimedTextUnit]] = None,\n    words: Optional[List[TimedTextUnit]] = None,\n    units: Optional[List[TimedTextUnit]] = None,\n    **kwargs\n):\n    \"\"\"\n    Custom initializer for TimedText.\n    If `units` is provided, granularity is inferred from the first unit unless explicitly set.\n    If only `segments` or `words` is provided, granularity is set accordingly.\n    If all are empty, granularity must be provided.\n    \"\"\"\n    segments = segments or []\n    words = words or []\n    if units is not None:\n        if units:\n            inferred_granularity = units[0].granularity\n            granularity = granularity or inferred_granularity\n            if granularity == Granularity.SEGMENT:\n                segments = units\n                words = []\n            elif granularity == Granularity.WORD:\n                words = units\n                segments = []\n            else:\n                raise ValueError(\"Invalid granularity inferred from units.\")\n        else:\n            if granularity is None:\n                raise ValueError(\"Must provide granularity for empty TimedText.\")\n    elif segments:\n        granularity = granularity or Granularity.SEGMENT\n        words = []\n    elif words:\n        granularity = granularity or Granularity.WORD\n        segments = []\n    elif granularity is None:\n        raise ValueError(\"Must provide granularity for empty TimedText.\")\n\n    super().__init__(granularity=granularity, segments=segments, words=words, **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of units.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of units.\"\"\"\n    return len(self.units)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.append","title":"<code>append(unit)</code>","text":"<p>Add a unit to the end.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def append(self, unit: TimedTextUnit):\n    \"\"\"Add a unit to the end.\"\"\"\n    if unit.granularity != self.granularity:\n        raise ValueError(f\"Cannot append unit with granularity {unit.granularity} \"\n                         \"to TimedText of granularity {self.granularity}.\")\n    self.units.append(unit)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.clear","title":"<code>clear()</code>","text":"<p>Remove all units.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def clear(self):\n    \"\"\"Remove all units.\"\"\"\n    self.units.clear()\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.export_text","title":"<code>export_text(separator='\\n', skip_empty=True, show_speaker=True)</code>","text":"<p>Export the text content of all units as a single string.</p> <p>Parameters:</p> Name Type Description Default <code>separator</code> <code>str</code> <p>String used to separate units (default: newline).</p> <code>'\\n'</code> <code>skip_empty</code> <code>bool</code> <p>If True, skip units with empty or whitespace-only text.</p> <code>True</code> <code>show_speaker</code> <code>bool</code> <p>If True, add speaker info.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Concatenated text of all units, separated by <code>separator</code>.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def export_text(self, separator: str = \"\\n\", skip_empty: bool = True, show_speaker: bool = True) -&gt; str:\n    \"\"\"\n    Export the text content of all units as a single string.\n\n    Args:\n        separator: String used to separate units (default: newline).\n        skip_empty: If True, skip units with empty or whitespace-only text.\n        show_speaker: If True, add speaker info.\n\n    Returns:\n        Concatenated text of all units, separated by `separator`.\n    \"\"\"\n    def _text_line(unit: TimedTextUnit) -&gt; str:\n        if show_speaker and unit.speaker:\n            return f\"[{unit.speaker}] {unit.text}\"\n        return unit.text\n\n    texts = [\n        _text_line(unit) for unit in self.units\n        if not skip_empty or unit.text.strip()\n    ]\n    return separator.join(texts)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.extend","title":"<code>extend(units)</code>","text":"<p>Add multiple units to the end.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def extend(self, units: List[TimedTextUnit]):\n    \"\"\"Add multiple units to the end.\"\"\"\n    for unit in units:\n        self.append(unit)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.filter_by_min_duration","title":"<code>filter_by_min_duration(min_duration_ms)</code>","text":"<p>Return a new TimedText object containing only units with a minimum duration.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def filter_by_min_duration(self, min_duration_ms: int) -&gt; \"TimedText\":\n    \"\"\"\n    Return a new TimedText object containing only units with a minimum duration.\n    \"\"\"\n    filtered_units = [\n        unit for unit in self.units\n        if unit.duration_ms &gt;= min_duration_ms\n    ]\n    return self._new_with_units(filtered_units, self.granularity)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.is_segment_granularity","title":"<code>is_segment_granularity()</code>","text":"<p>Return True if granularity is SEGMENT.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def is_segment_granularity(self) -&gt; bool:\n    \"\"\"Return True if granularity is SEGMENT.\"\"\"\n    return self.granularity == Granularity.SEGMENT\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.is_word_granularity","title":"<code>is_word_granularity()</code>","text":"<p>Return True if granularity is WORD.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def is_word_granularity(self) -&gt; bool:\n    \"\"\"Return True if granularity is WORD.\"\"\"\n    return self.granularity == Granularity.WORD\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.iter","title":"<code>iter()</code>","text":"<p>Unified iterator over the units of the correct granularity.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def iter(self) -&gt; Iterator[TimedTextUnit]:\n    \"\"\"\n    Unified iterator over the units of the correct granularity.\n    \"\"\"\n    return iter(self.units)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.iter_segments","title":"<code>iter_segments()</code>","text":"<p>Iterate over segment-level units.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If granularity is not SEGMENT.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def iter_segments(self) -&gt; Iterator[TimedTextUnit]:\n    \"\"\"\n    Iterate over segment-level units.\n\n    Raises:\n        ValueError: If granularity is not SEGMENT.\n    \"\"\"\n    if not self.is_segment_granularity():\n        raise ValueError(\"Cannot call iter_segments() on TimedText with WORD granularity.\")\n    return iter(self.segments)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.iter_words","title":"<code>iter_words()</code>","text":"<p>Iterate over word-level units.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If granularity is not WORD.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def iter_words(self) -&gt; Iterator[TimedTextUnit]:\n    \"\"\"\n    Iterate over word-level units.\n\n    Raises:\n        ValueError: If granularity is not WORD.\n    \"\"\"\n    if not self.is_word_granularity():\n        raise ValueError(\"Cannot call iter_words() on TimedText with SEGMENT granularity.\")\n    return iter(self.words)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.merge","title":"<code>merge(items)</code>  <code>classmethod</code>","text":"<p>Merge a list of TimedText objects of the same granularity into a single TimedText object.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>@classmethod\ndef merge(cls, items: List[\"TimedText\"]) -&gt; \"TimedText\":\n    \"\"\"\n    Merge a list of TimedText objects of the same granularity into a single TimedText object.\n    \"\"\"\n    if not items:\n        raise ValueError(\"No TimedText objects to merge.\")\n    granularity = items[0].granularity\n    for item in items:\n        if item.granularity != granularity:\n            raise ValueError(\"Cannot merge TimedText objects of different granularities.\")\n    all_units: List[TimedTextUnit] = []\n    for item in items:\n        all_units.extend(item.units)\n\n    # Use the classmethod to generate with units\n    return cls._new_with_units(all_units, granularity)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>After initialization, sort units by start time and normalize durations.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"\n    After initialization, sort units by start time and normalize durations.\n    \"\"\"\n    self.sort_by_start()\n    for unit in self.units:\n        unit.normalize()\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.set_all_speakers","title":"<code>set_all_speakers(speaker)</code>","text":"<p>Set the same speaker for all units.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def set_all_speakers(self, speaker: str) -&gt; None:\n    \"\"\"Set the same speaker for all units.\"\"\"\n    for unit in self.units:\n        unit.set_speaker(speaker)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.set_speaker","title":"<code>set_speaker(index, speaker)</code>","text":"<p>Set speaker for a specific unit by index.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def set_speaker(self, index: int, speaker: str) -&gt; None:\n    \"\"\"Set speaker for a specific unit by index.\"\"\"\n    if not (0 &lt;= index &lt; len(self.units)):\n        raise IndexError(f\"Index {index} out of range for units.\")\n    self.units[index].set_speaker(speaker)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.shift","title":"<code>shift(offset_ms)</code>","text":"<p>Shift all units by a given offset in milliseconds.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def shift(self, offset_ms: int) -&gt; None:\n    \"\"\"Shift all units by a given offset in milliseconds.\"\"\"\n    for i, unit in enumerate(self.units):\n        self.units[i] = unit.shift_time(offset_ms)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.slice","title":"<code>slice(start_ms, end_ms)</code>","text":"<p>Return a new TimedText object containing only units within [start_ms, end_ms]. Units must overlap with the interval to be included.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def slice(self, start_ms: int, end_ms: int) -&gt; \"TimedText\":\n    \"\"\"\n    Return a new TimedText object containing only units within [start_ms, end_ms].\n    Units must overlap with the interval to be included.\n    \"\"\"\n    sliced_units = [\n        unit for unit in self.units\n        if unit.end_ms &gt; start_ms and unit.start_ms &lt; end_ms\n    ]\n    return self._new_with_units(sliced_units, self.granularity)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedText.sort_by_start","title":"<code>sort_by_start()</code>","text":"<p>Sort units by start time.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def sort_by_start(self) -&gt; None:\n    \"\"\"Sort units by start time.\"\"\"\n    self.units.sort(key=lambda unit: unit.start_ms)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit","title":"<code>TimedTextUnit</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a timed unit with timestamps.</p> <p>A fundamental building block for subtitle and transcript processing that associates text content with start/end times and optional metadata. Can represent either a segment (phrase/sentence) or a word.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>class TimedTextUnit(BaseModel):\n    \"\"\"\n    Represents a timed unit with timestamps.\n\n    A fundamental building block for subtitle and transcript processing that\n    associates text content with start/end times and optional metadata.\n    Can represent either a segment (phrase/sentence) or a word.\n    \"\"\"\n    text: str = Field(..., description=\"The text content\")\n    start_ms: int = Field(..., description=\"Start time in milliseconds\")\n    end_ms: int = Field(..., description=\"End time in milliseconds\")\n    speaker: Optional[str] = Field(None, description=\"Speaker identifier if available\")\n    index: Optional[int] = Field(None, description=\"Entry index or sequence number\")\n    granularity: Granularity\n    confidence: Optional[float] = Field(None, description=\"Optional confidence score\")\n\n    @property\n    def duration_ms(self) -&gt; int:\n        \"\"\"Get duration in milliseconds.\"\"\"\n        return self.end_ms - self.start_ms\n\n    @property\n    def start_sec(self) -&gt; float:\n        \"\"\"Get start time in seconds.\"\"\"\n        return self.start_ms / 1000\n\n    @property\n    def end_sec(self) -&gt; float:\n        \"\"\"Get end time in seconds.\"\"\"\n        return self.end_ms / 1000\n\n    @property\n    def duration_sec(self) -&gt; float:\n        \"\"\"Get duration in seconds.\"\"\"\n        return self.duration_ms / 1000\n\n    def shift_time(self, offset_ms: int) -&gt; \"TimedTextUnit\":\n        \"\"\"Create a new TimedUnit with timestamps shifted by offset.\"\"\"\n        return self.model_copy(\n            update={\n                \"start_ms\": self.start_ms + offset_ms,\n                \"end_ms\": self.end_ms + offset_ms\n            }\n        )\n\n    def overlaps_with(self, other: \"TimedTextUnit\") -&gt; bool:\n        \"\"\"Check if this unit overlaps with another.\"\"\"\n        return (self.start_ms &lt;= other.end_ms and \n                other.start_ms &lt;= self.end_ms)\n\n    def set_speaker(self, speaker: str) -&gt; None:\n        \"\"\"Set the speaker label.\"\"\"\n        self.speaker = speaker\n\n    def normalize(self) -&gt; None:\n        \"\"\"Normalize the duration of the segment to be nonzero\"\"\"\n        if self.start_ms == self.end_ms:\n            self.end_ms = self.start_ms + 1 # minimum duration \n\n    @field_validator(\"start_ms\", \"end_ms\")\n    @classmethod\n    def _validate_time_non_negative(cls, v: int) -&gt; int:\n        if v &lt; 0:\n            raise ValueError(\"start_ms and end_ms must be non-negative.\")\n        return v\n\n    @field_validator(\"end_ms\")\n    @classmethod\n    def _validate_positive_duration(cls, end_ms: int, info) -&gt; int:\n        start_ms = info.data.get(\"start_ms\")\n        if start_ms is not None and end_ms &lt; start_ms:\n            raise ValueError(\n                f\"end_ms ({end_ms}) must be greater than start_ms ({start_ms}).\"\n            )\n        return end_ms\n\n    @field_validator(\"text\")\n    @classmethod\n    def _validate_word_text(cls, v: str, info):\n        granularity = info.data.get(\"granularity\", \"segment\")\n        if granularity == \"word\" and (\" \" in v.strip()):\n            raise ValueError(\n                \"Text for a word-level TimedUnit cannot contain whitespace.\"\n            )\n        return v\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.confidence","title":"<code>confidence = Field(None, description='Optional confidence score')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.duration_ms","title":"<code>duration_ms</code>  <code>property</code>","text":"<p>Get duration in milliseconds.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.duration_sec","title":"<code>duration_sec</code>  <code>property</code>","text":"<p>Get duration in seconds.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.end_ms","title":"<code>end_ms = Field(..., description='End time in milliseconds')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.end_sec","title":"<code>end_sec</code>  <code>property</code>","text":"<p>Get end time in seconds.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.granularity","title":"<code>granularity</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.index","title":"<code>index = Field(None, description='Entry index or sequence number')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.speaker","title":"<code>speaker = Field(None, description='Speaker identifier if available')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.start_ms","title":"<code>start_ms = Field(..., description='Start time in milliseconds')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.start_sec","title":"<code>start_sec</code>  <code>property</code>","text":"<p>Get start time in seconds.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.text","title":"<code>text = Field(..., description='The text content')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.normalize","title":"<code>normalize()</code>","text":"<p>Normalize the duration of the segment to be nonzero</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def normalize(self) -&gt; None:\n    \"\"\"Normalize the duration of the segment to be nonzero\"\"\"\n    if self.start_ms == self.end_ms:\n        self.end_ms = self.start_ms + 1 # minimum duration \n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.overlaps_with","title":"<code>overlaps_with(other)</code>","text":"<p>Check if this unit overlaps with another.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def overlaps_with(self, other: \"TimedTextUnit\") -&gt; bool:\n    \"\"\"Check if this unit overlaps with another.\"\"\"\n    return (self.start_ms &lt;= other.end_ms and \n            other.start_ms &lt;= self.end_ms)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.set_speaker","title":"<code>set_speaker(speaker)</code>","text":"<p>Set the speaker label.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def set_speaker(self, speaker: str) -&gt; None:\n    \"\"\"Set the speaker label.\"\"\"\n    self.speaker = speaker\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TimedTextUnit.shift_time","title":"<code>shift_time(offset_ms)</code>","text":"<p>Create a new TimedUnit with timestamps shifted by offset.</p> Source code in <code>src/tnh_scholar/audio_processing/timed_object/timed_text.py</code> <pre><code>def shift_time(self, offset_ms: int) -&gt; \"TimedTextUnit\":\n    \"\"\"Create a new TimedUnit with timestamps shifted by offset.\"\"\"\n    return self.model_copy(\n        update={\n            \"start_ms\": self.start_ms + offset_ms,\n            \"end_ms\": self.end_ms + offset_ms\n        }\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TranscriptionService","title":"<code>TranscriptionService</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for transcription services.</p> <p>This interface provides a standard way to interact with different transcription service providers (e.g., OpenAI Whisper, AssemblyAI).</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>class TranscriptionService(ABC):\n    \"\"\"\n    Abstract base class defining the interface for transcription services.\n\n    This interface provides a standard way to interact with different\n    transcription service providers (e.g., OpenAI Whisper, AssemblyAI).\n    \"\"\"\n\n    @abstractmethod\n    def transcribe(\n        self,\n        audio_file: Union[Path, BytesIO],\n        options: Optional[Dict[str, Any]] = None\n    ) -&gt; TranscriptionResult:\n        \"\"\"\n        Transcribe audio file to text.\n\n        Args:\n            audio_file: Path to audio file or file-like object\n            options: Provider-specific options for transcription\n\n        Returns:\n            TranscriptionResult\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_result(self, job_id: str) -&gt; TranscriptionResult:\n        \"\"\"\n        Get results for an existing transcription job.\n\n        Args:\n            job_id: ID of the transcription job\n\n        Returns:\n            Dictionary containing transcription results in the same\n            standardized format as transcribe()\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def transcribe_to_format(\n        self,\n        audio_file: Union[Path, BytesIO],\n        format_type: str = \"srt\",\n        transcription_options: Optional[Dict[str, Any]] = None,\n        format_options: Optional[Dict[str, Any]] = None\n    ) -&gt; str:\n        \"\"\"\n        Transcribe audio and return result in specified format.\n\n        Args:\n            audio_file: Path, file-like object, or URL of audio file\n            format_type: Format type (e.g., \"srt\", \"vtt\", \"text\")\n            transcription_options: Options for transcription\n            format_options: Format-specific options\n\n        Returns:\n            String representation in the requested format\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TranscriptionService.get_result","title":"<code>get_result(job_id)</code>  <code>abstractmethod</code>","text":"<p>Get results for an existing transcription job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>ID of the transcription job</p> required <p>Returns:</p> Type Description <code>TranscriptionResult</code> <p>Dictionary containing transcription results in the same</p> <code>TranscriptionResult</code> <p>standardized format as transcribe()</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>@abstractmethod\ndef get_result(self, job_id: str) -&gt; TranscriptionResult:\n    \"\"\"\n    Get results for an existing transcription job.\n\n    Args:\n        job_id: ID of the transcription job\n\n    Returns:\n        Dictionary containing transcription results in the same\n        standardized format as transcribe()\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TranscriptionService.transcribe","title":"<code>transcribe(audio_file, options=None)</code>  <code>abstractmethod</code>","text":"<p>Transcribe audio file to text.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Union[Path, BytesIO]</code> <p>Path to audio file or file-like object</p> required <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Provider-specific options for transcription</p> <code>None</code> <p>Returns:</p> Type Description <code>TranscriptionResult</code> <p>TranscriptionResult</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>@abstractmethod\ndef transcribe(\n    self,\n    audio_file: Union[Path, BytesIO],\n    options: Optional[Dict[str, Any]] = None\n) -&gt; TranscriptionResult:\n    \"\"\"\n    Transcribe audio file to text.\n\n    Args:\n        audio_file: Path to audio file or file-like object\n        options: Provider-specific options for transcription\n\n    Returns:\n        TranscriptionResult\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TranscriptionService.transcribe_to_format","title":"<code>transcribe_to_format(audio_file, format_type='srt', transcription_options=None, format_options=None)</code>  <code>abstractmethod</code>","text":"<p>Transcribe audio and return result in specified format.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Union[Path, BytesIO]</code> <p>Path, file-like object, or URL of audio file</p> required <code>format_type</code> <code>str</code> <p>Format type (e.g., \"srt\", \"vtt\", \"text\")</p> <code>'srt'</code> <code>transcription_options</code> <code>Optional[Dict[str, Any]]</code> <p>Options for transcription</p> <code>None</code> <code>format_options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation in the requested format</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>@abstractmethod\ndef transcribe_to_format(\n    self,\n    audio_file: Union[Path, BytesIO],\n    format_type: str = \"srt\",\n    transcription_options: Optional[Dict[str, Any]] = None,\n    format_options: Optional[Dict[str, Any]] = None\n) -&gt; str:\n    \"\"\"\n    Transcribe audio and return result in specified format.\n\n    Args:\n        audio_file: Path, file-like object, or URL of audio file\n        format_type: Format type (e.g., \"srt\", \"vtt\", \"text\")\n        transcription_options: Options for transcription\n        format_options: Format-specific options\n\n    Returns:\n        String representation in the requested format\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TranscriptionServiceFactory","title":"<code>TranscriptionServiceFactory</code>","text":"<p>Factory for creating transcription service instances.</p> <p>This factory provides a standard way to create transcription service instances based on the provider name and configuration.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>class TranscriptionServiceFactory:\n    \"\"\"\n    Factory for creating transcription service instances.\n\n    This factory provides a standard way to create transcription service\n    instances based on the provider name and configuration.\n    \"\"\"\n\n    # Mapping provider names to implementation classes\n    # Classes will be imported lazily when needed\n    _PROVIDER_MAP: Dict[str, Callable[..., TranscriptionService]] = {}\n\n    @classmethod\n    def register_provider(\n        cls, \n        name: str, \n        provider_class: Callable[..., TranscriptionService]\n    ) -&gt; None:\n        \"\"\"\n        Register a provider implementation with the factory.\n\n        Args:\n            name: Provider name (lowercase)\n            provider_class: Provider implementation class or factory function\n\n        Example:\n            &gt;&gt;&gt; from my_module import MyTranscriptionService\n            &gt;&gt;&gt; TranscriptionServiceFactory.register_provider(\"my_provider\", MyTranscriptionService)\n        \"\"\"  \n        cls._PROVIDER_MAP[name.lower()] = provider_class\n\n    @classmethod\n    def create_service(\n        cls,\n        provider: str = \"assemblyai\",\n        api_key: Optional[str] = None,\n        **kwargs: Any\n    ) -&gt; TranscriptionService:\n        \"\"\"\n        Create a transcription service instance.\n\n        Args:\n            provider: Service provider name (e.g., \"whisper\", \"assemblyai\")\n            api_key: API key for the service\n            **kwargs: Additional provider-specific configuration\n\n        Returns:\n            TranscriptionService instance\n\n        Raises:\n            ValueError: If the provider is not supported\n            ImportError: If the provider module cannot be imported\n        \"\"\"\n        provider = provider.lower()\n\n        # Initialize provider map if empty\n        if not cls._PROVIDER_MAP:\n            # Import lazily to avoid circular imports\n            from .assemblyai_service import AAITranscriptionService\n            from .whisper_service import WhisperTranscriptionService\n\n            cls._PROVIDER_MAP = {\n                \"whisper\": WhisperTranscriptionService,\n                \"assemblyai\": AAITranscriptionService,\n            }\n\n        # Get the provider implementation\n        provider_class = cls._PROVIDER_MAP.get(provider)\n\n        if provider_class is None:\n            raise ValueError(f\"Unsupported transcription provider: {provider}\")\n\n        # Create and return the service instance\n        return provider_class(api_key=api_key, **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TranscriptionServiceFactory.create_service","title":"<code>create_service(provider='assemblyai', api_key=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a transcription service instance.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Service provider name (e.g., \"whisper\", \"assemblyai\")</p> <code>'assemblyai'</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for the service</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>TranscriptionService</code> <p>TranscriptionService instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provider is not supported</p> <code>ImportError</code> <p>If the provider module cannot be imported</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>@classmethod\ndef create_service(\n    cls,\n    provider: str = \"assemblyai\",\n    api_key: Optional[str] = None,\n    **kwargs: Any\n) -&gt; TranscriptionService:\n    \"\"\"\n    Create a transcription service instance.\n\n    Args:\n        provider: Service provider name (e.g., \"whisper\", \"assemblyai\")\n        api_key: API key for the service\n        **kwargs: Additional provider-specific configuration\n\n    Returns:\n        TranscriptionService instance\n\n    Raises:\n        ValueError: If the provider is not supported\n        ImportError: If the provider module cannot be imported\n    \"\"\"\n    provider = provider.lower()\n\n    # Initialize provider map if empty\n    if not cls._PROVIDER_MAP:\n        # Import lazily to avoid circular imports\n        from .assemblyai_service import AAITranscriptionService\n        from .whisper_service import WhisperTranscriptionService\n\n        cls._PROVIDER_MAP = {\n            \"whisper\": WhisperTranscriptionService,\n            \"assemblyai\": AAITranscriptionService,\n        }\n\n    # Get the provider implementation\n    provider_class = cls._PROVIDER_MAP.get(provider)\n\n    if provider_class is None:\n        raise ValueError(f\"Unsupported transcription provider: {provider}\")\n\n    # Create and return the service instance\n    return provider_class(api_key=api_key, **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.TranscriptionServiceFactory.register_provider","title":"<code>register_provider(name, provider_class)</code>  <code>classmethod</code>","text":"<p>Register a provider implementation with the factory.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Provider name (lowercase)</p> required <code>provider_class</code> <code>Callable[..., TranscriptionService]</code> <p>Provider implementation class or factory function</p> required Example <p>from my_module import MyTranscriptionService TranscriptionServiceFactory.register_provider(\"my_provider\", MyTranscriptionService)</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>@classmethod\ndef register_provider(\n    cls, \n    name: str, \n    provider_class: Callable[..., TranscriptionService]\n) -&gt; None:\n    \"\"\"\n    Register a provider implementation with the factory.\n\n    Args:\n        name: Provider name (lowercase)\n        provider_class: Provider implementation class or factory function\n\n    Example:\n        &gt;&gt;&gt; from my_module import MyTranscriptionService\n        &gt;&gt;&gt; TranscriptionServiceFactory.register_provider(\"my_provider\", MyTranscriptionService)\n    \"\"\"  \n    cls._PROVIDER_MAP[name.lower()] = provider_class\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.patch_whisper_options","title":"<code>patch_whisper_options(options, file_extension)</code>","text":"<p>Patch routine to ensure 'file_extension' is present in transcription options dict. This is a workaround for OpenAI Whisper API, which requires file-like objects to have a filename/extension. Only allows known audio extensions.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Transcription options dictionary (will not be mutated)</p> required <code>file_extension</code> <code>str</code> <p>File extension string (with or without leading dot)</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>New options dictionary with 'file_extension' set appropriately</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file_extension is not in the allowed list</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/patches.py</code> <pre><code>def patch_whisper_options(\n    options: Optional[Dict[str, Any]],\n    file_extension: str\n    ) -&gt; Dict[str, Any]:\n    \"\"\"\n    Patch routine to ensure 'file_extension' is present in transcription options dict.\n    This is a workaround for OpenAI Whisper API, which requires file-like objects to have a\n    filename/extension. Only allows known audio extensions.\n\n    Args:\n        options: Transcription options dictionary (will not be mutated)\n        file_extension: File extension string (with or without leading dot)\n\n    Returns:\n        New options dictionary with 'file_extension' set appropriately\n\n    Raises:\n        ValueError: If file_extension is not in the allowed list\n    \"\"\"\n    patched = dict(options) if options is not None else {}\n    ext = file_extension.lstrip('.')\n    if ext.lower() not in _ALLOWED_EXTENSIONS:\n        raise ValueError(\n            f\"Unsupported file extension '{ext}'. Allowed extensions: {_ALLOWED_EXTENSIONS}\"\n        )\n    patched['file_extension'] = ext\n    return patched\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.assemblyai_service","title":"<code>assemblyai_service</code>","text":"<p>AssemblyAI implementation of the TranscriptionService interface.</p> <p>This module provides a complete implementation of the TranscriptionService interface using the AssemblyAI Python SDK, with support for all major features including:</p> <ul> <li>Transcription with configurable options</li> <li>Speaker diarization</li> <li>Automatic language detection</li> <li>Audio intelligence features</li> <li>Subtitle generation</li> <li>Regional endpoint support</li> <li>Webhook callbacks</li> </ul> <p>The implementation follows a modular design with single-action methods and supports both synchronous and asynchronous usage patterns.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.assemblyai_service.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.assemblyai_service.AAIConfig","title":"<code>AAIConfig</code>  <code>dataclass</code>","text":"<p>Comprehensive configuration for AssemblyAI transcription service.</p> <p>This class contains all configurable options for the AssemblyAI API, organized by feature category.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/assemblyai_service.py</code> <pre><code>@dataclass\nclass AAIConfig:\n    \"\"\"\n    Comprehensive configuration for AssemblyAI transcription service.\n\n    This class contains all configurable options for the AssemblyAI API,\n    organized by feature category.\n    \"\"\"\n\n    # Base configuration\n    api_key: Optional[str] = None\n    use_eu_endpoint: bool = False\n\n    # Connection configuration\n    polling_interval: int = 4\n\n    # Core transcription configuration\n    speech_model: SpeechModel = SpeechModel.BEST\n    language_code: Optional[str] = None\n    language_detection: bool = True\n    dual_channel: bool = False\n\n    # Text formatting options\n    format_text: bool = True\n    punctuate: bool = True\n    disfluencies: bool = False\n    filter_profanity: bool = False\n\n    # subtitle options\n    chars_per_caption: int = 60\n\n    # Speaker options\n    speaker_labels: bool = True\n    speakers_expected: Optional[int] = None\n\n    # Audio channel options\n    custom_spelling: Dict[str, str] = field(default_factory=dict)\n    word_boost: List[str] = field(default_factory=list)\n\n    # Audio intelligence configuration\n    auto_chapters: bool = False\n    auto_highlights: bool = False\n    entity_detection: bool = False\n    iab_categories: bool = False\n    sentiment_analysis: bool = False\n    summarization: bool = False\n    content_safety: bool = False\n\n    # Callback options (Webhook functionality currently not implemented)\n    # The transcribe_asynch method provides asynchronous processing\n    webhook_url: Optional[str] = None\n    webhook_auth_header_name: Optional[str] = None\n    webhook_auth_header_value: Optional[str] = None\n</code></pre> <code>api_key = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>auto_chapters = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>auto_highlights = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>chars_per_caption = 60</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>content_safety = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>custom_spelling = field(default_factory=dict)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>disfluencies = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>dual_channel = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>entity_detection = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>filter_profanity = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>format_text = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>iab_categories = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>language_code = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>language_detection = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>polling_interval = 4</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>punctuate = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>sentiment_analysis = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>speaker_labels = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>speakers_expected = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>speech_model = SpeechModel.BEST</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>summarization = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>use_eu_endpoint = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>webhook_auth_header_name = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>webhook_auth_header_value = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>webhook_url = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>word_boost = field(default_factory=list)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>__init__(api_key=None, use_eu_endpoint=False, polling_interval=4, speech_model=SpeechModel.BEST, language_code=None, language_detection=True, dual_channel=False, format_text=True, punctuate=True, disfluencies=False, filter_profanity=False, chars_per_caption=60, speaker_labels=True, speakers_expected=None, custom_spelling=dict(), word_boost=list(), auto_chapters=False, auto_highlights=False, entity_detection=False, iab_categories=False, sentiment_analysis=False, summarization=False, content_safety=False, webhook_url=None, webhook_auth_header_name=None, webhook_auth_header_value=None)</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.transcription.assemblyai_service.AAITranscriptionService","title":"<code>AAITranscriptionService</code>","text":"<p>               Bases: <code>TranscriptionService</code></p> <p>AssemblyAI implementation of the TranscriptionService interface.</p> <p>Provides comprehensive access to AssemblyAI's transcription services with support for all major features through the official Python SDK.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/assemblyai_service.py</code> <pre><code>class AAITranscriptionService(TranscriptionService):\n    \"\"\"\n    AssemblyAI implementation of the TranscriptionService interface.\n\n    Provides comprehensive access to AssemblyAI's transcription services\n    with support for all major features through the official Python SDK.\n    \"\"\"\n\n    def __init__(\n        self, \n        api_key: Optional[str] = None, \n        options: Optional[Dict[str, Any]] = None,\n        ):\n        \"\"\"\n        Initialize the AssemblyAI transcription service.\n\n        Args:\n            api_key: AssemblyAI API key (defaults to ASSEMBLYAI_API_KEY env var)\n            options: Additional transcription configuration overrides\n        \"\"\"\n        # Initialize format converter for fallback cases\n        self.format_converter = FormatConverter()\n\n        # Set and validate configuration\n        self.config = AAIConfig()\n\n        # Configure SDK\n        self._configure_sdk(api_key)\n\n        # Create transcriber instance\n        self.transcriber = aai.Transcriber(\n            config=self._create_transcription_config(options)\n            )\n\n        logger.debug(\"Initialized AssemblyAI service with SDK\")\n\n    def _configure_sdk(self, api_key: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Configure the AssemblyAI SDK with API key and regional settings.\n\n        Args:\n            api_key: AssemblyAI API key\n\n        Raises:\n            ValueError: If no API key is provided or found in environment\n        \"\"\"\n        # Set API key - priority: parameter &gt; config &gt; env var\n        api_key = api_key or self.config.api_key or os.getenv(\"ASSEMBLYAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"AssemblyAI API key is required. Set ASSEMBLYAI_API_KEY environment \"\n                \"variable, pass as api_key parameter, or include in config.\"\n            )\n\n        # Configure SDK settings\n        aai.settings.api_key = api_key\n        aai.settings.polling_interval = self.config.polling_interval\n\n\n\n        # Configure regional settings\n        if self.config.use_eu_endpoint:\n            aai.settings.base_url = \"https://api.eu.assemblyai.com/v2\"\n            logger.debug(\"Using EU endpoint for AssemblyAI API\")\n\n    def _create_transcription_config(\n        self, \n        options: Optional[Dict[str, Any]] = None\n    ) -&gt; aai.TranscriptionConfig:\n        \"\"\"\n        Create a TranscriptionConfig object from configuration and options.\n\n        Args:\n            options: Additional options to override configuration\n\n        Returns:\n            Configured TranscriptionConfig object\n        \"\"\"\n        # Start with empty config\n        config_params = {}\n\n        # Add core settings\n        if self.config.speech_model == SpeechModel.NANO:\n            config_params[\"speech_model\"] = \"nano\"\n\n        if self.config.language_code:\n            config_params[\"language_code\"] = self.config.language_code\n\n        config_params[\"language_detection\"] = self.config.language_detection\n        config_params[\"dual_channel\"] = self.config.dual_channel\n\n        # Add text formatting options\n        config_params[\"format_text\"] = self.config.format_text\n        config_params[\"punctuate\"] = self.config.punctuate\n        config_params[\"disfluencies\"] = self.config.disfluencies\n        config_params[\"filter_profanity\"] = self.config.filter_profanity\n\n        # Add speaker options\n        config_params[\"speaker_labels\"] = self.config.speaker_labels\n        if self.config.speakers_expected is not None:\n            config_params[\"speakers_expected\"] = self.config.speakers_expected\n\n        # Add audio intelligence options\n        config_params[\"auto_chapters\"] = self.config.auto_chapters\n        config_params[\"auto_highlights\"] = self.config.auto_highlights\n        config_params[\"entity_detection\"] = self.config.entity_detection\n        config_params[\"iab_categories\"] = self.config.iab_categories\n        config_params[\"sentiment_analysis\"] = self.config.sentiment_analysis\n        config_params[\"summarization\"] = self.config.summarization\n        config_params[\"content_safety\"] = self.config.content_safety\n\n        # Add custom vocabulary options\n        if self.config.word_boost:\n            config_params[\"word_boost\"] = self.config.word_boost\n\n        # Add custom spelling\n        if self.config.custom_spelling:\n            config_params[\"custom_spelling\"] = self.config.custom_spelling\n\n        # Add webhook config\n        if self.config.webhook_url:\n            config_params[\"webhook_url\"] = self.config.webhook_url\n            if self.config.webhook_auth_header_name and \\\n                self.config.webhook_auth_header_value:\n                config_params[\"webhook_auth_header_name\"] = \\\n                    self.config.webhook_auth_header_name\n                config_params[\"webhook_auth_header_value\"] = \\\n                    self.config.webhook_auth_header_value\n\n        # Override with any provided options\n        if options:\n            config_params |= options\n\n        # Create config object\n        return aai.TranscriptionConfig(**config_params)\n\n    def _get_file_path(\n        self, \n        audio_file: Union[Path, BinaryIO, str]\n        ) -&gt; Union[BinaryIO, str]:\n        \"\"\"\n        Get appropriate file path for different input types.\n\n        Args:\n            audio_file: Path, file-like object, or URL of audio file\n\n        Returns:\n            Path or string for SDK\n\n        Raises:\n            TypeError: If input type is not supported\n        \"\"\"\n        # Handle Path objects\n        if isinstance(audio_file, Path):\n            return str(audio_file)\n\n        # Handle URLs\n        if isinstance(audio_file, str) and (\n            audio_file.startswith(\"http://\") or \n            audio_file.startswith(\"https://\")\n        ):\n            return audio_file\n\n        # Handle file-like objects\n        if hasattr(audio_file, \"read\"):\n            # SDK handles file-like objects directly\n            return audio_file\n\n        raise TypeError(f\"Unsupported audio file type: {type(audio_file)}\")\n\n    def _extract_words(self, transcript: aai.Transcript) -&gt; TimedText:\n        \"\"\"\n        Extract words with timestamps from transcript and return a TimedText object.\n\n        Args:\n            transcript: AssemblyAI transcript object\n\n        Returns:\n            TimedText object containing word-level units\n        \"\"\"\n        if not transcript.words:\n            raise ValueError(f\"Transcript object has no words: {transcript}\")\n\n        units = [\n            TimedTextUnit(\n                index=None,\n                granularity=Granularity.WORD,\n                speaker=word.speaker,\n                text=word.text,\n                start_ms=word.start,\n                end_ms=word.end,\n                confidence=word.confidence,\n            )\n            for word in transcript.words\n        ]\n\n        # TimedText performs its own internal validation\n        return TimedText(words=units, granularity=Granularity.WORD)\n\n    def _extract_utterances(self, transcript: aai.Transcript) -&gt; TimedText:\n        \"\"\"\n        Extract utterances (speaker segments) from transcript and return a TimedText object.\n\n        Args:\n            transcript: AssemblyAI transcript object\n\n        Returns:\n            TimedText object containing utterance-level units\n        \"\"\"\n        if not (utterances := getattr(transcript, \"utterances\", None)):\n            # Return an empty TimedText if diarization wasn't requested\n            return TimedText(segments=[], granularity=Granularity.SEGMENT)\n\n        units = [\n            TimedTextUnit(\n                index=None,\n                granularity=Granularity.SEGMENT,\n                text=utterance.text,\n                start_ms=utterance.start,\n                end_ms=utterance.end,\n                speaker=utterance.speaker,\n                confidence=utterance.confidence,\n            )\n            for utterance in utterances\n        ]\n\n        return TimedText(segments=units, granularity=Granularity.SEGMENT)\n\n    def _extract_audio_intelligence(self, transcript: aai.Transcript) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extract audio intelligence features from transcript.\n\n        Args:\n            transcript: AssemblyAI transcript object\n\n        Returns:\n            Dictionary of audio intelligence features\n        \"\"\"\n        intelligence = {}\n\n        # Extract auto chapters\n        if hasattr(transcript, \"chapters\") and transcript.chapters:\n            chapters_data = []\n            chapters_data.extend(\n                {\n                    \"summary\": chapter.summary,\n                    \"headline\": chapter.headline,\n                    \"start_ms\": chapter.start,\n                    \"end_ms\": chapter.end,\n                }\n                for chapter in transcript.chapters\n            )\n            intelligence[\"chapters\"] = chapters_data\n\n        # Extract sentiment analysis\n        if hasattr(transcript, \"sentiment_analysis\") and transcript.sentiment_analysis:\n            sentiment_data = []\n            sentiment_data.extend(\n                {\n                    \"text\": sentiment.text,\n                    \"sentiment\": sentiment.sentiment,\n                    \"confidence\": sentiment.confidence,\n                    \"start_ms\": sentiment.start,\n                    \"end_ms\": sentiment.end,\n                }\n                for sentiment in transcript.sentiment_analysis\n            )\n            intelligence[\"sentiment_analysis\"] = sentiment_data\n\n        # Extract entity detection\n        if hasattr(transcript, \"entities\") and transcript.entities:\n            entities_data = []\n            entities_data.extend(\n                {\n                    \"text\": entity.text,\n                    \"entity_type\": entity.entity_type,\n                    \"start_ms\": entity.start,\n                    \"end_ms\": entity.end,\n                }\n                for entity in transcript.entities\n            )\n            intelligence[\"entities\"] = entities_data\n\n        # Extract topics (IAB categories)\n        if hasattr(transcript, \"iab_categories\") and transcript.iab_categories:\n            topics_data = {\n                \"results\": [],\n                \"summary\": transcript.iab_categories.summary\n            }\n\n            if not transcript.iab_categories.results:\n                return topics_data\n\n            for result in transcript.iab_categories.results:\n                topics_data[\"results\"].append({\n                    \"text\": result.text,\n                    \"labels\": [\n                        {\"label\": label.label, \"relevance\": label.relevance}\n                        for label in result.labels\n                    ],\n                    \"timestamp\": {\n                        \"start\": result.timestamp.start,\n                        \"end\": result.timestamp.end\n                    }\n                })\n\n            intelligence[\"topics\"] = topics_data\n\n        # Extract auto highlights\n        if hasattr(transcript, \"auto_highlights\") and transcript.auto_highlights:\n            intelligence[\"highlights\"] = {\n                \"results\": transcript.auto_highlights.results,\n                \"status\": transcript.auto_highlights.status\n            }\n\n        return intelligence\n\n    def standardize_result(self, transcript: aai.Transcript) -&gt; TranscriptionResult:\n        \"\"\"\n        Standardize AssemblyAI transcript to match common format.\n\n        Args:\n            transcript: AssemblyAI transcript object\n\n        Returns:\n            Standardized result dictionary\n        \"\"\"\n        # Extract words and utterances as TimedText\n        words = self._extract_words(transcript)\n        utterances = self._extract_utterances(transcript)\n\n        language = self.config.language_code or \\\n                (\"auto\" if self.config.language_detection else \"unknown\")\n\n        return TranscriptionResult(\n            text=transcript.text or \"\",\n            language=language,\n            word_timing=words,\n            utterance_timing=utterances,\n            confidence=getattr(transcript, \"confidence\", 0.0),\n            audio_duration_ms=getattr(transcript, \"audio_duration\", 0),\n            transcript_id=transcript.id,\n            status=transcript.status,\n            raw_result=transcript.json_response,\n        )\n\n    def transcribe(\n        self,\n        audio_file: Union[Path, BinaryIO, str],\n        options: Optional[Dict[str, Any]] = None\n    ) -&gt; TranscriptionResult:\n        \"\"\"\n        Transcribe audio file to text using AssemblyAI's synchronous SDK approach.\n\n        This method handles:\n        - File paths\n        - File-like objects\n        - URLs\n\n        Args:\n            audio_file: Path, file-like object, or URL of audio file\n            options: Provider-specific options for transcription\n\n        Returns:\n            Dictionary containing standardized transcription results\n        \"\"\"\n        try:\n            transcript = self._gen_transcript(options, audio_file)\n\n            # Standardize the result format\n            return self.standardize_result(transcript)\n\n        except Exception as e:\n            logger.error(f\"Transcription failed: {e}\")\n            raise RuntimeError(f\"AssemblyAI transcription failed: {e}\") from e\n\n    def transcribe_async(\n        self,\n        audio_file: Union[Path, BinaryIO, str],\n        options: Optional[Dict[str, Any]] = None\n    ) -&gt;  Future:\n        \"\"\"\n        Submit an asynchronous transcription job using AssemblyAI's SDK.\n\n        This method submits a transcription job and returns immediately with\n        a transcript ID that can be used to retrieve results later.\n\n        Args:\n            audio_file: Path, file-like object, or URL of audio file\n            options: Provider-specific options for transcription\n\n        Returns:\n            String containing the transcript ID for later retrieval\n\n        Notes:\n            The SDK's submit method returns a Future object, but this method\n            extracts just the transcript ID for simpler handling.\n        \"\"\"\n        try:\n            # Create configuration with options\n            tx_config = self._create_transcription_config(options)\n\n            # Get file path/object in the right format\n            file_path = self._get_file_path(audio_file)\n\n            logger.info(\"Submitting asynchronous transcription with AssemblyAI SDK\")\n\n            # Use the SDK's asynchronous submit method\n            # This returns a Future object containing a Transcript\n            return self.transcriber.transcribe_async(file_path, config=tx_config)\n\n        except Exception as e:\n            logger.error(f\"Transcription submission failed: {e}\")\n            raise RuntimeError(f\"AssemblyAI transcription submission failed: {e}\") \\\n                from e\n\n    def get_result(self, job_id: str) -&gt; TranscriptionResult:\n        \"\"\"\n        Get results for an existing transcription job.\n\n        This method blocks until the transcript is retrieved.\n\n        Args:\n            job_id: ID of the transcription job\n\n        Returns:\n            Dictionary containing transcription results\n        \"\"\"\n        try:\n            # Use the SDK's get_by_id method to retrieve the transcript\n            # This blocks until the transcript is retrieved\n            transcript = aai.Transcript.get_by_id(job_id)\n\n            # Standardize the result format\n            return self.standardize_result(transcript)\n\n        except Exception as e:\n            logger.error(f\"Failed to retrieve transcript {job_id}: {e}\")\n            raise RuntimeError(f\"Failed to retrieve transcript: {e}\") from e\n\n    def get_subtitles(\n        self, \n        transcript_id: str, \n        format_type: str = \"srt\",\n    ) -&gt; str:\n        \"\"\"\n        Get subtitles directly from AssemblyAI.\n\n        Args:\n            transcript_id: ID of the transcription job\n            format_type: Format type (\"srt\" or \"vtt\")\n\n        Returns:\n            String representation in the requested format\n\n        Raises:\n            ValueError: If the format type is not supported\n        \"\"\"\n        chars_per_caption = self.config.chars_per_caption\n\n        format_type = format_type.lower()\n\n        if format_type not in [\"srt\", \"vtt\"]:\n            raise ValueError(\n                f\"Unsupported subtitle format: {format_type}. \"\n                \"Supported formats: srt, vtt\"\n                )\n        # Create transcript object from ID\n        transcript = aai.Transcript(transcript_id=transcript_id)\n\n        # Get subtitles in requested format\n        if format_type == \"srt\":\n            return transcript.export_subtitles_srt(chars_per_caption=chars_per_caption)\n        else:  # format_type == \"vtt\"\n            return transcript.export_subtitles_vtt(chars_per_caption=chars_per_caption)\n\n    def transcribe_to_format(\n        self,\n        audio_file: Union[Path, BinaryIO, str],\n        format_type: str = \"srt\",\n        transcription_options: Optional[Dict[str, Any]] = None,\n        format_options: Optional[Dict[str, Any]] = None\n    ) -&gt; str:\n        \"\"\"\n        Transcribe audio and return result in specified format.\n\n        Takes advantage of the direct subtitle generation\n        functionality when requesting SRT or VTT formats.\n\n        Args:\n            audio_file: Path, file-like object, or URL of audio file\n            format_type: Format type (e.g., \"srt\", \"vtt\", \"text\")\n            transcription_options: Options for transcription\n            format_options: Format-specific options\n\n        Returns:\n            String representation in the requested format\n        \"\"\"\n        format_type = format_type.lower()\n        chars_per_caption = format_options.get(\n            'chars_per_caption', self.config.chars_per_caption) \\\n            if format_options else self.config.chars_per_caption\n\n        transcript = self._gen_transcript(\n                transcription_options, audio_file\n            )\n\n        # Check if we need direct subtitle generation\n        if format_type == \"srt\":  \n            return transcript.export_subtitles_srt(chars_per_caption=chars_per_caption)\n        elif format_type == \"vtt\":\n            return transcript.export_subtitles_vtt(chars_per_caption=chars_per_caption)\n\n        # For other formats, use the format converter\n        # First get a normal transcription result\n        result = self.transcribe(audio_file, transcription_options)\n\n        # Then convert to the requested format\n        return self.format_converter.convert(\n            result, format_type, format_options or {}\n        )\n\n    def _gen_transcript(self, transcription_options, audio_file):\n        # Create configuration with options\n        tx_config = self._create_transcription_config(transcription_options)\n\n        # Get file path/object in the right format\n        file_path = self._get_file_path(audio_file)\n\n        logger.info(\"Starting synchronous transcription with AssemblyAI SDK\")\n\n        # Use the SDK's synchronous transcribe method\n        # This will block until transcription is complete\n        return self.transcriber.transcribe(file_path, config=tx_config)\n</code></pre> <code>config = AAIConfig()</code> <code>instance-attribute</code> \u00b6 <code>format_converter = FormatConverter()</code> <code>instance-attribute</code> \u00b6 <code>transcriber = aai.Transcriber(config=(self._create_transcription_config(options)))</code> <code>instance-attribute</code> \u00b6 <code>__init__(api_key=None, options=None)</code> \u00b6 <p>Initialize the AssemblyAI transcription service.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>AssemblyAI API key (defaults to ASSEMBLYAI_API_KEY env var)</p> <code>None</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Additional transcription configuration overrides</p> <code>None</code> Source code in <code>src/tnh_scholar/audio_processing/transcription/assemblyai_service.py</code> <pre><code>def __init__(\n    self, \n    api_key: Optional[str] = None, \n    options: Optional[Dict[str, Any]] = None,\n    ):\n    \"\"\"\n    Initialize the AssemblyAI transcription service.\n\n    Args:\n        api_key: AssemblyAI API key (defaults to ASSEMBLYAI_API_KEY env var)\n        options: Additional transcription configuration overrides\n    \"\"\"\n    # Initialize format converter for fallback cases\n    self.format_converter = FormatConverter()\n\n    # Set and validate configuration\n    self.config = AAIConfig()\n\n    # Configure SDK\n    self._configure_sdk(api_key)\n\n    # Create transcriber instance\n    self.transcriber = aai.Transcriber(\n        config=self._create_transcription_config(options)\n        )\n\n    logger.debug(\"Initialized AssemblyAI service with SDK\")\n</code></pre> <code>get_result(job_id)</code> \u00b6 <p>Get results for an existing transcription job.</p> <p>This method blocks until the transcript is retrieved.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>ID of the transcription job</p> required <p>Returns:</p> Type Description <code>TranscriptionResult</code> <p>Dictionary containing transcription results</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/assemblyai_service.py</code> <pre><code>def get_result(self, job_id: str) -&gt; TranscriptionResult:\n    \"\"\"\n    Get results for an existing transcription job.\n\n    This method blocks until the transcript is retrieved.\n\n    Args:\n        job_id: ID of the transcription job\n\n    Returns:\n        Dictionary containing transcription results\n    \"\"\"\n    try:\n        # Use the SDK's get_by_id method to retrieve the transcript\n        # This blocks until the transcript is retrieved\n        transcript = aai.Transcript.get_by_id(job_id)\n\n        # Standardize the result format\n        return self.standardize_result(transcript)\n\n    except Exception as e:\n        logger.error(f\"Failed to retrieve transcript {job_id}: {e}\")\n        raise RuntimeError(f\"Failed to retrieve transcript: {e}\") from e\n</code></pre> <code>get_subtitles(transcript_id, format_type='srt')</code> \u00b6 <p>Get subtitles directly from AssemblyAI.</p> <p>Parameters:</p> Name Type Description Default <code>transcript_id</code> <code>str</code> <p>ID of the transcription job</p> required <code>format_type</code> <code>str</code> <p>Format type (\"srt\" or \"vtt\")</p> <code>'srt'</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation in the requested format</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the format type is not supported</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/assemblyai_service.py</code> <pre><code>def get_subtitles(\n    self, \n    transcript_id: str, \n    format_type: str = \"srt\",\n) -&gt; str:\n    \"\"\"\n    Get subtitles directly from AssemblyAI.\n\n    Args:\n        transcript_id: ID of the transcription job\n        format_type: Format type (\"srt\" or \"vtt\")\n\n    Returns:\n        String representation in the requested format\n\n    Raises:\n        ValueError: If the format type is not supported\n    \"\"\"\n    chars_per_caption = self.config.chars_per_caption\n\n    format_type = format_type.lower()\n\n    if format_type not in [\"srt\", \"vtt\"]:\n        raise ValueError(\n            f\"Unsupported subtitle format: {format_type}. \"\n            \"Supported formats: srt, vtt\"\n            )\n    # Create transcript object from ID\n    transcript = aai.Transcript(transcript_id=transcript_id)\n\n    # Get subtitles in requested format\n    if format_type == \"srt\":\n        return transcript.export_subtitles_srt(chars_per_caption=chars_per_caption)\n    else:  # format_type == \"vtt\"\n        return transcript.export_subtitles_vtt(chars_per_caption=chars_per_caption)\n</code></pre> <code>standardize_result(transcript)</code> \u00b6 <p>Standardize AssemblyAI transcript to match common format.</p> <p>Parameters:</p> Name Type Description Default <code>transcript</code> <code>Transcript</code> <p>AssemblyAI transcript object</p> required <p>Returns:</p> Type Description <code>TranscriptionResult</code> <p>Standardized result dictionary</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/assemblyai_service.py</code> <pre><code>def standardize_result(self, transcript: aai.Transcript) -&gt; TranscriptionResult:\n    \"\"\"\n    Standardize AssemblyAI transcript to match common format.\n\n    Args:\n        transcript: AssemblyAI transcript object\n\n    Returns:\n        Standardized result dictionary\n    \"\"\"\n    # Extract words and utterances as TimedText\n    words = self._extract_words(transcript)\n    utterances = self._extract_utterances(transcript)\n\n    language = self.config.language_code or \\\n            (\"auto\" if self.config.language_detection else \"unknown\")\n\n    return TranscriptionResult(\n        text=transcript.text or \"\",\n        language=language,\n        word_timing=words,\n        utterance_timing=utterances,\n        confidence=getattr(transcript, \"confidence\", 0.0),\n        audio_duration_ms=getattr(transcript, \"audio_duration\", 0),\n        transcript_id=transcript.id,\n        status=transcript.status,\n        raw_result=transcript.json_response,\n    )\n</code></pre> <code>transcribe(audio_file, options=None)</code> \u00b6 <p>Transcribe audio file to text using AssemblyAI's synchronous SDK approach.</p> <p>This method handles: - File paths - File-like objects - URLs</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Union[Path, BinaryIO, str]</code> <p>Path, file-like object, or URL of audio file</p> required <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Provider-specific options for transcription</p> <code>None</code> <p>Returns:</p> Type Description <code>TranscriptionResult</code> <p>Dictionary containing standardized transcription results</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/assemblyai_service.py</code> <pre><code>def transcribe(\n    self,\n    audio_file: Union[Path, BinaryIO, str],\n    options: Optional[Dict[str, Any]] = None\n) -&gt; TranscriptionResult:\n    \"\"\"\n    Transcribe audio file to text using AssemblyAI's synchronous SDK approach.\n\n    This method handles:\n    - File paths\n    - File-like objects\n    - URLs\n\n    Args:\n        audio_file: Path, file-like object, or URL of audio file\n        options: Provider-specific options for transcription\n\n    Returns:\n        Dictionary containing standardized transcription results\n    \"\"\"\n    try:\n        transcript = self._gen_transcript(options, audio_file)\n\n        # Standardize the result format\n        return self.standardize_result(transcript)\n\n    except Exception as e:\n        logger.error(f\"Transcription failed: {e}\")\n        raise RuntimeError(f\"AssemblyAI transcription failed: {e}\") from e\n</code></pre> <code>transcribe_async(audio_file, options=None)</code> \u00b6 <p>Submit an asynchronous transcription job using AssemblyAI's SDK.</p> <p>This method submits a transcription job and returns immediately with a transcript ID that can be used to retrieve results later.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Union[Path, BinaryIO, str]</code> <p>Path, file-like object, or URL of audio file</p> required <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Provider-specific options for transcription</p> <code>None</code> <p>Returns:</p> Type Description <code>Future</code> <p>String containing the transcript ID for later retrieval</p> Notes <p>The SDK's submit method returns a Future object, but this method extracts just the transcript ID for simpler handling.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/assemblyai_service.py</code> <pre><code>def transcribe_async(\n    self,\n    audio_file: Union[Path, BinaryIO, str],\n    options: Optional[Dict[str, Any]] = None\n) -&gt;  Future:\n    \"\"\"\n    Submit an asynchronous transcription job using AssemblyAI's SDK.\n\n    This method submits a transcription job and returns immediately with\n    a transcript ID that can be used to retrieve results later.\n\n    Args:\n        audio_file: Path, file-like object, or URL of audio file\n        options: Provider-specific options for transcription\n\n    Returns:\n        String containing the transcript ID for later retrieval\n\n    Notes:\n        The SDK's submit method returns a Future object, but this method\n        extracts just the transcript ID for simpler handling.\n    \"\"\"\n    try:\n        # Create configuration with options\n        tx_config = self._create_transcription_config(options)\n\n        # Get file path/object in the right format\n        file_path = self._get_file_path(audio_file)\n\n        logger.info(\"Submitting asynchronous transcription with AssemblyAI SDK\")\n\n        # Use the SDK's asynchronous submit method\n        # This returns a Future object containing a Transcript\n        return self.transcriber.transcribe_async(file_path, config=tx_config)\n\n    except Exception as e:\n        logger.error(f\"Transcription submission failed: {e}\")\n        raise RuntimeError(f\"AssemblyAI transcription submission failed: {e}\") \\\n            from e\n</code></pre> <code>transcribe_to_format(audio_file, format_type='srt', transcription_options=None, format_options=None)</code> \u00b6 <p>Transcribe audio and return result in specified format.</p> <p>Takes advantage of the direct subtitle generation functionality when requesting SRT or VTT formats.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Union[Path, BinaryIO, str]</code> <p>Path, file-like object, or URL of audio file</p> required <code>format_type</code> <code>str</code> <p>Format type (e.g., \"srt\", \"vtt\", \"text\")</p> <code>'srt'</code> <code>transcription_options</code> <code>Optional[Dict[str, Any]]</code> <p>Options for transcription</p> <code>None</code> <code>format_options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation in the requested format</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/assemblyai_service.py</code> <pre><code>def transcribe_to_format(\n    self,\n    audio_file: Union[Path, BinaryIO, str],\n    format_type: str = \"srt\",\n    transcription_options: Optional[Dict[str, Any]] = None,\n    format_options: Optional[Dict[str, Any]] = None\n) -&gt; str:\n    \"\"\"\n    Transcribe audio and return result in specified format.\n\n    Takes advantage of the direct subtitle generation\n    functionality when requesting SRT or VTT formats.\n\n    Args:\n        audio_file: Path, file-like object, or URL of audio file\n        format_type: Format type (e.g., \"srt\", \"vtt\", \"text\")\n        transcription_options: Options for transcription\n        format_options: Format-specific options\n\n    Returns:\n        String representation in the requested format\n    \"\"\"\n    format_type = format_type.lower()\n    chars_per_caption = format_options.get(\n        'chars_per_caption', self.config.chars_per_caption) \\\n        if format_options else self.config.chars_per_caption\n\n    transcript = self._gen_transcript(\n            transcription_options, audio_file\n        )\n\n    # Check if we need direct subtitle generation\n    if format_type == \"srt\":  \n        return transcript.export_subtitles_srt(chars_per_caption=chars_per_caption)\n    elif format_type == \"vtt\":\n        return transcript.export_subtitles_vtt(chars_per_caption=chars_per_caption)\n\n    # For other formats, use the format converter\n    # First get a normal transcription result\n    result = self.transcribe(audio_file, transcription_options)\n\n    # Then convert to the requested format\n    return self.format_converter.convert(\n        result, format_type, format_options or {}\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.assemblyai_service.SpeechModel","title":"<code>SpeechModel</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported AssemblyAI speech models.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/assemblyai_service.py</code> <pre><code>class SpeechModel(str, Enum):\n    \"\"\"Supported AssemblyAI speech models.\"\"\"\n    BEST = \"best\"\n    NANO = \"nano\"\n</code></pre> <code>BEST = 'best'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>NANO = 'nano'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.transcription.format_converter","title":"<code>format_converter</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.format_converter--tnh_scholaraudio_processingtranscriptionformat_converter","title":"tnh_scholar.audio_processing.transcription.format_converter","text":"<p>Thin facade that turns raw transcription-service output dictionaries into the formats requested by callers (plain-text, SRT - VTT coming later).</p> <p>Core heavy lifting now lives in:</p> <ul> <li><code>TimedText</code> / <code>TimedTextUnit</code> - canonical internal representation</li> <li><code>SegmentBuilder</code>            - word-level -&gt; sentence/segment chunking</li> <li><code>SRTProcessor</code>              - rendering to <code>.srt</code></li> </ul> <p>Only one public method remains: meth:<code>FormatConverter.convert</code>.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.format_converter.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.format_converter.FormatConverter","title":"<code>FormatConverter</code>","text":"<p>Convert a raw transcription result to text, SRT, or (placeholder) VTT.</p> <p>The raw result must follow the loose schema - <code>{\"utterances\": [...]}</code> -&gt; already speaker-segmented - <code>{\"words\":       [...]}</code> -&gt; word-level; we chunk via :class:<code>SegmentBuilder</code> - <code>{\"text\": \"...\", \"audio_duration_ms\": 12345}</code> -&gt; single blob fallback</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/format_converter.py</code> <pre><code>class FormatConverter:\n    \"\"\"\n    Convert a raw transcription result to *text*, *SRT*, or (placeholder) *VTT*.\n\n    The *raw* result must follow the loose schema\n    - ``{\"utterances\": [...]}`` -&gt; already speaker-segmented\n    - ``{\"words\":       [...]}`` -&gt; word-level; we chunk via :class:`SegmentBuilder`\n    - ``{\"text\": \"...\", \"audio_duration_ms\": 12345}`` -&gt; single blob fallback\n    \"\"\"\n\n    def __init__(self, config: Optional[FormatConverterConfig] = None):\n        self.config = config or FormatConverterConfig()\n        self._segment_builder = TextSegmentBuilder(\n            max_duration_ms=self.config.max_entry_duration_ms,\n            target_characters=self.config.characters_per_entry,\n            avoid_orphans=True,\n            ignore_speaker=not self.config.include_speaker,  \n            max_gap_duration_ms=self.config.max_gap_duration_ms\n        )\n\n    def convert(\n        self,\n        result: TranscriptionResult,\n        format_type: str = \"srt\",\n        format_options: Optional[Dict[str, Any]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Convert *result* to the given *format_type*.\n\n        Parameters\n        ----------\n        result : dict\n            Raw transcription output.\n        format_type : {\"srt\", \"text\", \"vtt\"}\n        format_options : dict | None\n            Currently only ``{\"include_speaker\": bool}`` recognized for *srt*.\n        \"\"\"\n        format_type = format_type.lower()\n        format_options = format_options or {}\n\n        timed_text = self._build_timed_text(result)\n\n        if format_type == \"text\":\n            return self._to_plain_text(timed_text)\n\n        if format_type == \"srt\":\n            include_speaker = format_options.get(\"include_speaker\", True)\n            processor = SRTProcessor()\n            return processor.generate(timed_text, include_speaker=include_speaker)\n\n        if format_type == \"vtt\":\n            raise NotImplementedError(\"VTT conversion not implemented yet.\")\n\n        raise ValueError(f\"Unsupported format_type: {format_type}\")\n\n    def _to_plain_text(self, timed_text: TimedText) -&gt; str:\n        \"\"\"Flatten ``TimedText`` into a newline-separated block of text.\"\"\"\n        return \"\\n\".join(unit.text for unit in timed_text.segments if unit.text)\n\n    def _build_timed_text(self, result: TranscriptionResult) -&gt; TimedText:\n        \"\"\"\n        Normalize *result* into :class:`TimedText`, handling three cases:\n\n        1. *utterance*-level input (already segmented)\n        2. *word*-level input  - chunk via :class:`SegmentBuilder`\n        3. plain *text* fallback\n        \"\"\"\n\n        if timed_text := result.utterance_timing:\n            units: List[TimedTextUnit] = []\n            for i, unit in enumerate(timed_text.iter_segments(), start=1):\n                data = unit.model_copy()\n\n                units.append(\n                    TimedTextUnit(\n                        granularity=Granularity.SEGMENT,\n                        text=data.text,\n                        start_ms=data.start_ms,\n                        end_ms=data.end_ms,\n                        speaker=data.speaker,\n                        index=i,\n                        confidence=data.confidence,\n                    )\n                )\n\n            return TimedText(segments=units, granularity=Granularity.SEGMENT)\n\n        if words := result.word_timing:\n            # *SegmentBuilder* returns a list[TimedTextUnit]\n            return self._segment_builder.create_segments(words)\n\n        if text := result.text:\n            duration_ms = result.audio_duration_ms\n\n            units = [\n                TimedTextUnit(\n                    granularity=Granularity.SEGMENT,\n                    text=text,\n                    start_ms=0,\n                    end_ms=duration_ms or 0,\n                    speaker=None,\n                    index=None,\n                    confidence=None,\n                )\n            ]\n            return TimedText(segments=units, granularity=Granularity.SEGMENT)\n\n        # If we arrived here \u2013 nothing to work with.\n        raise ValueError(\n            \"Cannot build TimedText: result contains no utterances, words, or text.\"\n        )\n</code></pre> <code>config = config or FormatConverterConfig()</code> <code>instance-attribute</code> \u00b6 <code>__init__(config=None)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/transcription/format_converter.py</code> <pre><code>def __init__(self, config: Optional[FormatConverterConfig] = None):\n    self.config = config or FormatConverterConfig()\n    self._segment_builder = TextSegmentBuilder(\n        max_duration_ms=self.config.max_entry_duration_ms,\n        target_characters=self.config.characters_per_entry,\n        avoid_orphans=True,\n        ignore_speaker=not self.config.include_speaker,  \n        max_gap_duration_ms=self.config.max_gap_duration_ms\n    )\n</code></pre> <code>convert(result, format_type='srt', format_options=None)</code> \u00b6 <p>Convert result to the given format_type.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.format_converter.FormatConverter.convert--parameters","title":"Parameters","text":"<p>result : dict     Raw transcription output. format_type : {\"srt\", \"text\", \"vtt\"} format_options : dict | None     Currently only <code>{\"include_speaker\": bool}</code> recognized for srt.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/format_converter.py</code> <pre><code>def convert(\n    self,\n    result: TranscriptionResult,\n    format_type: str = \"srt\",\n    format_options: Optional[Dict[str, Any]] = None,\n) -&gt; str:\n    \"\"\"\n    Convert *result* to the given *format_type*.\n\n    Parameters\n    ----------\n    result : dict\n        Raw transcription output.\n    format_type : {\"srt\", \"text\", \"vtt\"}\n    format_options : dict | None\n        Currently only ``{\"include_speaker\": bool}`` recognized for *srt*.\n    \"\"\"\n    format_type = format_type.lower()\n    format_options = format_options or {}\n\n    timed_text = self._build_timed_text(result)\n\n    if format_type == \"text\":\n        return self._to_plain_text(timed_text)\n\n    if format_type == \"srt\":\n        include_speaker = format_options.get(\"include_speaker\", True)\n        processor = SRTProcessor()\n        return processor.generate(timed_text, include_speaker=include_speaker)\n\n    if format_type == \"vtt\":\n        raise NotImplementedError(\"VTT conversion not implemented yet.\")\n\n    raise ValueError(f\"Unsupported format_type: {format_type}\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.format_converter.FormatConverterConfig","title":"<code>FormatConverterConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>User-tunable knobs for :class:<code>FormatConverter</code>.</p> <p>Only a handful remain now that the heavy logic moved to <code>SegmentBuilder</code>.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/format_converter.py</code> <pre><code>class FormatConverterConfig(BaseModel):\n    \"\"\"\n    User-tunable knobs for :class:`FormatConverter`.\n\n    Only a handful remain now that the heavy logic moved to `SegmentBuilder`.\n    \"\"\"\n\n    max_entry_duration_ms: int = 6_000\n    include_segment_index: bool = True\n    include_speaker: bool = True\n    characters_per_entry: int = 42\n    max_gap_duration_ms: int = 2_000\n</code></pre> <code>characters_per_entry = 42</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>include_segment_index = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>include_speaker = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>max_entry_duration_ms = 6000</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>max_gap_duration_ms = 2000</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.transcription.patches","title":"<code>patches</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.patches.patch_file_with_name","title":"<code>patch_file_with_name(file_obj, extension)</code>","text":"<p>Ensures the file-like object has a .name attribute with the correct extension.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/patches.py</code> <pre><code>def patch_file_with_name(file_obj: BytesIO, extension: str) -&gt; BinaryIO:\n    \"\"\"\n    Ensures the file-like object has a .name attribute with the correct extension.\n    \"\"\"\n    file_obj.name = f\"filename_placeholder.{extension}\"\n    return file_obj\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.patches.patch_whisper_options","title":"<code>patch_whisper_options(options, file_extension)</code>","text":"<p>Patch routine to ensure 'file_extension' is present in transcription options dict. This is a workaround for OpenAI Whisper API, which requires file-like objects to have a filename/extension. Only allows known audio extensions.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Transcription options dictionary (will not be mutated)</p> required <code>file_extension</code> <code>str</code> <p>File extension string (with or without leading dot)</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>New options dictionary with 'file_extension' set appropriately</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file_extension is not in the allowed list</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/patches.py</code> <pre><code>def patch_whisper_options(\n    options: Optional[Dict[str, Any]],\n    file_extension: str\n    ) -&gt; Dict[str, Any]:\n    \"\"\"\n    Patch routine to ensure 'file_extension' is present in transcription options dict.\n    This is a workaround for OpenAI Whisper API, which requires file-like objects to have a\n    filename/extension. Only allows known audio extensions.\n\n    Args:\n        options: Transcription options dictionary (will not be mutated)\n        file_extension: File extension string (with or without leading dot)\n\n    Returns:\n        New options dictionary with 'file_extension' set appropriately\n\n    Raises:\n        ValueError: If file_extension is not in the allowed list\n    \"\"\"\n    patched = dict(options) if options is not None else {}\n    ext = file_extension.lstrip('.')\n    if ext.lower() not in _ALLOWED_EXTENSIONS:\n        raise ValueError(\n            f\"Unsupported file extension '{ext}'. Allowed extensions: {_ALLOWED_EXTENSIONS}\"\n        )\n    patched['file_extension'] = ext\n    return patched\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.srt_processor","title":"<code>srt_processor</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.srt_processor.SRTConfig","title":"<code>SRTConfig</code>","text":"<p>Configuration options for SRT processing.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>class SRTConfig:\n    \"\"\"Configuration options for SRT processing.\"\"\"\n\n    def __init__(\n        self,\n        include_speaker: bool = False,\n        speaker_format: str = \"[{speaker}] {text}\",\n        reindex_entries: bool = True,\n        timestamp_format: str = \"{:02d}:{:02d}:{:02d},{:03d}\",\n        max_chars_per_line: int = 42,\n        use_pysrt: bool = False,\n    ):\n        \"\"\"\n        Initialize with default settings.\n\n        Args:\n            include_speaker: Whether to include speaker labels in output\n            speaker_format: Format string for speaker attribution\n            reindex_entries: Whether to reindex entries sequentially\n            timestamp_format: Format string for timestamp formatting\n            max_chars_per_line: Maximum characters per line before splitting\n        \"\"\"\n        self.include_speaker = include_speaker\n        self.speaker_format = speaker_format\n        self.reindex_entries = reindex_entries\n        self.timestamp_format = timestamp_format\n        self.max_chars_per_line = max_chars_per_line\n        self.use_pysrt = use_pysrt\n</code></pre> <code>include_speaker = include_speaker</code> <code>instance-attribute</code> \u00b6 <code>max_chars_per_line = max_chars_per_line</code> <code>instance-attribute</code> \u00b6 <code>reindex_entries = reindex_entries</code> <code>instance-attribute</code> \u00b6 <code>speaker_format = speaker_format</code> <code>instance-attribute</code> \u00b6 <code>timestamp_format = timestamp_format</code> <code>instance-attribute</code> \u00b6 <code>use_pysrt = use_pysrt</code> <code>instance-attribute</code> \u00b6 <code>__init__(include_speaker=False, speaker_format='[{speaker}] {text}', reindex_entries=True, timestamp_format='{:02d}:{:02d}:{:02d},{:03d}', max_chars_per_line=42, use_pysrt=False)</code> \u00b6 <p>Initialize with default settings.</p> <p>Parameters:</p> Name Type Description Default <code>include_speaker</code> <code>bool</code> <p>Whether to include speaker labels in output</p> <code>False</code> <code>speaker_format</code> <code>str</code> <p>Format string for speaker attribution</p> <code>'[{speaker}] {text}'</code> <code>reindex_entries</code> <code>bool</code> <p>Whether to reindex entries sequentially</p> <code>True</code> <code>timestamp_format</code> <code>str</code> <p>Format string for timestamp formatting</p> <code>'{:02d}:{:02d}:{:02d},{:03d}'</code> <code>max_chars_per_line</code> <code>int</code> <p>Maximum characters per line before splitting</p> <code>42</code> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>def __init__(\n    self,\n    include_speaker: bool = False,\n    speaker_format: str = \"[{speaker}] {text}\",\n    reindex_entries: bool = True,\n    timestamp_format: str = \"{:02d}:{:02d}:{:02d},{:03d}\",\n    max_chars_per_line: int = 42,\n    use_pysrt: bool = False,\n):\n    \"\"\"\n    Initialize with default settings.\n\n    Args:\n        include_speaker: Whether to include speaker labels in output\n        speaker_format: Format string for speaker attribution\n        reindex_entries: Whether to reindex entries sequentially\n        timestamp_format: Format string for timestamp formatting\n        max_chars_per_line: Maximum characters per line before splitting\n    \"\"\"\n    self.include_speaker = include_speaker\n    self.speaker_format = speaker_format\n    self.reindex_entries = reindex_entries\n    self.timestamp_format = timestamp_format\n    self.max_chars_per_line = max_chars_per_line\n    self.use_pysrt = use_pysrt\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.srt_processor.SRTProcessor","title":"<code>SRTProcessor</code>","text":"<p>Handles parsing and generating SRT format.</p> <p>Provides functionality to convert between SRT text format and TimedText objects, with various formatting options. Supports both native parsing/generation and pysrt backend.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>class SRTProcessor:\n    \"\"\"\n    Handles parsing and generating SRT format.\n\n    Provides functionality to convert between SRT text format and\n    TimedText objects, with various formatting options.\n    Supports both native parsing/generation and pysrt backend.\n    \"\"\"\n\n    def __init__(self, config: Optional[SRTConfig] = None):\n        \"\"\"\n        Initialize with optional configuration overrides.\n\n        Args:\n            config: Configuration options for SRT processing\n        \"\"\"\n        self.config = config or SRTConfig()\n\n    def merge_srts(self, srt_list: List[str]) -&gt; str:\n        \"\"\"Merge multiple SRT files into a single SRT string.\"\"\"\n        timed_text_list = [self.parse(srt) for srt in srt_list]\n        combined_timed_text = self.combine(timed_text_list)\n        return self.generate(combined_timed_text, self.config.include_speaker)\n\n    def generate(\n        self, \n        timed_text: TimedText, \n        include_speaker: Optional[bool] = None\n    ) -&gt; str:\n        \"\"\"\n        Generate SRT content from a TimedText object.\n        Uses internal generator or pysrt depending on configuration.\n        \"\"\"\n        if not include_speaker:\n            include_speaker = self.config.include_speaker\n        if self.config.use_pysrt:\n            return self._generate_with_pysrt(timed_text)\n\n        srt_parts = []\n        srt_parts.extend(\n            self._generate_entry(\n                entry, \n                index=i if self.config.reindex_entries else entry.index,\n                include_speaker=include_speaker\n            )\n            for i, entry in enumerate(timed_text.iter_segments(), start=1)\n        )\n        return \"\\n\".join(srt_parts)\n\n    def parse(self, srt_content: str) -&gt; TimedText:\n        \"\"\"\n        Parse SRT content into a new TimedText object.\n        Uses internal parser or pysrt depending on configuration.\n        \"\"\"\n        if self.config.use_pysrt:\n            return self._parse_with_pysrt(srt_content)\n        parser = self._SRTParser(srt_content)\n        return parser.parse()\n\n    def shift_timestamps(self, timed_text: TimedText, offset_ms: int) -&gt; TimedText:\n            \"\"\"\n            Shift all timestamps by the given offset.\n\n            Args:\n                timed_text: TimedText to shift\n                offset_ms: Offset in milliseconds to apply\n\n            Returns:\n                New TimedText object with adjusted timestamps\n            \"\"\"\n            new_segments = [\n                segment.shift_time(offset_ms) \n                for segment in timed_text.iter_segments()\n                ]\n            return TimedText(segments=new_segments)\n\n    def combine(self, timed_texts: List[TimedText]) -&gt; TimedText:\n        \"\"\"\n        Combine multiple lists of TimedText into one, with proper indexing.\n\n        Args:\n            timed_texts: List of TimedText to combine\n\n        Returns:\n            Combined TimedText object\n        \"\"\"\n        combined_segments = []\n        for timed_text in timed_texts:\n            combined_segments.extend(timed_text.segments)\n\n        # Sort by start time\n        combined_segments.sort(key=lambda x: x.start_ms)\n\n        return TimedText(segments=combined_segments)\n\n    def assign_single_speaker(self, srt_content: str, speaker: str) -&gt; str:\n        \"\"\"\n        Assign the same speaker to all segments in the SRT content.\n        \"\"\"\n        timed_text = self.parse(srt_content)\n        timed_text.set_all_speakers(speaker)\n        return self.generate(timed_text, include_speaker=True)\n\n    def assign_speaker_by_mapping(\n        self, srt_content: str, speaker_labels: dict[str, list[int]]\n        ) -&gt; str:\n        \"\"\"\n        Assign speakers to segments based on a mapping of speaker to segment indices.\n        (Not implemented yet.)\n        \"\"\"\n        raise NotImplementedError(\"assign_speaker_by_mapping is not implemented yet.\")\n\n    def add_speaker_labels(\n        self, \n        srt_content: str, \n        *, \n        speaker: Optional[str] = None, \n        speaker_labels: Optional[dict[str, list[int]]] = None\n        ) -&gt; str:\n        \"\"\"\n        Unified entry point for adding speaker labels. \n        (Not implemented yet.)\n        \"\"\"\n        raise NotImplementedError(\"add_speaker_labels is not implemented yet.\")\n\n    class _SRTParser:\n        \"\"\"Inner class to manage the state of the SRT parsing.\"\"\"\n\n        def __init__(self, srt_content: str):\n            self.lines = srt_content.splitlines()\n            self.current_index = 0\n            self.timed_segments = []\n\n        def parse(self) -&gt; TimedText:\n            while self.current_index &lt; len(self.lines):\n                if self.lines[self.current_index].strip():\n                    try:\n                        timed_segment = self._parse_entry()\n                        self.timed_segments.append(timed_segment)\n                    except (IndexError, ValueError) as e:\n                        raise ValueError(\n                            f\"Invalid SRT format at line {self.current_index}: {e}\"\n                        ) from e\n                self.current_index += 1  # Always increment to avoid infinite loops\n\n            return TimedText(segments=self.timed_segments)\n\n        def _parse_entry(self) -&gt; TimedTextUnit:\n            index = self._parse_index()\n            start_time, end_time = self._parse_timestamps()\n            text = self._parse_text()\n            start_ms = self._timestamp_to_ms(start_time)\n            end_ms = self._timestamp_to_ms(end_time)\n            speaker, text = _extract_speaker_from_text(text)\n\n            return TimedTextUnit(\n                text=text,\n                start_ms=start_ms,\n                end_ms=end_ms,\n                speaker=speaker,\n                index=index,\n                granularity=Granularity.SEGMENT,\n                confidence=None,\n            )\n\n        def _parse_index(self) -&gt; int:\n            try:\n                index = int(self.lines[self.current_index])\n                self.current_index += 1\n                return index\n            except ValueError as ve:\n                raise ValueError(\n                    f\"Invalid SRT entry index at line {self.current_index + 1}:\"\n                    f\" '{self.lines[self.current_index]}' is not an integer.\"\n                ) from ve\n\n        def _parse_timestamps(self) -&gt; Tuple[str, str]:\n            timestamps_line = self.lines[self.current_index]\n            start_time, end_time = timestamps_line.split(\"--&gt;\")\n            self.current_index += 1\n            return start_time.strip(), end_time.strip()\n\n        def _parse_text(self) -&gt; str:\n            text_lines = []\n            while self.current_index &lt; len(self.lines) \\\n                and self.lines[self.current_index].strip():\n                text_lines.append(self.lines[self.current_index])\n                self.current_index += 1\n            return \"\\n\".join(text_lines).strip()\n\n        def _timestamp_to_ms(self, timestamp: str) -&gt; int:\n            \"\"\"\n            Convert SRT timestamp (HH:MM:SS,mmm) to milliseconds.\n\n            Args:\n                timestamp: SRT format timestamp\n\n            Returns:\n                Timestamp in milliseconds\n            \"\"\"\n            pattern = r\"(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})\"\n            match = re.match(pattern, timestamp)\n            if not match:\n                raise ValueError(f\"Invalid timestamp format: {timestamp}\")\n\n            hours, minutes, seconds, milliseconds = map(int, match.groups())\n            return hours * 3600000 + minutes * 60000 + seconds * 1000 + milliseconds\n\n    def _generate_entry(\n        self, \n        entry: TimedTextUnit, \n        index: Optional[int] = None,\n        include_speaker: bool = False,\n        ) -&gt; str:\n        \"\"\"Generate a single SRT entry from a TimedUnit.\"\"\"\n        start_timestamp = self._ms_to_timestamp(entry.start_ms)\n        end_timestamp = self._ms_to_timestamp(entry.end_ms)\n\n        text = entry.text\n        if self.config.include_speaker and entry.speaker:\n            text = self.config.speaker_format.format(speaker=entry.speaker, text=text)\n\n        srt_entry = [\n            str(index or 0),\n            f\"{start_timestamp} --&gt; {end_timestamp}\",\n            text,\n            \"\",  # Empty line between entries\n        ]\n        return \"\\n\".join(srt_entry)\n\n    def _ms_to_timestamp(self, milliseconds: int) -&gt; str:\n        \"\"\"\n        Convert milliseconds to SRT timestamp format (HH:MM:SS,mmm).\n\n        Args:\n            milliseconds: Time in milliseconds\n\n        Returns:\n            Formatted timestamp string\n        \"\"\"\n        total_seconds, ms = divmod(milliseconds, 1000)\n        hours, remainder = divmod(total_seconds, 3600)\n        minutes, seconds = divmod(remainder, 60)\n\n        return self.config.timestamp_format.format(hours, minutes, seconds, ms)\n\n    def _parse_with_pysrt(self, srt_content: str) -&gt; TimedText:\n        \"\"\"Internal: Parse using pysrt, extracting speaker information.\"\"\"\n        subs = pysrt.from_string(srt_content)\n        segments = []\n        for item in subs:\n            speaker, text = _extract_speaker_from_text(item.text)\n            segments.append(\n                TimedTextUnit(\n                    text=text,\n                    speaker=speaker,\n                    start_ms=item.start.ordinal,\n                    end_ms=item.end.ordinal,\n                    index=item.index,\n                    granularity=Granularity.SEGMENT,\n                    confidence=None,\n                )\n            )\n        return TimedText(segments=segments)\n\n    def _generate_with_pysrt(self, timed_text: TimedText) -&gt; str:\n        \"\"\"Internal: Generate SRT using pysrt.\"\"\"\n        subs = pysrt.SubRipFile()\n        for i, segment in enumerate(timed_text.iter_segments(), start=1):\n            start = pysrt.SubRipTime(milliseconds=segment.start_ms)\n            end = pysrt.SubRipTime(milliseconds=segment.end_ms)\n            text = segment.text\n            if self.config.include_speaker and segment.speaker:\n                text = self.config.speaker_format.format(\n                    speaker=segment.speaker, text=text)\n            subs.append(pysrt.SubRipItem(index=i, start=start, end=end, text=text))\n        return subs.to_string()\n</code></pre> <code>config = config or SRTConfig()</code> <code>instance-attribute</code> \u00b6 <code>__init__(config=None)</code> \u00b6 <p>Initialize with optional configuration overrides.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[SRTConfig]</code> <p>Configuration options for SRT processing</p> <code>None</code> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>def __init__(self, config: Optional[SRTConfig] = None):\n    \"\"\"\n    Initialize with optional configuration overrides.\n\n    Args:\n        config: Configuration options for SRT processing\n    \"\"\"\n    self.config = config or SRTConfig()\n</code></pre> <code>add_speaker_labels(srt_content, *, speaker=None, speaker_labels=None)</code> \u00b6 <p>Unified entry point for adding speaker labels.  (Not implemented yet.)</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>def add_speaker_labels(\n    self, \n    srt_content: str, \n    *, \n    speaker: Optional[str] = None, \n    speaker_labels: Optional[dict[str, list[int]]] = None\n    ) -&gt; str:\n    \"\"\"\n    Unified entry point for adding speaker labels. \n    (Not implemented yet.)\n    \"\"\"\n    raise NotImplementedError(\"add_speaker_labels is not implemented yet.\")\n</code></pre> <code>assign_single_speaker(srt_content, speaker)</code> \u00b6 <p>Assign the same speaker to all segments in the SRT content.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>def assign_single_speaker(self, srt_content: str, speaker: str) -&gt; str:\n    \"\"\"\n    Assign the same speaker to all segments in the SRT content.\n    \"\"\"\n    timed_text = self.parse(srt_content)\n    timed_text.set_all_speakers(speaker)\n    return self.generate(timed_text, include_speaker=True)\n</code></pre> <code>assign_speaker_by_mapping(srt_content, speaker_labels)</code> \u00b6 <p>Assign speakers to segments based on a mapping of speaker to segment indices. (Not implemented yet.)</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>def assign_speaker_by_mapping(\n    self, srt_content: str, speaker_labels: dict[str, list[int]]\n    ) -&gt; str:\n    \"\"\"\n    Assign speakers to segments based on a mapping of speaker to segment indices.\n    (Not implemented yet.)\n    \"\"\"\n    raise NotImplementedError(\"assign_speaker_by_mapping is not implemented yet.\")\n</code></pre> <code>combine(timed_texts)</code> \u00b6 <p>Combine multiple lists of TimedText into one, with proper indexing.</p> <p>Parameters:</p> Name Type Description Default <code>timed_texts</code> <code>List[TimedText]</code> <p>List of TimedText to combine</p> required <p>Returns:</p> Type Description <code>TimedText</code> <p>Combined TimedText object</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>def combine(self, timed_texts: List[TimedText]) -&gt; TimedText:\n    \"\"\"\n    Combine multiple lists of TimedText into one, with proper indexing.\n\n    Args:\n        timed_texts: List of TimedText to combine\n\n    Returns:\n        Combined TimedText object\n    \"\"\"\n    combined_segments = []\n    for timed_text in timed_texts:\n        combined_segments.extend(timed_text.segments)\n\n    # Sort by start time\n    combined_segments.sort(key=lambda x: x.start_ms)\n\n    return TimedText(segments=combined_segments)\n</code></pre> <code>generate(timed_text, include_speaker=None)</code> \u00b6 <p>Generate SRT content from a TimedText object. Uses internal generator or pysrt depending on configuration.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>def generate(\n    self, \n    timed_text: TimedText, \n    include_speaker: Optional[bool] = None\n) -&gt; str:\n    \"\"\"\n    Generate SRT content from a TimedText object.\n    Uses internal generator or pysrt depending on configuration.\n    \"\"\"\n    if not include_speaker:\n        include_speaker = self.config.include_speaker\n    if self.config.use_pysrt:\n        return self._generate_with_pysrt(timed_text)\n\n    srt_parts = []\n    srt_parts.extend(\n        self._generate_entry(\n            entry, \n            index=i if self.config.reindex_entries else entry.index,\n            include_speaker=include_speaker\n        )\n        for i, entry in enumerate(timed_text.iter_segments(), start=1)\n    )\n    return \"\\n\".join(srt_parts)\n</code></pre> <code>merge_srts(srt_list)</code> \u00b6 <p>Merge multiple SRT files into a single SRT string.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>def merge_srts(self, srt_list: List[str]) -&gt; str:\n    \"\"\"Merge multiple SRT files into a single SRT string.\"\"\"\n    timed_text_list = [self.parse(srt) for srt in srt_list]\n    combined_timed_text = self.combine(timed_text_list)\n    return self.generate(combined_timed_text, self.config.include_speaker)\n</code></pre> <code>parse(srt_content)</code> \u00b6 <p>Parse SRT content into a new TimedText object. Uses internal parser or pysrt depending on configuration.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>def parse(self, srt_content: str) -&gt; TimedText:\n    \"\"\"\n    Parse SRT content into a new TimedText object.\n    Uses internal parser or pysrt depending on configuration.\n    \"\"\"\n    if self.config.use_pysrt:\n        return self._parse_with_pysrt(srt_content)\n    parser = self._SRTParser(srt_content)\n    return parser.parse()\n</code></pre> <code>shift_timestamps(timed_text, offset_ms)</code> \u00b6 <p>Shift all timestamps by the given offset.</p> <p>Parameters:</p> Name Type Description Default <code>timed_text</code> <code>TimedText</code> <p>TimedText to shift</p> required <code>offset_ms</code> <code>int</code> <p>Offset in milliseconds to apply</p> required <p>Returns:</p> Type Description <code>TimedText</code> <p>New TimedText object with adjusted timestamps</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>def shift_timestamps(self, timed_text: TimedText, offset_ms: int) -&gt; TimedText:\n        \"\"\"\n        Shift all timestamps by the given offset.\n\n        Args:\n            timed_text: TimedText to shift\n            offset_ms: Offset in milliseconds to apply\n\n        Returns:\n            New TimedText object with adjusted timestamps\n        \"\"\"\n        new_segments = [\n            segment.shift_time(offset_ms) \n            for segment in timed_text.iter_segments()\n            ]\n        return TimedText(segments=new_segments)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.srt_processor.SubtitleFormat","title":"<code>SubtitleFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported subtitle formats.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/srt_processor.py</code> <pre><code>class SubtitleFormat(str, Enum):\n    \"\"\"Supported subtitle formats.\"\"\"\n    SRT = \"srt\"\n    VTT = \"vtt\"\n    TEXT = \"text\"\n</code></pre> <code>SRT = 'srt'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>TEXT = 'text'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>VTT = 'vtt'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.transcription.text_segment_builder","title":"<code>text_segment_builder</code>","text":"<p>SegmentBuilder for creating phrase-level segments from word-level TimedText.</p> <p>This module builds higher-level segments from a TimedText object containing  word-level units, based on configurable criteria like duration, character count,  punctuation, pauses, and speaker changes.</p>"},{"location":"api/#tnh_scholar.audio_processing.transcription.text_segment_builder.COMMON_ABBREVIATIONS","title":"<code>COMMON_ABBREVIATIONS = frozenset({'adj.', 'adm.', 'adv.', 'al.', 'anon.', 'apr.', 'arc.', 'aug.', 'ave.', 'brig.', 'bros.', 'capt.', 'cmdr.', 'col.', 'comdr.', 'con.', 'corp.', 'cpl.', 'dr.', 'drs.', 'ed.', 'enc.', 'etc.', 'ex.', 'feb.', 'gen.', 'gov.', 'hon.', 'hosp.', 'hr.', 'inc.', 'jan.', 'jr.', 'maj.', 'mar.', 'messrs.', 'mlle.', 'mm.', 'mme.', 'mr.', 'mrs.', 'ms.', 'msgr.', 'nov.', 'oct.', 'op.', 'ord.', 'ph.d.', 'prof.', 'pvt.', 'rep.', 'reps.', 'res.', 'rev.', 'rt.', 'sen.', 'sens.', 'sep.', 'sfc.', 'sgt.', 'sr.', 'st.', 'supt.', 'surg.', 'u.s.', 'v.p.', 'vs.'})</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.text_segment_builder.TextSegmentBuilder","title":"<code>TextSegmentBuilder</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/transcription/text_segment_builder.py</code> <pre><code>class TextSegmentBuilder:\n    def __init__(\n        self,\n        *,\n        max_duration_ms: Optional[int] = None, # milliseconds\n        target_characters: Optional[int] = None,\n        avoid_orphans: bool = True,\n        max_gap_duration_ms: Optional[int] = None, # milliseconds\n        ignore_speaker: bool = True,\n    ):\n        self.max_duration = max_duration_ms\n        self.target_characters = target_characters\n        self.avoid_orphans = avoid_orphans\n        self.max_gap_duration = max_gap_duration_ms\n        self.ignore_speaker = ignore_speaker\n\n        self.segments: List[TimedTextUnit] = []\n        self.current_words: List[TimedTextUnit] = []\n        self.current_characters = 0\n\n    def create_segments(self, timed_text: TimedText) -&gt; TimedText:\n        # Validate\n        if not timed_text.words:\n            raise ValueError(\n                \"TimedText object must have word-level units to build segments.\"\n                )\n\n        for unit in timed_text.words:\n            if unit.granularity != Granularity.WORD:\n                raise ValueError(f\"Expected WORD units, got {unit.granularity}\")\n\n        # Process\n        for word in timed_text.words:\n            if self._should_start_new_segment(word):\n                self._flush_current_words()\n            self._add_word(word)\n\n        self._flush_current_words()  # Final flush\n        return TimedText(segments=self.segments, granularity=Granularity.SEGMENT)\n\n    def _add_word(self, word: TimedTextUnit):\n        if self.current_words:\n            self.current_characters += 1  # space before the new word\n        self.current_characters += len(word.text)\n        self.current_words.append(word)\n\n\n    def _should_start_new_segment(self, word: TimedTextUnit) -&gt; bool:\n        if not self.current_words:\n            return False\n\n        # Speaker change\n        last_word = self.current_words[-1]\n        if not self.ignore_speaker and (word.speaker != last_word.speaker):\n            return True\n\n        # Significant pause\n        if self.max_gap_duration is not None:\n            pause = word.start_ms - last_word.end_ms\n            if pause &gt; self.max_gap_duration:\n                return True\n\n        # End punctuation\n        if last_word.text and self._is_punctuation_word(last_word.text):\n            return True\n\n        # Max duration\n        if self.max_duration is not None:\n            duration = word.end_ms - self.current_words[0].start_ms\n            if duration &gt; self.max_duration:\n                return True\n\n        # Target character count\n        if self.target_characters is not None:\n            total_chars = self.current_characters + len(word.text) + 1\n            if total_chars &gt; self.target_characters:\n                return True\n\n        return False\n\n    def _flush_current_words(self):\n        if not self.current_words:\n            return\n\n        segment_text = \" \".join(word.text for word in self.current_words)\n        segment = TimedTextUnit(\n            text=segment_text,\n            start_ms=self.current_words[0].start_ms,\n            end_ms=self.current_words[-1].end_ms,\n            granularity=Granularity.SEGMENT,\n            speaker=None if self.ignore_speaker else self._find_speaker(),\n            confidence=None,\n            index=None,\n        )\n        self.segments.append(segment)\n        self.current_words = []\n        self.current_characters = 0\n\n    def _find_speaker(self) -&gt; Optional[str]:\n        \"\"\"\n        Only called when ignore_speakers is False; \n        in this case we always split on speaker. \n        So only one speaker is expected. \n        \"\"\"\n        speakers = {word.speaker for word in self.current_words}\n        assert len(speakers) == 1, \"Inconsistent speakers in segment\"\n        return speakers.pop()\n\n    def _is_punctuation_word(self, word_text: str) -&gt; bool:\n        \"\"\"\n        Check if a word ending in punctuation should trigger a new segment,\n        excluding common abbreviations.\n        \"\"\"\n        if not word_text:\n            return False\n        return word_text[-1] in \".!?\" and word_text.lower() not in COMMON_ABBREVIATIONS\n\n\n    def build_segments(\n        self,\n        *,\n        target_duration: Optional[int] = None,\n        target_characters: Optional[int] = None,\n        avoid_orphans: Optional[bool] = True,\n        max_gap_duration: Optional[int] = None,\n        ignore_speaker: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Build or rebuild `segments` from the contents of `words`.\n\n        Args:\n            target_duration: Maximum desired segment duration in milliseconds.\n            target_characters: Maximum desired character length of a segment.\n            avoid_orphans: If True, prevent extremely short trailing segments.\n\n        Note:\n            This is a stub.  Concrete algorithms will be implemented later.\n\n        Raises:\n            NotImplementedError: Always, until implemented.\n        \"\"\"\n        raise NotImplementedError(\"build_segments is not yet implemented.\")\n</code></pre> <code>avoid_orphans = avoid_orphans</code> <code>instance-attribute</code> \u00b6 <code>current_characters = 0</code> <code>instance-attribute</code> \u00b6 <code>current_words = []</code> <code>instance-attribute</code> \u00b6 <code>ignore_speaker = ignore_speaker</code> <code>instance-attribute</code> \u00b6 <code>max_duration = max_duration_ms</code> <code>instance-attribute</code> \u00b6 <code>max_gap_duration = max_gap_duration_ms</code> <code>instance-attribute</code> \u00b6 <code>segments = []</code> <code>instance-attribute</code> \u00b6 <code>target_characters = target_characters</code> <code>instance-attribute</code> \u00b6 <code>__init__(*, max_duration_ms=None, target_characters=None, avoid_orphans=True, max_gap_duration_ms=None, ignore_speaker=True)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/transcription/text_segment_builder.py</code> <pre><code>def __init__(\n    self,\n    *,\n    max_duration_ms: Optional[int] = None, # milliseconds\n    target_characters: Optional[int] = None,\n    avoid_orphans: bool = True,\n    max_gap_duration_ms: Optional[int] = None, # milliseconds\n    ignore_speaker: bool = True,\n):\n    self.max_duration = max_duration_ms\n    self.target_characters = target_characters\n    self.avoid_orphans = avoid_orphans\n    self.max_gap_duration = max_gap_duration_ms\n    self.ignore_speaker = ignore_speaker\n\n    self.segments: List[TimedTextUnit] = []\n    self.current_words: List[TimedTextUnit] = []\n    self.current_characters = 0\n</code></pre> <code>build_segments(*, target_duration=None, target_characters=None, avoid_orphans=True, max_gap_duration=None, ignore_speaker=False)</code> \u00b6 <p>Build or rebuild <code>segments</code> from the contents of <code>words</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target_duration</code> <code>Optional[int]</code> <p>Maximum desired segment duration in milliseconds.</p> <code>None</code> <code>target_characters</code> <code>Optional[int]</code> <p>Maximum desired character length of a segment.</p> <code>None</code> <code>avoid_orphans</code> <code>Optional[bool]</code> <p>If True, prevent extremely short trailing segments.</p> <code>True</code> Note <p>This is a stub.  Concrete algorithms will be implemented later.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always, until implemented.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/text_segment_builder.py</code> <pre><code>def build_segments(\n    self,\n    *,\n    target_duration: Optional[int] = None,\n    target_characters: Optional[int] = None,\n    avoid_orphans: Optional[bool] = True,\n    max_gap_duration: Optional[int] = None,\n    ignore_speaker: bool = False,\n) -&gt; None:\n    \"\"\"\n    Build or rebuild `segments` from the contents of `words`.\n\n    Args:\n        target_duration: Maximum desired segment duration in milliseconds.\n        target_characters: Maximum desired character length of a segment.\n        avoid_orphans: If True, prevent extremely short trailing segments.\n\n    Note:\n        This is a stub.  Concrete algorithms will be implemented later.\n\n    Raises:\n        NotImplementedError: Always, until implemented.\n    \"\"\"\n    raise NotImplementedError(\"build_segments is not yet implemented.\")\n</code></pre> <code>create_segments(timed_text)</code> \u00b6 Source code in <code>src/tnh_scholar/audio_processing/transcription/text_segment_builder.py</code> <pre><code>def create_segments(self, timed_text: TimedText) -&gt; TimedText:\n    # Validate\n    if not timed_text.words:\n        raise ValueError(\n            \"TimedText object must have word-level units to build segments.\"\n            )\n\n    for unit in timed_text.words:\n        if unit.granularity != Granularity.WORD:\n            raise ValueError(f\"Expected WORD units, got {unit.granularity}\")\n\n    # Process\n    for word in timed_text.words:\n        if self._should_start_new_segment(word):\n            self._flush_current_words()\n        self._add_word(word)\n\n    self._flush_current_words()  # Final flush\n    return TimedText(segments=self.segments, granularity=Granularity.SEGMENT)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.transcription_service","title":"<code>transcription_service</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.transcription_service.TranscriptionResult","title":"<code>TranscriptionResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>class TranscriptionResult(BaseModel):\n    text: str\n    language: str\n    word_timing: Optional[TimedText] = None\n    utterance_timing: Optional[TimedText] = None\n    confidence: Optional[float] = None\n    audio_duration_ms: Optional[int] = None\n    transcript_id: Optional[str] = None\n    status: Optional[str] = None\n    raw_result: Optional[Dict[str, Any]] = None\n</code></pre> <code>audio_duration_ms = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>confidence = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>language</code> <code>instance-attribute</code> \u00b6 <code>raw_result = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>status = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>text</code> <code>instance-attribute</code> \u00b6 <code>transcript_id = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>utterance_timing = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>word_timing = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.transcription.transcription_service.TranscriptionService","title":"<code>TranscriptionService</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for transcription services.</p> <p>This interface provides a standard way to interact with different transcription service providers (e.g., OpenAI Whisper, AssemblyAI).</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>class TranscriptionService(ABC):\n    \"\"\"\n    Abstract base class defining the interface for transcription services.\n\n    This interface provides a standard way to interact with different\n    transcription service providers (e.g., OpenAI Whisper, AssemblyAI).\n    \"\"\"\n\n    @abstractmethod\n    def transcribe(\n        self,\n        audio_file: Union[Path, BytesIO],\n        options: Optional[Dict[str, Any]] = None\n    ) -&gt; TranscriptionResult:\n        \"\"\"\n        Transcribe audio file to text.\n\n        Args:\n            audio_file: Path to audio file or file-like object\n            options: Provider-specific options for transcription\n\n        Returns:\n            TranscriptionResult\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_result(self, job_id: str) -&gt; TranscriptionResult:\n        \"\"\"\n        Get results for an existing transcription job.\n\n        Args:\n            job_id: ID of the transcription job\n\n        Returns:\n            Dictionary containing transcription results in the same\n            standardized format as transcribe()\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def transcribe_to_format(\n        self,\n        audio_file: Union[Path, BytesIO],\n        format_type: str = \"srt\",\n        transcription_options: Optional[Dict[str, Any]] = None,\n        format_options: Optional[Dict[str, Any]] = None\n    ) -&gt; str:\n        \"\"\"\n        Transcribe audio and return result in specified format.\n\n        Args:\n            audio_file: Path, file-like object, or URL of audio file\n            format_type: Format type (e.g., \"srt\", \"vtt\", \"text\")\n            transcription_options: Options for transcription\n            format_options: Format-specific options\n\n        Returns:\n            String representation in the requested format\n        \"\"\"\n        pass\n</code></pre> <code>get_result(job_id)</code> <code>abstractmethod</code> \u00b6 <p>Get results for an existing transcription job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>ID of the transcription job</p> required <p>Returns:</p> Type Description <code>TranscriptionResult</code> <p>Dictionary containing transcription results in the same</p> <code>TranscriptionResult</code> <p>standardized format as transcribe()</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>@abstractmethod\ndef get_result(self, job_id: str) -&gt; TranscriptionResult:\n    \"\"\"\n    Get results for an existing transcription job.\n\n    Args:\n        job_id: ID of the transcription job\n\n    Returns:\n        Dictionary containing transcription results in the same\n        standardized format as transcribe()\n    \"\"\"\n    pass\n</code></pre> <code>transcribe(audio_file, options=None)</code> <code>abstractmethod</code> \u00b6 <p>Transcribe audio file to text.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Union[Path, BytesIO]</code> <p>Path to audio file or file-like object</p> required <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Provider-specific options for transcription</p> <code>None</code> <p>Returns:</p> Type Description <code>TranscriptionResult</code> <p>TranscriptionResult</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>@abstractmethod\ndef transcribe(\n    self,\n    audio_file: Union[Path, BytesIO],\n    options: Optional[Dict[str, Any]] = None\n) -&gt; TranscriptionResult:\n    \"\"\"\n    Transcribe audio file to text.\n\n    Args:\n        audio_file: Path to audio file or file-like object\n        options: Provider-specific options for transcription\n\n    Returns:\n        TranscriptionResult\n\n    \"\"\"\n    pass\n</code></pre> <code>transcribe_to_format(audio_file, format_type='srt', transcription_options=None, format_options=None)</code> <code>abstractmethod</code> \u00b6 <p>Transcribe audio and return result in specified format.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Union[Path, BytesIO]</code> <p>Path, file-like object, or URL of audio file</p> required <code>format_type</code> <code>str</code> <p>Format type (e.g., \"srt\", \"vtt\", \"text\")</p> <code>'srt'</code> <code>transcription_options</code> <code>Optional[Dict[str, Any]]</code> <p>Options for transcription</p> <code>None</code> <code>format_options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation in the requested format</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>@abstractmethod\ndef transcribe_to_format(\n    self,\n    audio_file: Union[Path, BytesIO],\n    format_type: str = \"srt\",\n    transcription_options: Optional[Dict[str, Any]] = None,\n    format_options: Optional[Dict[str, Any]] = None\n) -&gt; str:\n    \"\"\"\n    Transcribe audio and return result in specified format.\n\n    Args:\n        audio_file: Path, file-like object, or URL of audio file\n        format_type: Format type (e.g., \"srt\", \"vtt\", \"text\")\n        transcription_options: Options for transcription\n        format_options: Format-specific options\n\n    Returns:\n        String representation in the requested format\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.transcription_service.TranscriptionServiceFactory","title":"<code>TranscriptionServiceFactory</code>","text":"<p>Factory for creating transcription service instances.</p> <p>This factory provides a standard way to create transcription service instances based on the provider name and configuration.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>class TranscriptionServiceFactory:\n    \"\"\"\n    Factory for creating transcription service instances.\n\n    This factory provides a standard way to create transcription service\n    instances based on the provider name and configuration.\n    \"\"\"\n\n    # Mapping provider names to implementation classes\n    # Classes will be imported lazily when needed\n    _PROVIDER_MAP: Dict[str, Callable[..., TranscriptionService]] = {}\n\n    @classmethod\n    def register_provider(\n        cls, \n        name: str, \n        provider_class: Callable[..., TranscriptionService]\n    ) -&gt; None:\n        \"\"\"\n        Register a provider implementation with the factory.\n\n        Args:\n            name: Provider name (lowercase)\n            provider_class: Provider implementation class or factory function\n\n        Example:\n            &gt;&gt;&gt; from my_module import MyTranscriptionService\n            &gt;&gt;&gt; TranscriptionServiceFactory.register_provider(\"my_provider\", MyTranscriptionService)\n        \"\"\"  \n        cls._PROVIDER_MAP[name.lower()] = provider_class\n\n    @classmethod\n    def create_service(\n        cls,\n        provider: str = \"assemblyai\",\n        api_key: Optional[str] = None,\n        **kwargs: Any\n    ) -&gt; TranscriptionService:\n        \"\"\"\n        Create a transcription service instance.\n\n        Args:\n            provider: Service provider name (e.g., \"whisper\", \"assemblyai\")\n            api_key: API key for the service\n            **kwargs: Additional provider-specific configuration\n\n        Returns:\n            TranscriptionService instance\n\n        Raises:\n            ValueError: If the provider is not supported\n            ImportError: If the provider module cannot be imported\n        \"\"\"\n        provider = provider.lower()\n\n        # Initialize provider map if empty\n        if not cls._PROVIDER_MAP:\n            # Import lazily to avoid circular imports\n            from .assemblyai_service import AAITranscriptionService\n            from .whisper_service import WhisperTranscriptionService\n\n            cls._PROVIDER_MAP = {\n                \"whisper\": WhisperTranscriptionService,\n                \"assemblyai\": AAITranscriptionService,\n            }\n\n        # Get the provider implementation\n        provider_class = cls._PROVIDER_MAP.get(provider)\n\n        if provider_class is None:\n            raise ValueError(f\"Unsupported transcription provider: {provider}\")\n\n        # Create and return the service instance\n        return provider_class(api_key=api_key, **kwargs)\n</code></pre> <code>create_service(provider='assemblyai', api_key=None, **kwargs)</code> <code>classmethod</code> \u00b6 <p>Create a transcription service instance.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Service provider name (e.g., \"whisper\", \"assemblyai\")</p> <code>'assemblyai'</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for the service</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>TranscriptionService</code> <p>TranscriptionService instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provider is not supported</p> <code>ImportError</code> <p>If the provider module cannot be imported</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>@classmethod\ndef create_service(\n    cls,\n    provider: str = \"assemblyai\",\n    api_key: Optional[str] = None,\n    **kwargs: Any\n) -&gt; TranscriptionService:\n    \"\"\"\n    Create a transcription service instance.\n\n    Args:\n        provider: Service provider name (e.g., \"whisper\", \"assemblyai\")\n        api_key: API key for the service\n        **kwargs: Additional provider-specific configuration\n\n    Returns:\n        TranscriptionService instance\n\n    Raises:\n        ValueError: If the provider is not supported\n        ImportError: If the provider module cannot be imported\n    \"\"\"\n    provider = provider.lower()\n\n    # Initialize provider map if empty\n    if not cls._PROVIDER_MAP:\n        # Import lazily to avoid circular imports\n        from .assemblyai_service import AAITranscriptionService\n        from .whisper_service import WhisperTranscriptionService\n\n        cls._PROVIDER_MAP = {\n            \"whisper\": WhisperTranscriptionService,\n            \"assemblyai\": AAITranscriptionService,\n        }\n\n    # Get the provider implementation\n    provider_class = cls._PROVIDER_MAP.get(provider)\n\n    if provider_class is None:\n        raise ValueError(f\"Unsupported transcription provider: {provider}\")\n\n    # Create and return the service instance\n    return provider_class(api_key=api_key, **kwargs)\n</code></pre> <code>register_provider(name, provider_class)</code> <code>classmethod</code> \u00b6 <p>Register a provider implementation with the factory.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Provider name (lowercase)</p> required <code>provider_class</code> <code>Callable[..., TranscriptionService]</code> <p>Provider implementation class or factory function</p> required Example <p>from my_module import MyTranscriptionService TranscriptionServiceFactory.register_provider(\"my_provider\", MyTranscriptionService)</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>@classmethod\ndef register_provider(\n    cls, \n    name: str, \n    provider_class: Callable[..., TranscriptionService]\n) -&gt; None:\n    \"\"\"\n    Register a provider implementation with the factory.\n\n    Args:\n        name: Provider name (lowercase)\n        provider_class: Provider implementation class or factory function\n\n    Example:\n        &gt;&gt;&gt; from my_module import MyTranscriptionService\n        &gt;&gt;&gt; TranscriptionServiceFactory.register_provider(\"my_provider\", MyTranscriptionService)\n    \"\"\"  \n    cls._PROVIDER_MAP[name.lower()] = provider_class\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.transcription_service.Utterance","title":"<code>Utterance</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>class Utterance(BaseModel):\n    speaker: Optional[str]\n    start_ms: int\n    end_ms: int\n    text: str\n    confidence: float\n</code></pre> <code>confidence</code> <code>instance-attribute</code> \u00b6 <code>end_ms</code> <code>instance-attribute</code> \u00b6 <code>speaker</code> <code>instance-attribute</code> \u00b6 <code>start_ms</code> <code>instance-attribute</code> \u00b6 <code>text</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.transcription.transcription_service.WordTiming","title":"<code>WordTiming</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/tnh_scholar/audio_processing/transcription/transcription_service.py</code> <pre><code>class WordTiming(BaseModel):\n    word: str\n    start_ms: int\n    end_ms: int\n    confidence: float\n</code></pre> <code>confidence</code> <code>instance-attribute</code> \u00b6 <code>end_ms</code> <code>instance-attribute</code> \u00b6 <code>start_ms</code> <code>instance-attribute</code> \u00b6 <code>word</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.transcription.vtt_processor","title":"<code>vtt_processor</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.vtt_processor.VTTConfig","title":"<code>VTTConfig</code>","text":"<p>Configuration options for WebVTT processing.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/vtt_processor.py</code> <pre><code>class VTTConfig:\n    \"\"\"Configuration options for WebVTT processing.\"\"\"\n\n    def __init__(\n        self,\n        include_speaker: bool = False,\n        speaker_format: str = \"&lt;v {speaker}&gt;{text}\",\n        reindex_entries: bool = False,\n        timestamp_format: str = \"{:02d}:{:02d}:{:02d}.{:03d}\",\n        max_chars_per_line: int = 42\n    ):\n        \"\"\"\n        Initialize with default settings.\n\n        Args:\n            include_speaker: Whether to include speaker labels in output\n            speaker_format: Format string for speaker attribution\n            reindex_entries: Whether to reindex entries sequentially\n            timestamp_format: Format string for timestamp formatting\n            max_chars_per_line: Maximum characters per line before splitting\n        \"\"\"\n        self.include_speaker = include_speaker\n        self.speaker_format = speaker_format\n        self.reindex_entries = reindex_entries\n        self.timestamp_format = timestamp_format\n        self.max_chars_per_line = max_chars_per_line\n</code></pre> <code>include_speaker = include_speaker</code> <code>instance-attribute</code> \u00b6 <code>max_chars_per_line = max_chars_per_line</code> <code>instance-attribute</code> \u00b6 <code>reindex_entries = reindex_entries</code> <code>instance-attribute</code> \u00b6 <code>speaker_format = speaker_format</code> <code>instance-attribute</code> \u00b6 <code>timestamp_format = timestamp_format</code> <code>instance-attribute</code> \u00b6 <code>__init__(include_speaker=False, speaker_format='&lt;v {speaker}&gt;{text}', reindex_entries=False, timestamp_format='{:02d}:{:02d}:{:02d}.{:03d}', max_chars_per_line=42)</code> \u00b6 <p>Initialize with default settings.</p> <p>Parameters:</p> Name Type Description Default <code>include_speaker</code> <code>bool</code> <p>Whether to include speaker labels in output</p> <code>False</code> <code>speaker_format</code> <code>str</code> <p>Format string for speaker attribution</p> <code>'&lt;v {speaker}&gt;{text}'</code> <code>reindex_entries</code> <code>bool</code> <p>Whether to reindex entries sequentially</p> <code>False</code> <code>timestamp_format</code> <code>str</code> <p>Format string for timestamp formatting</p> <code>'{:02d}:{:02d}:{:02d}.{:03d}'</code> <code>max_chars_per_line</code> <code>int</code> <p>Maximum characters per line before splitting</p> <code>42</code> Source code in <code>src/tnh_scholar/audio_processing/transcription/vtt_processor.py</code> <pre><code>def __init__(\n    self,\n    include_speaker: bool = False,\n    speaker_format: str = \"&lt;v {speaker}&gt;{text}\",\n    reindex_entries: bool = False,\n    timestamp_format: str = \"{:02d}:{:02d}:{:02d}.{:03d}\",\n    max_chars_per_line: int = 42\n):\n    \"\"\"\n    Initialize with default settings.\n\n    Args:\n        include_speaker: Whether to include speaker labels in output\n        speaker_format: Format string for speaker attribution\n        reindex_entries: Whether to reindex entries sequentially\n        timestamp_format: Format string for timestamp formatting\n        max_chars_per_line: Maximum characters per line before splitting\n    \"\"\"\n    self.include_speaker = include_speaker\n    self.speaker_format = speaker_format\n    self.reindex_entries = reindex_entries\n    self.timestamp_format = timestamp_format\n    self.max_chars_per_line = max_chars_per_line\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.vtt_processor.VTTProcessor","title":"<code>VTTProcessor</code>","text":"<p>Handles parsing and generating WebVTT format.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/vtt_processor.py</code> <pre><code>class VTTProcessor:\n    \"\"\"Handles parsing and generating WebVTT format.\"\"\"\n\n    def __init__(self, config: Optional[VTTConfig] = None):\n        \"\"\"\n        Initialize with optional configuration.\n\n        Args:\n            config: Configuration options for VTT processing\n        \"\"\"\n        self.config = config or VTTConfig()\n\n    def parse(self, vtt_content: str) -&gt; List[TimedTextUnit]:\n        \"\"\"\n        Parse VTT content into a list of TimedUnit objects.\n\n        Args:\n            vtt_content: String containing VTT formatted content\n\n        Returns:\n            List of TimedUnit objects\n        \"\"\"\n        # Implementation will go here\n        raise NotImplementedError(\"Not implemented.\")\n\n    def generate(self, timed_texts: List[TimedTextUnit]) -&gt; str:\n        \"\"\"\n        Generate VTT content from a list of TimedUnit objects.\n\n        Args:\n            timed_texts: List of TimedUnit objects\n\n        Returns:\n            String containing VTT formatted content\n        \"\"\"\n        # Implementation will go here\n        raise NotImplementedError(\"Not implemented.\")\n</code></pre> <code>config = config or VTTConfig()</code> <code>instance-attribute</code> \u00b6 <code>__init__(config=None)</code> \u00b6 <p>Initialize with optional configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[VTTConfig]</code> <p>Configuration options for VTT processing</p> <code>None</code> Source code in <code>src/tnh_scholar/audio_processing/transcription/vtt_processor.py</code> <pre><code>def __init__(self, config: Optional[VTTConfig] = None):\n    \"\"\"\n    Initialize with optional configuration.\n\n    Args:\n        config: Configuration options for VTT processing\n    \"\"\"\n    self.config = config or VTTConfig()\n</code></pre> <code>generate(timed_texts)</code> \u00b6 <p>Generate VTT content from a list of TimedUnit objects.</p> <p>Parameters:</p> Name Type Description Default <code>timed_texts</code> <code>List[TimedTextUnit]</code> <p>List of TimedUnit objects</p> required <p>Returns:</p> Type Description <code>str</code> <p>String containing VTT formatted content</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/vtt_processor.py</code> <pre><code>def generate(self, timed_texts: List[TimedTextUnit]) -&gt; str:\n    \"\"\"\n    Generate VTT content from a list of TimedUnit objects.\n\n    Args:\n        timed_texts: List of TimedUnit objects\n\n    Returns:\n        String containing VTT formatted content\n    \"\"\"\n    # Implementation will go here\n    raise NotImplementedError(\"Not implemented.\")\n</code></pre> <code>parse(vtt_content)</code> \u00b6 <p>Parse VTT content into a list of TimedUnit objects.</p> <p>Parameters:</p> Name Type Description Default <code>vtt_content</code> <code>str</code> <p>String containing VTT formatted content</p> required <p>Returns:</p> Type Description <code>List[TimedTextUnit]</code> <p>List of TimedUnit objects</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/vtt_processor.py</code> <pre><code>def parse(self, vtt_content: str) -&gt; List[TimedTextUnit]:\n    \"\"\"\n    Parse VTT content into a list of TimedUnit objects.\n\n    Args:\n        vtt_content: String containing VTT formatted content\n\n    Returns:\n        List of TimedUnit objects\n    \"\"\"\n    # Implementation will go here\n    raise NotImplementedError(\"Not implemented.\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.whisper_service","title":"<code>whisper_service</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.whisper_service--todo-major-refactor-planned","title":"TODO: MAJOR REFACTOR PLANNED","text":"<p>This module currently mixes persistent service configuration (WhisperConfig) with per-call runtime options, leading to complex validation and logic. Plan is to:</p> <ul> <li>Refactor so each WhisperTranscriptionService instance is configured once at construction, with all     relevant settings (including file-like/path-like mode, file extension, etc).</li> <li>Use Pydantic BaseSettings for configuration to normalize configuration and validation according to          TNH Scholar style.</li> <li>Remove ad-hoc runtime options from the transcribe() entrypoint; all config should be set at init.</li> <li>If a different configuration is needed, instantiate a new service object.</li> <li>This will simplify validation, error handling, and code logic, and make the contract clear and robust.</li> <li>NOTE: This will change the TranscriptionService contract and will require similar changes in other     transcription system implementations.</li> <li>Update all dependent code and tests accordingly.</li> </ul>"},{"location":"api/#tnh_scholar.audio_processing.transcription.whisper_service.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.transcription.whisper_service.WhisperBase","title":"<code>WhisperBase</code>","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>class WhisperBase(TypedDict):\n    text: str\n    language: str\n    duration: float\n</code></pre> <code>duration</code> <code>instance-attribute</code> \u00b6 <code>language</code> <code>instance-attribute</code> \u00b6 <code>text</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.transcription.whisper_service.WhisperConfig","title":"<code>WhisperConfig</code>  <code>dataclass</code>","text":"<p>Configuration for the Whisper transcription service.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>@dataclass\nclass WhisperConfig:\n    \"\"\"Configuration for the Whisper transcription service.\"\"\"\n    model: str = \"whisper-1\"\n    response_format: str = \"verbose_json\"\n    timestamp_granularities: Optional[List[str]] = field(\n        default_factory=lambda: [\"word\"]\n        )\n    chunking_strategy: str = \"auto\" # currently not usable\n    language: Optional[str] = None # language code\n    temperature: Optional[float] = None\n    prompt: Optional[str] = None\n\n    # Supported response formats\n    SUPPORTED_FORMATS = [\"json\", \"text\", \"srt\", \"vtt\", \"verbose_json\"]\n\n    # Parameters allowed for each format\n    FORMAT_PARAMS = {\n        \"verbose_json\": [\"timestamp_granularities\"],\n        \"json\": [],\n        \"text\": [],\n        \"srt\": [],\n        \"vtt\": []\n    }\n\n    # Basic parameters: always allowed\n    BASE_PARAMS = [\n            \"model\", \"language\", \"temperature\", \"prompt\", \"response_format\",\n        ]\n\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert configuration to dictionary for API call.\"\"\"\n        # Filter out None values to avoid sending unnecessary parameters\n        return {k: v for k, v in self.__dict__.items() \n                if v is not None and not k.startswith(\"_\") and k != \"SUPPORTED_FORMATS\"}\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate configuration values.\"\"\"\n        if self.response_format not in self.SUPPORTED_FORMATS:\n            logger.warning(\n                f\"Unsupported response format: {self.response_format}, \"\n                f\"defaulting to 'verbose_json'\"\n            )\n            self.response_format = \"verbose_json\"\n</code></pre> <code>BASE_PARAMS = ['model', 'language', 'temperature', 'prompt', 'response_format']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>FORMAT_PARAMS = {'verbose_json': ['timestamp_granularities'], 'json': [], 'text': [], 'srt': [], 'vtt': []}</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>SUPPORTED_FORMATS = ['json', 'text', 'srt', 'vtt', 'verbose_json']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>chunking_strategy = 'auto'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>language = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model = 'whisper-1'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>prompt = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>response_format = 'verbose_json'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>temperature = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>timestamp_granularities = field(default_factory=(lambda: ['word']))</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>__init__(model='whisper-1', response_format='verbose_json', timestamp_granularities=(lambda: ['word'])(), chunking_strategy='auto', language=None, temperature=None, prompt=None)</code> \u00b6 <code>to_dict()</code> \u00b6 <p>Convert configuration to dictionary for API call.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert configuration to dictionary for API call.\"\"\"\n    # Filter out None values to avoid sending unnecessary parameters\n    return {k: v for k, v in self.__dict__.items() \n            if v is not None and not k.startswith(\"_\") and k != \"SUPPORTED_FORMATS\"}\n</code></pre> <code>validate()</code> \u00b6 <p>Validate configuration values.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate configuration values.\"\"\"\n    if self.response_format not in self.SUPPORTED_FORMATS:\n        logger.warning(\n            f\"Unsupported response format: {self.response_format}, \"\n            f\"defaulting to 'verbose_json'\"\n        )\n        self.response_format = \"verbose_json\"\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.whisper_service.WhisperResponse","title":"<code>WhisperResponse</code>","text":"<p>               Bases: <code>WhisperBase</code></p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>class WhisperResponse(WhisperBase, total=False):\n    words: Optional[List[WordEntry]]\n    segments: Optional[List[WhisperSegment]]\n</code></pre> <code>segments</code> <code>instance-attribute</code> \u00b6 <code>words</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.transcription.whisper_service.WhisperSegment","title":"<code>WhisperSegment</code>","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>class WhisperSegment(TypedDict, total=False):\n    id: int\n    start: float\n    end: float\n    text: str\n    temperature: float\n    avg_logprob: float\n    compression_ratio: float\n    no_speech_prob: float\n</code></pre> <code>avg_logprob</code> <code>instance-attribute</code> \u00b6 <code>compression_ratio</code> <code>instance-attribute</code> \u00b6 <code>end</code> <code>instance-attribute</code> \u00b6 <code>id</code> <code>instance-attribute</code> \u00b6 <code>no_speech_prob</code> <code>instance-attribute</code> \u00b6 <code>start</code> <code>instance-attribute</code> \u00b6 <code>temperature</code> <code>instance-attribute</code> \u00b6 <code>text</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.transcription.whisper_service.WhisperTranscriptionService","title":"<code>WhisperTranscriptionService</code>","text":"<p>               Bases: <code>TranscriptionService</code></p> <p>OpenAI Whisper implementation of the TranscriptionService interface.</p> <p>Provides transcription services using the OpenAI Whisper API.</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>class WhisperTranscriptionService(TranscriptionService):\n    \"\"\"\n    OpenAI Whisper implementation of the TranscriptionService interface.\n\n    Provides transcription services using the OpenAI Whisper API.\n    \"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, **config_options: Any):\n        \"\"\"\n        Initialize the Whisper transcription service.\n\n        Args:\n            api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)\n            **config_options: Additional configuration options\n        \"\"\"\n        # Create configuration base\n        self.config = WhisperConfig()\n\n        # Set any configuration options provided\n        for key, value in config_options.items():\n            if hasattr(self.config, key):\n                setattr(self.config, key, value)\n\n        # Validate configuration\n        self.config.validate()\n\n        # Initialize format converter\n        self.format_converter = FormatConverter()\n\n        # Set API key\n        self.set_api_key(api_key)\n\n    def _create_jsonl_writer(self):\n        \"\"\"\n        Create a file-like object that captures JSONL output.\n\n        Returns:\n            A file-like object that captures writes\n        \"\"\"\n        class JsonlCapture:\n            def __init__(self):\n                self.data = []\n\n            def write(self, content):\n                try:\n                    # Try to parse as JSON\n                    json_obj = json.loads(content)\n                    self.data.append(json_obj)\n                except json.JSONDecodeError:\n                    # If not valid JSON, just append as string\n                    self.data.append(content)\n\n            def flush(self):\n                pass\n\n            def close(self):\n                pass\n\n        return JsonlCapture()\n\n    def _prepare_file_object(\n        self,\n        audio_file: Union[Path, BytesIO],\n        options: Optional[Dict[str, Any]] = None\n    ) -&gt; tuple[BinaryIO, bool]:\n        \"\"\"\n        Prepare file object for API call. PATCH: file-like objects require 'file_extension' in options.\n\n        Args:\n            audio_file: Path to audio file or file-like object\n            options: Dict containing 'file_extension' if audio_file is file-like\n\n        Returns:\n            Tuple of (file_object, should_close_file)\n\n        Raises:\n            ValueError: If file-like object is provided without 'file_extension' in options\n        \"\"\"\n        if isinstance(audio_file, Path):\n            try:\n                file_obj = open(audio_file, \"rb\")\n                should_close = True\n            except (IOError, OSError) as e:\n                raise RuntimeError(f\"Failed to open audio file '{audio_file}': {e}\") from e\n        else:\n            file_extension = options.get(\"file_extension\", None) if options else None\n            if not file_extension:\n                logger.error(f\"No file extension provided in options for file-like object: {audio_file}\")\n                raise ValueError(\n                    \"For file-like objects, options['file_extension'] \"\n                    \"must be provided. (PATCH for OpenAI API which requires).\"\n                )\n            file_obj = patch_file_with_name(audio_file, file_extension)\n            should_close = False\n\n        return file_obj, should_close\n\n    def _prepare_api_params(\n        self, options: Optional[Dict[str, Any]] = None\n        ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Prepare parameters for the Whisper API call.\n\n        Args:\n            options: Additional options for this specific transcription\n\n        Returns:\n            Dictionary of parameters for the API call\n        \"\"\"\n        base_params = self.config.to_dict()\n\n        # Defensive: ensure options is a dict\n        options = options or {}\n\n        # Determine which response format we're using\n        response_format = options.get(\n            \"response_format\", self.config.response_format\n        )\n\n        # Compute allowed parameters for the chosen format\n        allowed_params = (\n            set(self.config.FORMAT_PARAMS.get(response_format, []))\n            | \n            set(self.config.BASE_PARAMS)\n        )\n\n        # Start with base params, filtered to allowed\n        api_params = {k: v for k, v in base_params.items() if k in allowed_params}\n\n        # Overlay with options, but only allowed keys\n        for k, v in options.items():\n            if k in allowed_params:\n                api_params[k] = v\n\n        # Validate response format\n        if api_params.get(\"response_format\") not in self.config.SUPPORTED_FORMATS:\n            logger.warning(\n                f\"Unsupported response format: {api_params.get('response_format')}, \"\n                f\"defaulting to 'verbose_json'\"\n            )\n            api_params[\"response_format\"] = \"verbose_json\"\n\n        return api_params\n\n    def _to_whisper_response(self, response: Any) -&gt; WhisperResponse:\n        \"\"\"\n        Convert an OpenAI Whisper API response (JSON or Verbose JSON) into a clean,\n        type-safe WhisperResponse structure.\n\n        Args:\n            response: API response (should have .model_dump())\n\n        Returns:\n            A WhisperVerboseJson dictionary\n        \"\"\"\n\n        if hasattr(response, \"model_dump\"):\n            data = response.model_dump(exclude_unset=True)\n        elif hasattr(response, \"to_dict\"):\n            data = response.to_dict()\n        elif isinstance(response, dict):\n            data = response\n        elif isinstance(response, str):\n            data = {\"text\": response} # mimic minimal data response format.\n        else:\n            raise ValueError(f\"OpenAI response does not have a method to extract data \"\n                             f\"(missing 'model_dump' or 'to_dict'): {repr(response)}\")\n\n        # Required field: duration\n        duration = float(data.get(\"duration\", 0.0))\n\n        # Required field: text \n        text = data.get(\"text\")\n        if not isinstance(text, str):\n            raise ValueError(f\"Invalid response: 'text' must be a string, \"\n                             f\"got {type(text)}\")\n\n        # Optional fields with normalization \n        language = data.get(\"language\") or self.config.language or \"unknown\"\n        if not isinstance(language, str):\n            raise ValueError(f\"Unexpected OpenAI response: 'language' is not a string.\"\n                             f\"got {type(language)}\")\n\n        # Optional: words and segments (only present in verbose_json)\n        words = data.get(\"words\")\n        if words is not None and not isinstance(words, list):\n            raise ValueError(f\"Invalid 'words': expected list, got {type(words)}\")\n\n        segments = data.get(\"segments\")\n        if segments is not None and not isinstance(segments, list):\n            raise ValueError(f\"Invalid 'segments': expected list, got {type(segments)}\")\n\n        return WhisperResponse(\n            text=text,\n            language=language,\n            duration=duration,\n            words=words,\n            segments=segments,\n        )\n\n    def set_api_key(self, api_key: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Set or update the API key.\n\n        This method allows refreshing the API key without re-instantiating the class.\n\n        Args:\n            api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)\n\n        Raises:\n            ValueError: If no API key is provided or found in environment\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n\n        if not self.api_key:\n            raise ValueError(\n                \"OpenAI API key is required. Set OPENAI_API_KEY environment \"\n                \"variable or pass as api_key parameter.\"\n            )\n\n        # Configure OpenAI client\n        openai.api_key = self.api_key\n        logger.debug(\"API key updated\")\n\n    def _seconds_to_ms(self, seconds: Optional[float]) -&gt; Optional[int]:\n        \"\"\"\n        Convert seconds to milliseconds.\n\n        Args:\n            seconds: Time in seconds\n\n        Returns:\n            Time in milliseconds or None if seconds is None\n        \"\"\"\n        return None if seconds is None else int(seconds * 1000)\n\n    def _export_response(self, response: WhisperResponse) -&gt; TranscriptionResult:\n        \"\"\"Process and validate WhisperResponse into TranscriptionResult.\"\"\"       \n        return TranscriptionResult(\n            text=response[\"text\"],\n            language=response[\"language\"],\n            word_timing=self._extract_and_validate_words(response),\n            utterance_timing=self._extract_and_validate_utterances(response),\n            confidence=0.0,  # Whisper doesn't provide overall confidence\n            audio_duration_ms=self._seconds_to_ms(response.get(\"duration\")),\n            transcript_id=None,  # No ID from Whisper\n            status=\"completed\",  # You can set a static \"completed\" status\n            raw_result=dict(response),  # Store the original response for debugging\n        )\n\n    def _extract_and_validate_words(\n        self, response: WhisperResponse\n        ) -&gt; TimedText:\n        \"\"\"Extract, validate, and convert word data into WordTiming objects.\"\"\"\n        words_data = response.get(\"words\")\n        units: list[TimedTextUnit] = []\n\n        if words_data:\n            for i, word_entry in enumerate(words_data, start=1):\n                word = word_entry.get(\"word\")\n                start_ms = self._seconds_to_ms(word_entry.get(\"start\"))\n                end_ms = self._seconds_to_ms(word_entry.get(\"end\"))\n\n                if not isinstance(word, str) or not word:\n                    logger.warning(f\"Invalid or missing word: {word_entry}\")\n                    continue\n\n                if not isinstance(start_ms, int) or not isinstance(end_ms, int):\n                    logger.warning(f\"Invalid timestamps for word: {word_entry}\")\n                    continue\n\n                if start_ms &gt; end_ms:\n                    logger.warning(\n                        f\"Invalid timestamps: start ({start_ms}) &gt; end ({end_ms}) \"\n                        f\"for word: {word}. Setting end = start + 1.\"\n                    )\n                    end_ms = start_ms + 1\n\n                if start_ms == end_ms:\n                    # Workaround for OpenAI Whisper API bug:\n                    # Sometimes start == end for word timestamps, which is invalid for downstream consumers.\n                    logger.debug(\n                        f\"Whisper API returned identical start and end times \"\n                        f\"({start_ms} ms) for word '{word}'. \"\n                        \"Adjusting end_ms to start_ms + 1.\"\n                    )\n                    end_ms += 1\n\n                units.append(\n                    TimedTextUnit(\n                        index=i,\n                        text=word,\n                        start_ms=start_ms,\n                        end_ms=end_ms,\n                        speaker=None,\n                        granularity=Granularity.WORD,\n                        confidence=0.0,\n                    )\n                )\n\n        return TimedText(words=units, granularity=Granularity.WORD)\n\n    def _extract_and_validate_utterances(\n        self, response: WhisperResponse\n        ) -&gt; TimedText:\n        \"\"\"Extract and validate utterance segments into Utterance objects.\"\"\"\n        segments = response.get(\"segments\")\n        units: list[TimedTextUnit] = []\n\n        if segments:\n            for i, segment in enumerate(segments, start=1):\n                start_ms = self._seconds_to_ms(segment.get(\"start\"))\n                end_ms = self._seconds_to_ms(segment.get(\"end\"))\n                text = segment.get(\"text\", \"\")\n\n                if not isinstance(start_ms, int) or not isinstance(end_ms, int):\n                    logger.warning(f\"Invalid segment timestamps: {segment}\")\n                    continue\n\n                if not isinstance(text, str) or not text.strip():\n                    logger.warning(f\"Empty or invalid text for segment: {segment}\")\n                    continue\n\n                units.append(\n                    TimedTextUnit(\n                        index=i,\n                        text=text,\n                        start_ms=start_ms,\n                        end_ms=end_ms,\n                        speaker=None,\n                        granularity=Granularity.SEGMENT,\n                        confidence=_logprob_to_confidence(segment.get(\"avg_logprob\", 0.0)),\n                    )\n                )\n\n        return TimedText(segments=units, granularity=Granularity.SEGMENT)\n\n    def transcribe(\n        self,\n        audio_file: Union[Path, BytesIO],\n        options: Optional[Dict[str, Any]] = None,\n    ) -&gt; TranscriptionResult:\n        \"\"\"\n        Transcribe audio file to text using OpenAI Whisper API.\n\n        PATCH: If audio_file is a file-like object, options['file_extension'] must be provided \n        (OpenAI API quirk).\n\n        Args:\n            audio_file: Path to audio file or file-like object\n            options: Provider-specific options for transcription. \n                     If audio_file is file-like, must include 'file_extension'.\n\n        Returns:\n            Dictionary containing transcription results with standardized keys\n\n        Raises:\n            ValueError: If file-like object is provided without 'file_extension' in options\n        \"\"\"\n        # Prepare file object\n        file_obj, should_close = self._prepare_file_object(audio_file, options)\n        try:\n            return self._transcribe_execute(options, file_obj)\n        except Exception as e:\n            logger.error(f\"Error during transcription: {e}\")\n            raise\n        finally:\n            # Clean up file object if we opened it\n            if should_close:\n                file_obj.close()\n\n    def _transcribe_execute(self, options, file_obj):\n        # Prepare API parameters\n        api_params = self._prepare_api_params(options)\n        api_params[\"file\"] = file_obj\n\n        # Call OpenAI API\n        logger.info(f\"Transcribing audio with Whisper API \"\n                    f\"using model: {api_params['model']}\")\n        raw_response = openai.audio.transcriptions.create(**api_params)\n        response = self._to_whisper_response(raw_response)\n\n        result = self._export_response(response)\n\n        logger.info(\"Transcription completed successfully\")\n        return result\n\n    def get_result(self, job_id: str) -&gt; TranscriptionResult:\n        \"\"\"\n        Get results for an existing transcription job.\n\n        Whisper API operates synchronously and doesn't use job IDs,\n        so this method is not implemented.\n\n        Args:\n            job_id: ID of the transcription job\n\n        Returns:\n            Dictionary containing transcription results\n\n        Raises:\n            NotImplementedError: This method is not supported for Whisper\n        \"\"\"\n        raise NotImplementedError(\n            \"Whisper API operates synchronously.\\n\"\n            \"Does not support retrieving results by job ID.\\n\"\n            \"Use the transcribe() method for direct transcription.\"\n        )\n\n    def transcribe_to_format(\n        self,\n        audio_file: Union[Path, BytesIO],\n        format_type: str = \"srt\",\n        transcription_options: Optional[Dict[str, Any]] = None,\n        format_options: Optional[Dict[str, Any]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Transcribe audio and return result in specified format.\n\n        PATCH: If audio_file is a file-like object, transcription_options['file_extension'] must be provided \n        (OpenAI API quirk).\n\n        Takes advantage of the direct subtitle generation functionality when requesting SRT or VTT formats.\n\n        Args:\n            audio_file: Path, file-like object, or URL of audio file\n            format_type: Format type (e.g., \"srt\", \"vtt\", \"text\")\n            transcription_options: Options for transcription. If audio_file is file-like, must include \n                                   'file_extension'.\n            format_options: Format-specific options\n\n        Returns:\n            String representation in the requested format\n\n        Raises:\n            ValueError: If file-like object is provided without 'file_extension' in transcription_options\n        \"\"\"\n        format_type = format_type.lower()\n\n        # If requesting SRT or VTT directly, use native OpenAI capabilities\n        if format_type in {\"srt\", \"vtt\"}:\n            # Create options with format set to SRT or VTT\n            options = transcription_options.copy() if transcription_options else {}\n            options[\"response_format\"] = format_type\n\n            # Prepare file object\n            file_obj, should_close = self._prepare_file_object(audio_file, options)\n\n            try:\n                # Prepare API parameters\n                api_params = self._prepare_api_params(options)\n                api_params[\"file\"] = file_obj\n\n                # Call OpenAI API\n                logger.info(f\"Transcribing directly to {format_type} with Whisper API\")\n                return openai.audio.transcriptions.create(**api_params)\n            finally:\n                # Clean up file object if we opened it\n                if should_close:\n                    file_obj.close()\n\n        # For other formats, use the format converter\n        # First get a normal transcription result\n        result = self.transcribe(audio_file, transcription_options)\n\n        # Then convert to the requested format\n        return self.format_converter.convert(\n            result, format_type, format_options or {}\n        )\n</code></pre> <code>config = WhisperConfig()</code> <code>instance-attribute</code> \u00b6 <code>format_converter = FormatConverter()</code> <code>instance-attribute</code> \u00b6 <code>__init__(api_key=None, **config_options)</code> \u00b6 <p>Initialize the Whisper transcription service.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>OpenAI API key (defaults to OPENAI_API_KEY env var)</p> <code>None</code> <code>**config_options</code> <code>Any</code> <p>Additional configuration options</p> <code>{}</code> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>def __init__(self, api_key: Optional[str] = None, **config_options: Any):\n    \"\"\"\n    Initialize the Whisper transcription service.\n\n    Args:\n        api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)\n        **config_options: Additional configuration options\n    \"\"\"\n    # Create configuration base\n    self.config = WhisperConfig()\n\n    # Set any configuration options provided\n    for key, value in config_options.items():\n        if hasattr(self.config, key):\n            setattr(self.config, key, value)\n\n    # Validate configuration\n    self.config.validate()\n\n    # Initialize format converter\n    self.format_converter = FormatConverter()\n\n    # Set API key\n    self.set_api_key(api_key)\n</code></pre> <code>get_result(job_id)</code> \u00b6 <p>Get results for an existing transcription job.</p> <p>Whisper API operates synchronously and doesn't use job IDs, so this method is not implemented.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>ID of the transcription job</p> required <p>Returns:</p> Type Description <code>TranscriptionResult</code> <p>Dictionary containing transcription results</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not supported for Whisper</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>def get_result(self, job_id: str) -&gt; TranscriptionResult:\n    \"\"\"\n    Get results for an existing transcription job.\n\n    Whisper API operates synchronously and doesn't use job IDs,\n    so this method is not implemented.\n\n    Args:\n        job_id: ID of the transcription job\n\n    Returns:\n        Dictionary containing transcription results\n\n    Raises:\n        NotImplementedError: This method is not supported for Whisper\n    \"\"\"\n    raise NotImplementedError(\n        \"Whisper API operates synchronously.\\n\"\n        \"Does not support retrieving results by job ID.\\n\"\n        \"Use the transcribe() method for direct transcription.\"\n    )\n</code></pre> <code>set_api_key(api_key=None)</code> \u00b6 <p>Set or update the API key.</p> <p>This method allows refreshing the API key without re-instantiating the class.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>OpenAI API key (defaults to OPENAI_API_KEY env var)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no API key is provided or found in environment</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>def set_api_key(self, api_key: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Set or update the API key.\n\n    This method allows refreshing the API key without re-instantiating the class.\n\n    Args:\n        api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)\n\n    Raises:\n        ValueError: If no API key is provided or found in environment\n    \"\"\"\n    self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n\n    if not self.api_key:\n        raise ValueError(\n            \"OpenAI API key is required. Set OPENAI_API_KEY environment \"\n            \"variable or pass as api_key parameter.\"\n        )\n\n    # Configure OpenAI client\n    openai.api_key = self.api_key\n    logger.debug(\"API key updated\")\n</code></pre> <code>transcribe(audio_file, options=None)</code> \u00b6 <p>Transcribe audio file to text using OpenAI Whisper API.</p> <p>PATCH: If audio_file is a file-like object, options['file_extension'] must be provided  (OpenAI API quirk).</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Union[Path, BytesIO]</code> <p>Path to audio file or file-like object</p> required <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Provider-specific options for transcription.       If audio_file is file-like, must include 'file_extension'.</p> <code>None</code> <p>Returns:</p> Type Description <code>TranscriptionResult</code> <p>Dictionary containing transcription results with standardized keys</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file-like object is provided without 'file_extension' in options</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>def transcribe(\n    self,\n    audio_file: Union[Path, BytesIO],\n    options: Optional[Dict[str, Any]] = None,\n) -&gt; TranscriptionResult:\n    \"\"\"\n    Transcribe audio file to text using OpenAI Whisper API.\n\n    PATCH: If audio_file is a file-like object, options['file_extension'] must be provided \n    (OpenAI API quirk).\n\n    Args:\n        audio_file: Path to audio file or file-like object\n        options: Provider-specific options for transcription. \n                 If audio_file is file-like, must include 'file_extension'.\n\n    Returns:\n        Dictionary containing transcription results with standardized keys\n\n    Raises:\n        ValueError: If file-like object is provided without 'file_extension' in options\n    \"\"\"\n    # Prepare file object\n    file_obj, should_close = self._prepare_file_object(audio_file, options)\n    try:\n        return self._transcribe_execute(options, file_obj)\n    except Exception as e:\n        logger.error(f\"Error during transcription: {e}\")\n        raise\n    finally:\n        # Clean up file object if we opened it\n        if should_close:\n            file_obj.close()\n</code></pre> <code>transcribe_to_format(audio_file, format_type='srt', transcription_options=None, format_options=None)</code> \u00b6 <p>Transcribe audio and return result in specified format.</p> <p>PATCH: If audio_file is a file-like object, transcription_options['file_extension'] must be provided  (OpenAI API quirk).</p> <p>Takes advantage of the direct subtitle generation functionality when requesting SRT or VTT formats.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Union[Path, BytesIO]</code> <p>Path, file-like object, or URL of audio file</p> required <code>format_type</code> <code>str</code> <p>Format type (e.g., \"srt\", \"vtt\", \"text\")</p> <code>'srt'</code> <code>transcription_options</code> <code>Optional[Dict[str, Any]]</code> <p>Options for transcription. If audio_file is file-like, must include                     'file_extension'.</p> <code>None</code> <code>format_options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation in the requested format</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file-like object is provided without 'file_extension' in transcription_options</p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>def transcribe_to_format(\n    self,\n    audio_file: Union[Path, BytesIO],\n    format_type: str = \"srt\",\n    transcription_options: Optional[Dict[str, Any]] = None,\n    format_options: Optional[Dict[str, Any]] = None,\n) -&gt; str:\n    \"\"\"\n    Transcribe audio and return result in specified format.\n\n    PATCH: If audio_file is a file-like object, transcription_options['file_extension'] must be provided \n    (OpenAI API quirk).\n\n    Takes advantage of the direct subtitle generation functionality when requesting SRT or VTT formats.\n\n    Args:\n        audio_file: Path, file-like object, or URL of audio file\n        format_type: Format type (e.g., \"srt\", \"vtt\", \"text\")\n        transcription_options: Options for transcription. If audio_file is file-like, must include \n                               'file_extension'.\n        format_options: Format-specific options\n\n    Returns:\n        String representation in the requested format\n\n    Raises:\n        ValueError: If file-like object is provided without 'file_extension' in transcription_options\n    \"\"\"\n    format_type = format_type.lower()\n\n    # If requesting SRT or VTT directly, use native OpenAI capabilities\n    if format_type in {\"srt\", \"vtt\"}:\n        # Create options with format set to SRT or VTT\n        options = transcription_options.copy() if transcription_options else {}\n        options[\"response_format\"] = format_type\n\n        # Prepare file object\n        file_obj, should_close = self._prepare_file_object(audio_file, options)\n\n        try:\n            # Prepare API parameters\n            api_params = self._prepare_api_params(options)\n            api_params[\"file\"] = file_obj\n\n            # Call OpenAI API\n            logger.info(f\"Transcribing directly to {format_type} with Whisper API\")\n            return openai.audio.transcriptions.create(**api_params)\n        finally:\n            # Clean up file object if we opened it\n            if should_close:\n                file_obj.close()\n\n    # For other formats, use the format converter\n    # First get a normal transcription result\n    result = self.transcribe(audio_file, transcription_options)\n\n    # Then convert to the requested format\n    return self.format_converter.convert(\n        result, format_type, format_options or {}\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.transcription.whisper_service.WordEntry","title":"<code>WordEntry</code>","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/tnh_scholar/audio_processing/transcription/whisper_service.py</code> <pre><code>class WordEntry(TypedDict, total=False):\n    word: str\n    start: Optional[float]\n    end: Optional[float]\n</code></pre> <code>end</code> <code>instance-attribute</code> \u00b6 <code>start</code> <code>instance-attribute</code> \u00b6 <code>word</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.utils","title":"<code>utils</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.utils.__all__","title":"<code>__all__ = ['AudioEnhancer', 'get_segment_audio', 'play_audio_segment', 'play_bytes', 'play_from_file', 'play_diarization_segment', 'get_audio_from_file']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.utils.AudioEnhancer","title":"<code>AudioEnhancer</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>class AudioEnhancer:\n    def __init__(\n        self, \n        config: EnhancementConfig = EnhancementConfig(), \n        compression_settings: CompressionSettings = CompressionSettings()\n        ):\n        \"\"\"Initialize with enhancement configuration and compression settings.\"\"\"\n\n        # Check required tools\n        for tool in [\"sox\", \"ffmpeg\"]:\n            try:\n                subprocess.run([\"which\", tool], capture_output=True, text=True, check=True)\n            except (subprocess.SubprocessError, FileNotFoundError) as e:\n                raise RuntimeError(f\"{tool} is not installed. Please install it first.\") from e\n\n        self.config = config\n        self.compression_settings = compression_settings\n\n    def enhance(self, input_path: Path, output_path: Optional[Path] = None) -&gt; Path:\n        \"\"\"\n        Apply enhancement routines (compression, EQ, gating, etc.) in a modular fashion.\n        Converts input to FLAC working format for Whisper compatibility.\n        \"\"\"\n        input_path = Path(input_path)\n        if output_path is None:\n            output_path = input_path.parent / f\"{input_path.stem}_enhanced.flac\"\n\n        # Step 1: Convert to FLAC if needed\n        working_flac = input_path.with_suffix(\".flac\")\n        if not working_flac.exists():\n            self._convert_to_flac(input_path, working_flac)\n\n        # Step 2: Build SoX command modularly using helper methods\n        sox_cmd = [\"sox\", str(working_flac), str(output_path)]\n        sox_cmd.extend(self._set_remix())\n        sox_cmd.extend(self._set_rate())\n        sox_cmd.extend(self._set_gain())\n        sox_cmd.extend(self._set_freq())\n        sox_cmd.extend(self._set_eq())\n        sox_cmd.extend(self._set_compand())\n        sox_cmd.extend(self._set_gate())\n        sox_cmd.extend(self._set_contrast_bass_treble())\n        sox_cmd.extend(self._set_norm())\n\n        result = subprocess.run(sox_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            logger.info(f\"SoX Error: {result.stderr}\")\n            raise RuntimeError(f\"SoX processing failed: {result.stderr}\")\n        return output_path\n\n    def _set_remix(self) -&gt; list[str]:\n        \"\"\"Set remix channels if force_mono is enabled.\"\"\"\n        if self.config.force_mono:\n            return [\"remix\", self.config.remix.remix_channels]\n        return []\n\n    def _set_rate(self) -&gt; list[str]:\n        \"\"\"Set sample rate if target_rate is specified.\"\"\"\n        if self.config.target_rate:\n            return [\"rate\", *self.config.rate.rate_args, str(self.config.target_rate)]\n        return []\n\n    def _set_gain(self) -&gt; list[str]:\n        \"\"\"Set gain normalization.\"\"\"\n        return [\"gain\", \"-n\", str(self.config.norm.norm_level)]\n\n    def _set_freq(self) -&gt; list[str]:\n        \"\"\"Set highpass and lowpass frequencies.\"\"\"\n        return [\n            \"highpass\", \"-1\", str(self.config.eq.highpass_freq),\n            \"lowpass\", \"-1\", str(self.config.eq.lowpass_freq)\n        ]\n\n    def _set_eq(self) -&gt; list[str]:\n        \"\"\"Set equalizer bands.\"\"\"\n        eq_cmd = []\n        for freq, width, gain in self.config.eq.eq_bands:\n            eq_cmd.extend([\"equalizer\", str(freq), str(width), str(gain)])\n        return eq_cmd\n\n    def _set_compand(self) -&gt; list[str]:\n        \"\"\"Set compression arguments.\"\"\"\n        comp_args: list[str] = getattr(\n            self.compression_settings,\n            self.config.compression_level,\n            self.compression_settings.whisper_optimized\n        )\n        return [\"compand\", *comp_args, \":\"]\n\n    def _set_gate(self) -&gt; list[str]:\n        \"\"\"Set gate if enabled.\"\"\"\n        if self.config.include_gate:\n            return [\"gate\", *self.config.gate.gate_params]\n        return []\n\n    def _set_contrast_bass_treble(self) -&gt; list[str]:\n        \"\"\"Set contrast, bass, and treble if EQ is enabled.\"\"\"\n        if self.config.include_eq:\n            return [\n                \"contrast\", str(self.config.eq.contrast),\n                \"bass\", str(self.config.eq.bass[0]), str(self.config.eq.bass[1]),\n                \"treble\", f\"+{self.config.eq.treble[0]}\", str(self.config.eq.treble[1])\n            ]\n        return []\n\n    def _set_norm(self) -&gt; list[str]:\n        \"\"\"Set normalization.\"\"\"\n        return [\"norm\", str(self.config.norm.norm_level)]\n\n    def _convert_to_flac(self, input_path: Path, output_path: Path) -&gt; None:\n        \"\"\"\n        Convert input audio to FLAC format using ffmpeg, preserving maximal fidelity.\n        \"\"\"\n        cmd = [\n            \"ffmpeg\", \"-i\", str(input_path),\n            \"-map\", \"0:a:0\",\n            \"-c:a\", \"flac\",\n            \"-compression_level\", \"8\",\n            str(output_path),\n            \"-y\"\n        ]\n        result = subprocess.run(cmd, check=True, capture_output=True)\n        if result.returncode != 0:\n            print(f\"FFmpeg Error: {result.stderr.decode()}\")\n            raise RuntimeError(f\"FFmpeg conversion failed: {result.stderr.decode()}\")\n\n    def extract_sample(\n        self,\n        input_path: Path,\n        start: float,\n        duration: float,\n        output_path: Optional[Path] = None,\n        output_format: str = \"flac\",\n        codec: Optional[str] = None,\n        compression_level: int = 8,\n    ) -&gt; Path:\n        \"\"\"\n        Extract a sample segment from the audio file.\n\n        Parameters\n        ----------\n        input_path : Path\n            Path to the input audio file.\n        start : float\n            Start time in seconds.\n        duration : float\n            Duration in seconds.\n        output_path : Path, optional\n            Output file path. If None, auto-generated from input.\n        output_format : str, default=\"flac\"\n            Output audio format/extension.\n        codec : str, optional\n            Audio codec to use (default: \"flac\" if output_format is \"flac\", else None).\n        compression_level : int, default=8\n            Compression level for supported codecs.\n\n        Returns\n        -------\n        Path\n            Path to the extracted audio sample.\n        \"\"\"\n        input_path = Path(input_path)\n        output_path = self._sample_output_path(input_path, output_path, start, duration, output_format)\n\n        if codec is None:\n            codec = \"flac\" if output_format == \"flac\" else None\n\n        cmd = [\n            \"ffmpeg\", \"-y\",\n            \"-ss\", str(start),\n            \"-t\", str(duration),\n            \"-i\", str(input_path),\n        ]\n        if codec:\n            cmd += [\"-c:a\", codec]\n        if codec == \"flac\":\n            cmd += [\"-compression_level\", str(compression_level)]\n        cmd.append(str(output_path))\n\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            logger.error(f\"FFmpeg sample extraction failed: {result.stderr}\")\n            raise RuntimeError(f\"Sample extraction failed: {result.stderr}\")\n        return output_path\n\n    def _sample_output_path(self, input_path, output_path, start, duration, output_format) -&gt; Path:\n        if output_path is None:\n            return ( \n                    input_path.parent / \n                    f\"{input_path.stem}_sample_{int(start)}s_{int(duration)}s.{output_format}\"\n            )\n        return Path(output_path)\n\n    def play_audio(self, file_path: Path):\n        \"\"\"Play audio in notebook for quality assessment.\"\"\"\n        display(Audio(str(file_path)))\n\n    def get_audio_info(self, file_path: Path):\n        \"\"\"Get detailed audio information using ffprobe.\"\"\"\n        cmd = [\n            \"ffprobe\", \"-v\", \"quiet\", \"-print_format\", \"json\",\n            \"-show_streams\", \"-select_streams\", \"a:0\", str(file_path)\n        ]\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode == 0:\n            return self._display_stream_info(result, file_path)\n        logger.error(f\"FFprobe error: {result.stderr}\")\n        raise RuntimeError(\"Failed to retrieve audio info.\")\n\n    def _display_stream_info(self, result: subprocess.CompletedProcess, file_path: Path) -&gt; dict:\n        data = json.loads(result.stdout)\n        stream = data[\"streams\"][0]\n\n        logger.info(f\"File: {file_path}\")\n        logger.info(f\"Codec: {stream.get('codec_name', 'Unknown')}\")\n        logger.info(f\"Sample Rate: {stream.get('sample_rate', 'Unknown')} Hz\")\n        logger.info(f\"Channels: {stream.get('channels', 'Unknown')}\")\n        logger.info(f\"Bit Rate: {stream.get('bit_rate', 'Unknown')} bps\")\n        logger.info(f\"Duration: {stream.get('duration', 'Unknown')} seconds\")\n        logger.info(f\"Sample Format: {stream.get('sample_fmt', 'Unknown')}\")\n\n        return stream\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.AudioEnhancer.compression_settings","title":"<code>compression_settings = compression_settings</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.utils.AudioEnhancer.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.utils.AudioEnhancer.__init__","title":"<code>__init__(config=EnhancementConfig(), compression_settings=CompressionSettings())</code>","text":"<p>Initialize with enhancement configuration and compression settings.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def __init__(\n    self, \n    config: EnhancementConfig = EnhancementConfig(), \n    compression_settings: CompressionSettings = CompressionSettings()\n    ):\n    \"\"\"Initialize with enhancement configuration and compression settings.\"\"\"\n\n    # Check required tools\n    for tool in [\"sox\", \"ffmpeg\"]:\n        try:\n            subprocess.run([\"which\", tool], capture_output=True, text=True, check=True)\n        except (subprocess.SubprocessError, FileNotFoundError) as e:\n            raise RuntimeError(f\"{tool} is not installed. Please install it first.\") from e\n\n    self.config = config\n    self.compression_settings = compression_settings\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.AudioEnhancer.enhance","title":"<code>enhance(input_path, output_path=None)</code>","text":"<p>Apply enhancement routines (compression, EQ, gating, etc.) in a modular fashion. Converts input to FLAC working format for Whisper compatibility.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def enhance(self, input_path: Path, output_path: Optional[Path] = None) -&gt; Path:\n    \"\"\"\n    Apply enhancement routines (compression, EQ, gating, etc.) in a modular fashion.\n    Converts input to FLAC working format for Whisper compatibility.\n    \"\"\"\n    input_path = Path(input_path)\n    if output_path is None:\n        output_path = input_path.parent / f\"{input_path.stem}_enhanced.flac\"\n\n    # Step 1: Convert to FLAC if needed\n    working_flac = input_path.with_suffix(\".flac\")\n    if not working_flac.exists():\n        self._convert_to_flac(input_path, working_flac)\n\n    # Step 2: Build SoX command modularly using helper methods\n    sox_cmd = [\"sox\", str(working_flac), str(output_path)]\n    sox_cmd.extend(self._set_remix())\n    sox_cmd.extend(self._set_rate())\n    sox_cmd.extend(self._set_gain())\n    sox_cmd.extend(self._set_freq())\n    sox_cmd.extend(self._set_eq())\n    sox_cmd.extend(self._set_compand())\n    sox_cmd.extend(self._set_gate())\n    sox_cmd.extend(self._set_contrast_bass_treble())\n    sox_cmd.extend(self._set_norm())\n\n    result = subprocess.run(sox_cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        logger.info(f\"SoX Error: {result.stderr}\")\n        raise RuntimeError(f\"SoX processing failed: {result.stderr}\")\n    return output_path\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.AudioEnhancer.extract_sample","title":"<code>extract_sample(input_path, start, duration, output_path=None, output_format='flac', codec=None, compression_level=8)</code>","text":"<p>Extract a sample segment from the audio file.</p>"},{"location":"api/#tnh_scholar.audio_processing.utils.AudioEnhancer.extract_sample--parameters","title":"Parameters","text":"<p>input_path : Path     Path to the input audio file. start : float     Start time in seconds. duration : float     Duration in seconds. output_path : Path, optional     Output file path. If None, auto-generated from input. output_format : str, default=\"flac\"     Output audio format/extension. codec : str, optional     Audio codec to use (default: \"flac\" if output_format is \"flac\", else None). compression_level : int, default=8     Compression level for supported codecs.</p>"},{"location":"api/#tnh_scholar.audio_processing.utils.AudioEnhancer.extract_sample--returns","title":"Returns","text":"<p>Path     Path to the extracted audio sample.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def extract_sample(\n    self,\n    input_path: Path,\n    start: float,\n    duration: float,\n    output_path: Optional[Path] = None,\n    output_format: str = \"flac\",\n    codec: Optional[str] = None,\n    compression_level: int = 8,\n) -&gt; Path:\n    \"\"\"\n    Extract a sample segment from the audio file.\n\n    Parameters\n    ----------\n    input_path : Path\n        Path to the input audio file.\n    start : float\n        Start time in seconds.\n    duration : float\n        Duration in seconds.\n    output_path : Path, optional\n        Output file path. If None, auto-generated from input.\n    output_format : str, default=\"flac\"\n        Output audio format/extension.\n    codec : str, optional\n        Audio codec to use (default: \"flac\" if output_format is \"flac\", else None).\n    compression_level : int, default=8\n        Compression level for supported codecs.\n\n    Returns\n    -------\n    Path\n        Path to the extracted audio sample.\n    \"\"\"\n    input_path = Path(input_path)\n    output_path = self._sample_output_path(input_path, output_path, start, duration, output_format)\n\n    if codec is None:\n        codec = \"flac\" if output_format == \"flac\" else None\n\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-ss\", str(start),\n        \"-t\", str(duration),\n        \"-i\", str(input_path),\n    ]\n    if codec:\n        cmd += [\"-c:a\", codec]\n    if codec == \"flac\":\n        cmd += [\"-compression_level\", str(compression_level)]\n    cmd.append(str(output_path))\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        logger.error(f\"FFmpeg sample extraction failed: {result.stderr}\")\n        raise RuntimeError(f\"Sample extraction failed: {result.stderr}\")\n    return output_path\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.AudioEnhancer.get_audio_info","title":"<code>get_audio_info(file_path)</code>","text":"<p>Get detailed audio information using ffprobe.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def get_audio_info(self, file_path: Path):\n    \"\"\"Get detailed audio information using ffprobe.\"\"\"\n    cmd = [\n        \"ffprobe\", \"-v\", \"quiet\", \"-print_format\", \"json\",\n        \"-show_streams\", \"-select_streams\", \"a:0\", str(file_path)\n    ]\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode == 0:\n        return self._display_stream_info(result, file_path)\n    logger.error(f\"FFprobe error: {result.stderr}\")\n    raise RuntimeError(\"Failed to retrieve audio info.\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.AudioEnhancer.play_audio","title":"<code>play_audio(file_path)</code>","text":"<p>Play audio in notebook for quality assessment.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def play_audio(self, file_path: Path):\n    \"\"\"Play audio in notebook for quality assessment.\"\"\"\n    display(Audio(str(file_path)))\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.get_audio_from_file","title":"<code>get_audio_from_file(audio_file)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def get_audio_from_file(audio_file: Path) -&gt; AudioSegment:\n    audio_format_ext = audio_file.suffix.lstrip(\".\").lower()\n    return AudioSegment.from_file(audio_file, format=audio_format_ext)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.get_segment_audio","title":"<code>get_segment_audio(segment, audio)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def get_segment_audio(segment: DiarizedSegment, audio: AudioSegment):\n    return audio[segment.start:segment.end]\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.play_audio_segment","title":"<code>play_audio_segment(audio)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def play_audio_segment(audio: AudioSegment):\n    play(audio)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.play_bytes","title":"<code>play_bytes(data, format='wav')</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def play_bytes(data: BytesIO, format: str = \"wav\"):\n    audio = AudioSegment.from_file(data, format=format)\n    play(audio)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.play_diarization_segment","title":"<code>play_diarization_segment(segment, audio)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def play_diarization_segment(segment: DiarizedSegment, audio: AudioSegment):\n    play_audio_segment(audio[segment.start:segment.end]) \n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.play_from_file","title":"<code>play_from_file(path)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def play_from_file(path: Path):\n    audio = AudioSegment.from_file(path)\n    play(audio)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance","title":"<code>audio_enhance</code>","text":"<p>Module review and recommendations:</p> <p>Big Picture Approach:</p> <p>Modular, Configurable, and Extensible: Your use of Pydantic models for settings and configs is excellent.  It makes the pipeline flexible and easy to tune for different ASR or enhancement needs. Tooling: Leveraging SoX and FFmpeg is a pragmatic choice for robust, high-quality audio processing. Pipeline Structure: The AudioEnhancer class is well-structured,  with clear separation of concerns for each processing step (remix, rate, gain, EQ, compand, etc.). Notebook Integration: The play_audio method and use of IPython display is great for interactive,  iterative work.</p> <p>Details &amp; Points You Might Be Missing:</p> <p>Error Handling &amp; Logging:</p> <p>You print errors but could benefit from more structured logging (e.g., using Python\u2019s logging module). Consider more granular exception handling, especially for subprocess calls. Testing &amp; Validation:</p> <p>No unit tests or validation of output audio quality/format are present. Consider adding automated tests  (even if just for file existence, format, and basic properties). You could add a method to compare pre/post enhancement SNR, loudness, or other metrics. Documentation &amp; Examples:</p> <p>While docstrings are good, a usage example (in code or markdown) would help new users. Consider a README or notebook cell that demonstrates a full workflow. Performance:</p> <p>For large-scale or batch processing, consider parallelization or async processing. Temporary files (e.g., intermediate FLACs) could be managed/cleaned up more robustly. Extensibility:</p> <p>The pipeline is modular, but adding a \u201ccustom steps\u201d hook (e.g., user-defined SoX/FFmpeg args)  would make it even more flexible. You might want to support other codecs or output formats for downstream ASR models. Feature Gaps:</p> <p>The extract_sample method is a TODO. Implementing this would be useful for quick QA or dataset creation. Consider adding Voice Activity Detection (VAD) or silence trimming as optional steps. You could add a \u201cdry run\u201d mode to print the SoX/FFmpeg commands without executing, for debugging. ASR-Specific Enhancements:</p> <p>You might want to add preset configs for different ASR models (e.g., Whisper, Wav2Vec2, etc.),  as they may have different optimal preprocessing. Consider integrating with open-source ASR evaluation tools to close the loop on enhancement effectiveness. General Strategic Recommendations:</p> <p>Automate QA: Add methods to check output audio quality, duration, and format, and optionally compare to input. Batch Processing: Add a method to process a directory or list of files. Config Export/Import: Allow saving/loading configs as JSON/YAML for reproducibility. CLI/Script Interface: Consider a command-line interface for use outside notebooks. Unit Tests: Add basic tests for each method, especially for error cases. Summary Table:</p> <p>| Modularity | Good | Add custom step hooks |   | Configurability | Excellent | Presets for more ASR models |   | Error Handling | Basic | Use logging, more granular exceptions |   | Testing | Missing | Add unit tests, output validation |   | Documentation | Good | Add usage examples, README |   | Extensibility | Good | Support more codecs, batch processing |   | ASR Optimization | Good start | Add VAD, silence trim, model-specific configs |</p>"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.AudioEnhancer","title":"<code>AudioEnhancer</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>class AudioEnhancer:\n    def __init__(\n        self, \n        config: EnhancementConfig = EnhancementConfig(), \n        compression_settings: CompressionSettings = CompressionSettings()\n        ):\n        \"\"\"Initialize with enhancement configuration and compression settings.\"\"\"\n\n        # Check required tools\n        for tool in [\"sox\", \"ffmpeg\"]:\n            try:\n                subprocess.run([\"which\", tool], capture_output=True, text=True, check=True)\n            except (subprocess.SubprocessError, FileNotFoundError) as e:\n                raise RuntimeError(f\"{tool} is not installed. Please install it first.\") from e\n\n        self.config = config\n        self.compression_settings = compression_settings\n\n    def enhance(self, input_path: Path, output_path: Optional[Path] = None) -&gt; Path:\n        \"\"\"\n        Apply enhancement routines (compression, EQ, gating, etc.) in a modular fashion.\n        Converts input to FLAC working format for Whisper compatibility.\n        \"\"\"\n        input_path = Path(input_path)\n        if output_path is None:\n            output_path = input_path.parent / f\"{input_path.stem}_enhanced.flac\"\n\n        # Step 1: Convert to FLAC if needed\n        working_flac = input_path.with_suffix(\".flac\")\n        if not working_flac.exists():\n            self._convert_to_flac(input_path, working_flac)\n\n        # Step 2: Build SoX command modularly using helper methods\n        sox_cmd = [\"sox\", str(working_flac), str(output_path)]\n        sox_cmd.extend(self._set_remix())\n        sox_cmd.extend(self._set_rate())\n        sox_cmd.extend(self._set_gain())\n        sox_cmd.extend(self._set_freq())\n        sox_cmd.extend(self._set_eq())\n        sox_cmd.extend(self._set_compand())\n        sox_cmd.extend(self._set_gate())\n        sox_cmd.extend(self._set_contrast_bass_treble())\n        sox_cmd.extend(self._set_norm())\n\n        result = subprocess.run(sox_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            logger.info(f\"SoX Error: {result.stderr}\")\n            raise RuntimeError(f\"SoX processing failed: {result.stderr}\")\n        return output_path\n\n    def _set_remix(self) -&gt; list[str]:\n        \"\"\"Set remix channels if force_mono is enabled.\"\"\"\n        if self.config.force_mono:\n            return [\"remix\", self.config.remix.remix_channels]\n        return []\n\n    def _set_rate(self) -&gt; list[str]:\n        \"\"\"Set sample rate if target_rate is specified.\"\"\"\n        if self.config.target_rate:\n            return [\"rate\", *self.config.rate.rate_args, str(self.config.target_rate)]\n        return []\n\n    def _set_gain(self) -&gt; list[str]:\n        \"\"\"Set gain normalization.\"\"\"\n        return [\"gain\", \"-n\", str(self.config.norm.norm_level)]\n\n    def _set_freq(self) -&gt; list[str]:\n        \"\"\"Set highpass and lowpass frequencies.\"\"\"\n        return [\n            \"highpass\", \"-1\", str(self.config.eq.highpass_freq),\n            \"lowpass\", \"-1\", str(self.config.eq.lowpass_freq)\n        ]\n\n    def _set_eq(self) -&gt; list[str]:\n        \"\"\"Set equalizer bands.\"\"\"\n        eq_cmd = []\n        for freq, width, gain in self.config.eq.eq_bands:\n            eq_cmd.extend([\"equalizer\", str(freq), str(width), str(gain)])\n        return eq_cmd\n\n    def _set_compand(self) -&gt; list[str]:\n        \"\"\"Set compression arguments.\"\"\"\n        comp_args: list[str] = getattr(\n            self.compression_settings,\n            self.config.compression_level,\n            self.compression_settings.whisper_optimized\n        )\n        return [\"compand\", *comp_args, \":\"]\n\n    def _set_gate(self) -&gt; list[str]:\n        \"\"\"Set gate if enabled.\"\"\"\n        if self.config.include_gate:\n            return [\"gate\", *self.config.gate.gate_params]\n        return []\n\n    def _set_contrast_bass_treble(self) -&gt; list[str]:\n        \"\"\"Set contrast, bass, and treble if EQ is enabled.\"\"\"\n        if self.config.include_eq:\n            return [\n                \"contrast\", str(self.config.eq.contrast),\n                \"bass\", str(self.config.eq.bass[0]), str(self.config.eq.bass[1]),\n                \"treble\", f\"+{self.config.eq.treble[0]}\", str(self.config.eq.treble[1])\n            ]\n        return []\n\n    def _set_norm(self) -&gt; list[str]:\n        \"\"\"Set normalization.\"\"\"\n        return [\"norm\", str(self.config.norm.norm_level)]\n\n    def _convert_to_flac(self, input_path: Path, output_path: Path) -&gt; None:\n        \"\"\"\n        Convert input audio to FLAC format using ffmpeg, preserving maximal fidelity.\n        \"\"\"\n        cmd = [\n            \"ffmpeg\", \"-i\", str(input_path),\n            \"-map\", \"0:a:0\",\n            \"-c:a\", \"flac\",\n            \"-compression_level\", \"8\",\n            str(output_path),\n            \"-y\"\n        ]\n        result = subprocess.run(cmd, check=True, capture_output=True)\n        if result.returncode != 0:\n            print(f\"FFmpeg Error: {result.stderr.decode()}\")\n            raise RuntimeError(f\"FFmpeg conversion failed: {result.stderr.decode()}\")\n\n    def extract_sample(\n        self,\n        input_path: Path,\n        start: float,\n        duration: float,\n        output_path: Optional[Path] = None,\n        output_format: str = \"flac\",\n        codec: Optional[str] = None,\n        compression_level: int = 8,\n    ) -&gt; Path:\n        \"\"\"\n        Extract a sample segment from the audio file.\n\n        Parameters\n        ----------\n        input_path : Path\n            Path to the input audio file.\n        start : float\n            Start time in seconds.\n        duration : float\n            Duration in seconds.\n        output_path : Path, optional\n            Output file path. If None, auto-generated from input.\n        output_format : str, default=\"flac\"\n            Output audio format/extension.\n        codec : str, optional\n            Audio codec to use (default: \"flac\" if output_format is \"flac\", else None).\n        compression_level : int, default=8\n            Compression level for supported codecs.\n\n        Returns\n        -------\n        Path\n            Path to the extracted audio sample.\n        \"\"\"\n        input_path = Path(input_path)\n        output_path = self._sample_output_path(input_path, output_path, start, duration, output_format)\n\n        if codec is None:\n            codec = \"flac\" if output_format == \"flac\" else None\n\n        cmd = [\n            \"ffmpeg\", \"-y\",\n            \"-ss\", str(start),\n            \"-t\", str(duration),\n            \"-i\", str(input_path),\n        ]\n        if codec:\n            cmd += [\"-c:a\", codec]\n        if codec == \"flac\":\n            cmd += [\"-compression_level\", str(compression_level)]\n        cmd.append(str(output_path))\n\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            logger.error(f\"FFmpeg sample extraction failed: {result.stderr}\")\n            raise RuntimeError(f\"Sample extraction failed: {result.stderr}\")\n        return output_path\n\n    def _sample_output_path(self, input_path, output_path, start, duration, output_format) -&gt; Path:\n        if output_path is None:\n            return ( \n                    input_path.parent / \n                    f\"{input_path.stem}_sample_{int(start)}s_{int(duration)}s.{output_format}\"\n            )\n        return Path(output_path)\n\n    def play_audio(self, file_path: Path):\n        \"\"\"Play audio in notebook for quality assessment.\"\"\"\n        display(Audio(str(file_path)))\n\n    def get_audio_info(self, file_path: Path):\n        \"\"\"Get detailed audio information using ffprobe.\"\"\"\n        cmd = [\n            \"ffprobe\", \"-v\", \"quiet\", \"-print_format\", \"json\",\n            \"-show_streams\", \"-select_streams\", \"a:0\", str(file_path)\n        ]\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode == 0:\n            return self._display_stream_info(result, file_path)\n        logger.error(f\"FFprobe error: {result.stderr}\")\n        raise RuntimeError(\"Failed to retrieve audio info.\")\n\n    def _display_stream_info(self, result: subprocess.CompletedProcess, file_path: Path) -&gt; dict:\n        data = json.loads(result.stdout)\n        stream = data[\"streams\"][0]\n\n        logger.info(f\"File: {file_path}\")\n        logger.info(f\"Codec: {stream.get('codec_name', 'Unknown')}\")\n        logger.info(f\"Sample Rate: {stream.get('sample_rate', 'Unknown')} Hz\")\n        logger.info(f\"Channels: {stream.get('channels', 'Unknown')}\")\n        logger.info(f\"Bit Rate: {stream.get('bit_rate', 'Unknown')} bps\")\n        logger.info(f\"Duration: {stream.get('duration', 'Unknown')} seconds\")\n        logger.info(f\"Sample Format: {stream.get('sample_fmt', 'Unknown')}\")\n\n        return stream\n</code></pre> <code>compression_settings = compression_settings</code> <code>instance-attribute</code> \u00b6 <code>config = config</code> <code>instance-attribute</code> \u00b6 <code>__init__(config=EnhancementConfig(), compression_settings=CompressionSettings())</code> \u00b6 <p>Initialize with enhancement configuration and compression settings.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def __init__(\n    self, \n    config: EnhancementConfig = EnhancementConfig(), \n    compression_settings: CompressionSettings = CompressionSettings()\n    ):\n    \"\"\"Initialize with enhancement configuration and compression settings.\"\"\"\n\n    # Check required tools\n    for tool in [\"sox\", \"ffmpeg\"]:\n        try:\n            subprocess.run([\"which\", tool], capture_output=True, text=True, check=True)\n        except (subprocess.SubprocessError, FileNotFoundError) as e:\n            raise RuntimeError(f\"{tool} is not installed. Please install it first.\") from e\n\n    self.config = config\n    self.compression_settings = compression_settings\n</code></pre> <code>enhance(input_path, output_path=None)</code> \u00b6 <p>Apply enhancement routines (compression, EQ, gating, etc.) in a modular fashion. Converts input to FLAC working format for Whisper compatibility.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def enhance(self, input_path: Path, output_path: Optional[Path] = None) -&gt; Path:\n    \"\"\"\n    Apply enhancement routines (compression, EQ, gating, etc.) in a modular fashion.\n    Converts input to FLAC working format for Whisper compatibility.\n    \"\"\"\n    input_path = Path(input_path)\n    if output_path is None:\n        output_path = input_path.parent / f\"{input_path.stem}_enhanced.flac\"\n\n    # Step 1: Convert to FLAC if needed\n    working_flac = input_path.with_suffix(\".flac\")\n    if not working_flac.exists():\n        self._convert_to_flac(input_path, working_flac)\n\n    # Step 2: Build SoX command modularly using helper methods\n    sox_cmd = [\"sox\", str(working_flac), str(output_path)]\n    sox_cmd.extend(self._set_remix())\n    sox_cmd.extend(self._set_rate())\n    sox_cmd.extend(self._set_gain())\n    sox_cmd.extend(self._set_freq())\n    sox_cmd.extend(self._set_eq())\n    sox_cmd.extend(self._set_compand())\n    sox_cmd.extend(self._set_gate())\n    sox_cmd.extend(self._set_contrast_bass_treble())\n    sox_cmd.extend(self._set_norm())\n\n    result = subprocess.run(sox_cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        logger.info(f\"SoX Error: {result.stderr}\")\n        raise RuntimeError(f\"SoX processing failed: {result.stderr}\")\n    return output_path\n</code></pre> <code>extract_sample(input_path, start, duration, output_path=None, output_format='flac', codec=None, compression_level=8)</code> \u00b6 <p>Extract a sample segment from the audio file.</p> <code>get_audio_info(file_path)</code> \u00b6 <p>Get detailed audio information using ffprobe.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def get_audio_info(self, file_path: Path):\n    \"\"\"Get detailed audio information using ffprobe.\"\"\"\n    cmd = [\n        \"ffprobe\", \"-v\", \"quiet\", \"-print_format\", \"json\",\n        \"-show_streams\", \"-select_streams\", \"a:0\", str(file_path)\n    ]\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode == 0:\n        return self._display_stream_info(result, file_path)\n    logger.error(f\"FFprobe error: {result.stderr}\")\n    raise RuntimeError(\"Failed to retrieve audio info.\")\n</code></pre> <code>play_audio(file_path)</code> \u00b6 <p>Play audio in notebook for quality assessment.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def play_audio(self, file_path: Path):\n    \"\"\"Play audio in notebook for quality assessment.\"\"\"\n    display(Audio(str(file_path)))\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.AudioEnhancer.extract_sample--parameters","title":"Parameters","text":"<p>input_path : Path     Path to the input audio file. start : float     Start time in seconds. duration : float     Duration in seconds. output_path : Path, optional     Output file path. If None, auto-generated from input. output_format : str, default=\"flac\"     Output audio format/extension. codec : str, optional     Audio codec to use (default: \"flac\" if output_format is \"flac\", else None). compression_level : int, default=8     Compression level for supported codecs.</p>"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.AudioEnhancer.extract_sample--returns","title":"Returns","text":"<p>Path     Path to the extracted audio sample.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def extract_sample(\n    self,\n    input_path: Path,\n    start: float,\n    duration: float,\n    output_path: Optional[Path] = None,\n    output_format: str = \"flac\",\n    codec: Optional[str] = None,\n    compression_level: int = 8,\n) -&gt; Path:\n    \"\"\"\n    Extract a sample segment from the audio file.\n\n    Parameters\n    ----------\n    input_path : Path\n        Path to the input audio file.\n    start : float\n        Start time in seconds.\n    duration : float\n        Duration in seconds.\n    output_path : Path, optional\n        Output file path. If None, auto-generated from input.\n    output_format : str, default=\"flac\"\n        Output audio format/extension.\n    codec : str, optional\n        Audio codec to use (default: \"flac\" if output_format is \"flac\", else None).\n    compression_level : int, default=8\n        Compression level for supported codecs.\n\n    Returns\n    -------\n    Path\n        Path to the extracted audio sample.\n    \"\"\"\n    input_path = Path(input_path)\n    output_path = self._sample_output_path(input_path, output_path, start, duration, output_format)\n\n    if codec is None:\n        codec = \"flac\" if output_format == \"flac\" else None\n\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-ss\", str(start),\n        \"-t\", str(duration),\n        \"-i\", str(input_path),\n    ]\n    if codec:\n        cmd += [\"-c:a\", codec]\n    if codec == \"flac\":\n        cmd += [\"-compression_level\", str(compression_level)]\n    cmd.append(str(output_path))\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        logger.error(f\"FFmpeg sample extraction failed: {result.stderr}\")\n        raise RuntimeError(f\"Sample extraction failed: {result.stderr}\")\n    return output_path\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.CompressionSettings","title":"<code>CompressionSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Compression settings for audio enhancement routines.</p> <p>Attributes:</p> Name Type Description <code>minimal</code> <code>list[str]</code> <p>List of compand arguments for minimal compression.</p> <code>light</code> <code>list[str]</code> <p>List of compand arguments for light compression.</p> <code>moderate</code> <code>list[str]</code> <p>List of compand arguments for moderate compression.</p> <code>aggressive</code> <code>list[str]</code> <p>List of compand arguments for aggressive compression.</p> <code>whisper_optimized</code> <code>list[str]</code> <p>List of compand arguments for Whisper-optimized compression.</p> <code>whisper_aggressive</code> <code>list[str]</code> <p>List of compand arguments for aggressive Whisper compression.</p> <code>primary_speech_only</code> <code>list[str]</code> <p>List of compand arguments for primary speech only.</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>class CompressionSettings(BaseSettings):\n    \"\"\"Compression settings for audio enhancement routines.\n\n    Attributes:\n        minimal: List of compand arguments for minimal compression.\n        light: List of compand arguments for light compression.\n        moderate: List of compand arguments for moderate compression.\n        aggressive: List of compand arguments for aggressive compression.\n        whisper_optimized: List of compand arguments for Whisper-optimized compression.\n        whisper_aggressive: List of compand arguments for aggressive Whisper compression.\n        primary_speech_only: List of compand arguments for primary speech only.\n    \"\"\"\n    minimal: list[str] = [\"0.1,0.3\", \"3:-50,-40,-30,-20\", \"-3\", \"-80\", \"0.2\"]\n    light: list[str] = [\"0.05,0.2\", \"6:-60,-50,-40,-30,-20,-10\", \"-3\", \"-85\", \"0.1\"]\n    moderate: list[str] = [\"0.03,0.15\", \"6:-65,-50,-40,-30,-20,-10\", \"-4\", \"-85\", \"0.1\"]\n    aggressive: list[str] = [\"0.02,0.1\", \"8:-70,-55,-45,-35,-25,-15\", \"-5\", \"-90\", \"0.05\"]\n    whisper_optimized: list[str] = [\"0.005,0.06\", \"12:-75,-65,-55,-45,-35,-25,-15,-8\", \"-8\", \"-95\", \"0.03\"]\n    whisper_aggressive: list[str] = [\"0.005,0.06\", \"12:-75,-45,-55,-30,-35,-18,-15,-8\", \"-8\", \"-95\", \"0.03\"]\n    primary_speech_only: list[str] = [\"0.005,0.06\", \"12:-60,-45,-55,-30,-35,-18,-15,-8\", \"-8\", \"-60\", \"0.03\"]\n</code></pre> <code>aggressive = ['0.02,0.1', '8:-70,-55,-45,-35,-25,-15', '-5', '-90', '0.05']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>light = ['0.05,0.2', '6:-60,-50,-40,-30,-20,-10', '-3', '-85', '0.1']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>minimal = ['0.1,0.3', '3:-50,-40,-30,-20', '-3', '-80', '0.2']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>moderate = ['0.03,0.15', '6:-65,-50,-40,-30,-20,-10', '-4', '-85', '0.1']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>primary_speech_only = ['0.005,0.06', '12:-60,-45,-55,-30,-35,-18,-15,-8', '-8', '-60', '0.03']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>whisper_aggressive = ['0.005,0.06', '12:-75,-45,-55,-30,-35,-18,-15,-8', '-8', '-95', '0.03']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>whisper_optimized = ['0.005,0.06', '12:-75,-65,-55,-45,-35,-25,-15,-8', '-8', '-95', '0.03']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.EQSettings","title":"<code>EQSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>class EQSettings(BaseSettings):\n    highpass_freq: int = 175\n    lowpass_freq: int = 15000\n    eq_bands: list[tuple[int, float, int]] = [\n        (100, 0.9, -20),\n        (1500, 1, 4),\n        (4000, 0.6, 15),\n        (10000, 1, -10)\n    ]\n    contrast: int = 75\n    bass: tuple[int, int] = (-5, 200)\n    treble: tuple[int, int] = (3, 3000)\n</code></pre> <code>bass = (-5, 200)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>contrast = 75</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>eq_bands = [(100, 0.9, -20), (1500, 1, 4), (4000, 0.6, 15), (10000, 1, -10)]</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>highpass_freq = 175</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>lowpass_freq = 15000</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>treble = (3, 3000)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.EnhancementConfig","title":"<code>EnhancementConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>class EnhancementConfig(BaseModel):\n    codec: str = 'flac'\n    sample_rate: int = 48000\n    channels: int = 2\n    compression_level: str = 'aggressive'\n    force_mono: bool = False\n    target_rate: Optional[int] = None\n    eq: EQSettings = EQSettings()\n    gate: GateSettings = GateSettings()\n    norm: NormalizationSettings = NormalizationSettings()\n    remix: RemixSettings = RemixSettings()\n    rate: RateSettings = RateSettings()\n    include_gate: bool = True\n    include_eq: bool = True\n</code></pre> <code>channels = 2</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>codec = 'flac'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>compression_level = 'aggressive'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>eq = EQSettings()</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>force_mono = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>gate = GateSettings()</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>include_eq = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>include_gate = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>norm = NormalizationSettings()</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>rate = RateSettings()</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>remix = RemixSettings()</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>sample_rate = 48000</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>target_rate = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.GateSettings","title":"<code>GateSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>class GateSettings(BaseSettings):\n    gate_params: list[str] = [\"0.1\", \"0.05\", \"-inf\", \"0.1\", \"-90\", \"0.1\"]\n</code></pre> <code>gate_params = ['0.1', '0.05', '-inf', '0.1', '-90', '0.1']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.NormalizationSettings","title":"<code>NormalizationSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>class NormalizationSettings(BaseSettings):\n    norm_level: int = -1\n</code></pre> <code>norm_level = -1</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.RateSettings","title":"<code>RateSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>class RateSettings(BaseSettings):\n    rate_args: list[str] = [\"-v\"]\n</code></pre> <code>rate_args = ['-v']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.RemixSettings","title":"<code>RemixSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>class RemixSettings(BaseSettings):\n    remix_channels: str = \"1,2\"\n</code></pre> <code>remix_channels = '1,2'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.compress_wav_to_mp4_vbr","title":"<code>compress_wav_to_mp4_vbr(input_wav, output_path=None, quality=8)</code>","text":"<p>Compress WAV to M4A (AAC VBR) using ffmpeg.</p>"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.compress_wav_to_mp4_vbr--parameters","title":"Parameters:","text":"<p>input_wav : str or Path     Path to the input .wav file output_path : str or Path, optional     Output .mp4 file path. If None, auto-generated from input quality : int, default=8     VBR quality level: 1 = good (~96kbps), 2 = very good (~128kbps), 3+ = higher bitrate</p>"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.compress_wav_to_mp4_vbr--returns","title":"Returns:","text":"<p>Path     Path to the compressed .m4a file</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def compress_wav_to_mp4_vbr(\n    input_wav: str | Path, output_path: Optional[str | Path] = None, quality: int = 8\n    ) -&gt; Path:\n    \"\"\"\n    Compress WAV to M4A (AAC VBR) using ffmpeg.\n\n    Parameters:\n    -----------\n    input_wav : str or Path\n        Path to the input .wav file\n    output_path : str or Path, optional\n        Output .mp4 file path. If None, auto-generated from input\n    quality : int, default=8\n        VBR quality level: 1 = good (~96kbps), 2 = very good (~128kbps), 3+ = higher bitrate\n\n    Returns:\n    --------\n    Path\n        Path to the compressed .m4a file\n    \"\"\"\n    input_wav = Path(input_wav)\n    if output_path is None:\n        output_path = input_wav.with_suffix(\".mp4\")\n    else:\n        output_path = Path(output_path)\n\n    cmd = [\n        \"ffmpeg\", \"-y\", \"-i\", str(input_wav),\n        \"-c:a\", \"aac\",\n        \"-q:a\", str(quality),\n        str(output_path)\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    if result.returncode != 0:\n        logger.error(\"Error compressing audio:\")\n        logger.error(result.stderr)\n        raise RuntimeError(\"FFmpeg compression failed.\")\n\n    print(f\"Compressed audio saved to: {output_path}\")\n    return output_path\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.audio_enhance.get_sox_info","title":"<code>get_sox_info(file_path)</code>","text":"<p>Get audio info using SoX</p> Source code in <code>src/tnh_scholar/audio_processing/utils/audio_enhance.py</code> <pre><code>def get_sox_info(file_path):\n    \"\"\"Get audio info using SoX\"\"\"\n    result = subprocess.run([\"sox\", \"--info\", str(file_path)], \n                          capture_output=True, text=True)\n    if result.returncode == 0:\n        logger.error(result.stdout)\n    else:\n        logger.error(f\"Error: {result.stderr}\")\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.playback","title":"<code>playback</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.utils.playback.get_audio_from_file","title":"<code>get_audio_from_file(audio_file)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def get_audio_from_file(audio_file: Path) -&gt; AudioSegment:\n    audio_format_ext = audio_file.suffix.lstrip(\".\").lower()\n    return AudioSegment.from_file(audio_file, format=audio_format_ext)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.playback.get_segment_audio","title":"<code>get_segment_audio(segment, audio)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def get_segment_audio(segment: DiarizedSegment, audio: AudioSegment):\n    return audio[segment.start:segment.end]\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.playback.play_audio_segment","title":"<code>play_audio_segment(audio)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def play_audio_segment(audio: AudioSegment):\n    play(audio)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.playback.play_bytes","title":"<code>play_bytes(data, format='wav')</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def play_bytes(data: BytesIO, format: str = \"wav\"):\n    audio = AudioSegment.from_file(data, format=format)\n    play(audio)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.playback.play_diarization_segment","title":"<code>play_diarization_segment(segment, audio)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def play_diarization_segment(segment: DiarizedSegment, audio: AudioSegment):\n    play_audio_segment(audio[segment.start:segment.end]) \n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.utils.playback.play_from_file","title":"<code>play_from_file(path)</code>","text":"Source code in <code>src/tnh_scholar/audio_processing/utils/playback.py</code> <pre><code>def play_from_file(path: Path):\n    audio = AudioSegment.from_file(path)\n    play(audio)\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.whisper_security","title":"<code>whisper_security</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.whisper_security.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.audio_processing.whisper_security.load_whisper_model","title":"<code>load_whisper_model(model_name)</code>","text":"<p>Safely load a Whisper model with security best practices.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the Whisper model to load (e.g., \"tiny\", \"base\", \"small\")</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Loaded Whisper model</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If model loading fails</p> Source code in <code>src/tnh_scholar/audio_processing/whisper_security.py</code> <pre><code>def load_whisper_model(model_name: str) -&gt; Any:\n    \"\"\"\n    Safely load a Whisper model with security best practices.\n\n    Args:\n        model_name: Name of the Whisper model to load (e.g., \"tiny\", \"base\", \"small\")\n\n    Returns:\n        Loaded Whisper model\n\n    Raises:\n        RuntimeError: If model loading fails\n    \"\"\"\n    import whisper\n\n    try:\n        with safe_torch_load():\n            model = whisper.load_model(model_name)\n        return model\n    except Exception as e:\n        logger.error(\"Failed to load Whisper model %r: %s\", model_name, e)\n        raise RuntimeError(f\"Failed to load Whisper model: {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.audio_processing.whisper_security.safe_torch_load","title":"<code>safe_torch_load(weights_only=True)</code>","text":"<p>Context manager that temporarily modifies torch.load  to use weights_only=True by default.</p> <p>This addresses the FutureWarning in PyTorch regarding pickle security: https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models</p> <p>Parameters:</p> Name Type Description Default <code>weights_only</code> <code>bool</code> <p>If True, limits unpickling to tensor data only.</p> <code>True</code> <p>Yields:</p> Type Description <code>None</code> <p>None</p> Example <p>with safe_torch_load(): ...     model = whisper.load_model(\"tiny\")</p> Source code in <code>src/tnh_scholar/audio_processing/whisper_security.py</code> <pre><code>@contextlib.contextmanager\ndef safe_torch_load(weights_only: bool = True) -&gt; Generator[None, None, None]:\n    \"\"\"\n    Context manager that temporarily modifies torch.load \n    to use weights_only=True by default.\n\n    This addresses the FutureWarning in PyTorch regarding pickle security:\n    https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models\n\n    Args:\n        weights_only: If True, limits unpickling to tensor data only.\n\n    Yields:\n        None\n\n    Example:\n        &gt;&gt;&gt; with safe_torch_load():\n        ...     model = whisper.load_model(\"tiny\")\n    \"\"\"\n    original_torch_load = torch.load\n    try:\n        torch.load = partial(original_torch_load, weights_only=weights_only)\n        logger.debug(\"Modified torch.load to use weights_only=%s\", weights_only)\n        yield\n    finally:\n        torch.load = original_torch_load\n        logger.debug(\"Restored original torch.load\")\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools","title":"<code>cli_tools</code>","text":"<p>TNH Scholar CLI Tools</p> <p>Command-line interface tools for the TNH Scholar project:</p> <pre><code>audio-transcribe:\n    Audio processing pipeline that handles downloading, segmentation,\n    and transcription of Buddhist teachings.\n\ntnh-fab:\n    Text processing tool for texts, providing functionality for\n    punctuation, sectioning, translation, and pattern-based processing.\n</code></pre> <p>See individual tool documentation for usage details and examples.</p>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe","title":"<code>audio_transcribe</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe","title":"<code>audio_transcribe</code>","text":"<p>CLI tool for downloading audio (YouTube or local), and transcribing to text.</p> Usage <p>audio-transcribe [OPTIONS]</p> <p>e.g. audio-transcribe         --yt_url https://www.youtube.com/watch?v=EXAMPLE         --output_dir ./processed         --service whisper         --model whisper-1</p>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_CHUNK_DURATION","title":"<code>DEFAULT_CHUNK_DURATION = 120</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_MIN_CHUNK","title":"<code>DEFAULT_MIN_CHUNK = 10</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_MODEL","title":"<code>DEFAULT_MODEL = 'whisper-1'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_OUTPUT_PATH","title":"<code>DEFAULT_OUTPUT_PATH = './audio_transcriptions/transcript.txt'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_RESPONSE_FORMAT","title":"<code>DEFAULT_RESPONSE_FORMAT = 'text'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_SERVICE","title":"<code>DEFAULT_SERVICE = 'whisper'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.DEFAULT_TEMP_DIR","title":"<code>DEFAULT_TEMP_DIR = tempfile.gettempdir()</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.VIDEO_EXTENSIONS","title":"<code>VIDEO_EXTENSIONS = {'.mp4', '.avi', '.mov', '.mkv', '.wmv'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.AudioTranscribeApp","title":"<code>AudioTranscribeApp</code>","text":"<p>Main application class for audio transcription CLI. Organizes configuration, source resolution, and pipeline execution. All runtime options are supplied via a validated <code>AudioTranscribeConfig</code>.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/audio_transcribe.py</code> <pre><code>class AudioTranscribeApp:\n    \"\"\"\n    Main application class for audio transcription CLI.\n    Organizes configuration, source resolution, and pipeline execution. All\n    runtime options are supplied via a validated `AudioTranscribeConfig`.\n    \"\"\"\n    def __init__(self, config: AudioTranscribeConfig) -&gt; None:\n        \"\"\"\n        Args:\n            config: Validated AudioTranscribeConfig instance.\n        \"\"\"\n        self.config = config\n        self.yt_url = config.yt_url\n        self.yt_url_csv = config.yt_url_csv\n        self.file_ = config.file_\n        self.output_path = Path(config.output)\n        self.keep_artifacts = config.keep_artifacts\n        # Use output directory for all artifacts if keep_artifacts is True, else use system temp\n        if self.keep_artifacts:\n            self.temp_dir = self.output_path.parent\n        else:\n            self.temp_dir = Path(tempfile.mkdtemp(dir=DEFAULT_TEMP_DIR))\n        self.service = config.service\n        self.model = config.model\n        self.language = config.language\n        self.response_format = config.response_format\n        self.chunk_duration = TimeMs.from_seconds(config.chunk_duration)\n        self.min_chunk = TimeMs.from_seconds(config.min_chunk)\n        self.start_time = config.start_time\n        self.end_time = config.end_time\n        self.prompt = config.prompt\n        ensure_directory_exists(self.output_path.parent)\n        ensure_directory_exists(self.temp_dir)\n        self.audio_file: Path = self._resolve_audio_source()\n        self.transcription_options: dict = self._build_transcription_options()\n        self.diarization_config = self._build_diarization_config()\n\n    def run(self) -&gt; None:\n        \"\"\"\n        Run the transcription pipeline and print results, or just download audio if no_transcribe is set.\n        \"\"\"\n        if self.config.no_transcribe:\n            self._echo_settings()\n            click.echo(\"\\n[Download Only Mode]\")\n            click.echo(f\"Downloaded audio file: {self.audio_file}\")\n            return\n        pipeline = TranscriptionPipeline(\n            audio_file=self.audio_file,\n            output_dir=self.temp_dir,\n            diarization_config=self.diarization_config,\n            transcriber=self.service,\n            transcription_options=self.transcription_options,\n        )\n        self._echo_settings()\n        transcripts: list[str] = pipeline.run()\n        self._write_transcript(transcripts)\n        self._print_transcripts(transcripts)\n        self._cleanup_temp_dir()\n\n    def _cleanup_temp_dir(self) -&gt; None:\n        \"\"\"\n        Remove temp directory if not keeping artifacts.\n        \"\"\"\n        if not self.keep_artifacts and self.temp_dir and self.temp_dir.exists():\n            import shutil\n            try:\n                shutil.rmtree(self.temp_dir)\n            except Exception as e:\n                logger.warning(f\"Failed to clean up temp directory: {self.temp_dir} ({e})\")\n\n    def _write_transcript(self, transcripts: list[str]) -&gt; None:\n        \"\"\"\n        Write the full transcript to the output file.\n\n        Args:\n            transcripts: List of transcript strings.\n        \"\"\"\n        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n            for chunk in transcripts:\n                f.write(chunk.strip() + \"\\n\\n\")\n\n    def _echo_settings(self) -&gt; None:\n        \"\"\"\n        Display all runtime settings except transcription_options and diarization_config.\n        \"\"\"\n        click.echo(\"\\n[Settings]\")\n        click.echo(f\"  YouTube URL:         {self.yt_url}\")\n        click.echo(f\"  YouTube CSV:         {self.yt_url_csv}\")\n        click.echo(f\"  File:                {self.file_}\")\n        click.echo(f\"  Output Path:         {self.output_path}\")\n        click.echo(f\"  Temp Directory:      {self.temp_dir}\")\n        click.echo(f\"  Service:             {self.service}\")\n        click.echo(f\"  Model:               {self.model}\")\n        click.echo(f\"  Language:            {self.language}\")\n        click.echo(f\"  Response Format:     {self.response_format}\")\n        click.echo(f\"  Chunk Duration:      {self.chunk_duration.to_seconds()} sec\")\n        click.echo(f\"  Min Chunk:           {self.min_chunk.to_seconds()} sec\")\n        click.echo(f\"  Start Time:          {self.start_time}\")\n        click.echo(f\"  End Time:            {self.end_time}\")\n        click.echo(f\"  Audio File:          {self.audio_file}\")\n        click.echo(f\"  Prompt:              '{self.prompt}'\")\n\n\n    def _resolve_audio_source(self) -&gt; Path:\n        \"\"\"\n        Resolve and return the audio file to transcribe.\n\n        Returns:\n            Path: Path to the resolved audio file.\n        Raises:\n            FileNotFoundError: If no audio input is found.\n            RuntimeError: If youtube-dl version check fails.\n        \"\"\"\n        click.echo(\"[Resolving/Downloading Audio Source ...]\")\n        if self.yt_url_csv:\n            self._set_yt_url_from_csv()\n        if self.yt_url:\n            return self._get_audio_from_youtube()\n        if self.file_:\n            return self._get_audio_from_file()\n        logger.error(\"No audio input found.\")\n        raise FileNotFoundError(\"No audio input found.\")\n\n    def _set_yt_url_from_csv(self) -&gt; None:\n        \"\"\"\n        Set the YouTube URL from the first entry in the CSV file.\n        \"\"\"\n        assert self.yt_url_csv\n        urls: list[str] = get_youtube_urls_from_csv(Path(self.yt_url_csv))\n        self.yt_url = urls[0] if urls else None\n\n    def _get_audio_from_youtube(self) -&gt; Path:\n        \"\"\"\n        Download and return the audio file from YouTube.\n        \"\"\"\n        if not check_ytd_version():\n            logger.error(\"youtube-dl version check failed.\")\n            raise RuntimeError(\"youtube-dl version check failed.\")\n\n        dl = DLPDownloader()\n\n        assert self.yt_url\n        url_metadata = dl.get_metadata(self.yt_url)\n        default_name = dl.get_default_filename_stem(url_metadata)\n        download_path: Path = self.temp_dir / default_name\n        download_file: Path = download_path.with_suffix(\".mp3\")\n        if not download_file.exists():\n            return self._extract_yt_audio(dl, download_path)\n        click.echo(f\"Re-using existing downloaded audio file: {download_file}\")\n        return download_file\n\n    def _extract_yt_audio(self, dl, download_path):\n        video_data = dl.get_audio(\n                self.yt_url,\n                start=self.start_time,\n                output_path=download_path,\n            )\n        if not video_data or not video_data.filepath:\n            raise FileNotFoundError(\"Failed to download or locate audio file.\")\n        return Path(video_data.filepath)\n\n    def _get_audio_from_file(self) -&gt; Path:\n        \"\"\"\n        Return the audio file path, converting video if needed.\n        \"\"\"\n        assert self.file_\n        audio_file: Path = Path(self.file_)\n        if audio_file.suffix.lower() in VIDEO_EXTENSIONS:\n            logger.info(f\"Detected video file: {audio_file}. Auto-converting to mp3 ...\")\n            return convert_video_to_audio(audio_file, self.temp_dir)\n        return audio_file\n\n    def _build_transcription_options(self) -&gt; dict:\n        \"\"\"\n        Build transcription options dictionary for the pipeline.\n\n        Returns:\n            dict: Transcription options for the pipeline.\n        \"\"\"\n        options: dict = {\n            \"model\": self.model,\n            \"language\": self.language,\n            \"response_format\": self.response_format,\n            \"prompt\": self.prompt,\n        }\n        if self.service == \"whisper\" and self.response_format != \"text\":\n            options[\"timestamp_granularities\"] = [\"word\"]\n        return options\n\n    def _build_diarization_config(self) -&gt; DiarizationConfig:\n        \"\"\"\n        Build DiarizationConfig for chunking and language settings.\n\n        Returns:\n            DiarizationConfig: Configuration for diarization and chunking.\n        \"\"\"\n        from tnh_scholar.audio_processing.diarization.config import (\n            ChunkConfig,\n            DiarizationConfig,\n            LanguageConfig,\n            SpeakerConfig,\n        )\n        return DiarizationConfig(\n            chunk=ChunkConfig(\n                target_duration=self.chunk_duration,\n                min_duration=self.min_chunk,\n            ),\n            speaker=SpeakerConfig(single_speaker=True),\n            language=LanguageConfig(default_language=self.language),\n        )\n\n    def _print_transcripts(self, transcripts: list[str]) -&gt; None:\n        \"\"\"\n        Print each transcript chunk to stdout.\n\n        Args:\n            transcripts: List of transcript strings.\n        \"\"\"\n        for i, text in enumerate(transcripts, 1):\n            print(f\"\\n--- Transcript chunk {i} ---\\n{text}\\n\")\n</code></pre> <code>audio_file = self._resolve_audio_source()</code> <code>instance-attribute</code> \u00b6 <code>chunk_duration = TimeMs.from_seconds(config.chunk_duration)</code> <code>instance-attribute</code> \u00b6 <code>config = config</code> <code>instance-attribute</code> \u00b6 <code>diarization_config = self._build_diarization_config()</code> <code>instance-attribute</code> \u00b6 <code>end_time = config.end_time</code> <code>instance-attribute</code> \u00b6 <code>file_ = config.file_</code> <code>instance-attribute</code> \u00b6 <code>keep_artifacts = config.keep_artifacts</code> <code>instance-attribute</code> \u00b6 <code>language = config.language</code> <code>instance-attribute</code> \u00b6 <code>min_chunk = TimeMs.from_seconds(config.min_chunk)</code> <code>instance-attribute</code> \u00b6 <code>model = config.model</code> <code>instance-attribute</code> \u00b6 <code>output_path = Path(config.output)</code> <code>instance-attribute</code> \u00b6 <code>prompt = config.prompt</code> <code>instance-attribute</code> \u00b6 <code>response_format = config.response_format</code> <code>instance-attribute</code> \u00b6 <code>service = config.service</code> <code>instance-attribute</code> \u00b6 <code>start_time = config.start_time</code> <code>instance-attribute</code> \u00b6 <code>temp_dir = self.output_path.parent</code> <code>instance-attribute</code> \u00b6 <code>transcription_options = self._build_transcription_options()</code> <code>instance-attribute</code> \u00b6 <code>yt_url = config.yt_url</code> <code>instance-attribute</code> \u00b6 <code>yt_url_csv = config.yt_url_csv</code> <code>instance-attribute</code> \u00b6 <code>__init__(config)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AudioTranscribeConfig</code> <p>Validated AudioTranscribeConfig instance.</p> required Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/audio_transcribe.py</code> <pre><code>def __init__(self, config: AudioTranscribeConfig) -&gt; None:\n    \"\"\"\n    Args:\n        config: Validated AudioTranscribeConfig instance.\n    \"\"\"\n    self.config = config\n    self.yt_url = config.yt_url\n    self.yt_url_csv = config.yt_url_csv\n    self.file_ = config.file_\n    self.output_path = Path(config.output)\n    self.keep_artifacts = config.keep_artifacts\n    # Use output directory for all artifacts if keep_artifacts is True, else use system temp\n    if self.keep_artifacts:\n        self.temp_dir = self.output_path.parent\n    else:\n        self.temp_dir = Path(tempfile.mkdtemp(dir=DEFAULT_TEMP_DIR))\n    self.service = config.service\n    self.model = config.model\n    self.language = config.language\n    self.response_format = config.response_format\n    self.chunk_duration = TimeMs.from_seconds(config.chunk_duration)\n    self.min_chunk = TimeMs.from_seconds(config.min_chunk)\n    self.start_time = config.start_time\n    self.end_time = config.end_time\n    self.prompt = config.prompt\n    ensure_directory_exists(self.output_path.parent)\n    ensure_directory_exists(self.temp_dir)\n    self.audio_file: Path = self._resolve_audio_source()\n    self.transcription_options: dict = self._build_transcription_options()\n    self.diarization_config = self._build_diarization_config()\n</code></pre> <code>run()</code> \u00b6 <p>Run the transcription pipeline and print results, or just download audio if no_transcribe is set.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/audio_transcribe.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"\n    Run the transcription pipeline and print results, or just download audio if no_transcribe is set.\n    \"\"\"\n    if self.config.no_transcribe:\n        self._echo_settings()\n        click.echo(\"\\n[Download Only Mode]\")\n        click.echo(f\"Downloaded audio file: {self.audio_file}\")\n        return\n    pipeline = TranscriptionPipeline(\n        audio_file=self.audio_file,\n        output_dir=self.temp_dir,\n        diarization_config=self.diarization_config,\n        transcriber=self.service,\n        transcription_options=self.transcription_options,\n    )\n    self._echo_settings()\n    transcripts: list[str] = pipeline.run()\n    self._write_transcript(transcripts)\n    self._print_transcripts(transcripts)\n    self._cleanup_temp_dir()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.audio_transcribe","title":"<code>audio_transcribe(**kwargs)</code>","text":"<p>CLI entry point for audio transcription.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/audio_transcribe.py</code> <pre><code>@click.command()\n@click.option(\n    \"-y\", \"--yt_url\", type=str,\n    help=\"Single YouTube URL.\"\n)\n@click.option(\n    \"-v\", \"--yt_url_csv\", type=click.Path(exists=True),\n    help=\"CSV file with multiple YouTube URLs in first column.\"\n)\n@click.option(\n    \"-f\", \"--file\", \"file_\", type=click.Path(exists=True),\n    help=\"Path to a local audio file.\"\n)\n@click.option(\n    \"-o\", \"--output\", type=click.Path(), default=DEFAULT_OUTPUT_PATH,\n    help=\"Path to the output transcript file.\"\n)\n    # Removed temp_dir option, now handled by keep_artifacts only\n@click.option(\n    \"-s\", \"--service\", type=click.Choice([\"whisper\", \"assemblyai\"]), default=DEFAULT_SERVICE,\n    help=\"Transcription service to use.\"\n)\n@click.option(\n    \"-m\", \"--model\", type=str, default=DEFAULT_MODEL,\n    help=\"Model to use for transcription (for OpenAI only).\"\n)\n@click.option(\n    \"-l\", \"--language\", type=str, default=\"en\",\n    help=\"Language code (e.g., 'en', 'vi').\"\n)\n@click.option(\n    \"-r\", \"--response_format\", type=str, default=DEFAULT_RESPONSE_FORMAT,\n    help=\"Response format for Whisper (default: text).\"\n)\n@click.option(\n    \"--chunk_duration\", type=int, default=DEFAULT_CHUNK_DURATION,\n    help=\"Chunk duration in seconds (default: 7 minutes).\"\n)\n@click.option(\n    \"--min_chunk\", type=int, default=DEFAULT_MIN_CHUNK,\n    help=\"Minimum chunk duration in seconds.\"\n)\n@click.option(\n    \"--start_time\", type=str,\n    help=\"Start time offset for the input media (HH:MM:SS).\"\n)\n@click.option(\n    \"--end_time\", type=str,\n    help=\"End time offset for the input media (HH:MM:SS).\"\n)\n@click.option(\n    \"--prompt\", type=str, default=\"\",\n    help=\"Prompt or keywords to guide the transcription.\"\n)\n@click.option(\n    \"-n\", \"--no_transcribe\", is_flag=True, default=False,\n    help=\"Download YouTube audio to mp3 only, do not transcribe. Requires --yt_url or --yt_url_csv.\"\n)\n@click.option(\n    \"-k\", \"--keep_artifacts\", is_flag=True, default=False,\n    help=\"Keep all intermediate artifacts in the output directory instead of using a system temp directory.\"\n)\ndef audio_transcribe(**kwargs):\n    \"\"\"\n    CLI entry point for audio transcription.\n    \"\"\"\n    try:\n        config = AudioTranscribeConfig(**kwargs)\n    except NoAudioSourceError as e:\n        print(f\"\\n[INPUT ERROR] {e}\", flush=True)\n        raise SystemExit(1) from e\n    except MultipleAudioSourceError as e:\n        print(f\"\\n[INPUT ERROR] {e}\", flush=True)\n        raise SystemExit(1) from e\n    except ValidationError as e:\n        print(\"\\n[CONFIG VALIDATION ERROR]\\n\", e, flush=True)\n        raise SystemExit(1) from e\n    app = AudioTranscribeApp(config)\n    app.run()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.audio_transcribe.main","title":"<code>main()</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/audio_transcribe.py</code> <pre><code>def main():\n    audio_transcribe()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.config","title":"<code>config</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.config.DEFAULT_OUTPUT_PATH","title":"<code>DEFAULT_OUTPUT_PATH = './audio_transcriptions/transcript.txt'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.config.DEFAULT_SERVICE","title":"<code>DEFAULT_SERVICE = 'whisper'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.config.DEFAULT_TEMP_DIR","title":"<code>DEFAULT_TEMP_DIR = './audio_transcriptions/tmp'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.config.AudioTranscribeConfig","title":"<code>AudioTranscribeConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/config.py</code> <pre><code>class AudioTranscribeConfig(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        extra=\"ignore\"\n    )\n\n    yt_url: Optional[str] = Field(default=None, description=\"YouTube URL\")\n    yt_url_csv: Optional[str] = Field(default=None, description=\"CSV file with YouTube URLs\")\n    file_: Optional[str] = Field(default=None, description=\"Path to local audio file\")\n    output: str = Field(default=DEFAULT_OUTPUT_PATH, description=\"Path to output transcript file\")\n    temp_dir: Optional[str] = Field(default=None, description=\"Directory for temporary processing files\")\n    service: str = Field(\n        default=DEFAULT_SERVICE, pattern=\"^(whisper|assemblyai)$\", description=\"Transcription service\"\n    )\n    model: str = Field(description=\"Transcription model name\")\n    language: str = Field(default=\"en\", description=\"Language code\")\n    response_format: str = Field(description=\"Response format\")\n    chunk_duration: int = Field(description=\"Target chunk duration in seconds\")\n    min_chunk: int = Field(ge=10, description=\"Minimum chunk duration in seconds\")\n    start_time: Optional[str] = Field(default=None, description=\"Start time offset\")\n    end_time: Optional[str] = Field(default=None, description=\"End time offset\")\n    prompt: str = Field(default=\"\", description=\"Prompt or keywords\")\n\n    no_transcribe: bool = Field(default=False, \n                                description=\"If True, only download YouTube audio to mp3, no transcription.\")\n    keep_artifacts: bool = Field(default=False, \n                                 description=\"Keep all intermediate artifacts in the output directory \"\n                                 \"instead of using a system temp directory.\")\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self):\n        sources = [self.yt_url, self.yt_url_csv, self.file_]\n        num_sources = sum(bool(s) for s in sources)\n        if self.no_transcribe:\n            # Only allow yt_url or yt_url_csv for download-only mode\n            if not (self.yt_url or self.yt_url_csv):\n                raise ValidationError(\n                    \"--no_transcribe requires a YouTube URL or CSV (--yt_url or --yt_url_csv).\"\n                )\n            if self.file_:\n                raise ValidationError(\n                    \"--no_transcribe does not support local file input. Use --yt_url or --yt_url_csv only.\"\n                )\n        else:\n            if num_sources == 0:\n                raise NoAudioSourceError(\n                    \"No audio source provided: yt_url, yt_url_csv, or _file input\"\n                )\n            if num_sources &gt; 1:\n                raise MultipleAudioSourceError(\n                    \"Only one audio source may be provided at a time: yt_url, yt_url_csv, or _file input.\"\n                )\n        return self\n</code></pre> <code>chunk_duration = Field(description='Target chunk duration in seconds')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>end_time = Field(default=None, description='End time offset')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>file_ = Field(default=None, description='Path to local audio file')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>keep_artifacts = Field(default=False, description='Keep all intermediate artifacts in the output directory instead of using a system temp directory.')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>language = Field(default='en', description='Language code')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>min_chunk = Field(ge=10, description='Minimum chunk duration in seconds')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model = Field(description='Transcription model name')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>no_transcribe = Field(default=False, description='If True, only download YouTube audio to mp3, no transcription.')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>output = Field(default=DEFAULT_OUTPUT_PATH, description='Path to output transcript file')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>prompt = Field(default='', description='Prompt or keywords')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>response_format = Field(description='Response format')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>service = Field(default=DEFAULT_SERVICE, pattern='^(whisper|assemblyai)$', description='Transcription service')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>start_time = Field(default=None, description='Start time offset')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>temp_dir = Field(default=None, description='Directory for temporary processing files')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>yt_url = Field(default=None, description='YouTube URL')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>yt_url_csv = Field(default=None, description='CSV file with YouTube URLs')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>validate_sources()</code> \u00b6 Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_sources(self):\n    sources = [self.yt_url, self.yt_url_csv, self.file_]\n    num_sources = sum(bool(s) for s in sources)\n    if self.no_transcribe:\n        # Only allow yt_url or yt_url_csv for download-only mode\n        if not (self.yt_url or self.yt_url_csv):\n            raise ValidationError(\n                \"--no_transcribe requires a YouTube URL or CSV (--yt_url or --yt_url_csv).\"\n            )\n        if self.file_:\n            raise ValidationError(\n                \"--no_transcribe does not support local file input. Use --yt_url or --yt_url_csv only.\"\n            )\n    else:\n        if num_sources == 0:\n            raise NoAudioSourceError(\n                \"No audio source provided: yt_url, yt_url_csv, or _file input\"\n            )\n        if num_sources &gt; 1:\n            raise MultipleAudioSourceError(\n                \"Only one audio source may be provided at a time: yt_url, yt_url_csv, or _file input.\"\n            )\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.config.MultipleAudioSourceError","title":"<code>MultipleAudioSourceError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when audio source selection has multiple sources).</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/config.py</code> <pre><code>class MultipleAudioSourceError(ValueError):\n    \"\"\"Raised when audio source selection has multiple sources).\"\"\"\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.config.NoAudioSourceError","title":"<code>NoAudioSourceError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when no audio source is provided.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/config.py</code> <pre><code>class NoAudioSourceError(ValueError):\n    \"\"\"Raised when no audio source is provided.\"\"\"\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.convert_video","title":"<code>convert_video</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.convert_video.FFMPEG_VIDEO_CONV_DEFAULT_CONFIG","title":"<code>FFMPEG_VIDEO_CONV_DEFAULT_CONFIG = {'audio_codec': 'libmp3lame', 'audio_bitrate': '192k', 'audio_samplerate': '44100'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.convert_video.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.convert_video.convert_video_to_audio","title":"<code>convert_video_to_audio(video_file, output_dir, conversion_params=None)</code>","text":"<p>Convert a video file to an audio file using ffmpeg.</p> <p>Parameters:</p> Name Type Description Default <code>video_file</code> <code>Path</code> <p>Path to the video file</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the converted audio file</p> required <code>conversion_params</code> <code>Optional[Dict[str, str]]</code> <p>Optional dictionary to override default conversion parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the converted audio file</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/convert_video.py</code> <pre><code>def convert_video_to_audio(\n    video_file: Path, \n    output_dir: Path,\n    conversion_params: Optional[Dict[str, str]] = None\n) -&gt; Path:\n    \"\"\"\n    Convert a video file to an audio file using ffmpeg.\n\n    Args:\n        video_file: Path to the video file\n        output_dir: Directory to save the converted audio file\n        conversion_params: Optional dictionary to override default conversion parameters\n\n    Returns:\n        Path to the converted audio file\n    \"\"\"\n    output_file = output_dir / f\"{video_file.stem}.mp3\"\n\n    if output_file.exists():\n        logger.info(f\"Audio file already exists: {output_file}\")\n        return output_file\n\n    # Merge default config with any supplied parameters\n    params = {**FFMPEG_VIDEO_CONV_DEFAULT_CONFIG}\n    if conversion_params:\n        params |= conversion_params\n\n    logger.info(f\"Converting video to audio: {video_file} -&gt; {output_file}\")\n    logger.debug(f\"Using conversion parameters: {params}\")\n\n    try:\n        cmd = [\n            \"ffmpeg\", \n            \"-i\", str(video_file),\n            \"-vn\",\n            \"-acodec\", params[\"audio_codec\"],\n            \"-ab\", params[\"audio_bitrate\"],\n            \"-ar\", params[\"audio_samplerate\"],\n            \"-y\",  # Overwrite output file if it exists\n            str(output_file)\n        ]\n        subprocess.run(cmd, check=True, capture_output=True)\n        logger.info(f\"Conversion successful: {output_file}\")\n        return output_file\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Conversion failed: {e.stderr.decode() if e.stderr else str(e)}\")\n        raise RuntimeError(f\"Failed to convert video: {video_file}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.environment","title":"<code>environment</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.environment.env","title":"<code>env</code>","text":"<code>logger = get_child_logger(__name__)</code> <code>module-attribute</code> \u00b6 <code>check_env()</code> \u00b6 <p>Check the environment for necessary conditions: 1. Check OpenAI key is available. 2. Check that all requirements from requirements.txt are importable.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/environment/env.py</code> <pre><code>def check_env() -&gt; bool:\n    \"\"\"\n    Check the environment for necessary conditions:\n    1. Check OpenAI key is available.\n    2. Check that all requirements from requirements.txt are importable.\n    \"\"\"\n    logger.debug(\"checking environment.\")\n\n    if not check_openai_env():\n        return False\n\n    if shutil.which(\"ffmpeg\") is None:\n        logger.error(\"ffmpeg not found in PATH. ffmpeg required for audio processing.\")\n        return False\n\n    return True\n</code></pre> <code>check_requirements(requirements_file)</code> \u00b6 <p>Check that all requirements listed in requirements.txt can be imported. If any cannot be imported, print a warning.</p> <p>This is a heuristic check. Some packages may not share the same name as their importable module. Adjust the name mappings below as needed.</p> Example <p>check_requirements(Path(\"./requirements.txt\"))</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/environment/env.py</code> <pre><code>def check_requirements(requirements_file: Path) -&gt; None:\n    \"\"\"\n    Check that all requirements listed in requirements.txt can be imported.\n    If any cannot be imported, print a warning.\n\n    This is a heuristic check. Some packages may not share the same name as their importable module.\n    Adjust the name mappings below as needed.\n\n    Example:\n        &gt;&gt;&gt; check_requirements(Path(\"./requirements.txt\"))\n        # Prints warnings if imports fail, otherwise silent.\n    \"\"\"\n    # Map requirement names to their importable module names if they differ\n    name_map = {\n        \"python-dotenv\": \"dotenv\",\n        \"openai_whisper\": \"whisper\",\n        \"protobuf\": \"google.protobuf\",\n        # Add other mappings if needed\n    }\n\n    # Parse requirements.txt to get a list of package names\n    packages = []\n    with requirements_file.open(\"r\") as req_file:\n        for line in req_file:\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                continue\n            # Each line generally looks like 'package==version'\n            pkg_name = line.split(\"==\")[0].strip()\n            packages.append(pkg_name)\n\n    # Try importing each package\n    for pkg in packages:\n        mod_name = name_map.get(pkg, pkg)\n        try:\n            __import__(mod_name)\n        except ImportError:\n            print(\n                f\"WARNING: Could not import '{mod_name}' from '{pkg}'. Check that it is correctly installed.\"\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.environment.env.check_requirements--prints-warnings-if-imports-fail-otherwise-silent","title":"Prints warnings if imports fail, otherwise silent.","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.transcription_pipeline","title":"<code>transcription_pipeline</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.transcription_pipeline.TranscriptionPipeline","title":"<code>TranscriptionPipeline</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/transcription_pipeline.py</code> <pre><code>class TranscriptionPipeline:\n    def __init__(\n        self,\n        audio_file: Path,\n        output_dir: Path,\n        diarization_config: Optional[DiarizationConfig] = None,\n        transcriber: str = \"whisper\",\n        transcription_options: Optional[Dict[str, Any]] = None,\n        diarization_kwargs: Optional[Dict[str, Any]] = None,\n        save_diarization: bool = True,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the TranscriptionPipeline.\n\n        Args:\n            audio_file (Path): Path to the audio file to process.\n            output_dir (Path): Directory to store output files.\n            diarization_config (Optional[DiarizationConfig]): Diarization configuration.\n            transcriber (str): Transcription service provider.\n            transcription_options (Optional[Dict[str, Any]]): Options for transcription.\n            diarization_kwargs (Optional[Dict[str, Any]]): Additional diarization arguments.\n            save_diarization (bool): Whether to save raw diarization JSON results.\n            logger (Optional[logging.Logger]): Logger for pipeline events.\n        \"\"\"\n        self.logger = logger or logging.getLogger(__name__)\n        self._validate_audio_file(audio_file)\n        self._validate_output_dir(output_dir)\n\n        self.audio_file = audio_file\n        self.output_dir = output_dir\n        self.diarization_config = diarization_config or DiarizationConfig()\n        self.transcriber = transcriber\n        if transcriber == \"whisper\":\n            self.transcription_options = patch_whisper_options(\n                transcription_options,\n                file_extension=audio_file.suffix\n            )\n        else:\n            self.transcription_options = transcription_options\n        self.diarization_kwargs = diarization_kwargs or {}\n        self.save_diarization = save_diarization\n\n        if self.save_diarization:\n            self.diarization_dir = self.output_dir / f\"{self.audio_file.stem}_diarization\"\n            self.diarization_results_path = self.diarization_dir / \"raw_diarization_results.json\"\n        else:\n            self.diarization_dir = None\n            self.diarization_results_path = None\n\n        ensure_directory_writable(self.output_dir)\n        if self.save_diarization:\n            assert self.diarization_dir\n            ensure_directory_writable(self.diarization_dir)\n\n        self.audio_file_extension = audio_file.suffix\n\n    def _validate_audio_file(self, audio_file: Path | str) -&gt; None:\n        \"\"\"\n        Validate the audio file input.\n\n        Args:\n            audio_file (Union[str, Path]): Path to the audio file.\n\n        Raises:\n            TypeError: If not a str or Path instance.\n            FileNotFoundError: If file does not exist.\n        \"\"\"\n        if isinstance(audio_file, str):\n            audio_file = Path(audio_file)\n        elif not isinstance(audio_file, Path):\n            raise TypeError(\"audio_file must be a str or pathlib.Path instance\")\n        if not audio_file.exists() or not audio_file.is_file():\n            raise FileNotFoundError(f\"Audio file does not exist: {audio_file}\")\n\n    def _validate_output_dir(self, output_dir: Path | str) -&gt; None:\n        \"\"\"\n        Validate the output directory\n\n        Args:\n            output_dir (Path | str): Path to the output directory.\n\n        Raises:\n            TypeError: If not a Path or str instance.\n        \"\"\"\n        if isinstance(output_dir, str):\n            output_dir = Path(output_dir)\n        elif not isinstance(output_dir, Path):\n            raise TypeError(\"output_dir must be a str or pathlib.Path instance\")\n\n\n    def run(self) -&gt; Optional[List[Dict[str, Any]]]:\n        \"\"\"\n        Execute the full transcription pipeline with robust error handling.\n\n        Returns:\n            List[Dict[str, Any]]: List of transcript dicts with chunk metadata, or None on failure\n\n        Raises:\n            RuntimeError: If any pipeline step fails.\n        \"\"\"\n        try:\n            self.logger.info(\"Starting diarization step.\")\n            segments = self._run_diarization()\n            if not segments:\n                self.logger.warning(\"No diarization segments found.\")\n                return []\n            self.logger.info(\"Chunking segments.\")\n            chunk_list = self._chunk_segments(segments)\n            if not chunk_list:\n                self.logger.warning(\"No chunks produced from segments.\")\n                return []\n            self.logger.info(\"Extracting audio chunks.\")\n            self._extract_audio_chunks(chunk_list)\n            self.logger.info(\"Transcribing chunks.\")\n            return self._transcribe_chunks(chunk_list)\n        except Exception as exc:\n            self._handle_pipeline_error(exc)\n            return None\n\n    def _run_diarization(self) -&gt; List[Any]:\n        \"\"\"\n        Orchestrate diarization and return domain-level segments.\n        Uses structural pattern matching on the discriminated union.\n        \"\"\"\n        # local import to avoid cycles\n        from tnh_scholar.audio_processing.diarization import diarize, diarize_to_file\n\n        if self.save_diarization:\n            response: DiarizationResponse = diarize_to_file(\n                audio_file_path=self.audio_file,\n                output_path=self.diarization_results_path,\n                wait_until_complete=True, # for this module defaulting to unlimited processing time\n                **(self.diarization_kwargs or {})\n            )\n        else:\n            response: DiarizationResponse = diarize(\n                self.audio_file,\n                wait_until_complete=True,\n                **(self.diarization_kwargs or {})\n            )\n        if response is None:\n            raise RuntimeError(\"Diarizer returned None response\")\n\n        # Discriminated-union matching\n        match response:\n            case DiarizationSucceeded(output=out):\n                segments = getattr(out, \"segments\", None)\n                if segments is None:\n                    raise RuntimeError(\"DiarizationSucceeded missing 'segments'\")\n                self.logger.info(f\"Diarization succeeded: {len(segments)} segments.\")\n                return segments\n\n            case DiarizationFailed(error=err):\n                raise RuntimeError(f\"Diarization failed: {getattr(err, 'message', err)}\")\n\n            case DiarizationPending() | DiarizationRunning():\n                raise RuntimeError(\"Diarization incomplete (pending/running).\")\n\n            case _:\n                self.logger.error(\"Unhandled diarization response variant: \"\n                                  f\"{type(response).__name__} - {response!r}\"\n                                  )\n                raise RuntimeError(\"Unhandled diarization response variant\")\n\n    def _chunk_segments(self, segments):\n        \"\"\"\n        Chunk diarization segments with error handling.\n        \"\"\"\n        try:\n            chunker = TimeGapChunker(config=self.diarization_config)\n            chunks = chunker.extract(segments)\n            if not chunks:\n                self.logger.warning(\"No chunks produced from segments.\")\n            return chunks\n        except Exception as exc:\n            self.logger.error(f\"Chunking segments failed: {exc}\")\n            raise RuntimeError(f\"Chunking segments failed: {exc}\") from exc\n\n    def _extract_audio_chunks(self, chunk_list):\n        \"\"\"\n        Extract audio chunks with error handling.\n        Remove failed chunks from the list and add error metadata for traceability.\n        \"\"\"\n        audio_handler = AudioHandler()\n        successful_chunks = []\n        for chunk in chunk_list:\n            try:\n                audio_handler.build_audio_chunk(chunk, audio_file=self.audio_file)\n                successful_chunks.append(chunk)\n            except Exception as exc:\n                self.logger.error(f\"Audio chunk extraction failed for chunk {chunk}: {exc}\")\n                # Do not add chunk to successful_chunks, effectively removing it from further processing\n        # Update chunk_list in place to only include successful chunks\n        chunk_list[:] = successful_chunks\n\n    def _transcribe_chunks(self, chunk_list):\n        \"\"\"\n        Transcribe audio chunks with error handling.\n        \"\"\"\n        ts_service = TranscriptionServiceFactory.create_service(provider=self.transcriber)\n        transcripts = []\n        for chunk in chunk_list:\n            transcript_text = None\n            error_detail = None\n            try:\n                audio = chunk.audio\n                if not audio:\n                    self.logger.warning(f\"No audio data for chunk {chunk}. Skipping transcription.\")\n                    continue\n                audio_obj = audio.data\n                transcript = ts_service.transcribe(\n                    audio_obj,\n                    self.transcription_options,\n                )\n                transcript_text = transcript.text\n                error_detail = None\n            except Exception as exc:\n                self.logger.error(f\"Transcription failed for chunk {chunk}: {exc}\")\n                transcript_text = None\n                error_detail = str(exc)\n            transcripts.append({\n                \"chunk\": chunk,\n                \"transcript\": transcript_text,\n                \"error\": error_detail\n            })\n        return transcripts\n\n    def _handle_pipeline_error(self, exc: Exception) -&gt; None:\n        \"\"\"\n        Handle pipeline errors in a modular way.\n\n        Args:\n            exc (Exception): The exception to handle.\n\n        Raises:\n            RuntimeError: Always re-raises the error after logging.\n        \"\"\"\n        self.logger.error(f\"TranscriptionPipeline failed: {exc}\")\n        raise RuntimeError(f\"TranscriptionPipeline failed: {exc}\") from exc\n</code></pre> <code>audio_file = audio_file</code> <code>instance-attribute</code> \u00b6 <code>audio_file_extension = audio_file.suffix</code> <code>instance-attribute</code> \u00b6 <code>diarization_config = diarization_config or DiarizationConfig()</code> <code>instance-attribute</code> \u00b6 <code>diarization_dir = self.output_dir / f'{self.audio_file.stem}_diarization'</code> <code>instance-attribute</code> \u00b6 <code>diarization_kwargs = diarization_kwargs or {}</code> <code>instance-attribute</code> \u00b6 <code>diarization_results_path = self.diarization_dir / 'raw_diarization_results.json'</code> <code>instance-attribute</code> \u00b6 <code>logger = logger or logging.getLogger(__name__)</code> <code>instance-attribute</code> \u00b6 <code>output_dir = output_dir</code> <code>instance-attribute</code> \u00b6 <code>save_diarization = save_diarization</code> <code>instance-attribute</code> \u00b6 <code>transcriber = transcriber</code> <code>instance-attribute</code> \u00b6 <code>transcription_options = patch_whisper_options(transcription_options, file_extension=(audio_file.suffix))</code> <code>instance-attribute</code> \u00b6 <code>__init__(audio_file, output_dir, diarization_config=None, transcriber='whisper', transcription_options=None, diarization_kwargs=None, save_diarization=True, logger=None)</code> \u00b6 <p>Initialize the TranscriptionPipeline.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>Path</code> <p>Path to the audio file to process.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to store output files.</p> required <code>diarization_config</code> <code>Optional[DiarizationConfig]</code> <p>Diarization configuration.</p> <code>None</code> <code>transcriber</code> <code>str</code> <p>Transcription service provider.</p> <code>'whisper'</code> <code>transcription_options</code> <code>Optional[Dict[str, Any]]</code> <p>Options for transcription.</p> <code>None</code> <code>diarization_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional diarization arguments.</p> <code>None</code> <code>save_diarization</code> <code>bool</code> <p>Whether to save raw diarization JSON results.</p> <code>True</code> <code>logger</code> <code>Optional[Logger]</code> <p>Logger for pipeline events.</p> <code>None</code> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/transcription_pipeline.py</code> <pre><code>def __init__(\n    self,\n    audio_file: Path,\n    output_dir: Path,\n    diarization_config: Optional[DiarizationConfig] = None,\n    transcriber: str = \"whisper\",\n    transcription_options: Optional[Dict[str, Any]] = None,\n    diarization_kwargs: Optional[Dict[str, Any]] = None,\n    save_diarization: bool = True,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the TranscriptionPipeline.\n\n    Args:\n        audio_file (Path): Path to the audio file to process.\n        output_dir (Path): Directory to store output files.\n        diarization_config (Optional[DiarizationConfig]): Diarization configuration.\n        transcriber (str): Transcription service provider.\n        transcription_options (Optional[Dict[str, Any]]): Options for transcription.\n        diarization_kwargs (Optional[Dict[str, Any]]): Additional diarization arguments.\n        save_diarization (bool): Whether to save raw diarization JSON results.\n        logger (Optional[logging.Logger]): Logger for pipeline events.\n    \"\"\"\n    self.logger = logger or logging.getLogger(__name__)\n    self._validate_audio_file(audio_file)\n    self._validate_output_dir(output_dir)\n\n    self.audio_file = audio_file\n    self.output_dir = output_dir\n    self.diarization_config = diarization_config or DiarizationConfig()\n    self.transcriber = transcriber\n    if transcriber == \"whisper\":\n        self.transcription_options = patch_whisper_options(\n            transcription_options,\n            file_extension=audio_file.suffix\n        )\n    else:\n        self.transcription_options = transcription_options\n    self.diarization_kwargs = diarization_kwargs or {}\n    self.save_diarization = save_diarization\n\n    if self.save_diarization:\n        self.diarization_dir = self.output_dir / f\"{self.audio_file.stem}_diarization\"\n        self.diarization_results_path = self.diarization_dir / \"raw_diarization_results.json\"\n    else:\n        self.diarization_dir = None\n        self.diarization_results_path = None\n\n    ensure_directory_writable(self.output_dir)\n    if self.save_diarization:\n        assert self.diarization_dir\n        ensure_directory_writable(self.diarization_dir)\n\n    self.audio_file_extension = audio_file.suffix\n</code></pre> <code>run()</code> \u00b6 <p>Execute the full transcription pipeline with robust error handling.</p> <p>Returns:</p> Type Description <code>Optional[List[Dict[str, Any]]]</code> <p>List[Dict[str, Any]]: List of transcript dicts with chunk metadata, or None on failure</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If any pipeline step fails.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/transcription_pipeline.py</code> <pre><code>def run(self) -&gt; Optional[List[Dict[str, Any]]]:\n    \"\"\"\n    Execute the full transcription pipeline with robust error handling.\n\n    Returns:\n        List[Dict[str, Any]]: List of transcript dicts with chunk metadata, or None on failure\n\n    Raises:\n        RuntimeError: If any pipeline step fails.\n    \"\"\"\n    try:\n        self.logger.info(\"Starting diarization step.\")\n        segments = self._run_diarization()\n        if not segments:\n            self.logger.warning(\"No diarization segments found.\")\n            return []\n        self.logger.info(\"Chunking segments.\")\n        chunk_list = self._chunk_segments(segments)\n        if not chunk_list:\n            self.logger.warning(\"No chunks produced from segments.\")\n            return []\n        self.logger.info(\"Extracting audio chunks.\")\n        self._extract_audio_chunks(chunk_list)\n        self.logger.info(\"Transcribing chunks.\")\n        return self._transcribe_chunks(chunk_list)\n    except Exception as exc:\n        self._handle_pipeline_error(exc)\n        return None\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.validate","title":"<code>validate</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.validate.validate_inputs","title":"<code>validate_inputs(is_download, yt_url, yt_url_list, audio_file, split, transcribe, chunk_dir, no_chunks, silence_boundaries, whisper_boundaries)</code>","text":"<p>Validate the CLI inputs to ensure logical consistency given all the flags.</p> <p>Conditions &amp; Requirements: 1. At least one action (yt_download, split, transcribe) should be requested.    Otherwise, nothing is done, so raise an error.</p> <ol> <li>If yt_download is True:</li> <li> <p>Must specify either yt_process_url OR yt_process_url_list (not both, not none).</p> </li> <li> <p>If yt_download is False:</p> </li> <li>If split is requested, we need a local audio file (since no download will occur).</li> <li> <p>If transcribe is requested without split and without yt_download:</p> <ul> <li>If no_chunks = False, we must have chunk_dir to read existing chunks.</li> <li>If no_chunks = True, we must have a local audio file (direct transcription) or previously downloaded file    (but since yt_download=False, previously downloaded file scenario doesn't apply here,    so effectively we need local audio in that scenario).</li> </ul> </li> <li> <p>no_chunks flag:</p> </li> <li> <p>If no_chunks = True, we are doing direct transcription on entire audio without chunking.</p> <ul> <li>Cannot use split if no_chunks = True. (Mutually exclusive)</li> <li>chunk_dir is irrelevant if no_chunks = True; since we don't split into chunks,    requiring a chunk_dir doesn't make sense. If provided, it's not useful, but let's allow it silently    or raise an error for clarity. It's safer to raise an error to prevent user confusion.</li> </ul> </li> <li> <p>Boundaries flags (silence_boundaries, whisper_boundaries):</p> </li> <li>These flags control how splitting is done.</li> <li>If split = False, these are irrelevant. Not necessarily an error, but could be a no-op.      For robustness, raise an error if user specifies these without split, to avoid confusion.</li> <li>If split = True and no_chunks = True, that\u2019s contradictory already, so no need for boundary logic there.</li> <li>If split = True, exactly one method should be chosen:      If both silence_boundaries and whisper_boundaries are True simultaneously or both are False simultaneously,      we need a clear default or raise an error. By the code snippet logic, whisper_boundaries is default True      if not stated otherwise. To keep it robust:<ul> <li>If both are True, raise error.</li> <li>If both are False, that means user explicitly turned them off or never turned on whisper.      The code snippet sets whisper_boundaries True by default. If user sets it False somehow,      we can then default to silence. Just ensure at run-time we have a deterministic method:      If both are False, we can default to whisper or silence. Let's default to whisper if no flags given.      However, given the code snippet, whisper_boundaries has a default of True.      If the user sets whisper_boundaries to False and also does not set silence_boundaries,      then no method is chosen. Let's then raise an error if both ended up False to avoid ambiguity.</li> </ul> </li> </ol> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arguments are not logically consistent.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/validate.py</code> <pre><code>def validate_inputs(\n    is_download: bool,\n    yt_url: str | None,\n    yt_url_list: Path | None,\n    audio_file: Path | None,\n    split: bool,\n    transcribe: bool,\n    chunk_dir: Path | None,\n    no_chunks: bool,\n    silence_boundaries: bool,\n    whisper_boundaries: bool,\n) -&gt; None:\n    \"\"\"\n    Validate the CLI inputs to ensure logical consistency given all the flags.\n\n    Conditions &amp; Requirements:\n    1. At least one action (yt_download, split, transcribe) should be requested.\n       Otherwise, nothing is done, so raise an error.\n\n    2. If yt_download is True:\n       - Must specify either yt_process_url OR yt_process_url_list (not both, not none).\n\n    3. If yt_download is False:\n       - If split is requested, we need a local audio file (since no download will occur).\n       - If transcribe is requested without split and without yt_download:\n         - If no_chunks = False, we must have chunk_dir to read existing chunks.\n         - If no_chunks = True, we must have a local audio file (direct transcription) or previously downloaded file\n           (but since yt_download=False, previously downloaded file scenario doesn't apply here,\n           so effectively we need local audio in that scenario).\n\n    4. no_chunks flag:\n       - If no_chunks = True, we are doing direct transcription on entire audio without chunking.\n         - Cannot use split if no_chunks = True. (Mutually exclusive)\n         - chunk_dir is irrelevant if no_chunks = True; since we don't split into chunks,\n           requiring a chunk_dir doesn't make sense. If provided, it's not useful, but let's allow it silently\n           or raise an error for clarity. It's safer to raise an error to prevent user confusion.\n\n    5. Boundaries flags (silence_boundaries, whisper_boundaries):\n       - These flags control how splitting is done.\n       - If split = False, these are irrelevant. Not necessarily an error, but could be a no-op.\n         For robustness, raise an error if user specifies these without split, to avoid confusion.\n       - If split = True and no_chunks = True, that\u2019s contradictory already, so no need for boundary logic there.\n       - If split = True, exactly one method should be chosen:\n         If both silence_boundaries and whisper_boundaries are True simultaneously or both are False simultaneously,\n         we need a clear default or raise an error. By the code snippet logic, whisper_boundaries is default True\n         if not stated otherwise. To keep it robust:\n           - If both are True, raise error.\n           - If both are False, that means user explicitly turned them off or never turned on whisper.\n             The code snippet sets whisper_boundaries True by default. If user sets it False somehow,\n             we can then default to silence. Just ensure at run-time we have a deterministic method:\n             If both are False, we can default to whisper or silence. Let's default to whisper if no flags given.\n             However, given the code snippet, whisper_boundaries has a default of True.\n             If the user sets whisper_boundaries to False and also does not set silence_boundaries,\n             then no method is chosen. Let's then raise an error if both ended up False to avoid ambiguity.\n\n    Raises:\n        ValueError: If the input arguments are not logically consistent.\n    \"\"\"\n\n    # 1. Check that we have at least one action\n    if not is_download and not split and not transcribe:\n        raise ValueError(\n            \"No actions requested. At least one of --yt_download, --split, --transcribe, or --full must be set.\"\n        )\n\n    # 2. Validate YouTube download logic\n    if is_download:\n        if yt_url and yt_url_list:\n            raise ValueError(\n                \"Both --yt_process_url and --yt_process_url_list provided. Only one allowed.\"\n            )\n        if not yt_url and not yt_url_list:\n            raise ValueError(\n                \"When --yt_download is specified, you must provide --yt_process_url or --yt_process_url_list.\"\n            )\n\n    # 3. Logic when no YouTube download:\n    if not is_download:\n        # If splitting but no download, need an audio file\n        if split and audio_file is None:\n            raise ValueError(\n                \"Splitting requested but no audio file provided and no YouTube download source available.\"\n            )\n\n        if transcribe and not split:\n            if no_chunks:\n                # Direct transcription, need an audio file\n                if audio_file is None:\n                    raise ValueError(\n                        \"Transcription requested with no_chunks=True but no audio file provided.\"\n                    )\n            elif chunk_dir is None:\n                raise ValueError(\n                    \"Transcription requested without splitting or downloading and no_chunks=False. Must provide --chunk_dir with pre-split chunks.\"\n                )\n\n    # Check no_chunks scenario:\n    # no_chunks and split are mutually exclusive\n    # If transcribing but not splitting or downloading:\n    # If no_chunks and chunk_dir provided, it doesn't make sense since we won't use chunks at all.\n    # 4. no_chunks flag validation:\n    # no_chunks=False, we need chunks from chunk_dir\n    if no_chunks:\n        if split:\n            raise ValueError(\n                \"Cannot use --no_chunks and --split together. Choose one option.\"\n            )\n        if chunk_dir is not None:\n            raise ValueError(\"Cannot specify --chunk_dir when --no_chunks is set.\")\n\n    # 5. Boundaries flags:\n    # If splitting is not requested but boundaries flags are set, it's meaningless.\n    # The code snippet defaults whisper_boundaries to True, so if user tries to turn it off and sets silence?\n    # We'll require that boundaries only matter if split is True.\n    if not split and (silence_boundaries or whisper_boundaries):\n        raise ValueError(\n            \"Boundary detection flags given but splitting is not requested. Remove these flags or enable --split.\"\n        )\n\n    # If split is True, we must have a consistent boundary method:\n    if split:\n        # If both whisper and silence are somehow True:\n        if silence_boundaries and whisper_boundaries:\n            raise ValueError(\n                \"Cannot use both --silence_boundaries and --whisper_boundaries simultaneously.\"\n            )\n\n        # If both are False:\n        # Given the original snippet, whisper_boundaries is True by default.\n        # For the sake of robustness, let's say if user sets both off, we can't proceed:\n        if not silence_boundaries and not whisper_boundaries:\n            raise ValueError(\n                \"No boundary method selected for splitting. Enable either whisper or silence boundaries.\"\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.version_check","title":"<code>version_check</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.version_check.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.version_check.YTDVersionChecker","title":"<code>YTDVersionChecker</code>","text":"<p>Simple version checker for yt-dlp with robust version comparison.</p> <p>This is a prototype implementation may need expansion in these areas: - Caching to prevent frequent PyPI calls - More comprehensive error handling for:     - Missing/uninstalled packages     - Network timeouts     - JSON parsing errors     - Invalid version strings - Environment detection (virtualenv, conda, system Python) - Configuration options for version pinning - Proxy support for network requests</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/version_check.py</code> <pre><code>class YTDVersionChecker:\n    \"\"\"\n    Simple version checker for yt-dlp with robust version comparison.\n\n    This is a prototype implementation may need expansion in these areas:\n    - Caching to prevent frequent PyPI calls\n    - More comprehensive error handling for:\n        - Missing/uninstalled packages\n        - Network timeouts\n        - JSON parsing errors\n        - Invalid version strings\n    - Environment detection (virtualenv, conda, system Python)\n    - Configuration options for version pinning\n    - Proxy support for network requests\n    \"\"\"\n\n    PYPI_URL = \"https://pypi.org/pypi/yt-dlp/json\"\n    NETWORK_TIMEOUT = 5  # seconds\n\n    def _get_installed_version(self) -&gt; Version:\n        \"\"\"\n        Get installed yt-dlp version.\n\n        Returns:\n            Version object representing installed version\n\n        Raises:\n            ImportError: If yt-dlp is not installed\n            InvalidVersion: If installed version string is invalid\n        \"\"\"\n        try:\n            if version_str := str(importlib.metadata.version(\"yt-dlp\")):\n                return Version(version_str)\n            else:\n                raise InvalidVersion(\"yt-dlp version string is empty\")\n        except importlib.metadata.PackageNotFoundError as e:\n            raise ImportError(\"yt-dlp is not installed\") from e\n        except InvalidVersion:\n            raise\n\n    def _get_latest_version(self) -&gt; Version:\n        \"\"\"\n        Get latest version from PyPI.\n\n        Returns:\n            Version object representing latest available version\n\n        Raises:\n            requests.RequestException: For any network-related errors\n            InvalidVersion: If PyPI version string is invalid\n            KeyError: If PyPI response JSON is malformed\n        \"\"\"\n        try:\n            response = requests.get(self.PYPI_URL, timeout=self.NETWORK_TIMEOUT)\n            response.raise_for_status()\n            version_str = response.json()[\"info\"][\"version\"]\n            return Version(version_str)\n        except requests.RequestException as e:\n            raise requests.RequestException(\n                \"Failed to fetch version from PyPI. Check network connection.\"\n            ) from e\n\n    def check_version(self) -&gt; Tuple[bool, Version, Version]:\n        \"\"\"\n        Check if yt-dlp needs updating.\n\n        Returns:\n            Tuple of (needs_update, installed_version, latest_version)\n\n        Raises:\n            ImportError: If yt-dlp is not installed\n            requests.RequestException: For network-related errors\n            InvalidVersion: If version strings are invalid\n        \"\"\"\n        installed_version = self._get_installed_version()\n        latest_version = self._get_latest_version()\n\n        needs_update = installed_version &lt; latest_version\n        return needs_update, installed_version, latest_version\n</code></pre> <code>NETWORK_TIMEOUT = 5</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>PYPI_URL = 'https://pypi.org/pypi/yt-dlp/json'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>check_version()</code> \u00b6 <p>Check if yt-dlp needs updating.</p> <p>Returns:</p> Type Description <code>Tuple[bool, Version, Version]</code> <p>Tuple of (needs_update, installed_version, latest_version)</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If yt-dlp is not installed</p> <code>RequestException</code> <p>For network-related errors</p> <code>InvalidVersion</code> <p>If version strings are invalid</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/version_check.py</code> <pre><code>def check_version(self) -&gt; Tuple[bool, Version, Version]:\n    \"\"\"\n    Check if yt-dlp needs updating.\n\n    Returns:\n        Tuple of (needs_update, installed_version, latest_version)\n\n    Raises:\n        ImportError: If yt-dlp is not installed\n        requests.RequestException: For network-related errors\n        InvalidVersion: If version strings are invalid\n    \"\"\"\n    installed_version = self._get_installed_version()\n    latest_version = self._get_latest_version()\n\n    needs_update = installed_version &lt; latest_version\n    return needs_update, installed_version, latest_version\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.audio_transcribe.version_check.check_ytd_version","title":"<code>check_ytd_version()</code>","text":"<p>Check if yt-dlp needs updating and log appropriate messages.</p> <p>This function checks the installed version of yt-dlp against the latest version on PyPI and logs informational or error messages as appropriate. It handles network errors, missing packages, and version parsing issues gracefully.</p> <p>The function does not raise exceptions but logs them using the application's logging system.</p> Source code in <code>src/tnh_scholar/cli_tools/audio_transcribe/version_check.py</code> <pre><code>def check_ytd_version() -&gt; bool:\n    \"\"\"\n    Check if yt-dlp needs updating and log appropriate messages.\n\n    This function checks the installed version of yt-dlp against the latest version\n    on PyPI and logs informational or error messages as appropriate. It handles\n    network errors, missing packages, and version parsing issues gracefully.\n\n    The function does not raise exceptions but logs them using the application's\n    logging system.\n    \"\"\"\n    checker = YTDVersionChecker()\n    try:\n        needs_update, current, latest = checker.check_version()\n        if needs_update:\n            logger.info(f\"Update available: {current} -&gt; {latest}\")\n            logger.info(\"Please run the appropriate upgrade in your environment.\")\n            logger.info(\"   For example: pip install --upgrade yt-dlp \")\n            return False\n        else:\n            logger.info(f\"yt-dlp is up to date (version {current})\")\n\n    except ImportError as e:\n        logger.error(f\"In yt-dlp version check: Package error: {e}\")\n    except requests.RequestException as e:\n        logger.error(f\"In yt-dlp version check: Network error: {e}\")\n    except InvalidVersion as e:\n        logger.error(f\"In yt-dlp version check: Version parsing error: {e}\")\n    except Exception as e:\n        logger.error(f\"In yt-dlp version check: Unexpected error: {e}\")\n\n    return True\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt","title":"<code>json_to_srt</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.__all__","title":"<code>__all__ = ['main', 'json_to_srt']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.main","title":"<code>main()</code>","text":"<p>Entry point for the jsonl-to-srt CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def main():\n    \"\"\"Entry point for the jsonl-to-srt CLI tool.\"\"\"\n    json_to_srt()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt","title":"<code>json_to_srt</code>","text":"<p>Simple CLI tool for converting JSONL transcription files to SRT format.</p> <p>This module provides a command line interface for transforming JSONL transcription files (from audio-transcribe) into SRT subtitle format. Handles chunked transcriptions with proper timestamp accumulation.</p>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt.JsonlToSrtConverter","title":"<code>JsonlToSrtConverter</code>","text":"<p>Converts JSONL transcription files from audio-transcribe to SRT format.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>class JsonlToSrtConverter:\n    \"\"\"Converts JSONL transcription files from audio-transcribe to SRT format.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize converter state.\"\"\"\n        self.entry_index = 1\n        self.accumulated_time = 0.0\n\n    def format_timestamp(self, seconds: float) -&gt; str:\n        \"\"\"Convert seconds to SRT timestamp format (HH:MM:SS,mmm).\"\"\"\n        td = timedelta(seconds=seconds)\n        hours, remainder = divmod(td.seconds, 3600)\n        minutes, seconds = divmod(remainder, 60)\n        milliseconds = round(td.microseconds / 1000)\n        return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n\n    def parse_jsonl_line(self, line: str) -&gt; Dict:\n        \"\"\"Parse a single JSONL line into a dictionary.\"\"\"\n        try:\n            return json.loads(line.strip())\n        except json.JSONDecodeError as e:\n            logger.error(f\"Error parsing JSONL line: {e}\")\n            return {}\n\n    def build_srt_entry(self, index: int, start: float, end: float, text: str) -&gt; str:\n        \"\"\"Format a single SRT entry.\"\"\"\n        start_str = self.format_timestamp(start)\n        end_str = self.format_timestamp(end)\n        return f\"{index}\\n{start_str} --&gt; {end_str}\\n{text}\\n\"\n\n    def extract_segment_data(self, segment: Dict) -&gt; Tuple[float, float, str]:\n        \"\"\"Extract timestamp and text data from a segment.\"\"\"\n        start = segment.get(\"start\", 0) + self.accumulated_time\n        end = segment.get(\"end\", 0) + self.accumulated_time\n        text = segment.get(\"text\", \"\").strip()\n        return start, end, text\n\n    def process_segment(self, segment: Dict) -&gt; Optional[str]:\n        \"\"\"Process a single segment into SRT format.\"\"\"\n        start, end, text = self.extract_segment_data(segment)\n\n        if not text:\n            return None\n\n        entry = self.build_srt_entry(self.entry_index, start, end, text)\n        self.entry_index += 1\n        return entry\n\n    def process_segments_list(self, segments_list: List[Dict]) -&gt; List[str]:\n        \"\"\"Process a list of segments into SRT entries.\"\"\"\n        entries = []\n\n        for segment in segments_list:\n            if entry := self.process_segment(segment):\n                entries.append(entry)\n\n        return entries\n\n    def get_segments_from_data(self, data: Dict) -&gt; List[Dict]:\n        \"\"\"Extract segments from a data object.\"\"\"\n        return data.get(\"segments\", [])\n\n    def read_input_lines(self, input_file: TextIO) -&gt; List[str]:\n        \"\"\"Read and filter input lines from file.\"\"\"\n        return [line.strip() for line in input_file if line.strip()]\n\n    def process_jsonl_line(self, line: str) -&gt; List[str]:\n        \"\"\"Process a single JSONL line into SRT entries.\"\"\"\n        data = self.parse_jsonl_line(line)\n        if not data:\n            return []\n\n        # Extract duration for accumulation\n        chunk_duration = data.get(\"duration\", 0.0)\n\n        segments = self.get_segments_from_data(data)\n        entries = self.process_segments_list(segments)\n\n        # Update accumulated time after processing this chunk\n        self.accumulated_time += chunk_duration\n        return entries\n\n    def process_jsonl_content(self, lines: List[str]) -&gt; str:\n        \"\"\"Process all JSONL content into SRT format.\"\"\"\n        all_entries = []\n\n        for line in lines:\n            entries = self.process_jsonl_line(line)\n            all_entries.extend(entries)\n\n        return \"\\n\".join(all_entries)\n\n    def handle_output(self, srt_content: str, output_file: Optional[Path]) -&gt; None:\n        \"\"\"Write SRT content to file or stdout.\"\"\"\n        if output_file:\n            write_str_to_file(output_file, srt_content, overwrite=True)\n            logger.info(f\"SRT content written to {output_file}\")\n        else:\n            click.echo(srt_content)\n\n    def convert(self, input_file: TextIO, output_file: Optional[Path] = None) -&gt; str:\n        \"\"\"\n        Convert a JSONL transcription file to SRT format.\n\n        Args:\n            input_file: JSONL transcription file to parse\n            output_file: Optional output file path\n\n        Returns:\n            str: SRT formatted content\n        \"\"\"\n        input_lines = self.read_input_lines(input_file)\n        srt_content = self.process_jsonl_content(input_lines)\n        self.handle_output(srt_content, output_file)\n        return srt_content\n</code></pre> <code>accumulated_time = 0.0</code> <code>instance-attribute</code> \u00b6 <code>entry_index = 1</code> <code>instance-attribute</code> \u00b6 <code>__init__()</code> \u00b6 <p>Initialize converter state.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize converter state.\"\"\"\n    self.entry_index = 1\n    self.accumulated_time = 0.0\n</code></pre> <code>build_srt_entry(index, start, end, text)</code> \u00b6 <p>Format a single SRT entry.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def build_srt_entry(self, index: int, start: float, end: float, text: str) -&gt; str:\n    \"\"\"Format a single SRT entry.\"\"\"\n    start_str = self.format_timestamp(start)\n    end_str = self.format_timestamp(end)\n    return f\"{index}\\n{start_str} --&gt; {end_str}\\n{text}\\n\"\n</code></pre> <code>convert(input_file, output_file=None)</code> \u00b6 <p>Convert a JSONL transcription file to SRT format.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>TextIO</code> <p>JSONL transcription file to parse</p> required <code>output_file</code> <code>Optional[Path]</code> <p>Optional output file path</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>SRT formatted content</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def convert(self, input_file: TextIO, output_file: Optional[Path] = None) -&gt; str:\n    \"\"\"\n    Convert a JSONL transcription file to SRT format.\n\n    Args:\n        input_file: JSONL transcription file to parse\n        output_file: Optional output file path\n\n    Returns:\n        str: SRT formatted content\n    \"\"\"\n    input_lines = self.read_input_lines(input_file)\n    srt_content = self.process_jsonl_content(input_lines)\n    self.handle_output(srt_content, output_file)\n    return srt_content\n</code></pre> <code>extract_segment_data(segment)</code> \u00b6 <p>Extract timestamp and text data from a segment.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def extract_segment_data(self, segment: Dict) -&gt; Tuple[float, float, str]:\n    \"\"\"Extract timestamp and text data from a segment.\"\"\"\n    start = segment.get(\"start\", 0) + self.accumulated_time\n    end = segment.get(\"end\", 0) + self.accumulated_time\n    text = segment.get(\"text\", \"\").strip()\n    return start, end, text\n</code></pre> <code>format_timestamp(seconds)</code> \u00b6 <p>Convert seconds to SRT timestamp format (HH:MM:SS,mmm).</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def format_timestamp(self, seconds: float) -&gt; str:\n    \"\"\"Convert seconds to SRT timestamp format (HH:MM:SS,mmm).\"\"\"\n    td = timedelta(seconds=seconds)\n    hours, remainder = divmod(td.seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    milliseconds = round(td.microseconds / 1000)\n    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n</code></pre> <code>get_segments_from_data(data)</code> \u00b6 <p>Extract segments from a data object.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def get_segments_from_data(self, data: Dict) -&gt; List[Dict]:\n    \"\"\"Extract segments from a data object.\"\"\"\n    return data.get(\"segments\", [])\n</code></pre> <code>handle_output(srt_content, output_file)</code> \u00b6 <p>Write SRT content to file or stdout.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def handle_output(self, srt_content: str, output_file: Optional[Path]) -&gt; None:\n    \"\"\"Write SRT content to file or stdout.\"\"\"\n    if output_file:\n        write_str_to_file(output_file, srt_content, overwrite=True)\n        logger.info(f\"SRT content written to {output_file}\")\n    else:\n        click.echo(srt_content)\n</code></pre> <code>parse_jsonl_line(line)</code> \u00b6 <p>Parse a single JSONL line into a dictionary.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def parse_jsonl_line(self, line: str) -&gt; Dict:\n    \"\"\"Parse a single JSONL line into a dictionary.\"\"\"\n    try:\n        return json.loads(line.strip())\n    except json.JSONDecodeError as e:\n        logger.error(f\"Error parsing JSONL line: {e}\")\n        return {}\n</code></pre> <code>process_jsonl_content(lines)</code> \u00b6 <p>Process all JSONL content into SRT format.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def process_jsonl_content(self, lines: List[str]) -&gt; str:\n    \"\"\"Process all JSONL content into SRT format.\"\"\"\n    all_entries = []\n\n    for line in lines:\n        entries = self.process_jsonl_line(line)\n        all_entries.extend(entries)\n\n    return \"\\n\".join(all_entries)\n</code></pre> <code>process_jsonl_line(line)</code> \u00b6 <p>Process a single JSONL line into SRT entries.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def process_jsonl_line(self, line: str) -&gt; List[str]:\n    \"\"\"Process a single JSONL line into SRT entries.\"\"\"\n    data = self.parse_jsonl_line(line)\n    if not data:\n        return []\n\n    # Extract duration for accumulation\n    chunk_duration = data.get(\"duration\", 0.0)\n\n    segments = self.get_segments_from_data(data)\n    entries = self.process_segments_list(segments)\n\n    # Update accumulated time after processing this chunk\n    self.accumulated_time += chunk_duration\n    return entries\n</code></pre> <code>process_segment(segment)</code> \u00b6 <p>Process a single segment into SRT format.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def process_segment(self, segment: Dict) -&gt; Optional[str]:\n    \"\"\"Process a single segment into SRT format.\"\"\"\n    start, end, text = self.extract_segment_data(segment)\n\n    if not text:\n        return None\n\n    entry = self.build_srt_entry(self.entry_index, start, end, text)\n    self.entry_index += 1\n    return entry\n</code></pre> <code>process_segments_list(segments_list)</code> \u00b6 <p>Process a list of segments into SRT entries.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def process_segments_list(self, segments_list: List[Dict]) -&gt; List[str]:\n    \"\"\"Process a list of segments into SRT entries.\"\"\"\n    entries = []\n\n    for segment in segments_list:\n        if entry := self.process_segment(segment):\n            entries.append(entry)\n\n    return entries\n</code></pre> <code>read_input_lines(input_file)</code> \u00b6 <p>Read and filter input lines from file.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def read_input_lines(self, input_file: TextIO) -&gt; List[str]:\n    \"\"\"Read and filter input lines from file.\"\"\"\n    return [line.strip() for line in input_file if line.strip()]\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt.json_to_srt","title":"<code>json_to_srt(input_file, output=None)</code>","text":"<p>Convert JSONL transcription files to SRT subtitle format.</p> <p>Reads from stdin if no INPUT_FILE is specified. Writes to stdout if no output file is specified.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>@click.command()\n@click.argument(\"input_file\", type=click.File(\"r\"), default=\"-\")\n@click.option(\n    \"-o\", \n    \"--output\", \n    type=click.Path(path_type=Path), \n    help=\"Output file (default: stdout)\"\n)\ndef json_to_srt(input_file: TextIO, output: Optional[Path] = None) -&gt; None:\n    \"\"\"\n    Convert JSONL transcription files to SRT subtitle format.\n\n    Reads from stdin if no INPUT_FILE is specified.\n    Writes to stdout if no output file is specified.\n    \"\"\"\n    try:\n        converter = JsonlToSrtConverter()\n        converter.convert(input_file, output)\n    except Exception as e:\n        logger.error(f\"Error processing file: {e}\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt.main","title":"<code>main()</code>","text":"<p>Entry point for the jsonl-to-srt CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt.py</code> <pre><code>def main():\n    \"\"\"Entry point for the jsonl-to-srt CLI tool.\"\"\"\n    json_to_srt()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1","title":"<code>json_to_srt1</code>","text":"<p>Simple CLI tool for converting JSONL transcription files to SRT format.</p> <p>This module provides a command line interface for transforming JSONL transcription files (from audio-transcribe) into SRT subtitle format.</p>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.convert_to_srt","title":"<code>convert_to_srt(input_file, output_file=None)</code>","text":"<p>Convert a JSONL transcription file to SRT format.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>TextIO</code> <p>JSONL transcription file to parse</p> required <code>output_file</code> <code>Optional[Path]</code> <p>Optional output file path</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>SRT formatted content</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def convert_to_srt(input_file: TextIO, output_file: Optional[Path] = None) -&gt; str:\n    \"\"\"\n    Convert a JSONL transcription file to SRT format.\n\n    Args:\n        input_file: JSONL transcription file to parse\n        output_file: Optional output file path\n\n    Returns:\n        str: SRT formatted content\n    \"\"\"\n    input_lines = read_input_lines(input_file)\n    srt_content = process_jsonl_content(input_lines)\n    handle_output(srt_content, output_file)\n    return srt_content\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.extract_segment_data","title":"<code>extract_segment_data(segment)</code>","text":"<p>Extract timestamp and text data from a segment.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def extract_segment_data(segment: Dict) -&gt; Tuple[float, float, str]:\n    \"\"\"Extract timestamp and text data from a segment.\"\"\"\n    start = segment.get(\"start\", 0)\n    end = segment.get(\"end\", 0)\n    text = segment.get(\"text\", \"\").strip()\n    return start, end, text\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.format_srt_entry","title":"<code>format_srt_entry(index, start, end, text)</code>","text":"<p>Format a single SRT entry.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def format_srt_entry(index: int, start: float, end: float, text: str) -&gt; str:\n    \"\"\"Format a single SRT entry.\"\"\"\n    start_str = format_timestamp(start)\n    end_str = format_timestamp(end)\n    return f\"{index}\\n{start_str} --&gt; {end_str}\\n{text}\\n\"\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.format_timestamp","title":"<code>format_timestamp(seconds)</code>","text":"<p>Convert seconds to SRT timestamp format (HH:MM:SS,mmm).</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def format_timestamp(seconds: float) -&gt; str:\n    \"\"\"Convert seconds to SRT timestamp format (HH:MM:SS,mmm).\"\"\"\n    td = timedelta(seconds=seconds)\n    hours, remainder = divmod(td.seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    milliseconds = round(td.microseconds / 1000)\n    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.get_segments_from_data","title":"<code>get_segments_from_data(data)</code>","text":"<p>Extract segments from a data object.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def get_segments_from_data(data: Dict) -&gt; List[Dict]:\n    \"\"\"Extract segments from a data object.\"\"\"\n    return data[\"segments\"] if \"segments\" in data else []\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.handle_output","title":"<code>handle_output(srt_content, output_file)</code>","text":"<p>Write SRT content to file or stdout.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def handle_output(srt_content: str, output_file: Optional[Path]) -&gt; None:\n    \"\"\"Write SRT content to file or stdout.\"\"\"\n    if output_file:\n        write_str_to_file(output_file, srt_content)\n        logger.info(f\"SRT content written to {output_file}\")\n    else:\n        click.echo(srt_content)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.json_to_srt","title":"<code>json_to_srt(input_file, output=None)</code>","text":"<p>Convert JSONL transcription files to SRT subtitle format.</p> <p>Reads from stdin if no INPUT_FILE is specified. Writes to stdout if no output file is specified.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>@click.command()\n@click.argument(\"input_file\", type=click.File(\"r\"), default=\"-\")\n@click.option(\n    \"-o\", \n    \"--output\", \n    type=click.Path(path_type=Path), \n    help=\"Output file (default: stdout)\"\n)\ndef json_to_srt(input_file: TextIO, output: Optional[Path] = None) -&gt; None:\n    \"\"\"\n    Convert JSONL transcription files to SRT subtitle format.\n\n    Reads from stdin if no INPUT_FILE is specified.\n    Writes to stdout if no output file is specified.\n    \"\"\"\n    try:\n        convert_to_srt(input_file, output)\n    except Exception as e:\n        logger.error(f\"Error processing file: {e}\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.main","title":"<code>main()</code>","text":"<p>Entry point for the jsonl-to-srt CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def main():\n    \"\"\"Entry point for the jsonl-to-srt CLI tool.\"\"\"\n    json_to_srt()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.parse_jsonl_line","title":"<code>parse_jsonl_line(line)</code>","text":"<p>Parse a single JSONL line into a dictionary.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def parse_jsonl_line(line: str) -&gt; Dict:\n    \"\"\"Parse a single JSONL line into a dictionary.\"\"\"\n    try:\n        return json.loads(line.strip())\n    except json.JSONDecodeError as e:\n        logger.error(f\"Error parsing JSONL line: {e}\")\n        return {}\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.process_jsonl_content","title":"<code>process_jsonl_content(lines)</code>","text":"<p>Process all JSONL content into SRT format.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def process_jsonl_content(lines: List[str]) -&gt; str:\n    \"\"\"Process all JSONL content into SRT format.\"\"\"\n    all_entries = []\n    entry_index = 1\n    accumulated_time = 0.0  # Track total duration of processed chunks\n\n    for line in lines:\n        entries, entry_index, chunk_duration = process_jsonl_line(\n            line, entry_index, accumulated_time)\n        all_entries.extend(entries)\n\n        # Add this chunk's duration to accumulated time\n        accumulated_time += chunk_duration  \n\n    return \"\\n\".join(all_entries)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.process_jsonl_line","title":"<code>process_jsonl_line(line, entry_index)</code>","text":"<p>Process a single JSONL line into SRT entries.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def process_jsonl_line(line: str, entry_index: int) -&gt; Tuple[List[str], int]:\n    \"\"\"Process a single JSONL line into SRT entries.\"\"\"\n    data = parse_jsonl_line(line)\n    if not data:\n        return [], entry_index\n\n    segments = get_segments_from_data(data)\n    return process_segments_list(segments, entry_index)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.process_segment","title":"<code>process_segment(segment, entry_index)</code>","text":"<p>Process a single segment into SRT format.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def process_segment(segment: Dict, entry_index: int) -&gt; Tuple[str, int]:\n    \"\"\"Process a single segment into SRT format.\"\"\"\n    start, end, text = extract_segment_data(segment)\n\n    if not text:\n        return \"\", entry_index\n\n    entry = format_srt_entry(entry_index, start, end, text)\n    return entry, entry_index + 1\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.process_segments_list","title":"<code>process_segments_list(segments_list, entry_index)</code>","text":"<p>Process a list of segments into SRT entries.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def process_segments_list(segments_list: List[Dict], \n                          entry_index: int) -&gt; Tuple[List[str], int]:\n    \"\"\"Process a list of segments into SRT entries.\"\"\"\n    entries = []\n\n    for segment in segments_list:\n        entry, entry_index = process_segment(segment, entry_index)\n        if entry:\n            entries.append(entry)\n\n    return entries, entry_index\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.json_to_srt.json_to_srt1.read_input_lines","title":"<code>read_input_lines(input_file)</code>","text":"<p>Read and filter input lines from file.</p> Source code in <code>src/tnh_scholar/cli_tools/json_to_srt/json_to_srt1.py</code> <pre><code>def read_input_lines(input_file: TextIO) -&gt; List[str]:\n    \"\"\"Read and filter input lines from file.\"\"\"\n    return [line.strip() for line in input_file if line.strip()]\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.nfmt","title":"<code>nfmt</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.nfmt.nfmt","title":"<code>nfmt</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.nfmt.nfmt.main","title":"<code>main()</code>","text":"<p>Entry point for the nfmt CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/nfmt/nfmt.py</code> <pre><code>def main():\n    \"\"\"Entry point for the nfmt CLI tool.\"\"\"\n    nfmt()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.nfmt.nfmt.nfmt","title":"<code>nfmt(input_file, output, spacing)</code>","text":"<p>Normalize the number of newlines in a text file.</p> Source code in <code>src/tnh_scholar/cli_tools/nfmt/nfmt.py</code> <pre><code>@click.command()\n@click.argument(\"input_file\", type=click.File(\"r\"), default=\"-\")\n@click.option(\n    \"-o\",\n    \"--output\",\n    type=click.File(\"w\"),\n    default=\"-\",\n    help=\"Output file (default: stdout)\",\n)\n@click.option(\n    \"-s\", \"--spacing\", default=2, help=\"Number of newlines between blocks (default: 2)\"\n)\ndef nfmt(input_file, output, spacing):\n    \"\"\"Normalize the number of newlines in a text file.\"\"\"\n    text = input_file.read()\n    result = normalize_newlines(text, spacing)\n    output.write(result)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split","title":"<code>sent_split</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split","title":"<code>sent_split</code>","text":"<p>Simple CLI tool for sentence splitting.</p> <p>This module provides a command line interface for splitting text into sentences. Uses NLTK for robust sentence tokenization. Reads from stdin and writes to stdout by default, with optional file input/output.</p>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.SplitConfig","title":"<code>SplitConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>class SplitConfig(BaseModel):\n    separator: Literal[\"space\", \"newline\"] = \"newline\"\n    nltk_tokenizer: str = \"punkt\"\n</code></pre> <code>nltk_tokenizer = 'punkt'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>separator = 'newline'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.SplitIOData","title":"<code>SplitIOData</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>class SplitIOData(BaseModel):\n    input_path: Optional[Path] = None\n    output_path: Optional[Path] = None\n    content: Optional[str] = None\n\n    @classmethod\n    def from_io(\n        cls, input_file: Optional[Path], output: Optional[Path]\n        ) -&gt; \"SplitIOData\":\n        return cls(input_path=input_file, output_path=output)\n\n    def get_input_content(self) -&gt; str:\n        if self.content is not None:\n            return self.content\n        return read_str_from_file(self.input_path) if self.input_path \\\n            else sys.stdin.read()\n\n    def write_output(self, result: SplitResult) -&gt; None:\n        text = result.text_object\n        output_str = str(text)\n        if self.output_path:\n            write_str_to_file(self.output_path, output_str)\n            click.echo(f\"Output written to: {self.output_path.name}\")\n            click.echo(f\"Split into {result.stats['sentence_count']} sentences.\")\n        else:\n            click.echo(output_str)\n</code></pre> <code>content = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>input_path = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>output_path = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>from_io(input_file, output)</code> <code>classmethod</code> \u00b6 Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>@classmethod\ndef from_io(\n    cls, input_file: Optional[Path], output: Optional[Path]\n    ) -&gt; \"SplitIOData\":\n    return cls(input_path=input_file, output_path=output)\n</code></pre> <code>get_input_content()</code> \u00b6 Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>def get_input_content(self) -&gt; str:\n    if self.content is not None:\n        return self.content\n    return read_str_from_file(self.input_path) if self.input_path \\\n        else sys.stdin.read()\n</code></pre> <code>write_output(result)</code> \u00b6 Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>def write_output(self, result: SplitResult) -&gt; None:\n    text = result.text_object\n    output_str = str(text)\n    if self.output_path:\n        write_str_to_file(self.output_path, output_str)\n        click.echo(f\"Output written to: {self.output_path.name}\")\n        click.echo(f\"Split into {result.stats['sentence_count']} sentences.\")\n    else:\n        click.echo(output_str)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.SplitResult","title":"<code>SplitResult</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>@dataclass\nclass SplitResult:\n    text_object: TextObject\n    stats: Dict[str, Any] = {}\n</code></pre> <code>stats = {}</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>text_object</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.ensure_nltk_data","title":"<code>ensure_nltk_data(config)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>def ensure_nltk_data(config: SplitConfig) -&gt; None:\n    try:\n        nltk.data.find(f'tokenizers/{config.nltk_tokenizer}')\n    except LookupError:\n        try:\n            nltk.download(config.nltk_tokenizer, quiet=True)\n            nltk.data.find(f'tokenizers/{config.nltk_tokenizer}')\n        except Exception as e:\n            raise RuntimeError(\n                f\"Failed to download required NLTK data. \"\n                f\"Please run 'python -m nltk.downloader {config.nltk_tokenizer}' \"\n                f\"to install manually. Error: {e}\"\n            ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.main","title":"<code>main()</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>def main():\n    sent_split()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.sent_split","title":"<code>sent_split(input_file, output, space)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>@click.command()\n@click.argument(\n    \"input_file\", type=click.Path(exists=True, path_type=Path), required=False\n    )\n@click.option('-o', '--output', type=click.Path(path_type=Path), required=False,\n              help='Output file (default: stdout)')\n@click.option('-s', '--space', is_flag=True,\n              help='Separate sentences with spaces instead of newlines')\ndef sent_split(input_file: Optional[Path],\n               output: Optional[Path],\n               space: bool) -&gt; None:\n    try:\n        io_data = SplitIOData.from_io(input_file, output)\n        config = SplitConfig(separator=\"space\" if space else \"newline\")\n\n        input_text = io_data.get_input_content()\n        text = TextObject.from_str(input_text)\n\n        result = split_text(text, config, io_data)\n        io_data.write_output(result)\n\n    except Exception as e:\n        click.echo(f\"Error processing text: {e}\", err=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split.split_text","title":"<code>split_text(text, config, io_data)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split.py</code> <pre><code>def split_text(\n    text: TextObject, config: SplitConfig, io_data: SplitIOData\n    ) -&gt; SplitResult:\n    ensure_nltk_data(config)\n    sentences = sent_tokenize(text.content)\n\n    separator = \"\\n\" if config.separator == \"newline\" else \" \"\n    new_content = separator.join(sentences)\n\n    text.transform(\n        data_str=new_content,\n        process_metadata=ProcessMetadata(\n            step=\"split_text\",\n            processor=\"NLTK\",\n            tool=\"sent-split\",\n            source_file=io_data.input_path or None,\n        )\n    )\n\n    return SplitResult(\n        text_object=text,\n        stats={\"sentence_count\": len(sentences)}\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split_bak","title":"<code>sent_split_bak</code>","text":"<p>Simple CLI tool for sentence splitting.</p> <p>This module provides a command line interface for splitting text into sentences. Uses NLTK for robust sentence tokenization. Reads from stdin and writes to stdout by default, with optional file input/output.</p>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split_bak.ensure_nltk_data","title":"<code>ensure_nltk_data()</code>","text":"<p>Ensure NLTK punkt tokenizer is available.</p> Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split_bak.py</code> <pre><code>def ensure_nltk_data():\n    \"\"\"Ensure NLTK punkt tokenizer is available.\"\"\"\n    try:\n        # Try to find the resource\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        # If not found, try downloading\n        try:\n            nltk.download('punkt', quiet=True)\n            # Verify download\n            nltk.data.find('tokenizers/punkt')\n        except Exception as e:\n            raise RuntimeError(\n                \"Failed to download required NLTK data. \"\n                \"Please run 'python -m nltk.downloader punkt' \"\n                f\"to install manually. Error: {e}\"\n            ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split_bak.main","title":"<code>main()</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split_bak.py</code> <pre><code>def main():\n    sent_split()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split_bak.process_text","title":"<code>process_text(text, newline=True)</code>","text":"<p>Split text into sentences using NLTK.</p> Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split_bak.py</code> <pre><code>def process_text(text: TextObject, newline: bool = True) -&gt; None:\n    \"\"\"Split text into sentences using NLTK.\"\"\"\n    ensure_nltk_data()\n    sentences = sent_tokenize(text.content)\n\n    new_content = \"\\n\".join(sentences) if newline else \" \".join(sentences)\n    text.transform(data_str=new_content)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.sent_split.sent_split_bak.sent_split","title":"<code>sent_split(input_file, output, space)</code>","text":"<p>Split text into sentences using NLTK's sentence tokenizer.</p> <p>Reads from stdin if no input file is specified. Writes to stdout if no output file is specified.</p> Source code in <code>src/tnh_scholar/cli_tools/sent_split/sent_split_bak.py</code> <pre><code>@click.command()\n@click.argument(\n    \"input_file\", type=click.Path(exists=True, path_type=Path), required=False\n    )\n@click.option('-o', '--output', type=click.Path(path_type=Path), required=False,\n              help='Output file (default: stdout)')\n@click.option('-s', '--space', is_flag=True,\n              help='Separate sentences with spaces instead of newlines')\ndef sent_split(input_file: Optional[Path],\n               output: Optional[Path],\n               space: bool) -&gt; None:\n    \"\"\"Split text into sentences using NLTK's sentence tokenizer.\n\n    Reads from stdin if no input file is specified.\n    Writes to stdout if no output file is specified.\n    \"\"\"\n    try:\n        # Read from file or stdin\n        input_text = read_str_from_file(input_file) if input_file else sys.stdin.read()\n\n        # Process the text\n        text = TextObject.from_str(input_text)\n        process_text(text, newline=not space)\n\n        process_metadata = ProcessMetadata(\n            step=\"sentence-split\",\n            processor=\"NLTK\", \n        )\n        if input_file:\n            process_metadata.update({\"source_file\": path_as_str(input_file)})\n\n        text.transform(process_metadata=process_metadata)\n\n        # Write to file or stdout\n        if output:\n            write_str_to_file(output, str(text))\n        else:\n            click.echo(text)\n\n        if output:\n            click.echo(f\"Output written to: {output.name}\")\n\n    except Exception as e:\n        click.echo(f\"Error processing text: {e}\", err=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.srt_translate","title":"<code>srt_translate</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.srt_translate.__all__","title":"<code>__all__ = ['main', 'srt_translate']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.srt_translate.main","title":"<code>main()</code>","text":"<p>Entry point for the srt-translate CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def main():\n    \"\"\"Entry point for the srt-translate CLI tool.\"\"\"\n    srt_translate()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.srt_translate.srt_translate","title":"<code>srt_translate</code>","text":"<p>CLI tool for translating SRT subtitle files using tnh-scholar line translation.</p> <p>This module provides a command line interface for translating SRT subtitle files from one language to another while preserving timecodes and subtitle structure. Uses the same translation engine as tnh-fab translate.</p>"},{"location":"api/#tnh_scholar.cli_tools.srt_translate.srt_translate.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.srt_translate.srt_translate.SrtEntry","title":"<code>SrtEntry</code>","text":"<p>Represents a single subtitle entry from an SRT file.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>class SrtEntry:\n    \"\"\"Represents a single subtitle entry from an SRT file.\"\"\"\n\n    def __init__(self, index: int, start_time: str, end_time: str, text: str):\n        \"\"\"Initialize subtitle entry with timing and text.\"\"\"\n        self.index = index\n        self.start_time = start_time\n        self.end_time = end_time\n        self.text = text.strip()\n\n    def __str__(self) -&gt; str:\n        \"\"\"Format entry as SRT text.\"\"\"\n        return f\"{self.index}\\n{self.start_time} --&gt; {self.end_time}\\n{self.text}\\n\"\n\n    @property\n    def line_key(self) -&gt; str:\n        \"\"\"Generate a unique line key for this entry.\"\"\"\n        return f\"{self.index}\"\n</code></pre> <code>end_time = end_time</code> <code>instance-attribute</code> \u00b6 <code>index = index</code> <code>instance-attribute</code> \u00b6 <code>line_key</code> <code>property</code> \u00b6 <p>Generate a unique line key for this entry.</p> <code>start_time = start_time</code> <code>instance-attribute</code> \u00b6 <code>text = text.strip()</code> <code>instance-attribute</code> \u00b6 <code>__init__(index, start_time, end_time, text)</code> \u00b6 <p>Initialize subtitle entry with timing and text.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def __init__(self, index: int, start_time: str, end_time: str, text: str):\n    \"\"\"Initialize subtitle entry with timing and text.\"\"\"\n    self.index = index\n    self.start_time = start_time\n    self.end_time = end_time\n    self.text = text.strip()\n</code></pre> <code>__str__()</code> \u00b6 <p>Format entry as SRT text.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Format entry as SRT text.\"\"\"\n    return f\"{self.index}\\n{self.start_time} --&gt; {self.end_time}\\n{self.text}\\n\"\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.srt_translate.srt_translate.SrtTranslator","title":"<code>SrtTranslator</code>","text":"<p>Translates SRT files while preserving timecodes.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>class SrtTranslator:\n    \"\"\"Translates SRT files while preserving timecodes.\"\"\"\n\n    def __init__(self, \n                 source_language: Optional[str] = None,\n                 target_language: str = \"en\",\n                 pattern: Optional[Prompt] = None,\n                 model: Optional[str] = None,\n                 metadata: Optional[Metadata] = None):\n        \"\"\"Initialize translator with language, model settings, and metadata.\"\"\"\n        self.source_language = source_language\n        self.target_language = target_language\n        self.pattern = pattern\n        self.model = model\n        self.metadata = metadata\n\n    def parse_srt(self, content: str) -&gt; List[SrtEntry]:\n        \"\"\"Parse SRT content into structured entries.\"\"\"\n        # Pattern matches: index, start time, end time, and multiline text\n        pattern = r\"(\\d+)\\r?\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --&gt; (\\d{2}:\\d{2}:\\d{2},\\d{3})\\r?\\n((?:.+(?:\\r?\\n))+)(?:\\r?\\n)?\"  # noqa: E501\n        matches = re.findall(pattern, content, re.MULTILINE)\n\n        entries = []\n        for match in matches:\n            index = int(match[0])\n            start_time = match[1]\n            end_time = match[2]\n            text = match[3].strip()\n            entries.append(SrtEntry(index, start_time, end_time, text))\n\n        logger.info(f\"Parsed {len(entries)} subtitle entries\")\n        return entries\n\n    def entries_to_numbered_text(self, entries: List[SrtEntry]) -&gt; str:\n        \"\"\"Convert SRT entries to numbered text for TextObject.\"\"\"\n        lines = []\n        lines.extend(f\"{entry.text}\" for entry in entries)\n        return \"\\n\".join(lines)\n\n    def create_text_object(self, text: str) -&gt; TextObject:\n        \"\"\"Create a TextObject from the extracted SRT text with metadata.\"\"\"\n        return TextObject.from_str(\n            text, language=self.source_language, metadata=self.metadata\n            )\n\n    def translate_text_object(self, text_object: TextObject) -&gt; TextObject:\n        \"\"\"Translate the TextObject using line translation.\"\"\"\n        text_obj = translate_text_by_lines(\n            text_object,\n            source_language=self.source_language,\n            target_language=self.target_language,\n            pattern=self.pattern,\n            model=self.model\n        )\n        logger.debug(\"Text generated: \\n\"\n                      f\"{text_obj}\")\n        return text_obj\n\n    def extract_translated_lines(self, translated_object: TextObject) -&gt; Dict[str, str]:\n        \"\"\"Extract translated lines from TextObject with line keys.\"\"\"\n        # Get the properly numbered content instead of raw content\n        numbered_translation = translated_object.numbered_content\n        logger.debug(f\"Numbered translated text sample \"\n                     f\":\\n{numbered_translation[:500]}...\")\n\n        # Pattern matches line numbers and their text, \n        # accounting for the numbering format.\n        # This depends on a consistent pattern for the lines.\n        # This pattern will match the format like \"1: Translated text\"\n        pattern = rf\"(\\d+){re.escape(translated_object.num_text.separator)}(.*)\"\n\n        translations = {}\n        for line in numbered_translation.splitlines():\n            if match := re.match(pattern, line):\n                line_key = match[1]\n                text = match[2].strip()\n                translations[line_key] = text\n                logger.debug(f\"Found translation for key {line_key}: {text[:50]}...\")\n\n        logger.debug(f\"Extracted {len(translations)} translations\")\n        return translations\n\n    def update_entries_with_translations(self, \n                                        entries: List[SrtEntry], \n                                        translations: Dict[str, str]) -&gt; List[SrtEntry]:\n        \"\"\"Apply translations to original entries.\"\"\"\n        updated_entries = []\n        for entry in entries:\n            # Look up translation by line key\n            if entry.line_key in translations:\n                entry.text = translations[entry.line_key]\n            updated_entries.append(entry)\n\n        return updated_entries\n\n    def format_srt(self, entries: List[SrtEntry]) -&gt; str:\n        \"\"\"Format entries back to SRT content.\"\"\"\n        return \"\\n\".join(str(entry) for entry in entries)\n\n    def translate_srt(self, content: str) -&gt; str:\n        \"\"\"Process SRT content through complete translation pipeline.\"\"\"\n        entries = self.parse_srt(content)\n        numbered_text = self.entries_to_numbered_text(entries)\n        text_object = self.create_text_object(numbered_text)\n\n        logger.info(f\"Translating from {self.source_language or 'auto-detected'} \"\n                    f\"to {self.target_language}\")\n        translated_object = self.translate_text_object(text_object)\n\n        translations = self.extract_translated_lines(translated_object)\n        updated_entries = self.update_entries_with_translations(entries, translations)\n\n        return self.format_srt(updated_entries)\n\n    def translate_and_save(self, input_file: Path, output_path: Path):\n        \"\"\"Handles file reading, translation, and saving.\"\"\"\n\n        content = read_str_from_file(input_file)\n        logger.info(f\"Reading SRT file: {input_file}\")\n\n        translated_content = self.translate_srt(content)\n\n        write_str_to_file(output_path, translated_content, overwrite=True)\n        logger.info(f\"Translated SRT written to: {output_path}\")\n</code></pre> <code>metadata = metadata</code> <code>instance-attribute</code> \u00b6 <code>model = model</code> <code>instance-attribute</code> \u00b6 <code>pattern = pattern</code> <code>instance-attribute</code> \u00b6 <code>source_language = source_language</code> <code>instance-attribute</code> \u00b6 <code>target_language = target_language</code> <code>instance-attribute</code> \u00b6 <code>__init__(source_language=None, target_language='en', pattern=None, model=None, metadata=None)</code> \u00b6 <p>Initialize translator with language, model settings, and metadata.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def __init__(self, \n             source_language: Optional[str] = None,\n             target_language: str = \"en\",\n             pattern: Optional[Prompt] = None,\n             model: Optional[str] = None,\n             metadata: Optional[Metadata] = None):\n    \"\"\"Initialize translator with language, model settings, and metadata.\"\"\"\n    self.source_language = source_language\n    self.target_language = target_language\n    self.pattern = pattern\n    self.model = model\n    self.metadata = metadata\n</code></pre> <code>create_text_object(text)</code> \u00b6 <p>Create a TextObject from the extracted SRT text with metadata.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def create_text_object(self, text: str) -&gt; TextObject:\n    \"\"\"Create a TextObject from the extracted SRT text with metadata.\"\"\"\n    return TextObject.from_str(\n        text, language=self.source_language, metadata=self.metadata\n        )\n</code></pre> <code>entries_to_numbered_text(entries)</code> \u00b6 <p>Convert SRT entries to numbered text for TextObject.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def entries_to_numbered_text(self, entries: List[SrtEntry]) -&gt; str:\n    \"\"\"Convert SRT entries to numbered text for TextObject.\"\"\"\n    lines = []\n    lines.extend(f\"{entry.text}\" for entry in entries)\n    return \"\\n\".join(lines)\n</code></pre> <code>extract_translated_lines(translated_object)</code> \u00b6 <p>Extract translated lines from TextObject with line keys.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def extract_translated_lines(self, translated_object: TextObject) -&gt; Dict[str, str]:\n    \"\"\"Extract translated lines from TextObject with line keys.\"\"\"\n    # Get the properly numbered content instead of raw content\n    numbered_translation = translated_object.numbered_content\n    logger.debug(f\"Numbered translated text sample \"\n                 f\":\\n{numbered_translation[:500]}...\")\n\n    # Pattern matches line numbers and their text, \n    # accounting for the numbering format.\n    # This depends on a consistent pattern for the lines.\n    # This pattern will match the format like \"1: Translated text\"\n    pattern = rf\"(\\d+){re.escape(translated_object.num_text.separator)}(.*)\"\n\n    translations = {}\n    for line in numbered_translation.splitlines():\n        if match := re.match(pattern, line):\n            line_key = match[1]\n            text = match[2].strip()\n            translations[line_key] = text\n            logger.debug(f\"Found translation for key {line_key}: {text[:50]}...\")\n\n    logger.debug(f\"Extracted {len(translations)} translations\")\n    return translations\n</code></pre> <code>format_srt(entries)</code> \u00b6 <p>Format entries back to SRT content.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def format_srt(self, entries: List[SrtEntry]) -&gt; str:\n    \"\"\"Format entries back to SRT content.\"\"\"\n    return \"\\n\".join(str(entry) for entry in entries)\n</code></pre> <code>parse_srt(content)</code> \u00b6 <p>Parse SRT content into structured entries.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def parse_srt(self, content: str) -&gt; List[SrtEntry]:\n    \"\"\"Parse SRT content into structured entries.\"\"\"\n    # Pattern matches: index, start time, end time, and multiline text\n    pattern = r\"(\\d+)\\r?\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --&gt; (\\d{2}:\\d{2}:\\d{2},\\d{3})\\r?\\n((?:.+(?:\\r?\\n))+)(?:\\r?\\n)?\"  # noqa: E501\n    matches = re.findall(pattern, content, re.MULTILINE)\n\n    entries = []\n    for match in matches:\n        index = int(match[0])\n        start_time = match[1]\n        end_time = match[2]\n        text = match[3].strip()\n        entries.append(SrtEntry(index, start_time, end_time, text))\n\n    logger.info(f\"Parsed {len(entries)} subtitle entries\")\n    return entries\n</code></pre> <code>translate_and_save(input_file, output_path)</code> \u00b6 <p>Handles file reading, translation, and saving.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def translate_and_save(self, input_file: Path, output_path: Path):\n    \"\"\"Handles file reading, translation, and saving.\"\"\"\n\n    content = read_str_from_file(input_file)\n    logger.info(f\"Reading SRT file: {input_file}\")\n\n    translated_content = self.translate_srt(content)\n\n    write_str_to_file(output_path, translated_content, overwrite=True)\n    logger.info(f\"Translated SRT written to: {output_path}\")\n</code></pre> <code>translate_srt(content)</code> \u00b6 <p>Process SRT content through complete translation pipeline.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def translate_srt(self, content: str) -&gt; str:\n    \"\"\"Process SRT content through complete translation pipeline.\"\"\"\n    entries = self.parse_srt(content)\n    numbered_text = self.entries_to_numbered_text(entries)\n    text_object = self.create_text_object(numbered_text)\n\n    logger.info(f\"Translating from {self.source_language or 'auto-detected'} \"\n                f\"to {self.target_language}\")\n    translated_object = self.translate_text_object(text_object)\n\n    translations = self.extract_translated_lines(translated_object)\n    updated_entries = self.update_entries_with_translations(entries, translations)\n\n    return self.format_srt(updated_entries)\n</code></pre> <code>translate_text_object(text_object)</code> \u00b6 <p>Translate the TextObject using line translation.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def translate_text_object(self, text_object: TextObject) -&gt; TextObject:\n    \"\"\"Translate the TextObject using line translation.\"\"\"\n    text_obj = translate_text_by_lines(\n        text_object,\n        source_language=self.source_language,\n        target_language=self.target_language,\n        pattern=self.pattern,\n        model=self.model\n    )\n    logger.debug(\"Text generated: \\n\"\n                  f\"{text_obj}\")\n    return text_obj\n</code></pre> <code>update_entries_with_translations(entries, translations)</code> \u00b6 <p>Apply translations to original entries.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def update_entries_with_translations(self, \n                                    entries: List[SrtEntry], \n                                    translations: Dict[str, str]) -&gt; List[SrtEntry]:\n    \"\"\"Apply translations to original entries.\"\"\"\n    updated_entries = []\n    for entry in entries:\n        # Look up translation by line key\n        if entry.line_key in translations:\n            entry.text = translations[entry.line_key]\n        updated_entries.append(entry)\n\n    return updated_entries\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.srt_translate.srt_translate.load_metadata_from_file","title":"<code>load_metadata_from_file(metadata_file)</code>","text":"<p>Load metadata from a file if provided.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def load_metadata_from_file(metadata_file: Optional[Path]) -&gt; Optional[Metadata]:\n    \"\"\"Load metadata from a file if provided.\"\"\"\n    if not metadata_file:\n        return None\n\n    try:\n        metadata, _ = Frontmatter.extract_from_file(metadata_file)\n        logger.info(f\"Loaded metadata from {metadata_file}\")\n        return metadata\n    except FileNotFoundError:\n        logger.error(f\"Metadata file not found: {metadata_file}\")\n        exit(1)\n    except Exception as e:\n        logger.error(f\"Failed to load metadata from {metadata_file}: {e}\")\n        exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.srt_translate.srt_translate.main","title":"<code>main()</code>","text":"<p>Entry point for the srt-translate CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def main():\n    \"\"\"Entry point for the srt-translate CLI tool.\"\"\"\n    srt_translate()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.srt_translate.srt_translate.set_output_path","title":"<code>set_output_path(input_file, output, target_language)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def set_output_path(input_file: Path, output: Optional[Path], target_language):\n    if not output:\n        lang_suffix = target_language\n        return input_file.with_stem(f\"{input_file.stem}_{lang_suffix}\")\n    return output\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.srt_translate.srt_translate.set_pattern","title":"<code>set_pattern(pattern)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>def set_pattern(pattern: Optional[str]):\n    pattern_obj = None\n    if pattern:\n        try:\n            pattern_obj = get_pattern(pattern)\n        except Exception as e:\n            logger.error(f\"Failed to load pattern '{pattern}': {e}\")\n            sys.exit(1)\n    return pattern_obj   \n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.srt_translate.srt_translate.srt_translate","title":"<code>srt_translate(input_file, output=None, source_language=None, target_language='en', model=None, pattern=None, debug=False, metadata=None)</code>","text":"<p>Translate SRT subtitle files from one language to another.</p> <p>INPUT_FILE is the path to the SRT file to translate.</p> Source code in <code>src/tnh_scholar/cli_tools/srt_translate/srt_translate.py</code> <pre><code>@click.command()\n@click.argument(\"input_file\", type=click.Path(exists=True, path_type=Path))\n@click.option(\"-o\", \"--output\", type=click.Path(path_type=Path),\n              help=\"Output file path (default: adds language suffix to input filename)\")\n@click.option(\"-s\", \"--source-language\", \n              help=\"Source language code (auto-detected if not specified)\")\n@click.option(\"-t\", \"--target-language\", default=\"en\", \n              help=\"Target language code (default: en)\")\n@click.option(\"-m\", \"--model\", help=\"Optional model name to use for translation\")\n@click.option(\"-p\", \"--pattern\", help=\"Optional translation pattern name\")\n@click.option(\"-g\", \"--debug\", is_flag=True, help=\"Option to show debug output.\")\n@click.option(\"-d\", \"--metadata\", type=click.Path(exists=True, path_type=Path),\n              help=\"Path to file with YAML metadata as frontmatter, \"\n                   \"providing translation context\")\ndef srt_translate(\n    input_file: Path,\n    output: Optional[Path] = None,\n    source_language: Optional[str] = None,\n    target_language: str = \"en\",\n    model: Optional[str] = None,\n    pattern: Optional[str] = None,\n    debug: Optional[bool] = False,\n    metadata: Optional[Path] = None,\n) -&gt; None:\n    \"\"\"\n    Translate SRT subtitle files from one language to another.\n\n    INPUT_FILE is the path to the SRT file to translate.\n    \"\"\"\n\n    if debug:\n        setup_logging(log_level=logging.DEBUG)\n    else:\n        setup_logging()\n\n    try:\n        output_path = set_output_path(input_file, output, target_language)\n        pattern_obj = set_pattern(pattern)\n        if metadata_obj := load_metadata_from_file(metadata):\n            logger.info(f\"Using metadata for translation context from: {metadata}\")\n\n\n        translator = SrtTranslator(\n            source_language=source_language,\n            target_language=target_language,\n            pattern=pattern_obj,\n            model=model,\n            metadata=metadata_obj,\n        )\n\n        translator.translate_and_save(input_file, output_path)\n\n    except Exception as e:\n        logger.error(f\"Error translating SRT: {e}\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab","title":"<code>tnh_fab</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab","title":"<code>tnh_fab</code>","text":"<p>TNH-FAB Command Line Interface</p> <p>Part of the THICH NHAT HANH SCHOLAR (TNH_SCHOLAR) project. A rapid prototype implementation of the TNH-FAB command-line tool  for Open AI based text processing. Provides core functionality for text punctuation, sectioning,  translation, and general processing.</p>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.DEFAULT_SECTION_PATTERN","title":"<code>DEFAULT_SECTION_PATTERN = 'default_section'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.DEFAULT_TRANSLATE_PATTERN","title":"<code>DEFAULT_TRANSLATE_PATTERN = 'default_line_translate'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.pass_config","title":"<code>pass_config = click.make_pass_decorator(TNHFabConfig, ensure=True)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.TNHFabConfig","title":"<code>TNHFabConfig</code>","text":"<p>Holds configuration for the TNH-FAB CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>class TNHFabConfig:\n    \"\"\"Holds configuration for the TNH-FAB CLI tool.\"\"\"\n\n    def __init__(self):\n        self.verbose: bool = False\n        self.debug: bool = False\n        self.quiet: bool = False\n        # Initialize pattern manager with directory set in .env file or default.\n\n        load_dotenv()\n\n        if pattern_path_name := os.getenv(\"TNH_PATTERN_DIR\"):\n            pattern_dir = Path(pattern_path_name)\n            logger.debug(f\"pattern dir: {pattern_path_name}\")\n        else:\n            pattern_dir = TNH_DEFAULT_PATTERN_DIR\n\n        pattern_dir.mkdir(parents=True, exist_ok=True)\n        self.pattern_manager = PromptCatalog(pattern_dir)\n</code></pre> <code>debug = False</code> <code>instance-attribute</code> \u00b6 <code>pattern_manager = PromptCatalog(pattern_dir)</code> <code>instance-attribute</code> \u00b6 <code>quiet = False</code> <code>instance-attribute</code> \u00b6 <code>verbose = False</code> <code>instance-attribute</code> \u00b6 <code>__init__()</code> \u00b6 Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>def __init__(self):\n    self.verbose: bool = False\n    self.debug: bool = False\n    self.quiet: bool = False\n    # Initialize pattern manager with directory set in .env file or default.\n\n    load_dotenv()\n\n    if pattern_path_name := os.getenv(\"TNH_PATTERN_DIR\"):\n        pattern_dir = Path(pattern_path_name)\n        logger.debug(f\"pattern dir: {pattern_path_name}\")\n    else:\n        pattern_dir = TNH_DEFAULT_PATTERN_DIR\n\n    pattern_dir.mkdir(parents=True, exist_ok=True)\n    self.pattern_manager = PromptCatalog(pattern_dir)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.export_processed_sections","title":"<code>export_processed_sections(section_result, text_obj)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>def export_processed_sections(\n    section_result: Generator[ProcessedSection, None, None], \n    text_obj: TextObject) -&gt; None:\n    click.echo(f\"{Frontmatter.generate(text_obj.metadata)}\")\n    for processed_section in section_result:\n        click.echo(processed_section.processed_str)\n        click.echo(\"\\n\")  # newline separated output. \n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.gen_text_input","title":"<code>gen_text_input(ctx, input_file)</code>","text":"<p>Read input from file or stdin.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>def gen_text_input(ctx: Context, input_file: Optional[Path]) -&gt; TextObject:\n    \"\"\"Read input from file or stdin.\"\"\"\n    if input_file:\n        return TextObject.load(input_file)\n    if not sys.stdin.isatty():\n        return TextObject.from_str(sys.stdin.read())\n    raise UsageError(\"No input provided\")\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.get_pattern","title":"<code>get_pattern(pattern_manager, pattern_name)</code>","text":"<p>Get pattern from the pattern manager.</p> <p>Parameters:</p> Name Type Description Default <code>pattern_manager</code> <code>PromptCatalog</code> <p>Initialized PatternManager instance</p> required <code>pattern_name</code> <code>str</code> <p>Name of the pattern to load</p> required <p>Returns:</p> Name Type Description <code>Pattern</code> <code>Prompt</code> <p>Loaded pattern object</p> <p>Raises:</p> Type Description <code>ClickException</code> <p>If pattern cannot be loaded</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>def get_pattern(pattern_manager: PromptCatalog, pattern_name: str) -&gt; Prompt:\n    \"\"\"\n    Get pattern from the pattern manager.\n\n    Args:\n        pattern_manager: Initialized PatternManager instance\n        pattern_name: Name of the pattern to load\n\n    Returns:\n        Pattern: Loaded pattern object\n\n    Raises:\n        click.ClickException: If pattern cannot be loaded\n    \"\"\"\n    try:\n        return pattern_manager.load_pattern(pattern_name)\n    except FileNotFoundError as e:\n        raise click.ClickException(\n            f\"Pattern '{pattern_name}' not found in {pattern_manager.base_path}\"\n        ) from e\n    except Exception as e:\n        raise click.ClickException(f\"Error loading pattern: {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.main","title":"<code>main()</code>","text":"<p>Entry point for TNH-FAB CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>def main():\n    \"\"\"Entry point for TNH-FAB CLI tool.\"\"\"\n    tnh_fab()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.process","title":"<code>process(config, input_file, pattern, section, auto, paragraph, template)</code>","text":"<p>Apply custom pattern-based processing to text with flexible structuring options.</p> <p>This command provides flexible text processing using customizable patterns. It can process text either by sections (defined in a JSON file or auto-detected), by paragraphs, or can be used to process a text as a whole (this is the default). This is particularly useful for formatting, restructuring, or applying consistent transformations to text.</p> <p>Examples:</p> <pre><code>\b\n# Process using a specific pattern\n$ tnh-fab process -p format_xml input.txt\n\n\b\n# Process using paragraph mode\n$ tnh-fab process -p format_xml -g input.txt\n\n\b\n# Process with custom sections\n$ tnh-fab process -p format_xml -s sections.json input.txt\n\n\b\n# Process with template values\n$ tnh-fab process -p format_xml -t template.yaml input.txt\n</code></pre> <p>Processing Modes:</p> <pre><code>\b\n1. Single Input Mode (default)\n    - Processes entire input.\n\n\b\n2. Section Mode (-s):\n    - Uses sections from a JSON file\n    - Processes each section according to pattern\n\n\b\n3. Paragraph Mode (-g):\n    - Treats each line/paragraph as a separate unit\n    - Useful for simpler processing tasks\n    - More memory efficient for large files\n\n\b\n3. Auto Section Mode (-a):\n    - Automatically sections the input file \n    - Processes by section\n</code></pre> <p>\b Notes:     - Required pattern must exist in pattern directory     - Template values can customize pattern behavior</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>@tnh_fab.command()\n@click.argument(\n    \"input_file\", type=click.Path(exists=True, path_type=Path), required=False\n)\n@click.option(\"-p\", \"--pattern\", required=True, help=\"Pattern name for processing\")\n@click.option(\n    \"-s\",\n    \"--section\",\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Process using sections from JSON file.\",\n)\n@click.option(\n    \"-a\", \"--auto\", is_flag=True, help=\"Automatically generate and process by sections.\"\n)\n@click.option(\"-g\", \"--paragraph\", is_flag=True, help=\"Process text by paragraphs\")\n@click.option(\n    \"-t\",\n    \"--template\",\n    type=click.Path(exists=True, path_type=Path),\n    help=\"YAML file containing template values\",\n)\n@pass_config\ndef process(\n    config: TNHFabConfig,\n    input_file: Optional[Path],\n    pattern: str,\n    section: Optional[Path],\n    auto: bool,\n    paragraph: bool,\n    template: Optional[Path],\n):\n    \"\"\"Apply custom pattern-based processing to text with flexible structuring options.\n\n    This command provides flexible text processing using customizable patterns. It can\n    process text either by sections (defined in a JSON file or auto-detected), by\n    paragraphs, or can be used to process a text as a whole (this is the default).\n    This is particularly useful for formatting, restructuring, or applying\n    consistent transformations to text.\n\n    Examples:\n\n        \\b\n        # Process using a specific pattern\n        $ tnh-fab process -p format_xml input.txt\n\n        \\b\n        # Process using paragraph mode\n        $ tnh-fab process -p format_xml -g input.txt\n\n        \\b\n        # Process with custom sections\n        $ tnh-fab process -p format_xml -s sections.json input.txt\n\n        \\b\n        # Process with template values\n        $ tnh-fab process -p format_xml -t template.yaml input.txt\n\n\n    Processing Modes:\n\n        \\b\n        1. Single Input Mode (default)\n            - Processes entire input.\n\n        \\b\n        2. Section Mode (-s):\n            - Uses sections from a JSON file\n            - Processes each section according to pattern\n\n        \\b\n        3. Paragraph Mode (-g):\n            - Treats each line/paragraph as a separate unit\n            - Useful for simpler processing tasks\n            - More memory efficient for large files\n\n        \\b\n        3. Auto Section Mode (-a):\n            - Automatically sections the input file \n            - Processes by section\n\n    \\b\n    Notes:\n        - Required pattern must exist in pattern directory\n        - Template values can customize pattern behavior\n\n    \"\"\"\n    text_obj = gen_text_input(click, input_file)  # type: ignore\n\n    process_pattern = get_pattern(config.pattern_manager, pattern)\n\n    template_dict: Dict[str, str] = {}\n\n    if paragraph:\n        result = process_text_by_paragraphs(\n            text_obj, template_dict, pattern=process_pattern\n        )\n        export_processed_sections(result, text_obj)        \n    elif section is not None:  # Section mode (either file or auto-generate)    \n        text_obj = TextObject.from_section_file(section, text_obj.content)\n\n        result = process_text_by_sections(\n            text_obj, template_dict, pattern=process_pattern\n        )\n        export_processed_sections(result, text_obj)\n    elif auto:\n        # Auto-generate sections     \n        default_section_pattern = get_pattern(\n            config.pattern_manager, DEFAULT_SECTION_PATTERN\n        )\n        text_obj = find_sections(text_obj, section_pattern=default_section_pattern)\n\n        result = process_text_by_sections(\n            text_obj, template_dict, pattern=process_pattern\n        )\n        export_processed_sections(result, text_obj)\n\n    else:\n        result = process_text(\n            text_obj, pattern=process_pattern, template_dict=template_dict\n        )\n        click.echo(result)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.punctuate","title":"<code>punctuate(input_file, language, style, review_count, pattern)</code>","text":"<p>[DEPRECATED] Punctuation command is deprecated.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>@tnh_fab.command()\n@click.argument(\n    \"input_file\", type=click.Path(exists=True, path_type=Path), required=False\n)\n@click.option(\"-l\", \"--language\", help=\"[DEPRECATED] Source language code\")\n@click.option(\"-y\", \"--style\", help=\"[DEPRECATED] Punctuation style\")\n@click.option(\"-c\", \"--review-count\", type=int, \n              help=\"[DEPRECATED] Number of review passes\")\n@click.option(\"-p\", \"--pattern\", help=\"[DEPRECATED] Pattern name for punctuation\")\ndef punctuate(\n    input_file: Optional[Path],\n    language: Optional[str],\n    style: Optional[str],\n    review_count: Optional[int], \n    pattern: Optional[str],\n):\n    \"\"\"[DEPRECATED] Punctuation command is deprecated.\"\"\"\n    click.echo(\n        \"\\nDEPRECATED: The 'punctuate' command is deprecated.\\n\"\n        \"Please use: tnh-fab process -p &lt;punctuation_pattern&gt;\\n\\n\"\n        \"Example:\\n\"\n        \"  tnh-fab process -p default_punctuate input.txt\\n\"\n    )\n    sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.section","title":"<code>section(config, input_file, language, num_sections, review_count, pattern)</code>","text":"<p>Analyze and divide text into logical sections based on content.</p> <p>This command processes the input text to identify coherent sections based on content analysis. It generates a structured representation of the text with sections that maintain logical continuity. Each section includes metadata such as title and line range.</p> <p>Examples:</p> <pre><code>\b\n# Auto-detect sections in a file\n$ tnh-fab section input.txt\n\n\b\n# Specify desired number of sections\n$ tnh-fab section -n 5 input.txt\n\n\b\n# Process Vietnamese text with custom pattern\n$ tnh-fab section -l vi -p custom_section_pattern input.txt\n\n\b\n# Section text from stdin with increased review\n$ cat input.txt | tnh-fab section -c 5\n</code></pre> <p>\b Output Format:     JSON object containing:     - language: Detected or specified language code     - sections: Array of section objects, each with:         - title: Section title in original language         - start_line: Starting line number (inclusive)         - end_line: Ending line number (inclusive)</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>@tnh_fab.command()\n@click.argument(\n    \"input_file\", type=click.Path(exists=True, path_type=Path), required=False\n)\n@click.option(\n    \"-l\",\n    \"--language\",\n    help=\"Source language code (e.g., 'en', 'vi'). Auto-detected if not specified.\",\n)\n@click.option(\n    \"-n\",\n    \"--num-sections\",\n    type=int,\n    help=\"Target number of sections (auto-calculated if not specified)\",\n)\n@click.option(\n    \"-c\",\n    \"--review-count\",\n    type=int,\n    default=3,\n    help=\"Number of review passes (default: 3)\",\n)\n@click.option(\n    \"-p\",\n    \"--pattern\",\n    default=DEFAULT_SECTION_PATTERN,\n    help=f\"Pattern name for section analysis (default: '{DEFAULT_SECTION_PATTERN}')\",\n)\n@pass_config\ndef section(\n    config: TNHFabConfig,\n    input_file: Optional[Path],\n    language: Optional[str],\n    num_sections: Optional[int],\n    review_count: int,\n    pattern: str,\n):\n    \"\"\"Analyze and divide text into logical sections based on content.\n\n    This command processes the input text to identify coherent sections based on content\n    analysis. It generates a structured representation of the text with sections that\n    maintain logical continuity. Each section includes metadata such as title and line\n    range.\n\n    Examples:\n\n        \\b\n        # Auto-detect sections in a file\n        $ tnh-fab section input.txt\n\n        \\b\n        # Specify desired number of sections\n        $ tnh-fab section -n 5 input.txt\n\n        \\b\n        # Process Vietnamese text with custom pattern\n        $ tnh-fab section -l vi -p custom_section_pattern input.txt\n\n        \\b\n        # Section text from stdin with increased review\n        $ cat input.txt | tnh-fab section -c 5\n\n    \\b\n    Output Format:\n        JSON object containing:\n        - language: Detected or specified language code\n        - sections: Array of section objects, each with:\n            - title: Section title in original language\n            - start_line: Starting line number (inclusive)\n            - end_line: Ending line number (inclusive)\n    \"\"\"\n    input_text = gen_text_input(click, input_file)  # type: ignore\n    section_pattern = get_pattern(config.pattern_manager, pattern)\n\n    text_object = find_sections(\n        input_text,\n        section_pattern=section_pattern,\n        section_count=num_sections,\n        review_count=review_count,\n    )\n    # For prototype, just output the JSON representation\n    info = text_object.export_info(input_file)\n    click.echo(info.model_dump_json(indent=2))\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.tnh_fab","title":"<code>tnh_fab(ctx, verbose, debug, quiet)</code>","text":"<p>TNH-FAB: Thich Nhat Hanh Scholar Text processing command-line tool.</p> <p>CORE COMMANDS: punctuate, section, translate, process</p> <p>To Get help on any command and see its options:</p> <p>tnh-fab [COMMAND] --help</p> <p>Provides specialized processing for multi-lingual Dharma content.</p> <p>Offers functionalities for punctuation, sectioning, line-based translation, and general text processing based on predefined patterns. Input text can be provided either via a file or standard input.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>@click.group()\n@click.option(\"-v\", \"--verbose\", is_flag=True, \n              help=\"Enable detailed logging. (NOT implemented)\")\n@click.option(\"--debug\", is_flag=True, help=\"Enable debug output\")\n@click.option(\"--quiet\", is_flag=True, help=\"Suppress all non-error output\")\n@click.pass_context\ndef tnh_fab(ctx: Context, verbose: bool, debug: bool, quiet: bool):\n    \"\"\"TNH-FAB: Thich Nhat Hanh Scholar Text processing command-line tool.\n\n    CORE COMMANDS: punctuate, section, translate, process\n\n    To Get help on any command and see its options:\n\n    tnh-fab [COMMAND] --help\n\n    Provides specialized processing for multi-lingual Dharma content.\n\n    Offers functionalities for punctuation, sectioning, line-based translation,\n    and general text processing based on predefined patterns.\n    Input text can be provided either via a file or standard input.\n    \"\"\"        \n    config = ctx.ensure_object(TNHFabConfig)\n\n    if not check_openai_env():\n\n        raise click.ClickException(\"Missing OpenAI Credentials.\")\n\n    config.verbose = verbose\n    config.debug = debug\n    config.quiet = quiet\n\n    if not quiet:\n        if debug:\n            setup_logging(log_level=logging.DEBUG)\n        else:\n            setup_logging(log_level=logging.INFO)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_fab.tnh_fab.translate","title":"<code>translate(config, input_file, language, target, style, context_lines, segment_size, pattern)</code>","text":"<p>Translate text while preserving line numbers and contextual understanding.</p> <p>This command performs intelligent translation that maintains  line number correspondence between source and translated text.  It uses surrounding context to improve translation accuracy and consistency, particularly important for texts  where terminology and context are crucial.</p> <p>Examples:</p> <pre><code>\b\n# Translate Vietnamese text to English\n$ tnh-fab translate -l vi input.txt\n\n\b\n# Translate to French with specific style\n$ tnh-fab translate -l vi -r fr -y \"Formal\" input.txt\n\n\b\n# Translate with increased context\n$ tnh-fab translate --context-lines 5 input.txt\n\n\b\n# Translate using custom segment size\n$ tnh-fab translate --segment-size 10 input.txt\n</code></pre> <p>\b Notes:     - Line numbers are preserved in the output     - Context lines are used to improve translation accuracy     - Segment size affects processing speed and memory usage</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_fab/tnh_fab.py</code> <pre><code>@tnh_fab.command()\n@click.argument(\n    \"input_file\", type=click.Path(exists=True, path_type=Path), required=False\n)\n@click.option(\n    \"-l\", \"--language\", help=\"Source language code. Auto-detected if not specified.\"\n)\n@click.option(\n    \"-r\", \"--target\", default=\"en\", help=\"Target language code (default: 'en')\"\n)\n@click.option(\n    \"-y\", \"--style\", help=\"Translation style (e.g., 'American Dharma Teaching')\"\n)\n@click.option(\n    \"--context-lines\",\n    type=int,\n    default=3,\n    help=\"Number of context lines to consider (default: 3)\",\n)\n@click.option(\n    \"--segment-size\",\n    type=int,\n    help=\"Lines per translation segment (auto-calculated if not specified)\",\n)\n@click.option(\n    \"-p\",\n    \"--pattern\",\n    default=DEFAULT_TRANSLATE_PATTERN,\n    help=f\"Pattern name for translation (default: '{DEFAULT_TRANSLATE_PATTERN}')\",\n)\n@pass_config\ndef translate(\n    config: TNHFabConfig,\n    input_file: Optional[Path],\n    language: Optional[str],\n    target: str,\n    style: Optional[str],\n    context_lines: int,\n    segment_size: Optional[int],\n    pattern: str,\n):\n    \"\"\"Translate text while preserving line numbers and contextual understanding.\n\n    This command performs intelligent translation that maintains \n    line number correspondence between source and translated text. \n    It uses surrounding context to improve translation\n    accuracy and consistency, particularly important for texts \n    where terminology and context are crucial.\n\n    Examples:\n\n        \\b\n        # Translate Vietnamese text to English\n        $ tnh-fab translate -l vi input.txt\n\n        \\b\n        # Translate to French with specific style\n        $ tnh-fab translate -l vi -r fr -y \"Formal\" input.txt\n\n        \\b\n        # Translate with increased context\n        $ tnh-fab translate --context-lines 5 input.txt\n\n        \\b\n        # Translate using custom segment size\n        $ tnh-fab translate --segment-size 10 input.txt\n\n    \\b\n    Notes:\n        - Line numbers are preserved in the output\n        - Context lines are used to improve translation accuracy\n        - Segment size affects processing speed and memory usage\n    \"\"\"\n    text_obj = gen_text_input(click, input_file)  # type: ignore\n    translation_pattern = get_pattern(config.pattern_manager, pattern)\n\n    text_obj.update_metadata(source_file=input_file)\n\n    text_obj = translate_text_by_lines(\n        text_obj,\n        source_language=language,\n        target_language=target,\n        pattern=translation_pattern,\n        style=style,\n        context_lines=context_lines,\n        segment_size=segment_size,\n    )\n    click.echo(text_obj)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_setup","title":"<code>tnh_setup</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup","title":"<code>tnh_setup</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.OPENAI_ENV_HELP_MSG","title":"<code>OPENAI_ENV_HELP_MSG = \"\\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; OpenAI API key not found in environment. &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\\n\\nFor AI processing with TNH-scholar:\\n\\n1. Get an API key from https://platform.openai.com/api-keys\\n2. Set the OPENAI_API_KEY environment variable:\\n\\n   export OPENAI_API_KEY='your-api-key-here'  # Linux/Mac\\n   set OPENAI_API_KEY=your-api-key-here       # Windows\\n\\nFor OpenAI API access help: https://platform.openai.com/\\n\\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; -- &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\\n\"</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.PATTERNS_URL","title":"<code>PATTERNS_URL = 'https://github.com/aaronksolomon/patterns/archive/main.zip'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.create_config_dirs","title":"<code>create_config_dirs()</code>","text":"<p>Create required configuration directories.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_setup/tnh_setup.py</code> <pre><code>def create_config_dirs():\n    \"\"\"Create required configuration directories.\"\"\"\n    TNH_CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n    TNH_LOG_DIR.mkdir(exist_ok=True)\n    TNH_DEFAULT_PATTERN_DIR.mkdir(exist_ok=True)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.download_patterns","title":"<code>download_patterns()</code>","text":"<p>Download and extract pattern files from GitHub.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_setup/tnh_setup.py</code> <pre><code>def download_patterns() -&gt; bool:\n    \"\"\"Download and extract pattern files from GitHub.\"\"\"\n    try:\n        response = requests.get(PATTERNS_URL)\n        response.raise_for_status()\n\n        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n            root_dir = zip_ref.filelist[0].filename.split('/')[0]\n\n            for zip_info in zip_ref.filelist:\n                if zip_info.filename.endswith('.md'):\n                    rel_path = Path(zip_info.filename).relative_to(root_dir)\n                    target_path = TNH_DEFAULT_PATTERN_DIR / rel_path\n\n                    target_path.parent.mkdir(parents=True, exist_ok=True)\n\n                    with zip_ref.open(zip_info) as source, open(target_path, 'wb') as target:\n                        target.write(source.read())\n        return True\n\n    except Exception as e:\n        click.echo(f\"Pattern download failed: {e}\", err=True)\n        return False\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.main","title":"<code>main()</code>","text":"<p>Entry point for setup CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_setup/tnh_setup.py</code> <pre><code>def main():\n    \"\"\"Entry point for setup CLI tool.\"\"\"\n    tnh_setup()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_setup.tnh_setup.tnh_setup","title":"<code>tnh_setup(skip_env, skip_patterns)</code>","text":"<p>Set up TNH Scholar configuration.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_setup/tnh_setup.py</code> <pre><code>@click.command()\n@click.option('--skip-env', is_flag=True, help='Skip API key setup')\n@click.option('--skip-patterns', is_flag=True, help='Skip pattern download')\ndef tnh_setup(skip_env: bool, skip_patterns: bool):\n    \"\"\"Set up TNH Scholar configuration.\"\"\"\n    click.echo(\"Setting up TNH Scholar...\")\n\n    # Create config directories\n    create_config_dirs()\n    click.echo(f\"Created config directory: {TNH_CONFIG_DIR}\")\n\n    # Pattern download\n    if not skip_patterns and click.confirm(\n                \"\\nDownload pattern (markdown text) files from GitHub?\\n\"\n                f\"Source: {PATTERNS_URL}\\n\"\n                f\"Target: {TNH_DEFAULT_PATTERN_DIR}\"\n            ):\n        if download_patterns():\n            click.echo(\"Pattern files downloaded successfully\")\n        else:\n            click.echo(\"Pattern download failed\", err=True)\n\n    # Environment test:\n    if not skip_env:\n        load_dotenv()  # for development\n        if not check_openai_env(output=False):\n            print(OPENAI_ENV_HELP_MSG)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.tnh_tree","title":"<code>tnh_tree</code>","text":"<p>Developer tool for the tnh-scholar project.</p> <p>This script generates a directory tree for the entire project and for the src directory, saving the results to 'project_directory_tree.txt' and 'src_directory_tree.txt' respectively.</p> <p>Uses the generic module generate_tree which has a basic function build_tree that executes tree building.</p> <p>Exposed as a script via pyproject.toml under the name 'tnh-tree'.</p>"},{"location":"api/#tnh_scholar.cli_tools.tnh_tree.main","title":"<code>main()</code>","text":"<p>CLI entry point registered as <code>tnh-tree</code>.</p> Source code in <code>src/tnh_scholar/cli_tools/tnh_tree.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"CLI entry point registered as ``tnh-tree``.\"\"\"\n    build_tree(TNH_PROJECT_ROOT_DIR, TNH_PROJECT_ROOT_DIR / \"src\")\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.token_count","title":"<code>token_count</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.token_count.token_count","title":"<code>token_count</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.token_count.token_count.main","title":"<code>main()</code>","text":"<p>Entry point for the token-count CLI tool.</p> Source code in <code>src/tnh_scholar/cli_tools/token_count/token_count.py</code> <pre><code>def main():\n    \"\"\"Entry point for the token-count CLI tool.\"\"\"\n    token_count_cli()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.token_count.token_count.token_count_cli","title":"<code>token_count_cli(input_file)</code>","text":"<p>Return the Open AI API token count of a text file. Based on gpt-4o.</p> Source code in <code>src/tnh_scholar/cli_tools/token_count/token_count.py</code> <pre><code>@click.command()\n@click.argument(\"input_file\", type=click.File(\"r\"), default=\"-\")\ndef token_count_cli(input_file):\n    \"\"\"Return the Open AI API token count of a text file. Based on gpt-4o.\"\"\"\n    text = input_file.read()\n    result = token_count(text)\n    click.echo(result)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch","title":"<code>ytt_fetch</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.__all__","title":"<code>__all__ = ['main', 'ytt_fetch']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.main","title":"<code>main()</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>def main():\n    ytt_fetch()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch","title":"<code>ytt_fetch</code>","text":"<p>Simple CLI tool for retrieving video transcripts.</p> <p>This module provides a command line interface for downloading video transcripts in specified languages. It uses yt-dlp for video info extraction.</p>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.cleanup_files","title":"<code>cleanup_files(keep, filepath)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>def cleanup_files(keep: bool, filepath: Path) -&gt; None:\n    if not keep:\n        filepath.unlink()\n        logger.debug(f\"Removed local data file: {filepath}\")\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.export_data","title":"<code>export_data(output_path, data)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>def export_data(output_path, data):\n    if output_path:\n            write_str_to_file(output_path, data, overwrite=True)\n            click.echo(f\"Data written to: {output_path}\")\n    else:\n        click.echo(data)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.export_ttml_data","title":"<code>export_ttml_data(metadata, ttml_path, no_embed, output_path, keep)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>def export_ttml_data(\n    metadata: Metadata, \n    ttml_path: Optional[Path], \n    no_embed: bool, \n    output_path: Optional[Path], \n    keep: bool):\n    try:\n        # export transcript as text \n        if ttml_path:\n            transcript_text = extract_text_from_ttml(ttml_path)\n        else:\n            click.echo(\"Transcript Error. No ttml file found.\")\n            sys.exit(1)\n\n        if not no_embed:\n            transcript_text = Frontmatter.embed(metadata, transcript_text)\n\n        export_data(output_path, transcript_text)   \n        cleanup_files(keep, ttml_path)\n\n    except FileNotFoundError as e:\n        click.echo(f\"File not found error: {e}\", err=True)\n        sys.exit(1)\n    except (IOError, OSError) as e:\n        click.echo(f\"Error writing transcript to file: {e}\", err=True)\n        sys.exit(1)\n    except TypeError as e:\n        click.echo(f\"Type error: {e}\", err=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.generate_metadata","title":"<code>generate_metadata(dl, url, keep, output_path)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>def generate_metadata(\n    dl: DLPDownloader, \n    url: str, \n    keep: bool,\n    output_path: Optional[Path]\n    ) -&gt; None:\n    metadata = dl.get_metadata(url)\n    metadata_out = metadata.text_embed(\"\") # Only metadata\n\n    export_data(output_path, metadata_out)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.generate_transcript","title":"<code>generate_transcript(dl, url, lang, keep, no_embed, output_path)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>def generate_transcript(\n    dl: DLPDownloader, \n    url: str, \n    lang: str, \n    keep: bool, \n    no_embed: bool,\n    output_path: Optional[Path]\n    ) -&gt; None:\n\n    metadata, ttml_path = get_ttml_download(dl, url, lang, output_path)\n\n    process_metadata = ProcessMetadata(\n            step=\"generate_transcript\",\n            processor=\"DLPDownloader\",\n            tool=\"ytt-fetch\"\n            )\n    if output_path:\n        process_metadata.update(output_path=output_path)\n\n    metadata.add_process_info(process_metadata)\n\n    export_ttml_data(metadata, ttml_path, no_embed, output_path, keep)\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.get_ttml_download","title":"<code>get_ttml_download(dl, url, lang, output_path)</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>def get_ttml_download(dl, url, lang, output_path):\n    try:\n        transcript_data = dl.get_transcript(url, lang, output_path)\n        metadata = transcript_data.metadata\n        ttml_path = transcript_data.filepath\n\n    except TranscriptError as e:\n        click.echo(f\"Transcript error {e}\", err=True)\n        sys.exit(1)\n    except yt_dlp.utils.DownloadError as e:\n        click.echo(f\"Failed to extract video transcript: {e}\", err=True)\n        sys.exit(1)   \n\n    return metadata, ttml_path\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.main","title":"<code>main()</code>","text":"Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>def main():\n    ytt_fetch()\n</code></pre>"},{"location":"api/#tnh_scholar.cli_tools.ytt_fetch.ytt_fetch.ytt_fetch","title":"<code>ytt_fetch(url, lang, keep, info, no_embed, output)</code>","text":"<p>YouTube Transcript Fetch: Retrieve and  save transcripts for a Youtube video using yt-dlp.</p> Source code in <code>src/tnh_scholar/cli_tools/ytt_fetch/ytt_fetch.py</code> <pre><code>@click.command()\n@click.argument(\"url\")\n@click.option(\n    \"-l\", \"--lang\", default=\"en\", help=\"Language code for transcript (default: en)\"\n)\n@click.option(\n    \"-k\", \"--keep\",\n    is_flag=True,\n    help=\"Keep downloaded datafile: TTML transcript.\"\n)\n@click.option(\n    \"-i\", \"--info\",\n    is_flag=True,\n    help=\"Return only metadata in YAML frontmatter format.\" \n    )\n@click.option(\n    \"-n\", \"--no-embed\",\n    is_flag=True,\n    help=\"Do not embed metadata in transcript file.\"\n)\n@click.option(\n    \"-o\",\n    \"--output\",\n    type=click.Path(),\n    help=\"Save transcript text to file instead of printing.\",\n)\ndef ytt_fetch(\n    url: str, \n    lang: str, \n    keep: bool, \n    info: bool,\n    no_embed: bool,\n    output: Optional[str]) -&gt; None:\n    \"\"\"\n    YouTube Transcript Fetch: Retrieve and \n    save transcripts for a Youtube video using yt-dlp.\n    \"\"\"\n\n    dl = DLPDownloader()\n\n    output_path = Path(output) if output else None\n\n    if not info:  \n        generate_transcript(dl, url, lang, keep, no_embed, output_path)\n    else:\n        generate_metadata(dl, url, keep, output_path)\n</code></pre>"},{"location":"api/#tnh_scholar.exceptions","title":"<code>exceptions</code>","text":""},{"location":"api/#tnh_scholar.exceptions.__all__","title":"<code>__all__ = ['TnhScholarError', 'ConfigurationError', 'ValidationError', 'ExternalServiceError', 'RateLimitError', 'NotRetryable']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>TnhScholarError</code></p> <p>Configuration-related errors (missing env vars, invalid settings, etc.).</p> Source code in <code>src/tnh_scholar/exceptions.py</code> <pre><code>class ConfigurationError(TnhScholarError):\n    \"\"\"Configuration-related errors (missing env vars, invalid settings, etc.).\"\"\"\n</code></pre>"},{"location":"api/#tnh_scholar.exceptions.ExternalServiceError","title":"<code>ExternalServiceError</code>","text":"<p>               Bases: <code>TnhScholarError</code></p> <p>Upstream/provider errors (HTTP 5xx, transport, transient provider issues).</p> Source code in <code>src/tnh_scholar/exceptions.py</code> <pre><code>class ExternalServiceError(TnhScholarError):\n    \"\"\"Upstream/provider errors (HTTP 5xx, transport, transient provider issues).\"\"\"\n</code></pre>"},{"location":"api/#tnh_scholar.exceptions.NotRetryable","title":"<code>NotRetryable</code>","text":"<p>               Bases: <code>TnhScholarError</code></p> <p>Marker for errors where retry is known to be pointless (e.g., bad auth).</p> Source code in <code>src/tnh_scholar/exceptions.py</code> <pre><code>class NotRetryable(TnhScholarError):\n    \"\"\"Marker for errors where retry is known to be pointless (e.g., bad auth).\"\"\"\n</code></pre>"},{"location":"api/#tnh_scholar.exceptions.RateLimitError","title":"<code>RateLimitError</code>","text":"<p>               Bases: <code>ExternalServiceError</code></p> <p>Upstream rate limits; typically retryable after a backoff.</p> Source code in <code>src/tnh_scholar/exceptions.py</code> <pre><code>class RateLimitError(ExternalServiceError):\n    \"\"\"Upstream rate limits; typically retryable after a backoff.\"\"\"\n</code></pre>"},{"location":"api/#tnh_scholar.exceptions.TnhScholarError","title":"<code>TnhScholarError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all tnh_scholar errors.</p> <p>Attributes:</p> Name Type Description <code>message</code> <p>Human-readable summary.</p> <code>context</code> <p>Optional structured context to aid logging/diagnostics.      Keep this JSON-serializable.</p> <code>cause</code> <p>Optional underlying exception.</p> Source code in <code>src/tnh_scholar/exceptions.py</code> <pre><code>class TnhScholarError(Exception):\n    \"\"\"Base exception for all tnh_scholar errors.\n\n    Attributes:\n        message: Human-readable summary.\n        context: Optional structured context to aid logging/diagnostics.\n                 Keep this JSON-serializable.\n        cause:   Optional underlying exception.\n    \"\"\"\n    def __init__(\n        self,\n        message: str = \"\",\n        *,\n        context: Optional[Mapping[str, Any]] = None,\n        cause: Optional[BaseException] = None,\n    ) -&gt; None:\n        super().__init__(message)\n        self.message = message\n        self.context = dict(context) if context else {}\n        self.__cause__ = cause  # preserves exception chaining\n\n    def __str__(self) -&gt; str:\n        return self.message or self.__class__.__name__\n</code></pre>"},{"location":"api/#tnh_scholar.exceptions.TnhScholarError.__cause__","title":"<code>__cause__ = cause</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.exceptions.TnhScholarError.context","title":"<code>context = dict(context) if context else {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.exceptions.TnhScholarError.message","title":"<code>message = message</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.exceptions.TnhScholarError.__init__","title":"<code>__init__(message='', *, context=None, cause=None)</code>","text":"Source code in <code>src/tnh_scholar/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str = \"\",\n    *,\n    context: Optional[Mapping[str, Any]] = None,\n    cause: Optional[BaseException] = None,\n) -&gt; None:\n    super().__init__(message)\n    self.message = message\n    self.context = dict(context) if context else {}\n    self.__cause__ = cause  # preserves exception chaining\n</code></pre>"},{"location":"api/#tnh_scholar.exceptions.TnhScholarError.__str__","title":"<code>__str__()</code>","text":"Source code in <code>src/tnh_scholar/exceptions.py</code> <pre><code>def __str__(self) -&gt; str:\n    return self.message or self.__class__.__name__\n</code></pre>"},{"location":"api/#tnh_scholar.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>TnhScholarError</code></p> <p>Input/data validation errors (precondition failures before calling providers).</p> Source code in <code>src/tnh_scholar/exceptions.py</code> <pre><code>class ValidationError(TnhScholarError):\n    \"\"\"Input/data validation errors (precondition failures before calling providers).\"\"\"\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing","title":"<code>journal_processing</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process","title":"<code>journal_process</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.BATCH_RETRY_DELAY","title":"<code>BATCH_RETRY_DELAY = 5</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.DEFAULT_JOURNAL_MODEL","title":"<code>DEFAULT_JOURNAL_MODEL = 'gpt-4o'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.DEFAULT_MODEL_SETTINGS","title":"<code>DEFAULT_MODEL_SETTINGS = {'gpt-4o': {'max_tokens': 16000, 'temperature': 1.0}, 'gpt-3.5-turbo': {'max_tokens': 4096, 'temperature': 1.0}, 'gpt-4o-mini': {'max_tokens': 16000, 'temperature': 1.0}}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.MAX_BATCH_RETRIES","title":"<code>MAX_BATCH_RETRIES = 40</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.MAX_TOKEN_LIMIT","title":"<code>MAX_TOKEN_LIMIT = 60000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.journal_schema","title":"<code>journal_schema = {'type': 'object', 'properties': {'journal_summary': {'type': 'string'}, 'sections': {'type': 'array', 'items': {'type': 'object', 'properties': {'title_vi': {'type': 'string'}, 'title_en': {'type': 'string'}, 'author': {'type': ['string', 'null']}, 'summary': {'type': 'string'}, 'keywords': {'type': 'array', 'items': {'type': 'string'}}, 'start_page': {'type': 'integer', 'minimum': 1}, 'end_page': {'type': 'integer', 'minimum': 1}}, 'required': ['title_vi', 'title_en', 'summary', 'keywords', 'start_page', 'end_page']}}}, 'required': ['journal_summary', 'sections']}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.logger","title":"<code>logger = logging.getLogger('journal_process')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.ModelSettings","title":"<code>ModelSettings</code>","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>class ModelSettings(TypedDict):\n    max_tokens: int\n    temperature: float\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.ModelSettings.max_tokens","title":"<code>max_tokens</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.ModelSettings.temperature","title":"<code>temperature</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.journal_processing.journal_process.batch_section","title":"<code>batch_section(input_xml_path, batch_jsonl, system_message, journal_name)</code>","text":"<p>Splits the journal content into sections using GPT, with retries for both starting and completing the batch.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def batch_section(\n    input_xml_path: Path, batch_jsonl: Path, system_message, journal_name\n) -&gt; str:\n    \"\"\"\n    Splits the journal content into sections using GPT, with retries for both starting and completing the batch.\n    \"\"\"\n    try:\n        logger.info(\n            f\"Starting sectioning batch for {journal_name} with file:\\n\\t{input_xml_path}\"\n        )\n        # Load journal content\n        journal_pages = read_str_from_file(input_xml_path)\n\n        # Create GPT messages for sectioning\n        user_message_wrapper = lambda text: f\"{text}\"\n        messages = generate_messages(\n            system_message, user_message_wrapper, [journal_pages]\n        )\n\n        # Create JSONL file for batch processing\n        jsonl_file = create_jsonl_file_for_batch(messages, batch_jsonl, json_mode=True)\n\n    except Exception as e:\n        logger.error(\n            f\"Failed to initialize batch sectioning data for journal '{journal_name}'.\",\n            extra={\"input_xml_path\": input_xml_path},\n            exc_info=True,\n        )\n        raise RuntimeError(\n            f\"Error initializing batch sectioning data for journal '{journal_name}'.\"\n        ) from e\n\n    response = start_batch_with_retries(\n        jsonl_file,\n        description=f\"Batch for sectioning journal: {journal_name} | input file: {input_xml_path}\",\n    )\n\n    if response:\n        json_result = response[\n            0\n        ]  # should return json, just one batch so first response\n        # Log success and return output json\n        logger.info(\n            f\"Successfully batch sectioned journal '{journal_name}' with input file: {input_xml_path}.\"\n        )\n        return json_result\n    else:\n        logger.error(\"Section batch failed to get response.\")\n        return \"\"\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.batch_translate","title":"<code>batch_translate(input_xml_path, batch_json_path, metadata_path, system_message, journal_name)</code>","text":"<p>Translates the journal sections using the GPT model. Saves the translated content back to XML.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def batch_translate(\n    input_xml_path: Path,\n    batch_json_path: Path,\n    metadata_path: Path,\n    system_message,\n    journal_name: str,\n) -&gt; list:\n    \"\"\"\n    Translates the journal sections using the GPT model.\n    Saves the translated content back to XML.\n    \"\"\"\n    logger.info(\n        f\"Starting translation batch for journal '{journal_name}':\\n\\twith file: {input_xml_path}\\n\\tmetadata: {metadata_path}\"\n    )\n\n    # Data initialization:\n    try:\n        # load metadata\n        serial_json = read_str_from_file(metadata_path)\n\n        section_metadata = deserialize_json(serial_json)\n        if not section_metadata:\n            raise RuntimeError(f\"Metadata could not be loaded from {metadata_path}.\")\n\n        # Extract page groups and split XML content\n        page_groups = extract_page_groups_from_metadata(section_metadata)\n        xml_content = read_str_from_file(input_xml_path)\n        section_contents = split_xml_on_pagebreaks(xml_content, page_groups)\n\n        if section_contents:\n            logger.debug(f\"section_contents[0]:\\n{section_contents[0]}\")\n        else:\n            logger.error(\"No section contents.\")\n\n    except Exception as e:\n        logger.error(\n            f\"Failed to initialize data for translation batching for journal '{journal_name}'.\",\n            exc_info=True,\n        )\n        raise RuntimeError(\n            f\"Error during data initialization for journal '{journal_name}'.\"\n        ) from e\n\n    translation_data = translate_sections(\n        batch_json_path,\n        system_message,\n        section_contents,\n        section_metadata,\n        journal_name,\n    )\n    return translation_data\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.create_jsonl_file_for_batch","title":"<code>create_jsonl_file_for_batch(messages, output_file_path=None, max_token_list=None, model=DEFAULT_JOURNAL_MODEL, tools=None, json_mode=False)</code>","text":"<p>Write a JSONL batch file mirroring the legacy OpenAI format.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def create_jsonl_file_for_batch(\n    messages: list[list[dict[str, str]]],\n    output_file_path: Path | str | None = None,\n    max_token_list: list[int] | None = None,\n    model: str = DEFAULT_JOURNAL_MODEL,\n    tools=None,\n    json_mode: bool | None = False,\n):\n    \"\"\"Write a JSONL batch file mirroring the legacy OpenAI format.\"\"\"\n    model_settings = _get_model_settings(model)\n    if not max_token_list:\n        max_tokens = model_settings[\"max_tokens\"]\n        max_token_list = [max_tokens] * len(messages)\n\n    temperature = model_settings[\"temperature\"]\n    total_tokens = sum(max_token_list)\n\n    if output_file_path is None:\n        date_str = datetime.now().strftime(\"%m%d%Y\")\n        resolved_output = Path(f\"batch_requests_{date_str}.jsonl\")\n    else:\n        resolved_output = Path(output_file_path)\n\n    output_dir = resolved_output.parent\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    requests: list[dict[str, Any]] = []\n    for i, message in enumerate(messages):\n        max_tokens = max_token_list[i]\n        request_obj: dict[str, Any] = {\n            \"custom_id\": f\"request-{i+1}\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": model,\n                \"messages\": message,\n                \"max_tokens\": max_tokens,\n                \"temperature\": temperature,\n            },\n        }\n        if json_mode:\n            request_obj[\"body\"][\"response_format\"] = {\"type\": \"json_object\"}\n        if tools:\n            request_obj[\"body\"][\"tools\"] = tools\n\n        requests.append(request_obj)\n\n    with resolved_output.open(\"w\", encoding=\"utf-8\") as handle:\n        for request in requests:\n            json.dump(request, handle)\n            handle.write(\"\\n\")\n\n    logger.info(\n        \"JSONL batch file created at %s with ~%s requested tokens.\",\n        resolved_output,\n        total_tokens,\n    )\n    return resolved_output\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.deserialize_json","title":"<code>deserialize_json(serialized_data)</code>","text":"<p>Converts a serialized JSON string into a Python dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>serialized_data</code> <code>str</code> <p>The JSON string to deserialize.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The deserialized Python dictionary.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def deserialize_json(serialized_data: str) -&gt; dict:\n    \"\"\"\n    Converts a serialized JSON string into a Python dictionary.\n\n    Args:\n        serialized_data (str): The JSON string to deserialize.\n\n    Returns:\n        dict: The deserialized Python dictionary.\n    \"\"\"\n    if not isinstance(serialized_data, str):\n        logger.error(\n            f\"String input required for deserialize_json. Received: {type(serialized_data)}\"\n        )\n        raise ValueError(\"String input required.\")\n\n    try:\n        # Convert the JSON string into a dictionary\n        return json.loads(serialized_data)\n    except json.JSONDecodeError as e:\n        logger.error(f\"Failed to deserialize JSON: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.extract_page_groups_from_metadata","title":"<code>extract_page_groups_from_metadata(metadata)</code>","text":"<p>Extracts page groups from the section metadata for use with <code>split_xml_pages</code>.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The section metadata containing sections with start and end pages.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List[Tuple[int, int]]: A list of tuples, each representing a page range (start_page, end_page).</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def extract_page_groups_from_metadata(metadata) -&gt; list:\n    \"\"\"\n    Extracts page groups from the section metadata for use with `split_xml_pages`.\n\n    Parameters:\n        metadata (dict): The section metadata containing sections with start and end pages.\n\n    Returns:\n        List[Tuple[int, int]]: A list of tuples, each representing a page range (start_page, end_page).\n    \"\"\"\n    page_groups = []\n\n    # Ensure metadata contains sections\n    if \"sections\" not in metadata or not isinstance(metadata[\"sections\"], list):\n        raise ValueError(\n            \"Metadata does not contain a valid 'sections' key with a list of sections.\"\n        )\n\n    for section in metadata[\"sections\"]:\n        try:\n            # Extract start and end pages\n            start_page = section.get(\"start_page\")\n            end_page = section.get(\"end_page\")\n\n            # Ensure both start_page and end_page are integers\n            if not isinstance(start_page, int) or not isinstance(end_page, int):\n                raise ValueError(f\"Invalid page range in section: {section}\")\n\n            # Add the tuple to the page groups list\n            page_groups.append((start_page, end_page))\n\n        except KeyError as e:\n            print(f\"Missing key in section metadata: {e}\")\n        except ValueError as e:\n            print(f\"Error processing section metadata: {e}\")\n\n    logger.debug(f\"page groups found: {page_groups}\")\n\n    return page_groups\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.generate_all_batches","title":"<code>generate_all_batches(processed_document_dir, system_message, user_wrap_function, file_regex='.*\\\\.xml')</code>","text":"<p>Generate cleaning batches for all journals in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>processed_document_dir</code> <code>str</code> <p>Path to the directory containing processed journal data.</p> required <code>system_message</code> <code>str</code> <p>System message template for batch processing.</p> required <code>user_wrap_function</code> <code>callable</code> <p>Function to wrap user input for processing pages.</p> required <code>file_regex</code> <code>str</code> <p>Regex pattern to identify target files (default: \".*.xml\").</p> <code>'.*\\\\.xml'</code> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def generate_all_batches(\n    processed_document_dir: str,\n    system_message: str,\n    user_wrap_function,\n    file_regex: str = r\".*\\.xml\",\n) -&gt; None:\n    \"\"\"\n    Generate cleaning batches for all journals in the specified directory.\n\n    Parameters:\n        processed_document_dir (str): Path to the directory containing processed journal data.\n        system_message (str): System message template for batch processing.\n        user_wrap_function (callable): Function to wrap user input for processing pages.\n        file_regex (str): Regex pattern to identify target files (default: \".*\\\\.xml\").\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    document_dir = Path(processed_document_dir)\n    regex = re.compile(file_regex)\n\n    for journal_file in document_dir.iterdir():\n        if journal_file.is_file() and regex.search(journal_file.name):\n            try:\n                # Derive output file path\n                output_file = journal_file.with_suffix(\".jsonl\")\n                logger.info(f\"Generating batch for {journal_file}...\")\n\n                # Call single batch function\n                generate_single_oa_batch_from_pages(\n                    input_xml_file=str(journal_file),\n                    output_file=str(output_file),\n                    system_message=system_message,\n                    user_wrap_function=user_wrap_function,\n                )\n            except Exception as e:\n                logger.error(f\"Failed to process {journal_file}: {e}\")\n                continue\n\n    logger.info(\"Batch generation completed.\")\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.generate_clean_batch","title":"<code>generate_clean_batch(input_xml_file, output_file, system_message, user_wrap_function)</code>","text":"<p>Generate a batch file for the OpenAI (OA) API using a single input XML file.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def generate_clean_batch(\n    input_xml_file: str, output_file: str, system_message: str, user_wrap_function\n) -&gt; str:\n    \"\"\"\n    Generate a batch file for the OpenAI (OA) API using a single input XML file.\n    \"\"\"\n\n    try:\n        # Read the OCR text from the batch file\n        text = read_str_from_file(input_xml_file)\n        logger.info(f\"Processing file: {input_xml_file}\")\n\n        # Split the text into pages for processing\n        pages = split_xml_on_pagebreaks(text)\n        pages = wrap_all_lines(pages)  # wrap lines with brackets.\n        if not pages:\n            raise ValueError(f\"No pages found in XML file: {input_xml_file}\")\n        logger.info(f\"Found {len(pages)} pages in {input_xml_file}.\")\n\n        max_tokens = [_get_max_tokens_for_clean(page) for page in pages]\n\n        # Generate messages for the pages\n        batch_message_seq = generate_messages(system_message, user_wrap_function, pages)\n\n        # Save the batch file\n        create_jsonl_file_for_batch(\n            batch_message_seq, output_file, max_token_list=max_tokens\n        )\n        logger.info(f\"Batch file created successfully: {output_file}\")\n\n        return output_file\n\n    except FileNotFoundError:\n        logger.error(\"File not found.\")\n        raise\n    except ValueError as e:\n        logger.error(f\"Value error: {e}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error while processing {input_xml_file}: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.generate_messages","title":"<code>generate_messages(system_message, user_message_wrapper, data_list_to_process, log_system_message=True)</code>","text":"<p>Build OpenAI-style chat message payloads.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def generate_messages(\n    system_message: str,\n    user_message_wrapper: Callable[[object], str],\n    data_list_to_process: Sequence[object],\n    log_system_message: bool = True,\n) -&gt; list[list[dict[str, str]]]:\n    \"\"\"Build OpenAI-style chat message payloads.\"\"\"\n    if log_system_message:\n        logger.debug(\"System message:\\n%s\", system_message)\n\n    messages = []\n    for data_element in data_list_to_process:\n        message_block = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_message_wrapper(data_element)},\n        ]\n        messages.append(message_block)\n    return messages\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.generate_single_oa_batch_from_pages","title":"<code>generate_single_oa_batch_from_pages(input_xml_file, output_file, system_message, user_wrap_function)</code>","text":"<p>*** Deprecated *** Generate a batch file for the OpenAI (OA) API using a single input XML file.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def generate_single_oa_batch_from_pages(\n    input_xml_file: str,\n    output_file: str,\n    system_message: str,\n    user_wrap_function,\n):\n    \"\"\"\n    *** Deprecated ***\n    Generate a batch file for the OpenAI (OA) API using a single input XML file.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    try:\n        # Read the OCR text from the batch file\n        text = read_str_from_file(input_xml_file)\n        logger.info(f\"Processing file: {input_xml_file}\")\n\n        # Split the text into pages for processing\n        pages = split_xml_pages(text)\n        if not pages:\n            raise ValueError(f\"No pages found in XML file: {input_xml_file}\")\n        logger.info(f\"Found {len(pages)} pages in {input_xml_file}.\")\n\n        # Generate messages for the pages\n        batch_message_seq = generate_messages(system_message, user_wrap_function, pages)\n\n        # Save the batch file\n        create_jsonl_file_for_batch(batch_message_seq, output_file)\n        logger.info(f\"Batch file created successfully: {output_file}\")\n\n        return output_file\n\n    except FileNotFoundError:\n        logger.error(f\"File not found: {input_xml_file}\")\n        raise\n    except ValueError as e:\n        logger.error(f\"Value error: {e}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error while processing {input_xml_file}: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.run_immediate_chat_process","title":"<code>run_immediate_chat_process(messages, max_tokens=0, response_format=None, model=DEFAULT_JOURNAL_MODEL)</code>","text":"<p>Legacy-compatible immediate completion powered by GenAI simple_completion.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def run_immediate_chat_process(\n    messages: list[dict[str, str]],\n    max_tokens: int = 0,\n    response_format=None,\n    model: str = DEFAULT_JOURNAL_MODEL,\n):\n    \"\"\"Legacy-compatible immediate completion powered by GenAI simple_completion.\"\"\"\n    system_message, user_message = _extract_message_parts(messages)\n    if not max_tokens:\n        max_tokens = _get_model_settings(model)[\"max_tokens\"]\n\n    return simple_completion(\n        system_message=system_message,\n        user_message=user_message,\n        model=model,\n        max_tokens=max_tokens,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.save_cleaned_data","title":"<code>save_cleaned_data(cleaned_xml_path, cleaned_wrapped_pages, journal_name)</code>","text":"Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def save_cleaned_data(\n    cleaned_xml_path: Path, cleaned_wrapped_pages: List[str], journal_name\n):\n    try:\n        logger.info(f\"Saving cleaned content to XML for journal '{journal_name}'.\")\n        cleaned_wrapped_pages = unwrap_all_lines(cleaned_wrapped_pages)\n        save_pages_to_xml(cleaned_xml_path, cleaned_wrapped_pages, overwrite=True)\n        logger.info(f\"Cleaned journal saved successfully to:\\n\\t{cleaned_xml_path}\")\n    except Exception as e:\n        logger.error(\n            f\"Failed to save cleaned data for journal '{journal_name}'.\",\n            extra={\"cleaned_xml_path\": cleaned_xml_path},\n            exc_info=True,\n        )\n        raise RuntimeError(\n            f\"Failed to save cleaned data for journal '{journal_name}'.\"\n        ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.save_sectioning_data","title":"<code>save_sectioning_data(output_json_path, raw_output_path, serial_json, journal_name)</code>","text":"Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def save_sectioning_data(\n    output_json_path: Path, raw_output_path: Path, serial_json: str, journal_name\n):\n    try:\n        raw_output_path.write_text(serial_json, encoding=\"utf-8\")\n    except Exception as e:\n        logger.error(\n            f\"Failed to write raw response file for journal '{journal_name}'.\",\n            extra={\"raw_output_path\": raw_output_path},\n            exc_info=True,\n        )\n        raise RuntimeError(\n            f\"Failed to write raw response file for journal '{journal_name}'.\"\n        ) from e\n\n    # Validate and save metadata\n    try:\n        valid = validate_and_save_metadata(\n            output_json_path, serial_json, journal_schema\n        )\n        if not valid:\n            raise RuntimeError(\n                f\"Validation failed for metadata of journal '{journal_name}'.\"\n            )\n    except Exception as e:\n        logger.error(\n            f\"Error occurred while validating and saving metadata for journal '{journal_name}'.\",\n            extra={\"output_json_path\": output_json_path},\n            exc_info=True,\n        )\n        raise RuntimeError(f\"Validation error for journal '{journal_name}'.\") from e\n\n    return output_json_path\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.save_translation_data","title":"<code>save_translation_data(xml_output_path, translation_data, journal_name)</code>","text":"Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def save_translation_data(xml_output_path: Path, translation_data, journal_name):\n    # Save translated content back to XML\n    try:\n        logger.info(f\"Saving translated content to XML for journal '{journal_name}'.\")\n        join_xml_data_to_doc(xml_output_path, translation_data, overwrite=True)\n        logger.info(f\"Translated journal saved successfully to:\\n\\t{xml_output_path}\")\n\n    except Exception as e:\n        logger.error(\n            f\"Failed to save translation data for journal '{journal_name}'.\",\n            extra={\"xml_output_path\": xml_output_path},\n            exc_info=True,\n        )\n        raise RuntimeError(\n            f\"Failed to save translation data for journal '{journal_name}'.\"\n        ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.send_data_for_tx_batch","title":"<code>send_data_for_tx_batch(batch_jsonl_path, section_data_to_send, system_message, max_token_list, journal_name, immediate=False)</code>","text":"<p>Sends data for translation batch or immediate processing.</p> <p>Parameters:</p> Name Type Description Default <code>batch_jsonl_path</code> <code>Path</code> <p>Path for the JSONL file to save batch data.</p> required <code>section_data_to_send</code> <code>List</code> <p>List of section data to translate.</p> required <code>system_message</code> <code>str</code> <p>System message for the translation process.</p> required <code>max_token_list</code> <code>List</code> <p>List of max tokens for each section.</p> required <code>journal_name</code> <code>str</code> <p>Name of the journal being processed.</p> required <code>immediate</code> <code>bool</code> <p>If True, run immediate chat processing instead of batch.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>List</code> <code>list</code> <p>Translated data from the batch or immediate process.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def send_data_for_tx_batch(\n    batch_jsonl_path: Path,\n    section_data_to_send: List,\n    system_message,\n    max_token_list: List,\n    journal_name,\n    immediate=False,\n) -&gt; list:\n    \"\"\"\n    Sends data for translation batch or immediate processing.\n\n    Args:\n        batch_jsonl_path (Path): Path for the JSONL file to save batch data.\n        section_data_to_send (List): List of section data to translate.\n        system_message (str): System message for the translation process.\n        max_token_list (List): List of max tokens for each section.\n        journal_name (str): Name of the journal being processed.\n        immediate (bool): If True, run immediate chat processing instead of batch.\n\n    Returns:\n        List: Translated data from the batch or immediate process.\n    \"\"\"\n    try:\n        # Generate all messages using the generate_messages function\n        user_message_wrapper = (\n            lambda section_info: f\"Translate this section with title '{section_info.title}':\\n{section_info.content}\"\n        )\n        messages = generate_messages(\n            system_message, user_message_wrapper, section_data_to_send\n        )\n\n        if immediate:\n            logger.info(f\"Running immediate chat process for journal '{journal_name}'.\")\n            translated_data = []\n            for i, message in enumerate(messages):\n                max_tokens = max_token_list[i]\n                response = run_immediate_chat_process(message, max_tokens=max_tokens)\n                translated_data.append(response)\n            logger.info(\n                f\"Immediate translation completed for journal '{journal_name}'.\"\n            )\n            return translated_data\n        else:\n            logger.info(f\"Running batch processing for journal '{journal_name}'.\")\n            # Create batch file for batch processing\n            jsonl_file = create_jsonl_file_for_batch(\n                messages, batch_jsonl_path, max_token_list=max_token_list\n            )\n            if not jsonl_file:\n                raise RuntimeError(\"Failed to create JSONL file for translation batch.\")\n\n            # Process batch and return the result\n            translation_data = start_batch_with_retries(\n                jsonl_file,\n                description=f\"Batch for translating journal '{journal_name}'\",\n            )\n            logger.info(f\"Batch translation completed for journal '{journal_name}'.\")\n            return translation_data\n\n    except Exception as e:\n        logger.error(\n            f\"Error during translation processing for journal '{journal_name}'.\",\n            exc_info=True,\n        )\n        raise RuntimeError(\"Error in translation process.\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.setup_logger","title":"<code>setup_logger(log_file_path)</code>","text":"<p>Configures the logger to write to a log file and the console. Adds a custom \"PRIORITY_INFO\" logging level for important messages.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def setup_logger(log_file_path):\n    \"\"\"\n    Configures the logger to write to a log file and the console.\n    Adds a custom \"PRIORITY_INFO\" logging level for important messages.\n    \"\"\"\n    # Remove existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",  # Include logger name\n        handlers=[\n            logging.FileHandler(log_file_path, encoding=\"utf-8\"),\n            logging.StreamHandler(),  # Optional: to log to the console as well\n        ],\n    )\n\n    # Suppress DEBUG/INFO logs for specific noisy modules\n    modules_to_suppress = [\"httpx\", \"httpcore\", \"urllib3\", \"openai\", \"google\"]\n    for module in modules_to_suppress:\n        logger = logging.getLogger(module)\n        logger.setLevel(logging.WARNING)  # Suppress DEBUG and INFO logs\n\n    # Add a custom \"PRIORITY_INFO\" level\n    PRIORITY_INFO_LEVEL = 25  # Between INFO (20) and WARNING (30)\n    logging.addLevelName(PRIORITY_INFO_LEVEL, \"PRIORITY_INFO\")\n\n    def priority_info(self, message, *args, **kwargs):\n        if self.isEnabledFor(PRIORITY_INFO_LEVEL):\n            self._log(PRIORITY_INFO_LEVEL, f\"\\033[93m{message}\\033[0m\", args, **kwargs)\n\n    logging.Logger.priority_info = priority_info\n\n    return logging.getLogger(__name__)\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.start_batch_with_retries","title":"<code>start_batch_with_retries(jsonl_file, description='', max_retries=MAX_BATCH_RETRIES, retry_delay=BATCH_RETRY_DELAY, poll_interval=10, timeout=3600)</code>","text":"<p>Simulate the legacy batch runner using sequential simple_completion calls.</p> <p>The parameters mirror the old interface so callers remain unchanged, but the implementation now iterates through the JSONL requests locally.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def start_batch_with_retries(\n    jsonl_file: Path,\n    description: str = \"\",\n    max_retries: int = MAX_BATCH_RETRIES,\n    retry_delay: int = BATCH_RETRY_DELAY,\n    poll_interval: int = 10,\n    timeout: int = 3600,\n) -&gt; list[str]:\n    \"\"\"\n    Simulate the legacy batch runner using sequential simple_completion calls.\n\n    The parameters mirror the old interface so callers remain unchanged, but the\n    implementation now iterates through the JSONL requests locally.\n    \"\"\"\n    logger.info(\n        \"Running sequential batch for '%s' using %s\",\n        description,\n        jsonl_file,\n    )\n    responses: list[str] = []\n    try:\n        with jsonl_file.open(\"r\", encoding=\"utf-8\") as handle:\n            for line_no, line in enumerate(handle, start=1):\n                if not line.strip():\n                    continue\n                payload = json.loads(line)\n                body = payload.get(\"body\", {})\n                request_model = body.get(\"model\", DEFAULT_JOURNAL_MODEL)\n                messages = body.get(\"messages\", [])\n                max_tokens = body.get(\"max_tokens\") or body.get(\"max_completion_tokens\")\n                if not max_tokens:\n                    max_tokens = _get_model_settings(request_model)[\"max_tokens\"]\n                system_message, user_message = _extract_message_parts(messages)\n                response = simple_completion(\n                    system_message=system_message,\n                    user_message=user_message,\n                    model=request_model,\n                    max_tokens=max_tokens,\n                )\n                responses.append(response)\n                logger.debug(\"Processed request %s from batch file\", line_no)\n\n    except Exception as exc:\n        logger.error(\n            \"Failed to process batch '%s' from %s\",\n            description or jsonl_file,\n            jsonl_file,\n            exc_info=True,\n        )\n        raise RuntimeError(\"Failed to process batch sequentially\") from exc\n\n    logger.info(\n        \"Sequential batch for '%s' completed with %s responses.\",\n        description or jsonl_file,\n        len(responses),\n    )\n    return responses\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.translate_sections","title":"<code>translate_sections(batch_jsonl_path, system_message, section_contents, section_metadata, journal_name, immediate=False)</code>","text":"<p>build up sections in batches to translate</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def translate_sections(\n    batch_jsonl_path: Path,\n    system_message,\n    section_contents,\n    section_metadata,\n    journal_name,\n    immediate=False,\n) -&gt; list:\n    \"\"\"build up sections in batches to translate\"\"\"\n\n    section_mdata = section_metadata[\"sections\"]\n    if len(section_contents) != len(section_mdata):\n        raise RuntimeError(\"Section length mismatch.\")\n\n    # collate metadata and section content, calculate max_tokens per section:\n    section_data_to_send = []\n    max_token_list = []\n    current_token_count = 0\n    collected_translations = []\n    section_last_index = len(section_mdata) - 1\n\n    for i, section_info in enumerate(section_mdata):\n        section_content = section_contents[i]\n        max_tokens = floor(token_count(section_content) * 1.3) + 1000\n        max_token_list.append(max_tokens)\n        current_token_count += max_tokens\n        section_data = SimpleNamespace(\n            title=section_info[\"title_en\"], content=section_content\n        )\n        section_data_to_send.append(section_data)\n        logger.debug(f\"section {i}: {section_data.title} added for batch processing.\")\n\n        if current_token_count &gt;= MAX_TOKEN_LIMIT or i == section_last_index:\n            # send sections for batch processing since token limit reached.\n            batch_result = send_data_for_tx_batch(\n                batch_jsonl_path,\n                section_data_to_send,\n                system_message,\n                max_token_list,\n                journal_name,\n                immediate,\n            )\n            collected_translations.extend(batch_result)\n\n            # reset containers to start building up next batch.\n            section_data_to_send = []\n            max_token_list = []\n            current_token_count = 0\n\n    return collected_translations\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.unwrap_all_lines","title":"<code>unwrap_all_lines(pages)</code>","text":"Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def unwrap_all_lines(pages):\n    result = []\n    for page in pages:\n        if page == \"blank page\":\n            result.append(page)\n        else:\n            result.append(unwrap_lines(page))\n    return result\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.unwrap_lines","title":"<code>unwrap_lines(text)</code>","text":"<pre><code>Removes angle brackets (&lt; &gt;) from encapsulated lines and merges them into\na newline-separated string.\n\nParameters:\n    text (str): The input string with encapsulated lines.\n\nReturns:\n    str: A newline-separated string with the encapsulation removed.\n\nExample:\n    &gt;&gt;&gt; merge_encapsulated_lines(\"&lt;Line 1&gt; &lt;Line 2&gt; &lt;Line 3&gt;\")\n    'Line 1\n</code></pre> <p>Line 2 Line 3'         &gt;&gt;&gt; merge_encapsulated_lines(\" \")         'Line 1 Line 2 Line 3' Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def unwrap_lines(text: str) -&gt; str:\n    \"\"\"\n    Removes angle brackets (&lt; &gt;) from encapsulated lines and merges them into\n    a newline-separated string.\n\n    Parameters:\n        text (str): The input string with encapsulated lines.\n\n    Returns:\n        str: A newline-separated string with the encapsulation removed.\n\n    Example:\n        &gt;&gt;&gt; merge_encapsulated_lines(\"&lt;Line 1&gt; &lt;Line 2&gt; &lt;Line 3&gt;\")\n        'Line 1\\nLine 2\\nLine 3'\n        &gt;&gt;&gt; merge_encapsulated_lines(\"&lt;Line 1&gt;\\n&lt;Line 2&gt;\\n&lt;Line 3&gt;\")\n        'Line 1\\nLine 2\\nLine 3'\n    \"\"\"\n    # Find all content between &lt; and &gt; using regex\n    matches = re.findall(r\"&lt;(.*?)&gt;\", text)\n    # Join the extracted content with newlines\n    return \"\\n\".join(matches)\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.validate_and_clean_data","title":"<code>validate_and_clean_data(data, schema)</code>","text":"<p>Recursively validate and clean AI-generated data to fit the given schema. Any missing fields are filled with defaults, and extra fields are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The AI-generated data to validate and clean.</p> required <code>schema</code> <code>dict</code> <p>The schema defining the required structure.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The cleaned data adhering to the schema.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def validate_and_clean_data(data, schema) -&gt; dict:\n    \"\"\"\n    Recursively validate and clean AI-generated data to fit the given schema.\n    Any missing fields are filled with defaults, and extra fields are ignored.\n\n    Args:\n        data (dict): The AI-generated data to validate and clean.\n        schema (dict): The schema defining the required structure.\n\n    Returns:\n        dict: The cleaned data adhering to the schema.\n    \"\"\"\n\n    def clean_value(value, field_schema):\n        \"\"\"\n        Clean a single value based on its schema, attempting type conversions where necessary.\n        \"\"\"\n        field_type = field_schema[\"type\"]\n\n        # Handle type: string\n        if field_type == \"string\":\n            if isinstance(value, str):\n                return value\n            elif value is not None:\n                return str(value)\n            return \"unset\"\n\n        # Handle type: integer\n        elif field_type == \"integer\":\n            if isinstance(value, int):\n                return value\n            elif isinstance(value, str) and value.isdigit():\n                return int(value)\n            try:\n                return int(float(value))  # Handle cases like \"2.0\"\n            except (ValueError, TypeError):\n                return 0\n\n        # Handle type: array\n        elif field_type == \"array\":\n            if isinstance(value, list):\n                item_schema = field_schema.get(\"items\", {})\n                return [clean_value(item, item_schema) for item in value]\n            elif isinstance(value, str):\n                # Try splitting comma-separated strings into a list\n                return [v.strip() for v in value.split(\",\")]\n            return []\n\n        # Handle type: object\n        elif field_type == \"object\":\n            if isinstance(value, dict):\n                return validate_and_clean_data(value, field_schema)\n            return {}\n\n        # Handle nullable strings\n        elif field_type == [\"string\", \"null\"]:\n            if value is None or isinstance(value, str):\n                return value\n            return str(value)\n\n        # Default case for unknown or unsupported types\n        return \"unset\"\n\n    def clean_object(obj, obj_schema):\n        \"\"\"\n        Clean a dictionary object based on its schema.\n        \"\"\"\n        if not isinstance(obj, dict):\n            print(\n                f\"Expected dict but got: \\n{type(obj)}: {obj}\\nResetting to empty dict.\"\n            )\n            return {}\n        cleaned = {}\n        properties = obj_schema.get(\"properties\", {})\n        for key, field_schema in properties.items():\n            # Set default value for missing fields\n            cleaned[key] = clean_value(obj.get(key), field_schema)\n        return cleaned\n\n    # Handle the top-level object\n    if schema[\"type\"] == \"object\":\n        cleaned_data = clean_object(data, schema)\n        return cleaned_data\n    else:\n        raise ValueError(\"Top-level schema must be of type 'object'.\")\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.validate_and_save_metadata","title":"<code>validate_and_save_metadata(output_file_path, json_metadata_serial, schema)</code>","text":"<p>Validates and cleans journal data against the schema, then writes it to a JSON file.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successfully written to the file, False otherwise.</p> Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def validate_and_save_metadata(\n    output_file_path: Path, json_metadata_serial: str, schema\n) -&gt; bool:\n    \"\"\"\n    Validates and cleans journal data against the schema, then writes it to a JSON file.\n\n    Returns:\n        bool: True if successfully written to the file, False otherwise.\n    \"\"\"\n    try:\n        # Clean the data to fit the schema\n        data = deserialize_json(json_metadata_serial)\n        cleaned_data = validate_and_clean_data(data, schema)\n\n        # Write the parsed data to the specified JSON file\n        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n        logger.info(\n            f\"Parsed and validated metadata successfully written to {output_file_path}\"\n        )\n        return True\n    except Exception as e:\n        logger.error(f\"An error occurred during validation or writing: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.wrap_all_lines","title":"<code>wrap_all_lines(pages)</code>","text":"Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def wrap_all_lines(pages):\n    return [wrap_lines(page) for page in pages]\n</code></pre>"},{"location":"api/#tnh_scholar.journal_processing.journal_process.wrap_lines","title":"<code>wrap_lines(text)</code>","text":"<pre><code>Encloses each line of the input text with angle brackets.\n\nArgs:\n    text (str): The input string containing lines separated by '\n</code></pre> <p>'.</p> <pre><code>Returns:\n    str: A string where each line is enclosed in angle brackets.\n\nExample:\n    &gt;&gt;&gt; enclose_lines(\"This is a string with\n</code></pre> <p>two lines.\")         ' &lt;    two lines.&gt;' Source code in <code>src/tnh_scholar/journal_processing/journal_process.py</code> <pre><code>def wrap_lines(text: str) -&gt; str:\n    \"\"\"\n    Encloses each line of the input text with angle brackets.\n\n    Args:\n        text (str): The input string containing lines separated by '\\n'.\n\n    Returns:\n        str: A string where each line is enclosed in angle brackets.\n\n    Example:\n        &gt;&gt;&gt; enclose_lines(\"This is a string with   \\n   two lines.\")\n        '&lt;This is a string with  &gt;\\n&lt;    two lines.&gt;'\n    \"\"\"\n    return \"\\n\".join(f\"&lt;{line}&gt;\" for line in text.split(\"\\n\"))\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config","title":"<code>logging_config</code>","text":""},{"location":"api/#tnh_scholar.logging_config--tnh-scholar-logging-utilities","title":"TNH-Scholar Logging Utilities","text":"<p>A production-ready, environment-driven logging system for the TNH-Scholar project. It provides JSON logs in production, color/plain text in development, optional non-blocking queue logging, file rotation, noise suppression for chatty deps, and optional routing of Python warnings into the logging pipeline.</p> <p>This module is designed for application layer configuration and library layer usage:</p> <ul> <li>Applications (CLI, Streamlit, FastAPI, notebooks) call :func:<code>setup_logging</code>.</li> <li>Libraries / services (e.g., gen_ai_service, IssueHandler) only acquire a   logger via :func:<code>get_logger</code> (or legacy :func:<code>get_child_logger</code>) and never   configure global logging.</li> </ul>"},{"location":"api/#tnh_scholar.logging_config--quick-start","title":"Quick start","text":"<p>Application entry point (recommended):</p> <pre><code>&gt;&gt;&gt; from tnh_scholar.logging_config import setup_logging, get_logger\n&gt;&gt;&gt; setup_logging()  # reads env; see variables below\n&gt;&gt;&gt; log = get_logger(__name__)\n&gt;&gt;&gt; log.info(\"app started\", extra={\"service\": \"gen-ai\"})\n</code></pre> <p>Jupyter / dev (force color in non-TTY):</p> <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ[\"APP_ENV\"] = \"dev\"\n&gt;&gt;&gt; os.environ[\"LOG_JSON\"] = \"false\"\n&gt;&gt;&gt; os.environ[\"LOG_COLOR\"] = \"true\"]  # Jupyter isn't a TTY; force color\n&gt;&gt;&gt; from tnh_scholar.logging_config import setup_logging, get_logger\n&gt;&gt;&gt; setup_logging()\n&gt;&gt;&gt; get_logger(__name__).info(\"hello, color\")\n</code></pre> <p>Library / service modules (do NOT configure logging):</p> <pre><code>&gt;&gt;&gt; from tnh_scholar.logging_config import get_logger\n&gt;&gt;&gt; log = get_logger(__name__)\n&gt;&gt;&gt; log.info(\"library message\")\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config--behavior-by-environment","title":"Behavior by environment","text":"<ul> <li>dev (default):<ul> <li>Plain or color text to stdout by default.</li> <li>Queue logging disabled by default (synchronous).</li> <li>Color auto-detects TTY and Jupyter/IPython (can be forced).</li> </ul> </li> <li>prod:<ul> <li>JSON logs to stderr by default (suitable for log shippers).</li> <li>Queue logging enabled by default (can be disabled).</li> </ul> </li> </ul>"},{"location":"api/#tnh_scholar.logging_config--environment-variables","title":"Environment variables","text":"<p>Most behavior is controlled by environment variables (read when <code>setup_logging()</code> instantiates :class:<code>LogSettings</code>). Truthy values accept <code>true/1/yes/on</code> (case-insensitive).</p> <ul> <li><code>APP_ENV</code>: <code>dev</code> | <code>prod</code> | <code>test</code> (default: <code>dev</code>)</li> <li><code>LOG_LEVEL</code>: Logging level for the base project logger (default: <code>INFO</code>)</li> <li><code>LOG_STDOUT</code>: Emit logs to stdout (default: <code>true</code>)</li> <li><code>LOG_FILE_ENABLE</code>: Emit logs to a file (default: <code>false</code>)</li> <li><code>LOG_FILE_PATH</code>: File path for logs (default: <code>./logs/main.log</code>)</li> <li><code>LOG_ROTATE_BYTES</code>: Rotate at N bytes (e.g., 10485760) (default: unset)</li> <li><code>LOG_ROTATE_WHEN</code>: Timed rotation (e.g., <code>midnight</code>) (default: unset)</li> <li><code>LOG_BACKUPS</code>: Number of rotated file backups (default: <code>5</code>)</li> <li><code>LOG_JSON</code>: Use JSON formatter (recommended in prod) (default: <code>true</code>)</li> <li><code>LOG_COLOR</code>: <code>true</code> | <code>false</code> | <code>auto</code> (default: <code>auto</code>)</li> <li><code>LOG_STREAM</code>: <code>stdout</code> | <code>stderr</code> (default: <code>stderr</code>; dev defaults to <code>stdout</code>)</li> <li><code>LOG_USE_QUEUE</code>: Use QueueHandler/QueueListener (default: <code>true</code>; dev defaults to <code>false</code>)</li> <li><code>LOG_CAPTURE_WARNINGS</code>: Route Python warnings via logging (default: <code>false</code>)</li> <li><code>LOG_SUPPRESS</code>: Comma-separated list of noisy module names to set to WARNING                     (default includes <code>urllib3</code>, <code>httpx</code>, <code>openai</code>, <code>uvicorn.*</code>, etc.)</li> </ul>"},{"location":"api/#tnh_scholar.logging_config--backward-compatibility","title":"Backward compatibility","text":"<ul> <li><code>get_child_logger(name, console=False, separate_file=False)</code> remains available   and can attach ad-hoc console/file handlers without reconfiguring the project   base logger. When custom handlers are attached, the child\u2019s propagation is turned   off to avoid duplicate messages.</li> <li><code>setup_logging_legacy(...)</code> forwards to :func:<code>setup_logging</code> and emits   a DeprecationWarning to help locate legacy call sites.</li> <li> <p>Custom level <code>PRIORITY_INFO</code> (25) and :meth:<code>logger.priority_info</code> still exist   but are deprecated. Prefer:</p> <p>log.info(\"message\", extra={\"priority\": \"high\"})</p> </li> </ul> <p>This keeps level semantics standard and plays better with structured logging.</p>"},{"location":"api/#tnh_scholar.logging_config--queue-logging-notes","title":"Queue logging notes","text":"<ul> <li>When <code>LOG_USE_QUEUE=true</code>, the base logger uses a :class:<code>QueueHandler</code>.   A :class:<code>QueueListener</code> is started with sinks mirroring your configured   stdout/file handlers. This decouples log emission from I/O to minimize latency.</li> <li> <p>In notebooks or during debugging, you may prefer synchronous logs:</p> <p>os.environ[\"LOG_USE_QUEUE\"] = \"false\"</p> </li> </ul>"},{"location":"api/#tnh_scholar.logging_config--python-warnings-routing","title":"Python warnings routing","text":"<ul> <li>When <code>LOG_CAPTURE_WARNINGS=true</code>, Python warnings are captured and logged   through <code>py.warnings</code>. This module attaches the base logger\u2019s handlers to   that logger and disables propagation to avoid duplicate output.</li> </ul>"},{"location":"api/#tnh_scholar.logging_config--mixing-print-and-logging","title":"Mixing print() and logging","text":"<ul> <li><code>print()</code> writes to stdout; the logger can write to stdout or stderr   depending on <code>LOG_STREAM</code> and environment. Ordering is not guaranteed,   especially with queue logging enabled. Prefer logging for consistent output.</li> </ul>"},{"location":"api/#tnh_scholar.logging_config--minimal-examples","title":"Minimal examples","text":"<p>CLI / entrypoint:</p> <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ.setdefault(\"APP_ENV\", \"prod\")\n&gt;&gt;&gt; os.environ.setdefault(\"LOG_JSON\", \"true\")\n&gt;&gt;&gt; from tnh_scholar.logging_config import setup_logging, get_logger\n&gt;&gt;&gt; setup_logging()\n&gt;&gt;&gt; get_logger(__name__).info(\"ready\")\n</code></pre> <p>File logging with rotation:</p> <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ.update({\n...     \"LOG_FILE_ENABLE\": \"true\",\n...     \"LOG_FILE_PATH\": \"./logs/app.log\",\n...     \"LOG_ROTATE_BYTES\": \"10485760\",  # 10MB\n...     \"LOG_BACKUPS\": \"7\",\n... })\n&gt;&gt;&gt; setup_logging()\n&gt;&gt;&gt; get_logger(\"smoke\").info(\"to file\")\n</code></pre> <p>Jupyter with color:</p> <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ.update({\"APP_ENV\": \"dev\", \"LOG_JSON\": \"false\", \"LOG_COLOR\": \"true\"})\n&gt;&gt;&gt; setup_logging()\n&gt;&gt;&gt; get_logger(__name__).info(\"color in notebook\")\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config--notes","title":"Notes","text":"<ul> <li>JSON formatting requires <code>python-json-logger</code>; without it, we fall back to   plain/color format automatically.</li> <li>This module never configures the root logger; it configures the project   base logger (<code>tnh</code>) so your app can coexist with other libraries cleanly.</li> </ul>"},{"location":"api/#tnh_scholar.logging_config.BASE_LOG_DIR","title":"<code>BASE_LOG_DIR = Path('./logs')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.BASE_LOG_NAME","title":"<code>BASE_LOG_NAME = 'tnh'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.DEFAULT_CONSOLE_FORMAT_STRING","title":"<code>DEFAULT_CONSOLE_FORMAT_STRING = LOG_FMT_COLOR</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.DEFAULT_FILE_FORMAT_STRING","title":"<code>DEFAULT_FILE_FORMAT_STRING = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.DEFAULT_LOG_FILEPATH","title":"<code>DEFAULT_LOG_FILEPATH = Path('main.log')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.JsonFormatter","title":"<code>JsonFormatter = getattr(_pythonjsonlogger_json, 'JsonFormatter', None)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LOG_COLORS","title":"<code>LOG_COLORS = {'DEBUG': 'bold_green', 'INFO': 'cyan', 'PRIORITY_INFO': 'bold_cyan', 'WARNING': 'bold_yellow', 'ERROR': 'bold_red', 'CRITICAL': 'bold_red'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LOG_FMT_COLOR","title":"<code>LOG_FMT_COLOR = '%(asctime)s | %(log_color)s%(levelname)-8s%(reset)s | %(name)s | %(message)s'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LOG_FMT_JSON","title":"<code>LOG_FMT_JSON = '%(asctime)s %(levelname)s %(name)s %(message)s %(process)d %(thread)d %(module)s %(filename)s %(lineno)d'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LOG_FMT_PLAIN","title":"<code>LOG_FMT_PLAIN = '%(asctime)s | %(levelname)-8s | %(name)s | %(message)s'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.MAX_FILE_SIZE","title":"<code>MAX_FILE_SIZE = 10 * 1024 * 1024</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.PRIORITY_INFO_LEVEL","title":"<code>PRIORITY_INFO_LEVEL = 25</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.__all__","title":"<code>__all__ = ['BASE_LOG_NAME', 'BASE_LOG_DIR', 'DEFAULT_LOG_FILEPATH', 'MAX_FILE_SIZE', 'OMPFilter', 'setup_logging', 'setup_logging_legacy', 'get_logger', 'get_child_logger']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings","title":"<code>LogSettings</code>  <code>dataclass</code>","text":"<p>Environment-driven logging settings with sensible defaults.</p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>@dataclass\nclass LogSettings:\n    \"\"\"Environment-driven logging settings with sensible defaults.\"\"\"\n    # Mode\n    environment: str = field(default_factory=lambda: _env_str(\"APP_ENV\", \"dev\"))  # dev|prod|test\n    base_name: str = field(default_factory=lambda: _env_str(\"LOG_BASE\", BASE_LOG_NAME))\n\n    # Level\n    level: str = field(default_factory=lambda: _env_str(\"LOG_LEVEL\", \"INFO\"))\n\n    # Outputs\n    to_stdout: bool = field(default_factory=lambda: _env_bool(\"LOG_STDOUT\", \"true\"))\n    to_file: bool = field(default_factory=lambda: _env_bool(\"LOG_FILE_ENABLE\", \"false\"))\n    file_path: Path = field(\n        default_factory=lambda: Path(\n            _env_str(\"LOG_FILE_PATH\", str(BASE_LOG_DIR / DEFAULT_LOG_FILEPATH))\n            )\n        )\n\n    # File rotation\n    rotate_when: Optional[str] = field(default_factory=lambda: _env_str(\"LOG_ROTATE_WHEN\", \"\") or None)  \n        # e.g. 'midnight'\n    rotate_bytes: Optional[int] = field(default_factory=lambda: (_env_int(\"LOG_ROTATE_BYTES\", 0) or None))  \n        # e.g. 10485760\n    backups: int = field(default_factory=lambda: _env_int(\"LOG_BACKUPS\", 5))\n\n    # Format\n    json_format: bool = field(default_factory=lambda: _env_bool(\"LOG_JSON\", \"true\"))  # prod default\n    colorize: str = field(default_factory=lambda: _env_str(\"LOG_COLOR\", \"auto\"))  # true|false|auto\n\n    # Python warnings routing\n    capture_warnings: bool = field(default_factory=lambda: _env_bool(\"LOG_CAPTURE_WARNINGS\", \"false\"))\n\n    # Stream selection (stdout|stderr)\n    log_stream: str = field(default_factory=lambda: _env_str(\"LOG_STREAM\", \"stderr\"))\n\n    # Performance\n    use_queue: bool = field(default_factory=lambda: _env_bool(\"LOG_USE_QUEUE\", \"true\"))\n\n    # Noise suppression (comma-separated)\n    suppress_modules: str = field(default_factory=lambda: _env_str(\n        \"LOG_SUPPRESS\",\n        \"urllib3,httpx,openai,botocore,boto3,asyncio,uvicorn,uvicorn.error,uvicorn.access\",\n    ))\n\n    def is_dev(self) -&gt; bool:\n        return self.environment.lower() == \"dev\"\n\n    def should_color(self) -&gt; bool:\n        if self.colorize == \"true\":\n            return True\n        if self.colorize == \"false\":\n            return False\n        # auto: TTY or Jupyter/IPython\n        if _is_tty(self.selected_stream()):\n            return True\n        try:\n            from IPython.core.getipython import get_ipython\n            return get_ipython() is not None  # in a notebook/console\n        except Exception:\n            return False\n\n    def selected_stream(self):\n        \"\"\"Return the Python stream object to emit logs to (stdout or stderr).\"\"\"\n        return sys.stdout if self.log_stream.lower() == \"stdout\" else sys.stderr\n\n    def __post_init__(self):\n        # Default to stdout and no-queue in dev, unless explicitly overridden by env\n        if self.is_dev():\n            if \"LOG_STREAM\" not in os.environ:\n                self.log_stream = \"stdout\"\n            if \"LOG_USE_QUEUE\" not in os.environ:\n                self.use_queue = False\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LogSettings.backups","title":"<code>backups = field(default_factory=(lambda: _env_int('LOG_BACKUPS', 5)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.base_name","title":"<code>base_name = field(default_factory=(lambda: _env_str('LOG_BASE', BASE_LOG_NAME)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.capture_warnings","title":"<code>capture_warnings = field(default_factory=(lambda: _env_bool('LOG_CAPTURE_WARNINGS', 'false')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.colorize","title":"<code>colorize = field(default_factory=(lambda: _env_str('LOG_COLOR', 'auto')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.environment","title":"<code>environment = field(default_factory=(lambda: _env_str('APP_ENV', 'dev')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.file_path","title":"<code>file_path = field(default_factory=(lambda: Path(_env_str('LOG_FILE_PATH', str(BASE_LOG_DIR / DEFAULT_LOG_FILEPATH)))))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.json_format","title":"<code>json_format = field(default_factory=(lambda: _env_bool('LOG_JSON', 'true')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.level","title":"<code>level = field(default_factory=(lambda: _env_str('LOG_LEVEL', 'INFO')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.log_stream","title":"<code>log_stream = field(default_factory=(lambda: _env_str('LOG_STREAM', 'stderr')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.rotate_bytes","title":"<code>rotate_bytes = field(default_factory=(lambda: _env_int('LOG_ROTATE_BYTES', 0) or None))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.rotate_when","title":"<code>rotate_when = field(default_factory=(lambda: _env_str('LOG_ROTATE_WHEN', '') or None))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.suppress_modules","title":"<code>suppress_modules = field(default_factory=(lambda: _env_str('LOG_SUPPRESS', 'urllib3,httpx,openai,botocore,boto3,asyncio,uvicorn,uvicorn.error,uvicorn.access')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.to_file","title":"<code>to_file = field(default_factory=(lambda: _env_bool('LOG_FILE_ENABLE', 'false')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.to_stdout","title":"<code>to_stdout = field(default_factory=(lambda: _env_bool('LOG_STDOUT', 'true')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.use_queue","title":"<code>use_queue = field(default_factory=(lambda: _env_bool('LOG_USE_QUEUE', 'true')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.__init__","title":"<code>__init__(environment=(lambda: _env_str('APP_ENV', 'dev'))(), base_name=(lambda: _env_str('LOG_BASE', BASE_LOG_NAME))(), level=(lambda: _env_str('LOG_LEVEL', 'INFO'))(), to_stdout=(lambda: _env_bool('LOG_STDOUT', 'true'))(), to_file=(lambda: _env_bool('LOG_FILE_ENABLE', 'false'))(), file_path=(lambda: Path(_env_str('LOG_FILE_PATH', str(BASE_LOG_DIR / DEFAULT_LOG_FILEPATH))))(), rotate_when=(lambda: _env_str('LOG_ROTATE_WHEN', '') or None)(), rotate_bytes=(lambda: _env_int('LOG_ROTATE_BYTES', 0) or None)(), backups=(lambda: _env_int('LOG_BACKUPS', 5))(), json_format=(lambda: _env_bool('LOG_JSON', 'true'))(), colorize=(lambda: _env_str('LOG_COLOR', 'auto'))(), capture_warnings=(lambda: _env_bool('LOG_CAPTURE_WARNINGS', 'false'))(), log_stream=(lambda: _env_str('LOG_STREAM', 'stderr'))(), use_queue=(lambda: _env_bool('LOG_USE_QUEUE', 'true'))(), suppress_modules=(lambda: _env_str('LOG_SUPPRESS', 'urllib3,httpx,openai,botocore,boto3,asyncio,uvicorn,uvicorn.error,uvicorn.access'))())</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LogSettings.__post_init__","title":"<code>__post_init__()</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def __post_init__(self):\n    # Default to stdout and no-queue in dev, unless explicitly overridden by env\n    if self.is_dev():\n        if \"LOG_STREAM\" not in os.environ:\n            self.log_stream = \"stdout\"\n        if \"LOG_USE_QUEUE\" not in os.environ:\n            self.use_queue = False\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LogSettings.is_dev","title":"<code>is_dev()</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def is_dev(self) -&gt; bool:\n    return self.environment.lower() == \"dev\"\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LogSettings.selected_stream","title":"<code>selected_stream()</code>","text":"<p>Return the Python stream object to emit logs to (stdout or stderr).</p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def selected_stream(self):\n    \"\"\"Return the Python stream object to emit logs to (stdout or stderr).\"\"\"\n    return sys.stdout if self.log_stream.lower() == \"stdout\" else sys.stderr\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LogSettings.should_color","title":"<code>should_color()</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def should_color(self) -&gt; bool:\n    if self.colorize == \"true\":\n        return True\n    if self.colorize == \"false\":\n        return False\n    # auto: TTY or Jupyter/IPython\n    if _is_tty(self.selected_stream()):\n        return True\n    try:\n        from IPython.core.getipython import get_ipython\n        return get_ipython() is not None  # in a notebook/console\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator","title":"<code>LoggingConfigurator</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>class LoggingConfigurator:\n    _queue: Optional[queue.Queue] = None\n\n    # ----- Private helpers (handlers) -----\n    def _stdout_handler_config(self, fmt_key: str) -&gt; dict:\n        stream_path = \"ext://sys.stdout\" if self.settings.log_stream.lower() == \"stdout\" else \"ext://sys.stderr\"\n        return {\n            \"class\": \"logging.StreamHandler\",\n            \"stream\": stream_path,\n            \"formatter\": fmt_key,\n            \"filters\": [\"omp_filter\"],\n        }\n\n    def _file_handler_config(self, *, formatter_key: str) -&gt; dict:\n        s = self.settings\n        s.file_path.parent.mkdir(parents=True, exist_ok=True)\n        if s.rotate_bytes:\n            return {\n                \"class\": \"logging.handlers.RotatingFileHandler\",\n                \"maxBytes\": s.rotate_bytes,\n                \"backupCount\": s.backups,\n                \"filename\": str(s.file_path),\n                \"formatter\": formatter_key,\n                \"encoding\": \"utf-8\",\n                \"filters\": [\"omp_filter\"],\n            }\n        if s.rotate_when:\n            return {\n                \"class\": \"logging.handlers.TimedRotatingFileHandler\",\n                \"when\": s.rotate_when,\n                \"backupCount\": s.backups,\n                \"filename\": str(s.file_path),\n                \"formatter\": formatter_key,\n                \"encoding\": \"utf-8\",\n                \"filters\": [\"omp_filter\"],\n            }\n        return {\n            \"class\": \"logging.FileHandler\",\n            \"filename\": str(s.file_path),\n            \"formatter\": formatter_key,\n            \"encoding\": \"utf-8\",\n            \"filters\": [\"omp_filter\"],\n        }\n    \"\"\"Modular builder for project-wide logging configuration.\"\"\"\n\n    def __init__(self, settings: Optional[LogSettings] = None):\n        self.settings = settings or LogSettings()\n        # persistent queue instance for QueueHandler/Listener pairing\n        self._queue = queue.Queue() if self.settings.use_queue else None\n\n    # ----- Legacy-args bridge -----\n    def apply_legacy_args(\n        self,\n        *,\n        log_level,\n        log_filepath,\n        max_log_file_size,\n        backup_count,\n        console,\n    ) -&gt; None:\n        s = self.settings\n        s.level = (logging.getLevelName(log_level) if isinstance(log_level, int) else str(log_level)).upper()\n        if console is False:\n            s.to_stdout = False\n            s.to_file = True\n        if log_filepath != DEFAULT_LOG_FILEPATH:\n            s.to_file = True\n            s.file_path = BASE_LOG_DIR / Path(log_filepath)\n        if max_log_file_size and max_log_file_size != MAX_FILE_SIZE:\n            s.rotate_bytes = int(max_log_file_size)\n        s.backups = backup_count or s.backups\n\n    # ----- Builders -----\n    def build_formatters(self) -&gt; dict:\n        s = self.settings\n        fmts: dict[str, dict] = {}\n        if s.json_format and JsonFormatter is not None:\n            fmts[\"json\"] = {\n                \"()\": \"pythonjsonlogger.json.JsonFormatter\",\n                \"fmt\": LOG_FMT_JSON,\n                \"json_ensure_ascii\": False,\n            }\n        else:\n            fmts[\"plain\"] = {\n                \"()\": f\"{__name__}.UtcFormatter\",\n                \"fmt\": LOG_FMT_PLAIN,\n            }\n            if s.is_dev() and colorlog and s.should_color():\n                fmts[\"color\"] = {\n                    \"()\": \"colorlog.ColoredFormatter\",\n                    \"format\": LOG_FMT_COLOR,\n                    \"log_colors\": LOG_COLORS,\n                }\n        return fmts\n\n    def build_filters(self) -&gt; dict:\n        return {\"omp_filter\": {\"()\": f\"{__name__}.OMPFilter\"}}\n\n    def build_handlers(self, formatters: dict) -&gt; dict:\n        s = self.settings\n        handlers: dict[str, dict] = {}\n\n        # stdout handler\n        if s.to_stdout:\n            if s.json_format and JsonFormatter is not None:\n                fmt = \"json\"\n            elif s.is_dev() and colorlog and s.should_color():\n                fmt = \"color\"\n            else:\n                fmt = \"plain\"\n            handlers[\"stdout\"] = self._stdout_handler_config(fmt)\n\n        # file handler\n        formatter_key = \"json\" if (s.json_format and JsonFormatter is not None) else \"plain\"\n        if s.to_file:\n            handlers[\"file\"] = self._file_handler_config(formatter_key=formatter_key)\n\n        # queue wrapper\n        if s.use_queue and handlers:\n            if self._queue is None:\n                self._queue = queue.Queue()\n            handlers[\"queue\"] = {\n                \"class\": \"logging.handlers.QueueHandler\",\n                \"queue\": self._queue,\n            }\n        return handlers\n\n    # ----- Private helpers (queue sinks) -----\n    def _make_stream_sink(self) -&gt; logging.Handler:\n        s = self.settings\n        sh = logging.StreamHandler(self.settings.selected_stream())\n        if s.json_format and JsonFormatter is not None:\n            sh.setFormatter(JsonFormatter(LOG_FMT_JSON))\n        elif s.is_dev() and colorlog and s.should_color():\n            sh.setFormatter(colorlog.ColoredFormatter(LOG_FMT_COLOR, log_colors=LOG_COLORS))\n        else:\n            sh.setFormatter(UtcFormatter(LOG_FMT_PLAIN))\n        sh.addFilter(OMPFilter())\n        return sh\n\n    def _make_file_sink(self) -&gt; logging.Handler:\n        s = self.settings\n        if s.rotate_bytes:\n            fh: logging.Handler = RotatingFileHandler(\n                str(s.file_path),\n                maxBytes=s.rotate_bytes,\n                backupCount=s.backups,\n                encoding=\"utf-8\",\n            )\n        elif s.rotate_when:\n            fh = TimedRotatingFileHandler(\n                str(s.file_path),\n                when=s.rotate_when,\n                backupCount=s.backups,\n                encoding=\"utf-8\",\n            )\n        else:\n            fh = logging.FileHandler(str(s.file_path), encoding=\"utf-8\")\n\n        if s.json_format and JsonFormatter is not None:\n            fh.setFormatter(JsonFormatter(LOG_FMT_JSON))\n        else:\n            fh.setFormatter(UtcFormatter(LOG_FMT_PLAIN))\n        fh.addFilter(OMPFilter())\n        return fh\n\n    def select_base_handlers(self, handlers: dict) -&gt; list[str]:\n        s = self.settings\n        base_handlers: list[str] = []\n        if s.use_queue and (\"queue\" in handlers):\n            base_handlers.append(\"queue\")\n        else:\n            if \"stdout\" in handlers:\n                base_handlers.append(\"stdout\")\n            if \"file\" in handlers:\n                base_handlers.append(\"file\")\n        return base_handlers\n\n    def build_config(self, *, filters: dict, formatters: dict, handlers: dict) -&gt; dict:\n        s = self.settings\n        return {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"filters\": filters,\n            \"formatters\": formatters,\n            \"handlers\": handlers,\n            \"loggers\": {\n                s.base_name: {\n                    \"level\": s.level,\n                    \"handlers\": self.select_base_handlers(handlers),\n                    \"propagate\": False,\n                }\n            },\n        }\n\n    def apply_config(self, config: dict) -&gt; None:\n        logging.config.dictConfig(config)\n        logging.captureWarnings(self.settings.capture_warnings)\n        # When routing Python warnings into logging, the records go to 'py.warnings'.\n        # Attach our base handlers so warnings are visible.\n        if self.settings.capture_warnings:\n            base = logging.getLogger(self.settings.base_name)\n            pyw = logging.getLogger(\"py.warnings\")\n            # Avoid duplicate handlers on re-configure\n            existing = {id(h) for h in pyw.handlers}\n            for h in base.handlers:\n                if id(h) not in existing:\n                    pyw.addHandler(h)\n            # Ensure records are emitted even if root has no handlers\n            pyw.setLevel(logging.WARNING)\n            pyw.propagate = False\n\n    def start_queue_listener(self, handlers: dict) -&gt; None:\n        global _queue_listener\n        s = self.settings\n        if not (s.use_queue and (\"queue\" in handlers)):\n            return\n        q_logger = logging.getLogger(s.base_name)\n        qh = next((h for h in q_logger.handlers if isinstance(h, QueueHandler)), None)\n        if qh is None:\n            return\n        q = qh.queue  # type: ignore[attr-defined]\n\n        sink_handlers: list[logging.Handler] = []\n        if \"stdout\" in handlers:\n            sink_handlers.append(self._make_stream_sink())\n        if \"file\" in handlers and s.to_file:\n            sink_handlers.append(self._make_file_sink())\n\n        if _queue_listener:\n            with contextlib.suppress(Exception):\n                _queue_listener.stop()\n        _queue_listener = QueueListener(q, *sink_handlers, respect_handler_level=True)\n        _queue_listener.start()\n\n    def suppress_noise(self, modules_override, force: bool = False) -&gt; None:\n        s = self.settings\n        modules = modules_override\n        # Normalize to a list of module names (strings)\n        if modules is None:\n            modules = s.suppress_modules  # env string by default\n        if isinstance(modules, str):\n            modules_list = [m.strip() for m in modules.split(\",\") if m.strip()]\n        else:\n            # Attempt to iterate; if not iterable, coerce to single-item list\n            try:\n                modules_list = [str(m).strip() for m in modules if str(m).strip()]\n            except TypeError:\n                modules_list = [str(modules).strip()] if str(modules).strip() else []\n        for module in modules_list:\n            logger = logging.getLogger(module)\n            if force or logger.level == logging.NOTSET:\n                logger.setLevel(logging.WARNING)\n\n    # ----- Facade -----\n    def configure(\n        self,\n        *,\n        legacy_args: dict,\n        suppressed_modules,\n    ) -&gt; logging.Logger:\n        self.apply_legacy_args(**legacy_args)\n        formatters = self.build_formatters()\n        filters = self.build_filters()\n        handlers = self.build_handlers(formatters)\n        config = self.build_config(filters=filters, formatters=formatters, handlers=handlers)\n        self.apply_config(config)\n        self.start_queue_listener(handlers)\n        self.suppress_noise(suppressed_modules, force=False)\n        return logging.getLogger(self.settings.base_name)\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.settings","title":"<code>settings = settings or LogSettings()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.__init__","title":"<code>__init__(settings=None)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def __init__(self, settings: Optional[LogSettings] = None):\n    self.settings = settings or LogSettings()\n    # persistent queue instance for QueueHandler/Listener pairing\n    self._queue = queue.Queue() if self.settings.use_queue else None\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.apply_config","title":"<code>apply_config(config)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def apply_config(self, config: dict) -&gt; None:\n    logging.config.dictConfig(config)\n    logging.captureWarnings(self.settings.capture_warnings)\n    # When routing Python warnings into logging, the records go to 'py.warnings'.\n    # Attach our base handlers so warnings are visible.\n    if self.settings.capture_warnings:\n        base = logging.getLogger(self.settings.base_name)\n        pyw = logging.getLogger(\"py.warnings\")\n        # Avoid duplicate handlers on re-configure\n        existing = {id(h) for h in pyw.handlers}\n        for h in base.handlers:\n            if id(h) not in existing:\n                pyw.addHandler(h)\n        # Ensure records are emitted even if root has no handlers\n        pyw.setLevel(logging.WARNING)\n        pyw.propagate = False\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.apply_legacy_args","title":"<code>apply_legacy_args(*, log_level, log_filepath, max_log_file_size, backup_count, console)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def apply_legacy_args(\n    self,\n    *,\n    log_level,\n    log_filepath,\n    max_log_file_size,\n    backup_count,\n    console,\n) -&gt; None:\n    s = self.settings\n    s.level = (logging.getLevelName(log_level) if isinstance(log_level, int) else str(log_level)).upper()\n    if console is False:\n        s.to_stdout = False\n        s.to_file = True\n    if log_filepath != DEFAULT_LOG_FILEPATH:\n        s.to_file = True\n        s.file_path = BASE_LOG_DIR / Path(log_filepath)\n    if max_log_file_size and max_log_file_size != MAX_FILE_SIZE:\n        s.rotate_bytes = int(max_log_file_size)\n    s.backups = backup_count or s.backups\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.build_config","title":"<code>build_config(*, filters, formatters, handlers)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def build_config(self, *, filters: dict, formatters: dict, handlers: dict) -&gt; dict:\n    s = self.settings\n    return {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"filters\": filters,\n        \"formatters\": formatters,\n        \"handlers\": handlers,\n        \"loggers\": {\n            s.base_name: {\n                \"level\": s.level,\n                \"handlers\": self.select_base_handlers(handlers),\n                \"propagate\": False,\n            }\n        },\n    }\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.build_filters","title":"<code>build_filters()</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def build_filters(self) -&gt; dict:\n    return {\"omp_filter\": {\"()\": f\"{__name__}.OMPFilter\"}}\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.build_formatters","title":"<code>build_formatters()</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def build_formatters(self) -&gt; dict:\n    s = self.settings\n    fmts: dict[str, dict] = {}\n    if s.json_format and JsonFormatter is not None:\n        fmts[\"json\"] = {\n            \"()\": \"pythonjsonlogger.json.JsonFormatter\",\n            \"fmt\": LOG_FMT_JSON,\n            \"json_ensure_ascii\": False,\n        }\n    else:\n        fmts[\"plain\"] = {\n            \"()\": f\"{__name__}.UtcFormatter\",\n            \"fmt\": LOG_FMT_PLAIN,\n        }\n        if s.is_dev() and colorlog and s.should_color():\n            fmts[\"color\"] = {\n                \"()\": \"colorlog.ColoredFormatter\",\n                \"format\": LOG_FMT_COLOR,\n                \"log_colors\": LOG_COLORS,\n            }\n    return fmts\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.build_handlers","title":"<code>build_handlers(formatters)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def build_handlers(self, formatters: dict) -&gt; dict:\n    s = self.settings\n    handlers: dict[str, dict] = {}\n\n    # stdout handler\n    if s.to_stdout:\n        if s.json_format and JsonFormatter is not None:\n            fmt = \"json\"\n        elif s.is_dev() and colorlog and s.should_color():\n            fmt = \"color\"\n        else:\n            fmt = \"plain\"\n        handlers[\"stdout\"] = self._stdout_handler_config(fmt)\n\n    # file handler\n    formatter_key = \"json\" if (s.json_format and JsonFormatter is not None) else \"plain\"\n    if s.to_file:\n        handlers[\"file\"] = self._file_handler_config(formatter_key=formatter_key)\n\n    # queue wrapper\n    if s.use_queue and handlers:\n        if self._queue is None:\n            self._queue = queue.Queue()\n        handlers[\"queue\"] = {\n            \"class\": \"logging.handlers.QueueHandler\",\n            \"queue\": self._queue,\n        }\n    return handlers\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.configure","title":"<code>configure(*, legacy_args, suppressed_modules)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def configure(\n    self,\n    *,\n    legacy_args: dict,\n    suppressed_modules,\n) -&gt; logging.Logger:\n    self.apply_legacy_args(**legacy_args)\n    formatters = self.build_formatters()\n    filters = self.build_filters()\n    handlers = self.build_handlers(formatters)\n    config = self.build_config(filters=filters, formatters=formatters, handlers=handlers)\n    self.apply_config(config)\n    self.start_queue_listener(handlers)\n    self.suppress_noise(suppressed_modules, force=False)\n    return logging.getLogger(self.settings.base_name)\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.select_base_handlers","title":"<code>select_base_handlers(handlers)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def select_base_handlers(self, handlers: dict) -&gt; list[str]:\n    s = self.settings\n    base_handlers: list[str] = []\n    if s.use_queue and (\"queue\" in handlers):\n        base_handlers.append(\"queue\")\n    else:\n        if \"stdout\" in handlers:\n            base_handlers.append(\"stdout\")\n        if \"file\" in handlers:\n            base_handlers.append(\"file\")\n    return base_handlers\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.start_queue_listener","title":"<code>start_queue_listener(handlers)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def start_queue_listener(self, handlers: dict) -&gt; None:\n    global _queue_listener\n    s = self.settings\n    if not (s.use_queue and (\"queue\" in handlers)):\n        return\n    q_logger = logging.getLogger(s.base_name)\n    qh = next((h for h in q_logger.handlers if isinstance(h, QueueHandler)), None)\n    if qh is None:\n        return\n    q = qh.queue  # type: ignore[attr-defined]\n\n    sink_handlers: list[logging.Handler] = []\n    if \"stdout\" in handlers:\n        sink_handlers.append(self._make_stream_sink())\n    if \"file\" in handlers and s.to_file:\n        sink_handlers.append(self._make_file_sink())\n\n    if _queue_listener:\n        with contextlib.suppress(Exception):\n            _queue_listener.stop()\n    _queue_listener = QueueListener(q, *sink_handlers, respect_handler_level=True)\n    _queue_listener.start()\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.LoggingConfigurator.suppress_noise","title":"<code>suppress_noise(modules_override, force=False)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def suppress_noise(self, modules_override, force: bool = False) -&gt; None:\n    s = self.settings\n    modules = modules_override\n    # Normalize to a list of module names (strings)\n    if modules is None:\n        modules = s.suppress_modules  # env string by default\n    if isinstance(modules, str):\n        modules_list = [m.strip() for m in modules.split(\",\") if m.strip()]\n    else:\n        # Attempt to iterate; if not iterable, coerce to single-item list\n        try:\n            modules_list = [str(m).strip() for m in modules if str(m).strip()]\n        except TypeError:\n            modules_list = [str(modules).strip()] if str(modules).strip() else []\n    for module in modules_list:\n        logger = logging.getLogger(module)\n        if force or logger.level == logging.NOTSET:\n            logger.setLevel(logging.WARNING)\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.OMPFilter","title":"<code>OMPFilter</code>","text":"<p>               Bases: <code>Filter</code></p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>class OMPFilter(logging.Filter):\n    def filter(self, record):\n        # Suppress messages containing \"OMP:\"\n        return \"OMP:\" not in record.getMessage()\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.OMPFilter.filter","title":"<code>filter(record)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def filter(self, record):\n    # Suppress messages containing \"OMP:\"\n    return \"OMP:\" not in record.getMessage()\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.UtcFormatter","title":"<code>UtcFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>UTC ISO-8601 timestamps for plain text logging.</p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>class UtcFormatter(logging.Formatter):\n    \"\"\"UTC ISO-8601 timestamps for plain text logging.\"\"\"\n    # logging.Formatter.converter must accept (float | None) and return struct_time;\n    # time.gmtime satisfies that contract and returns a UTC struct_time.\n    converter = time.gmtime\n\n    def formatTime(self, record, datefmt=None):\n        if datefmt:\n            return super().formatTime(record, datefmt)\n        return datetime.fromtimestamp(record.created, tz=timezone.utc).isoformat()\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.UtcFormatter.converter","title":"<code>converter = time.gmtime</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.logging_config.UtcFormatter.formatTime","title":"<code>formatTime(record, datefmt=None)</code>","text":"Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def formatTime(self, record, datefmt=None):\n    if datefmt:\n        return super().formatTime(record, datefmt)\n    return datetime.fromtimestamp(record.created, tz=timezone.utc).isoformat()\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.get_child_logger","title":"<code>get_child_logger(name, console=False, separate_file=False)</code>","text":"<p>Get a child logger that writes logs to a console or a specified file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the child logger (e.g., module name).</p> required <code>console</code> <code>bool</code> <p>If True, log to the console. If False, do not log to the console.                       If None, inherit console behavior from the parent logger.</p> <code>False</code> <p>Returns:</p> Type Description <code>Logger</code> <p>logging.Logger: Configured child logger.</p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def get_child_logger(name: str, console: bool = False, separate_file: bool = False) -&gt; logging.Logger:\n    \"\"\"\n    Get a child logger that writes logs to a console or a specified file.\n\n    Args:\n        name (str): The name of the child logger (e.g., module name).\n        console (bool, optional): If True, log to the console. If False, do not log to the console.\n                                  If None, inherit console behavior from the parent logger.\n\n    Returns:\n        logging.Logger: Configured child logger.\n    \"\"\"\n\n    def _setup_logfile(name, logger):\n        logfile = BASE_LOG_DIR / f\"{name}.log\"\n        logfile.parent.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n        file_handler = RotatingFileHandler(\n            filename=str(logfile),\n            maxBytes=MAX_FILE_SIZE,  # Use the global MAX_FILE_SIZE\n            backupCount=5,\n            encoding=\"utf-8\",\n        )\n        file_formatter = logging.Formatter(DEFAULT_FILE_FORMAT_STRING)\n        file_handler.setFormatter(file_formatter)\n        logger.addHandler(file_handler)\n\n    # Create the fully qualified child logger name\n    full_name = f\"{BASE_LOG_NAME}.{name}\"\n    logger = logging.getLogger(full_name)\n\n    # Check if the logger already has handlers to avoid duplication\n    if not logger.handlers:\n        # Add console handler if specified\n        if console:\n            console_handler = colorlog.StreamHandler()\n            console_formatter = colorlog.ColoredFormatter(\n                DEFAULT_CONSOLE_FORMAT_STRING,\n                log_colors=LOG_COLORS,\n            )\n            console_handler.setFormatter(console_formatter)\n            logger.addHandler(console_handler)\n\n        # Add file handler if a file path is provided\n        if separate_file:\n            _setup_logfile(name, logger)\n        # Prevent duplication if we've attached custom handlers\n        logger.propagate = not console and not separate_file\n\n    return logger\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.get_logger","title":"<code>get_logger(name)</code>","text":"<p>Preferred helper: returns a namespaced logger under the base project name.</p> <p>Backwards-compatible with existing call sites that used get_child_logger(name).</p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def get_logger(name: str) -&gt; logging.Logger:\n    \"\"\"Preferred helper: returns a namespaced logger under the base project name.\n\n    Backwards-compatible with existing call sites that used get_child_logger(__name__).\n    \"\"\"\n    return logging.getLogger(f\"{BASE_LOG_NAME}.{name}\")\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.priority_info","title":"<code>priority_info(self, message, *args, **kwargs)</code>","text":"<p>Deprecated: use <code>logger.info(msg, extra={\"priority\": \"high\"})</code> instead.</p> <p>This custom level (25) was introduced for highlighting important informational events, but it complicates interoperability with external log shippers and structured log processing. The recommended migration path is to log at the standard INFO level with an added <code>extra</code> field indicating priority.</p> Example <p>logger.info(\"Important event\", extra={\"priority\": \"high\"})</p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def priority_info(self, message, *args, **kwargs):\n    \"\"\"\n    Deprecated: use `logger.info(msg, extra={\"priority\": \"high\"})` instead.\n\n    This custom level (25) was introduced for highlighting important informational\n    events, but it complicates interoperability with external log shippers and\n    structured log processing. The recommended migration path is to log at the\n    standard INFO level with an added `extra` field indicating priority.\n\n    Example:\n        &gt;&gt;&gt; logger.info(\"Important event\", extra={\"priority\": \"high\"})\n    \"\"\"\n    warnings.warn(\n        \"logger.priority_info() is deprecated; use logger.info(..., extra={'priority': 'high'}) instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if self.isEnabledFor(PRIORITY_INFO_LEVEL):\n        # Log normally at PRIORITY_INFO_LEVEL for backward compatibility\n        self._log(PRIORITY_INFO_LEVEL, message, args, **kwargs)\n    else:\n        # Fallback to standard INFO level if not explicitly handled\n        self.info(message, *args, **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.setup_logging","title":"<code>setup_logging(log_level=logging.INFO, log_filepath=DEFAULT_LOG_FILEPATH, max_log_file_size=MAX_FILE_SIZE, backup_count=5, console=True, suppressed_modules=None, *, settings=None)</code>","text":"<p>Initialize project-wide logging using dictConfig, with JSON in prod and colorized/plain text in dev.</p> <p>Backward compatible with previous signature. Prefer using env vars or pass a LogSettings via the keyword-only <code>settings</code> parameter.</p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def setup_logging(\n    log_level=logging.INFO,\n    log_filepath=DEFAULT_LOG_FILEPATH,\n    max_log_file_size=MAX_FILE_SIZE,  # 10MB\n    backup_count=5,\n    console=True,\n    suppressed_modules=None,\n    *,\n    settings: \"LogSettings|None\" = None,\n) -&gt; logging.Logger:\n    \"\"\"\n    Initialize project-wide logging using dictConfig, with JSON in prod and colorized/plain text in dev.\n\n    Backward compatible with previous signature. Prefer using env vars or pass a LogSettings via the\n    keyword-only `settings` parameter.\n    \"\"\"\n    global _queue_listener\n    configurator = LoggingConfigurator(settings=settings)\n    legacy_args = {\n        \"log_level\": log_level,\n        \"log_filepath\": log_filepath,\n        \"max_log_file_size\": max_log_file_size,\n        \"backup_count\": backup_count,\n        \"console\": console,\n    }\n    return configurator.configure(legacy_args=legacy_args, suppressed_modules=suppressed_modules)\n</code></pre>"},{"location":"api/#tnh_scholar.logging_config.setup_logging_legacy","title":"<code>setup_logging_legacy(*args, **kwargs)</code>","text":"<p>Deprecated: use setup_logging().</p> <p>This wrapper preserves old call sites during migration. It emits a DeprecationWarning (once per process) and forwards all arguments to the current setup_logging().</p> Source code in <code>src/tnh_scholar/logging_config.py</code> <pre><code>def setup_logging_legacy(*args, **kwargs) -&gt; logging.Logger:\n    \"\"\"Deprecated: use setup_logging().\n\n    This wrapper preserves old call sites during migration. It emits a DeprecationWarning\n    (once per process) and forwards all arguments to the current setup_logging().\n    \"\"\"\n    warnings.warn(\n        \"setup_logging_legacy() is deprecated; migrate to setup_logging() and get_logger().\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return setup_logging(*args, **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata","title":"<code>metadata</code>","text":""},{"location":"api/#tnh_scholar.metadata.metadata","title":"<code>metadata</code>","text":""},{"location":"api/#tnh_scholar.metadata.metadata.JsonValue","title":"<code>JsonValue = Union[str, int, float, bool, list, dict, None]</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.metadata.metadata.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.metadata.metadata.Frontmatter","title":"<code>Frontmatter</code>","text":"<p>Handles YAML frontmatter embedding and extraction.</p> <p>Note: <code>extract</code> is pure (no I/O). <code>extract_from_file</code> performs I/O and should be treated as adapter-level convenience, not domain-level parsing.</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>class Frontmatter:\n    \"\"\"Handles YAML frontmatter embedding and extraction.\n\n    Note: `extract` is pure (no I/O). `extract_from_file` performs I/O and should be\n    treated as adapter-level convenience, not domain-level parsing.\n    \"\"\"\n    @staticmethod\n    def extract(content: str) -&gt; tuple[Metadata, str]:\n        \"\"\"Extract frontmatter and content from text.\n\n        Args:\n            content: Text with optional YAML frontmatter\n\n        Returns:\n            Tuple of (metadata object, remaining content)\n        \"\"\"\n        pattern = r'^---\\s*\\n(.*?)\\n---\\s*\\n(.*)$'\n        if match := re.match(pattern, content, re.DOTALL):\n            try:\n                yaml_data = safe_yaml_load(match[1], context=\"Frontmatter.extract\")\n                return Metadata(yaml_data or {}), match[2]\n            except yaml.YAMLError:\n                logger.warning(\"YAML Error in Frontmatter extraction.\")\n                return Metadata(), content\n        return Metadata(), content\n\n    @classmethod\n    def extract_from_file(cls, file: Path) -&gt; tuple[Metadata, str]:\n        \"\"\"Adapter-level convenience wrapper that reads from disk then parses.\"\"\"\n        text_str = read_str_from_file(file)\n        return cls.extract(text_str)\n\n    @classmethod\n    def embed(cls, metadata: Metadata, content: str) -&gt; str:\n        \"\"\"Embed metadata as YAML frontmatter.\n\n        Args:\n            metadata: Dictionary of metadata\n            content: Content text\n\n        Returns:\n            Text with embedded frontmatter\n        \"\"\"\n\n        # Combine with content\n        return (\n            f\"{cls.generate(metadata)}\"\n            f\"{content.strip()}\"\n        )\n\n    @staticmethod\n    def generate(metadata: Metadata) -&gt; str:\n        if not metadata:\n            return \"\"\n\n        yaml_str = metadata.to_yaml() \n        return (\n            f\"---\\n\"\n            f\"{yaml_str}---\\n\\n\"\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Frontmatter.embed","title":"<code>embed(metadata, content)</code>  <code>classmethod</code>","text":"<p>Embed metadata as YAML frontmatter.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Metadata</code> <p>Dictionary of metadata</p> required <code>content</code> <code>str</code> <p>Content text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with embedded frontmatter</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>@classmethod\ndef embed(cls, metadata: Metadata, content: str) -&gt; str:\n    \"\"\"Embed metadata as YAML frontmatter.\n\n    Args:\n        metadata: Dictionary of metadata\n        content: Content text\n\n    Returns:\n        Text with embedded frontmatter\n    \"\"\"\n\n    # Combine with content\n    return (\n        f\"{cls.generate(metadata)}\"\n        f\"{content.strip()}\"\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Frontmatter.extract","title":"<code>extract(content)</code>  <code>staticmethod</code>","text":"<p>Extract frontmatter and content from text.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text with optional YAML frontmatter</p> required <p>Returns:</p> Type Description <code>tuple[Metadata, str]</code> <p>Tuple of (metadata object, remaining content)</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>@staticmethod\ndef extract(content: str) -&gt; tuple[Metadata, str]:\n    \"\"\"Extract frontmatter and content from text.\n\n    Args:\n        content: Text with optional YAML frontmatter\n\n    Returns:\n        Tuple of (metadata object, remaining content)\n    \"\"\"\n    pattern = r'^---\\s*\\n(.*?)\\n---\\s*\\n(.*)$'\n    if match := re.match(pattern, content, re.DOTALL):\n        try:\n            yaml_data = safe_yaml_load(match[1], context=\"Frontmatter.extract\")\n            return Metadata(yaml_data or {}), match[2]\n        except yaml.YAMLError:\n            logger.warning(\"YAML Error in Frontmatter extraction.\")\n            return Metadata(), content\n    return Metadata(), content\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Frontmatter.extract_from_file","title":"<code>extract_from_file(file)</code>  <code>classmethod</code>","text":"<p>Adapter-level convenience wrapper that reads from disk then parses.</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>@classmethod\ndef extract_from_file(cls, file: Path) -&gt; tuple[Metadata, str]:\n    \"\"\"Adapter-level convenience wrapper that reads from disk then parses.\"\"\"\n    text_str = read_str_from_file(file)\n    return cls.extract(text_str)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Frontmatter.generate","title":"<code>generate(metadata)</code>  <code>staticmethod</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>@staticmethod\ndef generate(metadata: Metadata) -&gt; str:\n    if not metadata:\n        return \"\"\n\n    yaml_str = metadata.to_yaml() \n    return (\n        f\"---\\n\"\n        f\"{yaml_str}---\\n\\n\"\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata","title":"<code>Metadata</code>","text":"<p>               Bases: <code>MutableMapping</code></p> <p>Flexible metadata container that behaves like a dict while ensuring JSON serializability. Designed for AI processing pipelines where schema flexibility is prioritized over structure.</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>class Metadata(MutableMapping):\n    \"\"\"\n    Flexible metadata container that behaves like a dict while ensuring\n    JSON serializability. Designed for AI processing pipelines where schema\n    flexibility is prioritized over structure.\n    \"\"\"\n    # Type processors at class level\n    _type_processors = {\n        Path: lambda p: path_as_str(p),\n        datetime: lambda d: d.isoformat(),\n    }\n\n    def __init__(\n        self, \n        data: Optional[Union[Dict[str, Any], 'Metadata']] = None\n        ) -&gt; None:\n        self._data: Dict[str, JsonValue] = {}\n        if data is not None:\n            raw_data = data._data if isinstance(data, Metadata) else data\n            processed_data = {\n                k: self._process_value(v) for k, v in raw_data.items()\n            }\n            self.update(processed_data)\n\n    def _process_value(self, value: Any) -&gt; JsonValue:\n        \"\"\"Convert input values to JSON-serializable format.\"\"\"\n        if isinstance(value, tuple(self._type_processors.keys())):\n            for type_, processor in self._type_processors.items():\n                if isinstance(value, type_):\n                    return processor(value)\n        if not isinstance(value, (str, int, float, bool, list, dict, type(None))):\n            raise ValueError(\n                f\"Value {value} of type {type(value)} has no conversion to JsonValue.\")\n        return value\n\n    # Core dict interface\n    def __getitem__(self, key: str) -&gt; JsonValue:\n        return self._data[key]\n\n    def __setitem__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Process and set value, ensuring JSON serializability.\"\"\"\n        self._data[key] = self._process_value(value)\n\n    def __delitem__(self, key: str) -&gt; None:\n        del self._data[key]\n\n    def __iter__(self) -&gt; Iterator[str]:\n        return iter(self._data)\n\n    def __len__(self) -&gt; int:\n        return len(self._data)\n\n    def __str__(self) -&gt; str:\n        return self.to_yaml()\n\n    # Dict union operations (|, |=)\n    def __or__(self, other: Union[Mapping[str, JsonValue], 'Metadata']) -&gt; 'Metadata':\n        if isinstance(other, (Metadata, Mapping)):\n            other_dict = other._data if isinstance(other, Metadata) else other\n            return Metadata(self._data | other_dict) # type: ignore\n        return NotImplemented\n\n    def __ror__(self, other: Mapping[str, JsonValue]) -&gt; 'Metadata':\n        if isinstance(other, Mapping):\n            return Metadata(other | self._data) # type: ignore\n        return NotImplemented\n\n    def __ior__(self, other: Union[Mapping[str, JsonValue], 'Metadata']) -&gt; 'Metadata':\n        if isinstance(other, (Metadata, Mapping)):\n            self._data |= (other._data if isinstance(other, Metadata) else other)\n            return self\n        return NotImplemented\n\n    def __repr__(self) -&gt; str:\n        return f\"Metadata({self._data})\"\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls,\n        source_type: Any,\n        handler: Callable[[Any], core_schema.CoreSchema],\n    ) -&gt; core_schema.CoreSchema:\n        \"\"\"Defines the Pydantic core schema for the `Metadata` class.\n\n        This method allows Pydantic to validate `Metadata` objects as dictionaries.\n        It handles both direct `Metadata` instances and dictionaries during validation,\n        providing flexibility for data input.\n\n        Args:\n            source_type: The source type being validated.\n            handler: A callable to handle schema generation for other types.\n\n        Returns:\n            A Pydantic core schema that validates either a Metadata instance\n            (by converting it to a dictionary) or a standard dictionary.\n        \"\"\"\n        return core_schema.union_schema(\n            choices=[\n                # Handle Metadata instances with serialization\n                core_schema.is_instance_schema(\n                    cls,\n                    serialization=core_schema.plain_serializer_function_ser_schema(\n                        lambda x: x.to_dict()  # Use our to_dict method\n                    )\n                ),\n                # Handle dictionary input\n                handler(dict),\n            ],\n        )\n\n    # JSON serialization\n    def to_dict(self) -&gt; Dict[str, JsonValue]:\n        \"\"\"Convert to plain dict for JSON serialization.\"\"\"\n        return self._data.copy()\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, JsonValue]) -&gt; 'Metadata':\n        \"\"\"Create from a plain dict.\"\"\"\n        return cls(data)\n\n    def copy(self) -&gt; 'Metadata':\n        \"\"\"Create a deep copy of the metadata object.\"\"\"\n        return Metadata(deepcopy(self._data))\n\n    @classmethod\n    def from_fields(cls, data: dict, fields: list[str]) -&gt; \"Metadata\":\n        \"\"\"Create a Metadata object by extracting specified fields from a dictionary.\n\n        Args:\n            data: Source dictionary\n            fields: List of field names to extract\n\n        Returns:\n            New Metadata instance with only specified fields\n        \"\"\"\n        filtered = {k: data.get(k) for k in fields if k in data}\n        return cls(filtered)\n\n    @classmethod\n    def from_yaml(cls, yaml_str: str) -&gt; 'Metadata':\n        \"\"\"Create Metadata instance from YAML string.\n\n        Args:\n            yaml_str: YAML formatted string\n\n        Returns:\n            New Metadata instance\n\n        Raises:\n            yaml.YAMLError: If YAML parsing fails\n        \"\"\"\n        if not yaml_str.strip():\n            return cls()\n\n        data = safe_yaml_load(yaml_str, context=\"Metadata.from_yaml()\")\n        return cls(data) if isinstance(data, dict) else cls()\n\n    def text_embed(self, content: str):\n        return Frontmatter.embed(self, content)\n\n    def add_process_info(self, process_metadata: 'ProcessMetadata') -&gt; None:\n        \"\"\"Add process metadata to history.\"\"\"\n        history = self.get(TNH_METADATA_PROCESS_FIELD, [])\n        if not isinstance(history, list):\n            history = []\n        history.append(process_metadata.to_dict())  # Store as dict for serialization\n        self[TNH_METADATA_PROCESS_FIELD] = history\n\n    @property\n    def process_history(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Access process history with proper typing.\"\"\"\n        return self.get(TNH_METADATA_PROCESS_FIELD, [])\n\n    def to_yaml(self) -&gt; str:\n        \"\"\"Return metadata as YAML formatted string\"\"\"\n        return yaml.dump(\n            self._data,\n            default_flow_style=False,\n            allow_unicode=True\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.process_history","title":"<code>process_history</code>  <code>property</code>","text":"<p>Access process history with proper typing.</p>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__delitem__","title":"<code>__delitem__(key)</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __delitem__(self, key: str) -&gt; None:\n    del self._data[key]\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__get_pydantic_core_schema__","title":"<code>__get_pydantic_core_schema__(source_type, handler)</code>  <code>classmethod</code>","text":"<p>Defines the Pydantic core schema for the <code>Metadata</code> class.</p> <p>This method allows Pydantic to validate <code>Metadata</code> objects as dictionaries. It handles both direct <code>Metadata</code> instances and dictionaries during validation, providing flexibility for data input.</p> <p>Parameters:</p> Name Type Description Default <code>source_type</code> <code>Any</code> <p>The source type being validated.</p> required <code>handler</code> <code>Callable[[Any], CoreSchema]</code> <p>A callable to handle schema generation for other types.</p> required <p>Returns:</p> Type Description <code>CoreSchema</code> <p>A Pydantic core schema that validates either a Metadata instance</p> <code>CoreSchema</code> <p>(by converting it to a dictionary) or a standard dictionary.</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(\n    cls,\n    source_type: Any,\n    handler: Callable[[Any], core_schema.CoreSchema],\n) -&gt; core_schema.CoreSchema:\n    \"\"\"Defines the Pydantic core schema for the `Metadata` class.\n\n    This method allows Pydantic to validate `Metadata` objects as dictionaries.\n    It handles both direct `Metadata` instances and dictionaries during validation,\n    providing flexibility for data input.\n\n    Args:\n        source_type: The source type being validated.\n        handler: A callable to handle schema generation for other types.\n\n    Returns:\n        A Pydantic core schema that validates either a Metadata instance\n        (by converting it to a dictionary) or a standard dictionary.\n    \"\"\"\n    return core_schema.union_schema(\n        choices=[\n            # Handle Metadata instances with serialization\n            core_schema.is_instance_schema(\n                cls,\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    lambda x: x.to_dict()  # Use our to_dict method\n                )\n            ),\n            # Handle dictionary input\n            handler(dict),\n        ],\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__getitem__","title":"<code>__getitem__(key)</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __getitem__(self, key: str) -&gt; JsonValue:\n    return self._data[key]\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__init__","title":"<code>__init__(data=None)</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __init__(\n    self, \n    data: Optional[Union[Dict[str, Any], 'Metadata']] = None\n    ) -&gt; None:\n    self._data: Dict[str, JsonValue] = {}\n    if data is not None:\n        raw_data = data._data if isinstance(data, Metadata) else data\n        processed_data = {\n            k: self._process_value(v) for k, v in raw_data.items()\n        }\n        self.update(processed_data)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__ior__","title":"<code>__ior__(other)</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __ior__(self, other: Union[Mapping[str, JsonValue], 'Metadata']) -&gt; 'Metadata':\n    if isinstance(other, (Metadata, Mapping)):\n        self._data |= (other._data if isinstance(other, Metadata) else other)\n        return self\n    return NotImplemented\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__iter__","title":"<code>__iter__()</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:\n    return iter(self._data)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__len__","title":"<code>__len__()</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._data)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__or__","title":"<code>__or__(other)</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __or__(self, other: Union[Mapping[str, JsonValue], 'Metadata']) -&gt; 'Metadata':\n    if isinstance(other, (Metadata, Mapping)):\n        other_dict = other._data if isinstance(other, Metadata) else other\n        return Metadata(self._data | other_dict) # type: ignore\n    return NotImplemented\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__repr__","title":"<code>__repr__()</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"Metadata({self._data})\"\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__ror__","title":"<code>__ror__(other)</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __ror__(self, other: Mapping[str, JsonValue]) -&gt; 'Metadata':\n    if isinstance(other, Mapping):\n        return Metadata(other | self._data) # type: ignore\n    return NotImplemented\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Process and set value, ensuring JSON serializability.</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __setitem__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Process and set value, ensuring JSON serializability.\"\"\"\n    self._data[key] = self._process_value(value)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.__str__","title":"<code>__str__()</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __str__(self) -&gt; str:\n    return self.to_yaml()\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.add_process_info","title":"<code>add_process_info(process_metadata)</code>","text":"<p>Add process metadata to history.</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def add_process_info(self, process_metadata: 'ProcessMetadata') -&gt; None:\n    \"\"\"Add process metadata to history.\"\"\"\n    history = self.get(TNH_METADATA_PROCESS_FIELD, [])\n    if not isinstance(history, list):\n        history = []\n    history.append(process_metadata.to_dict())  # Store as dict for serialization\n    self[TNH_METADATA_PROCESS_FIELD] = history\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.copy","title":"<code>copy()</code>","text":"<p>Create a deep copy of the metadata object.</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def copy(self) -&gt; 'Metadata':\n    \"\"\"Create a deep copy of the metadata object.\"\"\"\n    return Metadata(deepcopy(self._data))\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create from a plain dict.</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, JsonValue]) -&gt; 'Metadata':\n    \"\"\"Create from a plain dict.\"\"\"\n    return cls(data)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.from_fields","title":"<code>from_fields(data, fields)</code>  <code>classmethod</code>","text":"<p>Create a Metadata object by extracting specified fields from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Source dictionary</p> required <code>fields</code> <code>list[str]</code> <p>List of field names to extract</p> required <p>Returns:</p> Type Description <code>Metadata</code> <p>New Metadata instance with only specified fields</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>@classmethod\ndef from_fields(cls, data: dict, fields: list[str]) -&gt; \"Metadata\":\n    \"\"\"Create a Metadata object by extracting specified fields from a dictionary.\n\n    Args:\n        data: Source dictionary\n        fields: List of field names to extract\n\n    Returns:\n        New Metadata instance with only specified fields\n    \"\"\"\n    filtered = {k: data.get(k) for k in fields if k in data}\n    return cls(filtered)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.from_yaml","title":"<code>from_yaml(yaml_str)</code>  <code>classmethod</code>","text":"<p>Create Metadata instance from YAML string.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_str</code> <code>str</code> <p>YAML formatted string</p> required <p>Returns:</p> Type Description <code>Metadata</code> <p>New Metadata instance</p> <p>Raises:</p> Type Description <code>YAMLError</code> <p>If YAML parsing fails</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>@classmethod\ndef from_yaml(cls, yaml_str: str) -&gt; 'Metadata':\n    \"\"\"Create Metadata instance from YAML string.\n\n    Args:\n        yaml_str: YAML formatted string\n\n    Returns:\n        New Metadata instance\n\n    Raises:\n        yaml.YAMLError: If YAML parsing fails\n    \"\"\"\n    if not yaml_str.strip():\n        return cls()\n\n    data = safe_yaml_load(yaml_str, context=\"Metadata.from_yaml()\")\n    return cls(data) if isinstance(data, dict) else cls()\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.text_embed","title":"<code>text_embed(content)</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def text_embed(self, content: str):\n    return Frontmatter.embed(self, content)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to plain dict for JSON serialization.</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, JsonValue]:\n    \"\"\"Convert to plain dict for JSON serialization.\"\"\"\n    return self._data.copy()\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.Metadata.to_yaml","title":"<code>to_yaml()</code>","text":"<p>Return metadata as YAML formatted string</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def to_yaml(self) -&gt; str:\n    \"\"\"Return metadata as YAML formatted string\"\"\"\n    return yaml.dump(\n        self._data,\n        default_flow_style=False,\n        allow_unicode=True\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.ProcessMetadata","title":"<code>ProcessMetadata</code>","text":"<p>               Bases: <code>Metadata</code></p> <p>Records information about a specific processing operation.</p> Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>class ProcessMetadata(Metadata):\n    \"\"\"Records information about a specific processing operation.\"\"\"\n    def __init__(\n        self,\n        step: str,\n        processor: str, \n        tool: Optional[str] = None,\n        **additional_params\n    ):\n        # Initialize base Metadata with our process data structure\n        super().__init__({\n            \"step\": step,\n            \"timestamp\": datetime.now(),\n            \"processor\": processor,\n            \"tool\": tool,\n        })\n\n        # Add any additional parameters at top level\n        self.update(additional_params)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.ProcessMetadata.__init__","title":"<code>__init__(step, processor, tool=None, **additional_params)</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def __init__(\n    self,\n    step: str,\n    processor: str, \n    tool: Optional[str] = None,\n    **additional_params\n):\n    # Initialize base Metadata with our process data structure\n    super().__init__({\n        \"step\": step,\n        \"timestamp\": datetime.now(),\n        \"processor\": processor,\n        \"tool\": tool,\n    })\n\n    # Add any additional parameters at top level\n    self.update(additional_params)\n</code></pre>"},{"location":"api/#tnh_scholar.metadata.metadata.safe_yaml_load","title":"<code>safe_yaml_load(yaml_str, *, context='unknown')</code>","text":"Source code in <code>src/tnh_scholar/metadata/metadata.py</code> <pre><code>def safe_yaml_load(yaml_str: str, *, context: str = \"unknown\") -&gt; dict:\n    try:\n        data = yaml.safe_load(yaml_str)\n        if not isinstance(data, dict):\n            logger.warning(\n                \"YAML in [%s] is not a dict. Returning empty metadata.\", context\n                )\n            return {}\n        return data\n    except ScannerError as e:\n        snippet = yaml_str.replace(\"\\n\", \"\\\\n\")\n        logger.error(\"YAML ScannerError in [%s]: %s\\nSnippet:\\n%s\", context, e, snippet)\n    except yaml.YAMLError as e:\n        logger.error(\"General YAML error in [%s]: %s\", context, e)\n    return {}\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing","title":"<code>ocr_processing</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.DEFAULT_ANNOTATION_FONT_PATH","title":"<code>DEFAULT_ANNOTATION_FONT_PATH = Path('/System/Library/Fonts/Supplemental/Arial.ttf')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.DEFAULT_ANNOTATION_FONT_SIZE","title":"<code>DEFAULT_ANNOTATION_FONT_SIZE = 12</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.DEFAULT_ANNOTATION_LANGUAGE_HINTS","title":"<code>DEFAULT_ANNOTATION_LANGUAGE_HINTS = ['vi']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.DEFAULT_ANNOTATION_METHOD","title":"<code>DEFAULT_ANNOTATION_METHOD = 'DOCUMENT_TEXT_DETECTION'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.DEFAULT_ANNOTATION_OFFSET","title":"<code>DEFAULT_ANNOTATION_OFFSET = 2</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.logger","title":"<code>logger = logging.getLogger('ocr_processing')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.PDFParseWarning","title":"<code>PDFParseWarning</code>","text":"<p>               Bases: <code>Warning</code></p> <p>Custom warning class for PDF parsing issues. Encapsulates minimal logic for displaying warnings with a custom format.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>class PDFParseWarning(Warning):\n    \"\"\"\n    Custom warning class for PDF parsing issues.\n    Encapsulates minimal logic for displaying warnings with a custom format.\n    \"\"\"\n\n    @staticmethod\n    def warn(message: str):\n        \"\"\"\n        Display a warning message with custom formatting.\n\n        Parameters:\n            message (str): The warning message to display.\n        \"\"\"\n        formatted_message = f\"\\033[93mPDFParseWarning: {message}\\033[0m\"\n        print(formatted_message)  # Simply prints the warning\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.PDFParseWarning.warn","title":"<code>warn(message)</code>  <code>staticmethod</code>","text":"<p>Display a warning message with custom formatting.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The warning message to display.</p> required Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>@staticmethod\ndef warn(message: str):\n    \"\"\"\n    Display a warning message with custom formatting.\n\n    Parameters:\n        message (str): The warning message to display.\n    \"\"\"\n    formatted_message = f\"\\033[93mPDFParseWarning: {message}\\033[0m\"\n    print(formatted_message)  # Simply prints the warning\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.annotate_image_with_text","title":"<code>annotate_image_with_text(image, text_annotations, annotation_font_path, font_size=12)</code>","text":"<p>Annotates a PIL image with bounding boxes and text descriptions from OCR results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input PIL image to annotate.</p> required <code>text_annotations</code> <code>List[EntityAnnotation]</code> <p>OCR results containing bounding boxes and text.</p> required <code>annotation_font_path</code> <code>str</code> <p>Path to the font file for text annotations.</p> required <code>font_size</code> <code>int</code> <p>Font size for text annotations.</p> <code>12</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The annotated PIL image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input image is None.</p> <code>IOError</code> <p>If the font file cannot be loaded.</p> <code>Exception</code> <p>For any other unexpected errors.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def annotate_image_with_text(\n    image: Image.Image,\n    text_annotations: List[EntityAnnotation],\n    annotation_font_path: str,\n    font_size: int = 12,\n) -&gt; Image.Image:\n    \"\"\"\n    Annotates a PIL image with bounding boxes and text descriptions from OCR results.\n\n    Parameters:\n        image (Image.Image): The input PIL image to annotate.\n        text_annotations (List[EntityAnnotation]): OCR results containing bounding boxes and text.\n        annotation_font_path (str): Path to the font file for text annotations.\n        font_size (int): Font size for text annotations.\n\n    Returns:\n        Image.Image: The annotated PIL image.\n\n    Raises:\n        ValueError: If the input image is None.\n        IOError: If the font file cannot be loaded.\n        Exception: For any other unexpected errors.\n    \"\"\"\n    if image is None:\n        raise ValueError(\"The input image is None.\")\n\n    try:\n        font = ImageFont.truetype(annotation_font_path, font_size)\n    except IOError as e:\n        raise IOError(f\"Failed to load the font from '{annotation_font_path}': {e}\")\n\n    draw = ImageDraw.Draw(image)\n\n    try:\n        for i, text_obj in enumerate(text_annotations):\n            vertices = [\n                (vertex.x, vertex.y) for vertex in text_obj.bounding_poly.vertices\n            ]\n            if (\n                len(vertices) == 4\n            ):  # Ensure there are exactly 4 vertices for a rectangle\n                # Draw the bounding box\n                draw.polygon(vertices, outline=\"red\", width=2)\n\n                # Skip the first bounding box (whole text region)\n                if i &gt; 0:\n                    # Offset the text position slightly for clarity\n                    text_position = (vertices[0][0] + 2, vertices[0][1] + 2)\n                    draw.text(\n                        text_position, text_obj.description, fill=\"red\", font=font\n                    )\n\n    except AttributeError as e:\n        raise ValueError(f\"Invalid text annotation structure: {e}\")\n    except Exception as e:\n        raise Exception(f\"An error occurred during image annotation: {e}\")\n\n    return image\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.build_processed_pdf","title":"<code>build_processed_pdf(pdf_path, client, preprocessor=None, annotation_font_path=DEFAULT_ANNOTATION_FONT_PATH)</code>","text":"<p>Processes a PDF document, extracting text, word locations, annotated images, and unannotated images.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>Path</code> <p>Path to the PDF file.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>annotation_font_path</code> <code>Path</code> <p>Path to the font file for annotations.</p> <code>DEFAULT_ANNOTATION_FONT_PATH</code> <p>Returns:</p> Type Description <code>Tuple[List[str], List[List[EntityAnnotation]], List[Image], List[Image]]</code> <p>Tuple[List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]]: - List of extracted full-page texts (one entry per page). - List of word locations (list of <code>vision.EntityAnnotation</code> objects for each page). - List of annotated images (with bounding boxes and text annotations). - List of unannotated images (raw page images).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified PDF file does not exist.</p> <code>ValueError</code> <p>If the PDF file is invalid or contains no pages.</p> <code>Exception</code> <p>For any unexpected errors during processing.</p> Example <p>from pathlib import Path from google.cloud import vision pdf_path = Path(\"/path/to/example.pdf\") font_path = Path(\"/path/to/fonts/Arial.ttf\") client = vision.ImageAnnotatorClient() try:     text_pages, word_locations_list, annotated_images, unannotated_images = build_processed_pdf(         pdf_path, client, font_path     )     print(f\"Processed {len(text_pages)} pages successfully!\") except Exception as e:     print(f\"Error processing PDF: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def build_processed_pdf(\n    pdf_path: Path,\n    client: vision.ImageAnnotatorClient,\n    preprocessor: Callable = None,\n    annotation_font_path: Path = DEFAULT_ANNOTATION_FONT_PATH,\n) -&gt; Tuple[\n    List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]\n]:\n    \"\"\"\n    Processes a PDF document, extracting text, word locations, annotated images, and unannotated images.\n\n    Parameters:\n        pdf_path (Path): Path to the PDF file.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        annotation_font_path (Path): Path to the font file for annotations.\n\n    Returns:\n        Tuple[List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]]:\n            - List of extracted full-page texts (one entry per page).\n            - List of word locations (list of `vision.EntityAnnotation` objects for each page).\n            - List of annotated images (with bounding boxes and text annotations).\n            - List of unannotated images (raw page images).\n\n    Raises:\n        FileNotFoundError: If the specified PDF file does not exist.\n        ValueError: If the PDF file is invalid or contains no pages.\n        Exception: For any unexpected errors during processing.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from google.cloud import vision\n        &gt;&gt;&gt; pdf_path = Path(\"/path/to/example.pdf\")\n        &gt;&gt;&gt; font_path = Path(\"/path/to/fonts/Arial.ttf\")\n        &gt;&gt;&gt; client = vision.ImageAnnotatorClient()\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     text_pages, word_locations_list, annotated_images, unannotated_images = build_processed_pdf(\n        &gt;&gt;&gt;         pdf_path, client, font_path\n        &gt;&gt;&gt;     )\n        &gt;&gt;&gt;     print(f\"Processed {len(text_pages)} pages successfully!\")\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error processing PDF: {e}\")\n    \"\"\"\n    try:\n        doc = load_pdf_pages(pdf_path)\n    except FileNotFoundError as fnf_error:\n        raise FileNotFoundError(f\"Error loading PDF: {fnf_error}\")\n    except ValueError as ve:\n        raise ValueError(f\"Invalid PDF file: {ve}\")\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred while loading the PDF: {e}\")\n\n    if doc.page_count == 0:\n        raise ValueError(f\"The PDF file '{pdf_path}' contains no pages.\")\n\n    logger.info(f\"Processing file with {doc.page_count} pages:\\n\\t{pdf_path}\")\n\n    text_pages = []\n    word_locations_list = []\n    annotated_images = []\n    unannotated_images = []\n    first_page_dimensions = None\n\n    for page_num in range(doc.page_count):\n        logger.info(f\"Processing page {page_num + 1}/{doc.page_count}...\")\n\n        try:\n            page = doc.load_page(page_num)\n            (\n                full_page_text,\n                word_locations,\n                annotated_image,\n                unannotated_image,\n                page_dimensions,\n            ) = process_page(page, client, annotation_font_path, preprocessor)\n\n            if full_page_text:  # this is not an empty page\n\n                if page_num == 0:  # save first page info\n                    first_page_dimensions = page_dimensions\n                elif (\n                    page_dimensions != first_page_dimensions\n                ):  # verify page dimensions are consistent\n                    PDFParseWarning.warn(\n                        f\"Page {page_num + 1} has different dimensions than page 1.\"\n                        f\"({page_dimensions}) compared to the first page: ({first_page_dimensions}).\"\n                    )\n\n                text_pages.append(full_page_text)\n                word_locations_list.append(word_locations)\n                annotated_images.append(annotated_image)\n                unannotated_images.append(unannotated_image)\n            else:\n                PDFParseWarning.warn(\n                    f\"Page {page_num + 1} empty, added empty datastructures...\\n\"\n                    # f\"  (Note that total document length will be reduced.)\"\n                )\n\n        except ValueError as ve:\n            print(f\"ValueError on page {page_num + 1}: {ve}\")\n        except OSError as oe:\n            print(f\"OSError on page {page_num + 1}: {oe}\")\n        except Exception as e:\n            print(f\"Unexpected error on page {page_num + 1}: {e}\")\n\n    print(f\"page dimensions: {page_dimensions}\")\n    return text_pages, word_locations_list, annotated_images, unannotated_images\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.deserialize_entity_annotations_from_json","title":"<code>deserialize_entity_annotations_from_json(data)</code>","text":"<p>Deserializes JSON data into a nested list of EntityAnnotation objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The JSON string containing serialized annotations.</p> required <p>Returns:</p> Type Description <code>List[List[EntityAnnotation]]</code> <p>List[List[EntityAnnotation]]: The reconstructed nested list of EntityAnnotation objects.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def deserialize_entity_annotations_from_json(data: str) -&gt; List[List[EntityAnnotation]]:\n    \"\"\"\n    Deserializes JSON data into a nested list of EntityAnnotation objects.\n\n    Parameters:\n        data (str): The JSON string containing serialized annotations.\n\n    Returns:\n        List[List[EntityAnnotation]]: The reconstructed nested list of EntityAnnotation objects.\n    \"\"\"\n    serialized_data = json.loads(data)\n    deserialized_data = []\n\n    for serialized_page in serialized_data:\n        page_annotations = [\n            EntityAnnotation.deserialize(base64.b64decode(serialized_annotation))\n            for serialized_annotation in serialized_page\n        ]\n        deserialized_data.append(page_annotations)\n\n    return deserialized_data\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.extract_image_from_page","title":"<code>extract_image_from_page(page)</code>","text":"<p>Extracts the first image from the given PDF page and returns it as a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The PDF page object.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The first image on the page as a Pillow Image object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no images are found on the page or the image data is incomplete.</p> <code>Exception</code> <p>For unexpected errors during image extraction.</p> Example <p>import fitz from PIL import Image doc = fitz.open(\"/path/to/document.pdf\") page = doc.load_page(0)  # Load the first page try:     image = extract_image_from_page(page)     image.show()  # Display the image except Exception as e:     print(f\"Error extracting image: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def extract_image_from_page(page: fitz.Page) -&gt; Image.Image:\n    \"\"\"\n    Extracts the first image from the given PDF page and returns it as a PIL Image.\n\n    Parameters:\n        page (fitz.Page): The PDF page object.\n\n    Returns:\n        Image.Image: The first image on the page as a Pillow Image object.\n\n    Raises:\n        ValueError: If no images are found on the page or the image data is incomplete.\n        Exception: For unexpected errors during image extraction.\n\n    Example:\n        &gt;&gt;&gt; import fitz\n        &gt;&gt;&gt; from PIL import Image\n        &gt;&gt;&gt; doc = fitz.open(\"/path/to/document.pdf\")\n        &gt;&gt;&gt; page = doc.load_page(0)  # Load the first page\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     image = extract_image_from_page(page)\n        &gt;&gt;&gt;     image.show()  # Display the image\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error extracting image: {e}\")\n    \"\"\"\n    try:\n        # Get images from the page\n        images = page.get_images(full=True)\n        if not images:\n            raise ValueError(\"No images found on the page.\")\n\n        # Extract the first image reference\n        xref = images[0][0]  # Get the first image's xref\n        base_image = page.parent.extract_image(xref)\n\n        # Validate the extracted image data\n        if (\n            \"image\" not in base_image\n            or \"width\" not in base_image\n            or \"height\" not in base_image\n        ):\n            raise ValueError(\"The extracted image data is incomplete.\")\n\n        # Convert the raw image bytes into a Pillow image\n        image_bytes = base_image[\"image\"]\n        pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n\n        return pil_image\n\n    except ValueError as ve:\n        raise ve  # Re-raise for calling functions to handle\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred during image extraction: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.get_page_dimensions","title":"<code>get_page_dimensions(page)</code>","text":"<p>Extracts the width and height of a single PDF page in both inches and pixels.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>A single PDF page object from PyMuPDF.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the width and height of the page in inches and pixels.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def get_page_dimensions(page: fitz.Page) -&gt; dict:\n    \"\"\"\n    Extracts the width and height of a single PDF page in both inches and pixels.\n\n    Args:\n        page (fitz.Page): A single PDF page object from PyMuPDF.\n\n    Returns:\n        dict: A dictionary containing the width and height of the page in inches and pixels.\n    \"\"\"\n    # Get page dimensions in points and convert to inches\n    page_width_pts, page_height_pts = page.rect.width, page.rect.height\n    page_width_in = page_width_pts / 72  # Convert points to inches\n    page_height_in = page_height_pts / 72\n\n    # Extract the first image on the page (if any) to get pixel dimensions\n    images = page.get_images(full=True)\n    if images:\n        xref = images[0][0]\n        base_image = page.parent.extract_image(xref)\n        width_px = base_image[\"width\"]\n        height_px = base_image[\"height\"]\n    else:\n        width_px, height_px = None, None  # No image found on the page\n\n    # Return dimensions\n    return {\n        \"width_in\": page_width_in,\n        \"height_in\": page_height_in,\n        \"width_px\": width_px,\n        \"height_px\": height_px,\n    }\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.load_pdf_pages","title":"<code>load_pdf_pages(pdf_path)</code>","text":"<p>Opens the PDF document and returns the fitz Document object.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>Path</code> <p>The path to the PDF file.</p> required <p>Returns:</p> Type Description <code>Document</code> <p>fitz.Document: The loaded PDF document.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>ValueError</code> <p>If the file is not a valid PDF document.</p> <code>Exception</code> <p>For any unexpected error.</p> Example <p>from pathlib import Path pdf_path = Path(\"/path/to/example.pdf\") try:     pdf_doc = load_pdf_pages(pdf_path)     print(f\"PDF contains {pdf_doc.page_count} pages.\") except Exception as e:     print(f\"Error loading PDF: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def load_pdf_pages(pdf_path: Path) -&gt; fitz.Document:\n    \"\"\"\n    Opens the PDF document and returns the fitz Document object.\n\n    Parameters:\n        pdf_path (Path): The path to the PDF file.\n\n    Returns:\n        fitz.Document: The loaded PDF document.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        ValueError: If the file is not a valid PDF document.\n        Exception: For any unexpected error.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; pdf_path = Path(\"/path/to/example.pdf\")\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     pdf_doc = load_pdf_pages(pdf_path)\n        &gt;&gt;&gt;     print(f\"PDF contains {pdf_doc.page_count} pages.\")\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error loading PDF: {e}\")\n    \"\"\"\n    if not pdf_path.exists():\n        raise FileNotFoundError(f\"The file '{pdf_path}' does not exist.\")\n\n    if not pdf_path.suffix.lower() == \".pdf\":\n        raise ValueError(\n            f\"The file '{pdf_path}' is not a valid PDF document (expected '.pdf').\"\n        )\n\n    try:\n        return fitz.open(str(pdf_path))  # PyMuPDF expects a string path\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred while opening the PDF: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.load_processed_PDF_data","title":"<code>load_processed_PDF_data(base_path)</code>","text":"<p>Loads processed PDF data from files using metadata for file references.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>Base path where processed assets are stored.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[List[EntityAnnotation]], List[Image], List[Image]]</code> <p>Tuple[List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]]: - Loaded text pages. - Word locations (list of <code>EntityAnnotation</code> objects for each page). - Annotated images. - Unannotated images.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If any required files are missing.</p> <code>ValueError</code> <p>If the metadata file is incomplete or invalid.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def load_processed_PDF_data(\n    base_path: Path,\n) -&gt; Tuple[\n    List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]\n]:\n    \"\"\"\n    Loads processed PDF data from files using metadata for file references.\n\n    Parameters:\n        base_path (Path): Base path where processed assets are stored.\n\n    Returns:\n        Tuple[List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]]:\n            - Loaded text pages.\n            - Word locations (list of `EntityAnnotation` objects for each page).\n            - Annotated images.\n            - Unannotated images.\n\n    Raises:\n        FileNotFoundError: If any required files are missing.\n        ValueError: If the metadata file is incomplete or invalid.\n    \"\"\"\n    metadata_file = base_path / \"metadata.json\"\n\n    # Load metadata\n    try:\n        with metadata_file.open(\"r\", encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Metadata file '{metadata_file}' not found.\")\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid metadata file format: {e}\")\n\n    # Extract file paths from metadata\n    text_pages_file = base_path / metadata.get(\"files\", {}).get(\n        \"text_pages\", \"text_pages.json\"\n    )\n    word_locations_file = base_path / metadata.get(\"files\", {}).get(\n        \"word_locations\", \"word_locations.json\"\n    )\n    images_dir = Path(metadata.get(\"images_directory\", base_path / \"images\"))\n\n    # Validate file paths\n    if not text_pages_file.exists():\n        raise FileNotFoundError(f\"Text pages file '{text_pages_file}' not found.\")\n    if not word_locations_file.exists():\n        raise FileNotFoundError(\n            f\"Word locations file '{word_locations_file}' not found.\"\n        )\n    if not images_dir.exists() or not images_dir.is_dir():\n        raise FileNotFoundError(f\"Images directory '{images_dir}' not found.\")\n\n    # Load text pages\n    with text_pages_file.open(\"r\", encoding=\"utf-8\") as f:\n        text_pages = json.load(f)\n\n    # Load word locations\n    with word_locations_file.open(\"r\", encoding=\"utf-8\") as f:\n        serialized_word_locations = f.read()\n        word_locations = deserialize_entity_annotations_from_json(\n            serialized_word_locations\n        )\n\n    # Load images\n    annotated_images = []\n    unannotated_images = []\n    for file in sorted(\n        images_dir.iterdir()\n    ):  # Iterate over files in the images directory\n        if file.name.startswith(\"annotated_page_\") and file.suffix == \".png\":\n            annotated_images.append(Image.open(file))\n        elif file.name.startswith(\"unannotated_page_\") and file.suffix == \".png\":\n            unannotated_images.append(Image.open(file))\n\n    # Ensure images were loaded correctly\n    if not annotated_images or not unannotated_images:\n        raise ValueError(f\"No images found in the directory '{images_dir}'.\")\n\n    return text_pages, word_locations, annotated_images, unannotated_images\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.make_image_preprocess_mask","title":"<code>make_image_preprocess_mask(mask_height)</code>","text":"<p>Creates a preprocessing function that masks a specified height at the bottom of the image.</p> <p>Parameters:</p> Name Type Description Default <code>mask_height</code> <code>float</code> <p>The proportion of the image height to mask at the bottom (0.0 to 1.0).</p> required <p>Returns:</p> Type Description <code>Callable[[Image, int], Image]</code> <p>Callable[[Image.Image, int], Image.Image]: A preprocessing function that takes an image</p> <code>Callable[[Image, int], Image]</code> <p>and page number as input and returns the processed image.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def make_image_preprocess_mask(\n    mask_height: float,\n) -&gt; Callable[[Image.Image, int], Image.Image]:\n    \"\"\"\n    Creates a preprocessing function that masks a specified height at the bottom of the image.\n\n    Parameters:\n        mask_height (float): The proportion of the image height to mask at the bottom (0.0 to 1.0).\n\n    Returns:\n        Callable[[Image.Image, int], Image.Image]: A preprocessing function that takes an image\n        and page number as input and returns the processed image.\n    \"\"\"\n\n    def pre_process_image(image: Image.Image, page_number: int) -&gt; Image.Image:\n        \"\"\"\n        Preprocesses the image by masking the bottom region or performing other preprocessing steps.\n\n        Parameters:\n            image (Image.Image): The input image as a Pillow object.\n            page_number (int): The page number of the image (useful for conditional preprocessing).\n\n        Returns:\n            Image.Image: The preprocessed image.\n        \"\"\"\n\n        if page_number &gt; 0:  # don't apply mask to cover page.\n            # Make a copy of the image to avoid modifying the original\n            draw = ImageDraw.Draw(image)\n\n            # Get image dimensions\n            width, height = image.size\n\n            # Mask the bottom region based on the specified height proportion\n            mask_pixels = int(height * mask_height)\n            draw.rectangle([(0, height - mask_pixels), (width, height)], fill=\"black\")\n\n        return image\n\n    return pre_process_image\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.pil_to_bytes","title":"<code>pil_to_bytes(image, format='PNG')</code>","text":"<p>Converts a Pillow image to raw bytes.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The Pillow image object to convert.</p> required <code>format</code> <code>str</code> <p>The format to save the image as (e.g., \"PNG\", \"JPEG\"). Default is \"PNG\".</p> <code>'PNG'</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The raw bytes of the image.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def pil_to_bytes(image: Image.Image, format: str = \"PNG\") -&gt; bytes:\n    \"\"\"\n    Converts a Pillow image to raw bytes.\n\n    Parameters:\n        image (Image.Image): The Pillow image object to convert.\n        format (str): The format to save the image as (e.g., \"PNG\", \"JPEG\"). Default is \"PNG\".\n\n    Returns:\n        bytes: The raw bytes of the image.\n    \"\"\"\n    with io.BytesIO() as output:\n        image.save(output, format=format)\n        return output.getvalue()\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.process_page","title":"<code>process_page(page, client, annotation_font_path, preprocessor=None)</code>","text":"<p>Processes a single PDF page, extracting text, word locations, and annotated images.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The PDF page object.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>preprocessor</code> <code>Callable[[Image, int], Image]</code> <p>Preprocessing function for the image.</p> <code>None</code> <code>annotation_font_path</code> <code>str</code> <p>Path to the font file for annotations.</p> required <p>Returns:</p> Type Description <code>Tuple[str, List[EntityAnnotation], Image, Image, dict]</code> <p>Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]: - Full page text (str) - Word locations (List of vision.EntityAnnotation) - Annotated image (Pillow Image object) - Original unprocessed image (Pillow Image object) - Page dimensions (dict)</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def process_page(\n    page: fitz.Page,\n    client: vision.ImageAnnotatorClient,\n    annotation_font_path: str,\n    preprocessor: Callable[[Image.Image, int], Image.Image] = None,\n) -&gt; Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]:\n    \"\"\"\n    Processes a single PDF page, extracting text, word locations, and annotated images.\n\n    Parameters:\n        page (fitz.Page): The PDF page object.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        preprocessor (Callable[[Image.Image, int], Image.Image]): Preprocessing function for the image.\n        annotation_font_path (str): Path to the font file for annotations.\n\n    Returns:\n        Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]:\n            - Full page text (str)\n            - Word locations (List of vision.EntityAnnotation)\n            - Annotated image (Pillow Image object)\n            - Original unprocessed image (Pillow Image object)\n            - Page dimensions (dict)\n    \"\"\"\n    # Extract the original image from the PDF page\n    original_image = extract_image_from_page(page)\n\n    # Make a copy of the original image for processing\n    processed_image = original_image.copy()\n\n    # Apply the preprocessing function (if provided)\n    if preprocessor:\n        # print(\"preprocessing...\") # debug\n        processed_image = preprocessor(processed_image, page.number)\n        # processed_image.show() # debug\n\n    # Annotate the processed image using the Vision API\n    response = process_single_image(processed_image, client)\n\n    if response:\n        text_annotations = response.text_annotations\n        # Extract full text and word locations\n        full_page_text = text_annotations[0].description if text_annotations else \"\"\n        word_locations = text_annotations[1:] if len(text_annotations) &gt; 1 else []\n    else:\n        # return empty data\n        full_page_text = \"\"\n        word_locations = [EntityAnnotation()]\n        text_annotations = [\n            EntityAnnotation()\n        ]  # create empty data structures to allow storing to proceed.\n\n    # Create an annotated image with bounding boxes and labels\n    annotated_image = annotate_image_with_text(\n        processed_image, text_annotations, annotation_font_path\n    )\n\n    # Get page dimensions (from the original PDF page, not the image)\n    page_dimensions = get_page_dimensions(page)\n\n    return (\n        full_page_text,\n        word_locations,\n        annotated_image,\n        original_image,\n        page_dimensions,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.process_single_image","title":"<code>process_single_image(image, client, feature_type=DEFAULT_ANNOTATION_METHOD, language_hints=DEFAULT_ANNOTATION_LANGUAGE_HINTS)</code>","text":"<p>Processes a single image with the Google Vision API and returns text annotations.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The preprocessed Pillow image object.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>feature_type</code> <code>str</code> <p>Type of text detection to use ('TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION').</p> <code>DEFAULT_ANNOTATION_METHOD</code> <code>language_hints</code> <code>List</code> <p>Language hints for OCR.</p> <code>DEFAULT_ANNOTATION_LANGUAGE_HINTS</code> <p>Returns:</p> Type Description <code>List[EntityAnnotation]</code> <p>List[vision.EntityAnnotation]: Text annotations from the Vision API response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no text is detected.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def process_single_image(\n    image: Image.Image,\n    client: vision.ImageAnnotatorClient,\n    feature_type: str = DEFAULT_ANNOTATION_METHOD,\n    language_hints: List = DEFAULT_ANNOTATION_LANGUAGE_HINTS,\n) -&gt; List[vision.EntityAnnotation]:\n    \"\"\"\n    Processes a single image with the Google Vision API and returns text annotations.\n\n    Parameters:\n        image (Image.Image): The preprocessed Pillow image object.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        feature_type (str): Type of text detection to use ('TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION').\n        language_hints (List): Language hints for OCR.\n\n    Returns:\n        List[vision.EntityAnnotation]: Text annotations from the Vision API response.\n\n    Raises:\n        ValueError: If no text is detected.\n    \"\"\"\n    # Convert the Pillow image to bytes\n    image_bytes = pil_to_bytes(image, format=\"PNG\")\n\n    # Map feature type\n    feature_map = {\n        \"TEXT_DETECTION\": vision.Feature.Type.TEXT_DETECTION,\n        \"DOCUMENT_TEXT_DETECTION\": vision.Feature.Type.DOCUMENT_TEXT_DETECTION,\n    }\n    if feature_type not in feature_map:\n        raise ValueError(\n            f\"Invalid feature type '{feature_type}'. Use 'TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION'.\"\n        )\n\n    # Prepare Vision API request\n    vision_image = vision.Image(content=image_bytes)\n    features = [vision.Feature(type=feature_map[feature_type])]\n    image_context = vision.ImageContext(language_hints=language_hints)\n\n    # Make the API call\n    response = client.annotate_image(\n        {\"image\": vision_image, \"features\": features, \"image_context\": image_context}\n    )\n\n    return response\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.save_processed_pdf_data","title":"<code>save_processed_pdf_data(output_dir, journal_name, text_pages, word_locations, annotated_images, unannotated_images)</code>","text":"<p>Saves processed PDF data to files for later reloading.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>Directory to save the data (as a Path object).</p> required <code>journal_name</code> <code>str</code> <p>Name for the output directory (usually the PDF name without extension).</p> required <code>text_pages</code> <code>List[str]</code> <p>Extracted full-page text.</p> required <code>word_locations</code> <code>List[List[EntityAnnotation]]</code> <p>Word locations and annotations from Vision API.</p> required <code>annotated_images</code> <code>List[Image]</code> <p>Annotated images with bounding boxes.</p> required <code>unannotated_images</code> <code>List[Image]</code> <p>Raw unannotated images.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def save_processed_pdf_data(\n    output_dir: Path,\n    journal_name: str,\n    text_pages: List[str],\n    word_locations: List[List[EntityAnnotation]],\n    annotated_images: List[Image.Image],\n    unannotated_images: List[Image.Image],\n) -&gt; None:\n    \"\"\"\n    Saves processed PDF data to files for later reloading.\n\n    Parameters:\n        output_dir (Path): Directory to save the data (as a Path object).\n        journal_name (str): Name for the output directory (usually the PDF name without extension).\n        text_pages (List[str]): Extracted full-page text.\n        word_locations (List[List[EntityAnnotation]]): Word locations and annotations from Vision API.\n        annotated_images (List[PIL.Image.Image]): Annotated images with bounding boxes.\n        unannotated_images (List[PIL.Image.Image]): Raw unannotated images.\n\n    Returns:\n        None\n    \"\"\"\n    # Create output directories\n    base_path = output_dir / journal_name / \"ocr_data\"\n    images_dir = base_path / \"images\"\n\n    base_path.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save text data\n    text_pages_file = base_path / \"text_pages.json\"\n    with text_pages_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(text_pages, f, indent=4, ensure_ascii=False)\n\n    # Save word locations as JSON\n    word_locations_file = base_path / \"word_locations.json\"\n    serialized_word_locations = serialize_entity_annotations_to_json(word_locations)\n    with word_locations_file.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(serialized_word_locations)\n\n    # Save images\n    for i, annotated_image in enumerate(annotated_images):\n        annotated_file = images_dir / f\"annotated_page_{i + 1}.png\"\n        annotated_image.save(annotated_file)\n    for i, unannotated_image in enumerate(unannotated_images):\n        unannotated_file = images_dir / f\"unannotated_page_{i + 1}.png\"\n        unannotated_image.save(unannotated_file)\n\n    # Save metadata\n    metadata = {\n        \"source_pdf\": journal_name,\n        \"page_count\": len(text_pages),\n        \"images_directory\": str(\n            images_dir\n        ),  # Convert Path to string for JSON serialization\n        \"files\": {\n            \"text_pages\": \"text_pages.json\",\n            \"word_locations\": \"word_locations.json\",\n        },\n    }\n    metadata_file = base_path / \"metadata.json\"\n    with metadata_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(metadata, f, indent=4)\n\n    print(f\"Processed data saved in: {base_path}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.serialize_entity_annotations_to_json","title":"<code>serialize_entity_annotations_to_json(annotations)</code>","text":"<p>Serializes a nested list of EntityAnnotation objects into a JSON-compatible format using Base64 encoding.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>List[List[EntityAnnotation]]</code> <p>The nested list of EntityAnnotation objects.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The serialized data in JSON format as a string.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def serialize_entity_annotations_to_json(\n    annotations: List[List[EntityAnnotation]],\n) -&gt; str:\n    \"\"\"\n    Serializes a nested list of EntityAnnotation objects into a JSON-compatible format using Base64 encoding.\n\n    Parameters:\n        annotations (List[List[EntityAnnotation]]): The nested list of EntityAnnotation objects.\n\n    Returns:\n        str: The serialized data in JSON format as a string.\n    \"\"\"\n    serialized_data = []\n    for page_annotations in annotations:\n        serialized_page = [\n            base64.b64encode(annotation.SerializeToString()).decode(\"utf-8\")\n            for annotation in page_annotations\n        ]\n        serialized_data.append(serialized_page)\n\n    # Convert to a JSON string\n    return json.dumps(serialized_data, indent=4)\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.start_image_annotator_client","title":"<code>start_image_annotator_client(credentials_file=None, api_endpoint='vision.googleapis.com', timeout=(10, 30), enable_logging=False)</code>","text":"<p>Starts and returns a Google Vision API ImageAnnotatorClient with optional configuration.</p> <p>Parameters:</p> Name Type Description Default <code>credentials_file</code> <code>str</code> <p>Path to the credentials JSON file. If None, uses the default environment variable.</p> <code>None</code> <code>api_endpoint</code> <code>str</code> <p>Custom API endpoint for the Vision API. Default is the global endpoint.</p> <code>'vision.googleapis.com'</code> <code>timeout</code> <code>Tuple[int, int]</code> <p>Connection and read timeouts in seconds. Default is (10, 30).</p> <code>(10, 30)</code> <code>enable_logging</code> <code>bool</code> <p>Enable detailed logging for debugging. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ImageAnnotatorClient</code> <p>vision.ImageAnnotatorClient: Configured Vision API client.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified credentials file is not found.</p> <code>Exception</code> <p>For unexpected errors during client setup.</p> Example <p>client = start_image_annotator_client(     credentials_file=\"/path/to/credentials.json\",     api_endpoint=\"vision.googleapis.com\",     timeout=(10, 30),     enable_logging=True ) print(\"Google Vision API client initialized.\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def start_image_annotator_client(\n    credentials_file: str = None,\n    api_endpoint: str = \"vision.googleapis.com\",\n    timeout: Tuple[int, int] = (10, 30),\n    enable_logging: bool = False,\n) -&gt; vision.ImageAnnotatorClient:\n    \"\"\"\n    Starts and returns a Google Vision API ImageAnnotatorClient with optional configuration.\n\n    Parameters:\n        credentials_file (str): Path to the credentials JSON file. If None, uses the default environment variable.\n        api_endpoint (str): Custom API endpoint for the Vision API. Default is the global endpoint.\n        timeout (Tuple[int, int]): Connection and read timeouts in seconds. Default is (10, 30).\n        enable_logging (bool): Enable detailed logging for debugging. Default is False.\n\n    Returns:\n        vision.ImageAnnotatorClient: Configured Vision API client.\n\n    Raises:\n        FileNotFoundError: If the specified credentials file is not found.\n        Exception: For unexpected errors during client setup.\n\n    Example:\n        &gt;&gt;&gt; client = start_image_annotator_client(\n        &gt;&gt;&gt;     credentials_file=\"/path/to/credentials.json\",\n        &gt;&gt;&gt;     api_endpoint=\"vision.googleapis.com\",\n        &gt;&gt;&gt;     timeout=(10, 30),\n        &gt;&gt;&gt;     enable_logging=True\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; print(\"Google Vision API client initialized.\")\n    \"\"\"\n    try:\n        # Set up credentials\n        if credentials_file:\n            if not os.path.exists(credentials_file):\n                raise FileNotFoundError(\n                    f\"Credentials file '{credentials_file}' not found.\"\n                )\n            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_file\n\n        # Configure client options\n        client_options = {\"api_endpoint\": api_endpoint}\n        client = vision.ImageAnnotatorClient(client_options=client_options)\n\n        # Optionally enable logging\n        if enable_logging:\n            print(f\"Vision API Client started with endpoint: {api_endpoint}\")\n            print(f\"Timeout settings: Connect={timeout[0]}s, Read={timeout[1]}s\")\n\n        return client\n\n    except Exception as e:\n        raise Exception(f\"Failed to initialize ImageAnnotatorClient: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor","title":"<code>ocr_editor</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.current_image","title":"<code>current_image = st.session_state.current_image</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.current_page_index","title":"<code>current_page_index = st.session_state.current_page_index</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.current_text","title":"<code>current_text = pages[current_page_index]</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.edited_text","title":"<code>edited_text = st.text_area('Edit OCR Text', value=(st.session_state.current_text), key=f'text_area_{st.session_state.current_page_index}', height=400)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.image_directory","title":"<code>image_directory = st.sidebar.text_input('Image Directory', value='./images')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.ocr_text_directory","title":"<code>ocr_text_directory = st.sidebar.text_input('OCR Text Directory', value='./ocr_text')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.pages","title":"<code>pages = st.session_state.pages</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.save_path","title":"<code>save_path = os.path.join(ocr_text_directory, 'updated_ocr.xml')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.tree","title":"<code>tree = st.session_state.tree</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.uploaded_image_file","title":"<code>uploaded_image_file = st.sidebar.file_uploader('Upload an Image', type=['jpg', 'jpeg', 'png', 'pdf'])</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.uploaded_text_file","title":"<code>uploaded_text_file = st.sidebar.file_uploader('Upload OCR Text File', type=['xml'])</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.extract_pages","title":"<code>extract_pages(tree)</code>","text":"<p>Extract page data from the XML tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>ElementTree</code> <p>Parsed XML tree.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of dictionaries containing 'number' and 'text' for each page.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_editor.py</code> <pre><code>def extract_pages(tree) -&gt; list:\n    \"\"\"\n    Extract page data from the XML tree.\n\n    Args:\n        tree (etree.ElementTree): Parsed XML tree.\n\n    Returns:\n        list: A list of dictionaries containing 'number' and 'text' for each page.\n    \"\"\"\n    pages = []\n    for page in tree.xpath(\"//page\"):\n        page_number = page.get(\"page\")\n        ocr_text = page.text.strip() if page.text else \"\"\n        pages.append({\"number\": page_number, \"text\": ocr_text})\n    return pages\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.load_xml","title":"<code>load_xml(file_obj)</code>","text":"<p>Load an XML file from a file-like object.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_editor.py</code> <pre><code>def load_xml(file_obj):\n    \"\"\"\n    Load an XML file from a file-like object.\n    \"\"\"\n    try:\n        tree = etree.parse(file_obj)  # Directly parse the file-like object\n        return tree\n    except etree.XMLSyntaxError as e:\n        st.error(f\"Error parsing XML file: {e}\")\n        return None\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_editor.save_xml","title":"<code>save_xml(tree, file_path)</code>","text":"<p>Save the modified XML tree to a file.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_editor.py</code> <pre><code>def save_xml(tree, file_path):\n    \"\"\"\n    Save the modified XML tree to a file.\n    \"\"\"\n    with open(file_path, \"wb\") as file:\n        tree.write(file, pretty_print=True, encoding=\"utf-8\", xml_declaration=True)\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing","title":"<code>ocr_processing</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.DEFAULT_ANNOTATION_FONT_PATH","title":"<code>DEFAULT_ANNOTATION_FONT_PATH = Path('/System/Library/Fonts/Supplemental/Arial.ttf')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.DEFAULT_ANNOTATION_FONT_SIZE","title":"<code>DEFAULT_ANNOTATION_FONT_SIZE = 12</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.DEFAULT_ANNOTATION_LANGUAGE_HINTS","title":"<code>DEFAULT_ANNOTATION_LANGUAGE_HINTS = ['vi']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.DEFAULT_ANNOTATION_METHOD","title":"<code>DEFAULT_ANNOTATION_METHOD = 'DOCUMENT_TEXT_DETECTION'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.DEFAULT_ANNOTATION_OFFSET","title":"<code>DEFAULT_ANNOTATION_OFFSET = 2</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.logger","title":"<code>logger = logging.getLogger('ocr_processing')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.PDFParseWarning","title":"<code>PDFParseWarning</code>","text":"<p>               Bases: <code>Warning</code></p> <p>Custom warning class for PDF parsing issues. Encapsulates minimal logic for displaying warnings with a custom format.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>class PDFParseWarning(Warning):\n    \"\"\"\n    Custom warning class for PDF parsing issues.\n    Encapsulates minimal logic for displaying warnings with a custom format.\n    \"\"\"\n\n    @staticmethod\n    def warn(message: str):\n        \"\"\"\n        Display a warning message with custom formatting.\n\n        Parameters:\n            message (str): The warning message to display.\n        \"\"\"\n        formatted_message = f\"\\033[93mPDFParseWarning: {message}\\033[0m\"\n        print(formatted_message)  # Simply prints the warning\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.PDFParseWarning.warn","title":"<code>warn(message)</code>  <code>staticmethod</code>","text":"<p>Display a warning message with custom formatting.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The warning message to display.</p> required Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>@staticmethod\ndef warn(message: str):\n    \"\"\"\n    Display a warning message with custom formatting.\n\n    Parameters:\n        message (str): The warning message to display.\n    \"\"\"\n    formatted_message = f\"\\033[93mPDFParseWarning: {message}\\033[0m\"\n    print(formatted_message)  # Simply prints the warning\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.annotate_image_with_text","title":"<code>annotate_image_with_text(image, text_annotations, annotation_font_path, font_size=12)</code>","text":"<p>Annotates a PIL image with bounding boxes and text descriptions from OCR results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input PIL image to annotate.</p> required <code>text_annotations</code> <code>List[EntityAnnotation]</code> <p>OCR results containing bounding boxes and text.</p> required <code>annotation_font_path</code> <code>str</code> <p>Path to the font file for text annotations.</p> required <code>font_size</code> <code>int</code> <p>Font size for text annotations.</p> <code>12</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The annotated PIL image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input image is None.</p> <code>IOError</code> <p>If the font file cannot be loaded.</p> <code>Exception</code> <p>For any other unexpected errors.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def annotate_image_with_text(\n    image: Image.Image,\n    text_annotations: List[EntityAnnotation],\n    annotation_font_path: str,\n    font_size: int = 12,\n) -&gt; Image.Image:\n    \"\"\"\n    Annotates a PIL image with bounding boxes and text descriptions from OCR results.\n\n    Parameters:\n        image (Image.Image): The input PIL image to annotate.\n        text_annotations (List[EntityAnnotation]): OCR results containing bounding boxes and text.\n        annotation_font_path (str): Path to the font file for text annotations.\n        font_size (int): Font size for text annotations.\n\n    Returns:\n        Image.Image: The annotated PIL image.\n\n    Raises:\n        ValueError: If the input image is None.\n        IOError: If the font file cannot be loaded.\n        Exception: For any other unexpected errors.\n    \"\"\"\n    if image is None:\n        raise ValueError(\"The input image is None.\")\n\n    try:\n        font = ImageFont.truetype(annotation_font_path, font_size)\n    except IOError as e:\n        raise IOError(f\"Failed to load the font from '{annotation_font_path}': {e}\")\n\n    draw = ImageDraw.Draw(image)\n\n    try:\n        for i, text_obj in enumerate(text_annotations):\n            vertices = [\n                (vertex.x, vertex.y) for vertex in text_obj.bounding_poly.vertices\n            ]\n            if (\n                len(vertices) == 4\n            ):  # Ensure there are exactly 4 vertices for a rectangle\n                # Draw the bounding box\n                draw.polygon(vertices, outline=\"red\", width=2)\n\n                # Skip the first bounding box (whole text region)\n                if i &gt; 0:\n                    # Offset the text position slightly for clarity\n                    text_position = (vertices[0][0] + 2, vertices[0][1] + 2)\n                    draw.text(\n                        text_position, text_obj.description, fill=\"red\", font=font\n                    )\n\n    except AttributeError as e:\n        raise ValueError(f\"Invalid text annotation structure: {e}\")\n    except Exception as e:\n        raise Exception(f\"An error occurred during image annotation: {e}\")\n\n    return image\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.build_processed_pdf","title":"<code>build_processed_pdf(pdf_path, client, preprocessor=None, annotation_font_path=DEFAULT_ANNOTATION_FONT_PATH)</code>","text":"<p>Processes a PDF document, extracting text, word locations, annotated images, and unannotated images.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>Path</code> <p>Path to the PDF file.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>annotation_font_path</code> <code>Path</code> <p>Path to the font file for annotations.</p> <code>DEFAULT_ANNOTATION_FONT_PATH</code> <p>Returns:</p> Type Description <code>Tuple[List[str], List[List[EntityAnnotation]], List[Image], List[Image]]</code> <p>Tuple[List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]]: - List of extracted full-page texts (one entry per page). - List of word locations (list of <code>vision.EntityAnnotation</code> objects for each page). - List of annotated images (with bounding boxes and text annotations). - List of unannotated images (raw page images).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified PDF file does not exist.</p> <code>ValueError</code> <p>If the PDF file is invalid or contains no pages.</p> <code>Exception</code> <p>For any unexpected errors during processing.</p> Example <p>from pathlib import Path from google.cloud import vision pdf_path = Path(\"/path/to/example.pdf\") font_path = Path(\"/path/to/fonts/Arial.ttf\") client = vision.ImageAnnotatorClient() try:     text_pages, word_locations_list, annotated_images, unannotated_images = build_processed_pdf(         pdf_path, client, font_path     )     print(f\"Processed {len(text_pages)} pages successfully!\") except Exception as e:     print(f\"Error processing PDF: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def build_processed_pdf(\n    pdf_path: Path,\n    client: vision.ImageAnnotatorClient,\n    preprocessor: Callable = None,\n    annotation_font_path: Path = DEFAULT_ANNOTATION_FONT_PATH,\n) -&gt; Tuple[\n    List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]\n]:\n    \"\"\"\n    Processes a PDF document, extracting text, word locations, annotated images, and unannotated images.\n\n    Parameters:\n        pdf_path (Path): Path to the PDF file.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        annotation_font_path (Path): Path to the font file for annotations.\n\n    Returns:\n        Tuple[List[str], List[List[vision.EntityAnnotation]], List[Image.Image], List[Image.Image]]:\n            - List of extracted full-page texts (one entry per page).\n            - List of word locations (list of `vision.EntityAnnotation` objects for each page).\n            - List of annotated images (with bounding boxes and text annotations).\n            - List of unannotated images (raw page images).\n\n    Raises:\n        FileNotFoundError: If the specified PDF file does not exist.\n        ValueError: If the PDF file is invalid or contains no pages.\n        Exception: For any unexpected errors during processing.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from google.cloud import vision\n        &gt;&gt;&gt; pdf_path = Path(\"/path/to/example.pdf\")\n        &gt;&gt;&gt; font_path = Path(\"/path/to/fonts/Arial.ttf\")\n        &gt;&gt;&gt; client = vision.ImageAnnotatorClient()\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     text_pages, word_locations_list, annotated_images, unannotated_images = build_processed_pdf(\n        &gt;&gt;&gt;         pdf_path, client, font_path\n        &gt;&gt;&gt;     )\n        &gt;&gt;&gt;     print(f\"Processed {len(text_pages)} pages successfully!\")\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error processing PDF: {e}\")\n    \"\"\"\n    try:\n        doc = load_pdf_pages(pdf_path)\n    except FileNotFoundError as fnf_error:\n        raise FileNotFoundError(f\"Error loading PDF: {fnf_error}\")\n    except ValueError as ve:\n        raise ValueError(f\"Invalid PDF file: {ve}\")\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred while loading the PDF: {e}\")\n\n    if doc.page_count == 0:\n        raise ValueError(f\"The PDF file '{pdf_path}' contains no pages.\")\n\n    logger.info(f\"Processing file with {doc.page_count} pages:\\n\\t{pdf_path}\")\n\n    text_pages = []\n    word_locations_list = []\n    annotated_images = []\n    unannotated_images = []\n    first_page_dimensions = None\n\n    for page_num in range(doc.page_count):\n        logger.info(f\"Processing page {page_num + 1}/{doc.page_count}...\")\n\n        try:\n            page = doc.load_page(page_num)\n            (\n                full_page_text,\n                word_locations,\n                annotated_image,\n                unannotated_image,\n                page_dimensions,\n            ) = process_page(page, client, annotation_font_path, preprocessor)\n\n            if full_page_text:  # this is not an empty page\n\n                if page_num == 0:  # save first page info\n                    first_page_dimensions = page_dimensions\n                elif (\n                    page_dimensions != first_page_dimensions\n                ):  # verify page dimensions are consistent\n                    PDFParseWarning.warn(\n                        f\"Page {page_num + 1} has different dimensions than page 1.\"\n                        f\"({page_dimensions}) compared to the first page: ({first_page_dimensions}).\"\n                    )\n\n                text_pages.append(full_page_text)\n                word_locations_list.append(word_locations)\n                annotated_images.append(annotated_image)\n                unannotated_images.append(unannotated_image)\n            else:\n                PDFParseWarning.warn(\n                    f\"Page {page_num + 1} empty, added empty datastructures...\\n\"\n                    # f\"  (Note that total document length will be reduced.)\"\n                )\n\n        except ValueError as ve:\n            print(f\"ValueError on page {page_num + 1}: {ve}\")\n        except OSError as oe:\n            print(f\"OSError on page {page_num + 1}: {oe}\")\n        except Exception as e:\n            print(f\"Unexpected error on page {page_num + 1}: {e}\")\n\n    print(f\"page dimensions: {page_dimensions}\")\n    return text_pages, word_locations_list, annotated_images, unannotated_images\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.deserialize_entity_annotations_from_json","title":"<code>deserialize_entity_annotations_from_json(data)</code>","text":"<p>Deserializes JSON data into a nested list of EntityAnnotation objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The JSON string containing serialized annotations.</p> required <p>Returns:</p> Type Description <code>List[List[EntityAnnotation]]</code> <p>List[List[EntityAnnotation]]: The reconstructed nested list of EntityAnnotation objects.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def deserialize_entity_annotations_from_json(data: str) -&gt; List[List[EntityAnnotation]]:\n    \"\"\"\n    Deserializes JSON data into a nested list of EntityAnnotation objects.\n\n    Parameters:\n        data (str): The JSON string containing serialized annotations.\n\n    Returns:\n        List[List[EntityAnnotation]]: The reconstructed nested list of EntityAnnotation objects.\n    \"\"\"\n    serialized_data = json.loads(data)\n    deserialized_data = []\n\n    for serialized_page in serialized_data:\n        page_annotations = [\n            EntityAnnotation.deserialize(base64.b64decode(serialized_annotation))\n            for serialized_annotation in serialized_page\n        ]\n        deserialized_data.append(page_annotations)\n\n    return deserialized_data\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.extract_image_from_page","title":"<code>extract_image_from_page(page)</code>","text":"<p>Extracts the first image from the given PDF page and returns it as a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The PDF page object.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The first image on the page as a Pillow Image object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no images are found on the page or the image data is incomplete.</p> <code>Exception</code> <p>For unexpected errors during image extraction.</p> Example <p>import fitz from PIL import Image doc = fitz.open(\"/path/to/document.pdf\") page = doc.load_page(0)  # Load the first page try:     image = extract_image_from_page(page)     image.show()  # Display the image except Exception as e:     print(f\"Error extracting image: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def extract_image_from_page(page: fitz.Page) -&gt; Image.Image:\n    \"\"\"\n    Extracts the first image from the given PDF page and returns it as a PIL Image.\n\n    Parameters:\n        page (fitz.Page): The PDF page object.\n\n    Returns:\n        Image.Image: The first image on the page as a Pillow Image object.\n\n    Raises:\n        ValueError: If no images are found on the page or the image data is incomplete.\n        Exception: For unexpected errors during image extraction.\n\n    Example:\n        &gt;&gt;&gt; import fitz\n        &gt;&gt;&gt; from PIL import Image\n        &gt;&gt;&gt; doc = fitz.open(\"/path/to/document.pdf\")\n        &gt;&gt;&gt; page = doc.load_page(0)  # Load the first page\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     image = extract_image_from_page(page)\n        &gt;&gt;&gt;     image.show()  # Display the image\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error extracting image: {e}\")\n    \"\"\"\n    try:\n        # Get images from the page\n        images = page.get_images(full=True)\n        if not images:\n            raise ValueError(\"No images found on the page.\")\n\n        # Extract the first image reference\n        xref = images[0][0]  # Get the first image's xref\n        base_image = page.parent.extract_image(xref)\n\n        # Validate the extracted image data\n        if (\n            \"image\" not in base_image\n            or \"width\" not in base_image\n            or \"height\" not in base_image\n        ):\n            raise ValueError(\"The extracted image data is incomplete.\")\n\n        # Convert the raw image bytes into a Pillow image\n        image_bytes = base_image[\"image\"]\n        pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n\n        return pil_image\n\n    except ValueError as ve:\n        raise ve  # Re-raise for calling functions to handle\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred during image extraction: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.get_page_dimensions","title":"<code>get_page_dimensions(page)</code>","text":"<p>Extracts the width and height of a single PDF page in both inches and pixels.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>A single PDF page object from PyMuPDF.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the width and height of the page in inches and pixels.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def get_page_dimensions(page: fitz.Page) -&gt; dict:\n    \"\"\"\n    Extracts the width and height of a single PDF page in both inches and pixels.\n\n    Args:\n        page (fitz.Page): A single PDF page object from PyMuPDF.\n\n    Returns:\n        dict: A dictionary containing the width and height of the page in inches and pixels.\n    \"\"\"\n    # Get page dimensions in points and convert to inches\n    page_width_pts, page_height_pts = page.rect.width, page.rect.height\n    page_width_in = page_width_pts / 72  # Convert points to inches\n    page_height_in = page_height_pts / 72\n\n    # Extract the first image on the page (if any) to get pixel dimensions\n    images = page.get_images(full=True)\n    if images:\n        xref = images[0][0]\n        base_image = page.parent.extract_image(xref)\n        width_px = base_image[\"width\"]\n        height_px = base_image[\"height\"]\n    else:\n        width_px, height_px = None, None  # No image found on the page\n\n    # Return dimensions\n    return {\n        \"width_in\": page_width_in,\n        \"height_in\": page_height_in,\n        \"width_px\": width_px,\n        \"height_px\": height_px,\n    }\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.load_pdf_pages","title":"<code>load_pdf_pages(pdf_path)</code>","text":"<p>Opens the PDF document and returns the fitz Document object.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>Path</code> <p>The path to the PDF file.</p> required <p>Returns:</p> Type Description <code>Document</code> <p>fitz.Document: The loaded PDF document.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>ValueError</code> <p>If the file is not a valid PDF document.</p> <code>Exception</code> <p>For any unexpected error.</p> Example <p>from pathlib import Path pdf_path = Path(\"/path/to/example.pdf\") try:     pdf_doc = load_pdf_pages(pdf_path)     print(f\"PDF contains {pdf_doc.page_count} pages.\") except Exception as e:     print(f\"Error loading PDF: {e}\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def load_pdf_pages(pdf_path: Path) -&gt; fitz.Document:\n    \"\"\"\n    Opens the PDF document and returns the fitz Document object.\n\n    Parameters:\n        pdf_path (Path): The path to the PDF file.\n\n    Returns:\n        fitz.Document: The loaded PDF document.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        ValueError: If the file is not a valid PDF document.\n        Exception: For any unexpected error.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; pdf_path = Path(\"/path/to/example.pdf\")\n        &gt;&gt;&gt; try:\n        &gt;&gt;&gt;     pdf_doc = load_pdf_pages(pdf_path)\n        &gt;&gt;&gt;     print(f\"PDF contains {pdf_doc.page_count} pages.\")\n        &gt;&gt;&gt; except Exception as e:\n        &gt;&gt;&gt;     print(f\"Error loading PDF: {e}\")\n    \"\"\"\n    if not pdf_path.exists():\n        raise FileNotFoundError(f\"The file '{pdf_path}' does not exist.\")\n\n    if not pdf_path.suffix.lower() == \".pdf\":\n        raise ValueError(\n            f\"The file '{pdf_path}' is not a valid PDF document (expected '.pdf').\"\n        )\n\n    try:\n        return fitz.open(str(pdf_path))  # PyMuPDF expects a string path\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred while opening the PDF: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.load_processed_PDF_data","title":"<code>load_processed_PDF_data(base_path)</code>","text":"<p>Loads processed PDF data from files using metadata for file references.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>Base path where processed assets are stored.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[List[EntityAnnotation]], List[Image], List[Image]]</code> <p>Tuple[List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]]: - Loaded text pages. - Word locations (list of <code>EntityAnnotation</code> objects for each page). - Annotated images. - Unannotated images.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If any required files are missing.</p> <code>ValueError</code> <p>If the metadata file is incomplete or invalid.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def load_processed_PDF_data(\n    base_path: Path,\n) -&gt; Tuple[\n    List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]\n]:\n    \"\"\"\n    Loads processed PDF data from files using metadata for file references.\n\n    Parameters:\n        base_path (Path): Base path where processed assets are stored.\n\n    Returns:\n        Tuple[List[str], List[List[EntityAnnotation]], List[Image.Image], List[Image.Image]]:\n            - Loaded text pages.\n            - Word locations (list of `EntityAnnotation` objects for each page).\n            - Annotated images.\n            - Unannotated images.\n\n    Raises:\n        FileNotFoundError: If any required files are missing.\n        ValueError: If the metadata file is incomplete or invalid.\n    \"\"\"\n    metadata_file = base_path / \"metadata.json\"\n\n    # Load metadata\n    try:\n        with metadata_file.open(\"r\", encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Metadata file '{metadata_file}' not found.\")\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid metadata file format: {e}\")\n\n    # Extract file paths from metadata\n    text_pages_file = base_path / metadata.get(\"files\", {}).get(\n        \"text_pages\", \"text_pages.json\"\n    )\n    word_locations_file = base_path / metadata.get(\"files\", {}).get(\n        \"word_locations\", \"word_locations.json\"\n    )\n    images_dir = Path(metadata.get(\"images_directory\", base_path / \"images\"))\n\n    # Validate file paths\n    if not text_pages_file.exists():\n        raise FileNotFoundError(f\"Text pages file '{text_pages_file}' not found.\")\n    if not word_locations_file.exists():\n        raise FileNotFoundError(\n            f\"Word locations file '{word_locations_file}' not found.\"\n        )\n    if not images_dir.exists() or not images_dir.is_dir():\n        raise FileNotFoundError(f\"Images directory '{images_dir}' not found.\")\n\n    # Load text pages\n    with text_pages_file.open(\"r\", encoding=\"utf-8\") as f:\n        text_pages = json.load(f)\n\n    # Load word locations\n    with word_locations_file.open(\"r\", encoding=\"utf-8\") as f:\n        serialized_word_locations = f.read()\n        word_locations = deserialize_entity_annotations_from_json(\n            serialized_word_locations\n        )\n\n    # Load images\n    annotated_images = []\n    unannotated_images = []\n    for file in sorted(\n        images_dir.iterdir()\n    ):  # Iterate over files in the images directory\n        if file.name.startswith(\"annotated_page_\") and file.suffix == \".png\":\n            annotated_images.append(Image.open(file))\n        elif file.name.startswith(\"unannotated_page_\") and file.suffix == \".png\":\n            unannotated_images.append(Image.open(file))\n\n    # Ensure images were loaded correctly\n    if not annotated_images or not unannotated_images:\n        raise ValueError(f\"No images found in the directory '{images_dir}'.\")\n\n    return text_pages, word_locations, annotated_images, unannotated_images\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.make_image_preprocess_mask","title":"<code>make_image_preprocess_mask(mask_height)</code>","text":"<p>Creates a preprocessing function that masks a specified height at the bottom of the image.</p> <p>Parameters:</p> Name Type Description Default <code>mask_height</code> <code>float</code> <p>The proportion of the image height to mask at the bottom (0.0 to 1.0).</p> required <p>Returns:</p> Type Description <code>Callable[[Image, int], Image]</code> <p>Callable[[Image.Image, int], Image.Image]: A preprocessing function that takes an image</p> <code>Callable[[Image, int], Image]</code> <p>and page number as input and returns the processed image.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def make_image_preprocess_mask(\n    mask_height: float,\n) -&gt; Callable[[Image.Image, int], Image.Image]:\n    \"\"\"\n    Creates a preprocessing function that masks a specified height at the bottom of the image.\n\n    Parameters:\n        mask_height (float): The proportion of the image height to mask at the bottom (0.0 to 1.0).\n\n    Returns:\n        Callable[[Image.Image, int], Image.Image]: A preprocessing function that takes an image\n        and page number as input and returns the processed image.\n    \"\"\"\n\n    def pre_process_image(image: Image.Image, page_number: int) -&gt; Image.Image:\n        \"\"\"\n        Preprocesses the image by masking the bottom region or performing other preprocessing steps.\n\n        Parameters:\n            image (Image.Image): The input image as a Pillow object.\n            page_number (int): The page number of the image (useful for conditional preprocessing).\n\n        Returns:\n            Image.Image: The preprocessed image.\n        \"\"\"\n\n        if page_number &gt; 0:  # don't apply mask to cover page.\n            # Make a copy of the image to avoid modifying the original\n            draw = ImageDraw.Draw(image)\n\n            # Get image dimensions\n            width, height = image.size\n\n            # Mask the bottom region based on the specified height proportion\n            mask_pixels = int(height * mask_height)\n            draw.rectangle([(0, height - mask_pixels), (width, height)], fill=\"black\")\n\n        return image\n\n    return pre_process_image\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.pil_to_bytes","title":"<code>pil_to_bytes(image, format='PNG')</code>","text":"<p>Converts a Pillow image to raw bytes.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The Pillow image object to convert.</p> required <code>format</code> <code>str</code> <p>The format to save the image as (e.g., \"PNG\", \"JPEG\"). Default is \"PNG\".</p> <code>'PNG'</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The raw bytes of the image.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def pil_to_bytes(image: Image.Image, format: str = \"PNG\") -&gt; bytes:\n    \"\"\"\n    Converts a Pillow image to raw bytes.\n\n    Parameters:\n        image (Image.Image): The Pillow image object to convert.\n        format (str): The format to save the image as (e.g., \"PNG\", \"JPEG\"). Default is \"PNG\".\n\n    Returns:\n        bytes: The raw bytes of the image.\n    \"\"\"\n    with io.BytesIO() as output:\n        image.save(output, format=format)\n        return output.getvalue()\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.process_page","title":"<code>process_page(page, client, annotation_font_path, preprocessor=None)</code>","text":"<p>Processes a single PDF page, extracting text, word locations, and annotated images.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The PDF page object.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>preprocessor</code> <code>Callable[[Image, int], Image]</code> <p>Preprocessing function for the image.</p> <code>None</code> <code>annotation_font_path</code> <code>str</code> <p>Path to the font file for annotations.</p> required <p>Returns:</p> Type Description <code>Tuple[str, List[EntityAnnotation], Image, Image, dict]</code> <p>Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]: - Full page text (str) - Word locations (List of vision.EntityAnnotation) - Annotated image (Pillow Image object) - Original unprocessed image (Pillow Image object) - Page dimensions (dict)</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def process_page(\n    page: fitz.Page,\n    client: vision.ImageAnnotatorClient,\n    annotation_font_path: str,\n    preprocessor: Callable[[Image.Image, int], Image.Image] = None,\n) -&gt; Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]:\n    \"\"\"\n    Processes a single PDF page, extracting text, word locations, and annotated images.\n\n    Parameters:\n        page (fitz.Page): The PDF page object.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        preprocessor (Callable[[Image.Image, int], Image.Image]): Preprocessing function for the image.\n        annotation_font_path (str): Path to the font file for annotations.\n\n    Returns:\n        Tuple[str, List[vision.EntityAnnotation], Image.Image, Image.Image, dict]:\n            - Full page text (str)\n            - Word locations (List of vision.EntityAnnotation)\n            - Annotated image (Pillow Image object)\n            - Original unprocessed image (Pillow Image object)\n            - Page dimensions (dict)\n    \"\"\"\n    # Extract the original image from the PDF page\n    original_image = extract_image_from_page(page)\n\n    # Make a copy of the original image for processing\n    processed_image = original_image.copy()\n\n    # Apply the preprocessing function (if provided)\n    if preprocessor:\n        # print(\"preprocessing...\") # debug\n        processed_image = preprocessor(processed_image, page.number)\n        # processed_image.show() # debug\n\n    # Annotate the processed image using the Vision API\n    response = process_single_image(processed_image, client)\n\n    if response:\n        text_annotations = response.text_annotations\n        # Extract full text and word locations\n        full_page_text = text_annotations[0].description if text_annotations else \"\"\n        word_locations = text_annotations[1:] if len(text_annotations) &gt; 1 else []\n    else:\n        # return empty data\n        full_page_text = \"\"\n        word_locations = [EntityAnnotation()]\n        text_annotations = [\n            EntityAnnotation()\n        ]  # create empty data structures to allow storing to proceed.\n\n    # Create an annotated image with bounding boxes and labels\n    annotated_image = annotate_image_with_text(\n        processed_image, text_annotations, annotation_font_path\n    )\n\n    # Get page dimensions (from the original PDF page, not the image)\n    page_dimensions = get_page_dimensions(page)\n\n    return (\n        full_page_text,\n        word_locations,\n        annotated_image,\n        original_image,\n        page_dimensions,\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.process_single_image","title":"<code>process_single_image(image, client, feature_type=DEFAULT_ANNOTATION_METHOD, language_hints=DEFAULT_ANNOTATION_LANGUAGE_HINTS)</code>","text":"<p>Processes a single image with the Google Vision API and returns text annotations.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The preprocessed Pillow image object.</p> required <code>client</code> <code>ImageAnnotatorClient</code> <p>Google Vision API client for text detection.</p> required <code>feature_type</code> <code>str</code> <p>Type of text detection to use ('TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION').</p> <code>DEFAULT_ANNOTATION_METHOD</code> <code>language_hints</code> <code>List</code> <p>Language hints for OCR.</p> <code>DEFAULT_ANNOTATION_LANGUAGE_HINTS</code> <p>Returns:</p> Type Description <code>List[EntityAnnotation]</code> <p>List[vision.EntityAnnotation]: Text annotations from the Vision API response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no text is detected.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def process_single_image(\n    image: Image.Image,\n    client: vision.ImageAnnotatorClient,\n    feature_type: str = DEFAULT_ANNOTATION_METHOD,\n    language_hints: List = DEFAULT_ANNOTATION_LANGUAGE_HINTS,\n) -&gt; List[vision.EntityAnnotation]:\n    \"\"\"\n    Processes a single image with the Google Vision API and returns text annotations.\n\n    Parameters:\n        image (Image.Image): The preprocessed Pillow image object.\n        client (vision.ImageAnnotatorClient): Google Vision API client for text detection.\n        feature_type (str): Type of text detection to use ('TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION').\n        language_hints (List): Language hints for OCR.\n\n    Returns:\n        List[vision.EntityAnnotation]: Text annotations from the Vision API response.\n\n    Raises:\n        ValueError: If no text is detected.\n    \"\"\"\n    # Convert the Pillow image to bytes\n    image_bytes = pil_to_bytes(image, format=\"PNG\")\n\n    # Map feature type\n    feature_map = {\n        \"TEXT_DETECTION\": vision.Feature.Type.TEXT_DETECTION,\n        \"DOCUMENT_TEXT_DETECTION\": vision.Feature.Type.DOCUMENT_TEXT_DETECTION,\n    }\n    if feature_type not in feature_map:\n        raise ValueError(\n            f\"Invalid feature type '{feature_type}'. Use 'TEXT_DETECTION' or 'DOCUMENT_TEXT_DETECTION'.\"\n        )\n\n    # Prepare Vision API request\n    vision_image = vision.Image(content=image_bytes)\n    features = [vision.Feature(type=feature_map[feature_type])]\n    image_context = vision.ImageContext(language_hints=language_hints)\n\n    # Make the API call\n    response = client.annotate_image(\n        {\"image\": vision_image, \"features\": features, \"image_context\": image_context}\n    )\n\n    return response\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.save_processed_pdf_data","title":"<code>save_processed_pdf_data(output_dir, journal_name, text_pages, word_locations, annotated_images, unannotated_images)</code>","text":"<p>Saves processed PDF data to files for later reloading.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>Directory to save the data (as a Path object).</p> required <code>journal_name</code> <code>str</code> <p>Name for the output directory (usually the PDF name without extension).</p> required <code>text_pages</code> <code>List[str]</code> <p>Extracted full-page text.</p> required <code>word_locations</code> <code>List[List[EntityAnnotation]]</code> <p>Word locations and annotations from Vision API.</p> required <code>annotated_images</code> <code>List[Image]</code> <p>Annotated images with bounding boxes.</p> required <code>unannotated_images</code> <code>List[Image]</code> <p>Raw unannotated images.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def save_processed_pdf_data(\n    output_dir: Path,\n    journal_name: str,\n    text_pages: List[str],\n    word_locations: List[List[EntityAnnotation]],\n    annotated_images: List[Image.Image],\n    unannotated_images: List[Image.Image],\n) -&gt; None:\n    \"\"\"\n    Saves processed PDF data to files for later reloading.\n\n    Parameters:\n        output_dir (Path): Directory to save the data (as a Path object).\n        journal_name (str): Name for the output directory (usually the PDF name without extension).\n        text_pages (List[str]): Extracted full-page text.\n        word_locations (List[List[EntityAnnotation]]): Word locations and annotations from Vision API.\n        annotated_images (List[PIL.Image.Image]): Annotated images with bounding boxes.\n        unannotated_images (List[PIL.Image.Image]): Raw unannotated images.\n\n    Returns:\n        None\n    \"\"\"\n    # Create output directories\n    base_path = output_dir / journal_name / \"ocr_data\"\n    images_dir = base_path / \"images\"\n\n    base_path.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save text data\n    text_pages_file = base_path / \"text_pages.json\"\n    with text_pages_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(text_pages, f, indent=4, ensure_ascii=False)\n\n    # Save word locations as JSON\n    word_locations_file = base_path / \"word_locations.json\"\n    serialized_word_locations = serialize_entity_annotations_to_json(word_locations)\n    with word_locations_file.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(serialized_word_locations)\n\n    # Save images\n    for i, annotated_image in enumerate(annotated_images):\n        annotated_file = images_dir / f\"annotated_page_{i + 1}.png\"\n        annotated_image.save(annotated_file)\n    for i, unannotated_image in enumerate(unannotated_images):\n        unannotated_file = images_dir / f\"unannotated_page_{i + 1}.png\"\n        unannotated_image.save(unannotated_file)\n\n    # Save metadata\n    metadata = {\n        \"source_pdf\": journal_name,\n        \"page_count\": len(text_pages),\n        \"images_directory\": str(\n            images_dir\n        ),  # Convert Path to string for JSON serialization\n        \"files\": {\n            \"text_pages\": \"text_pages.json\",\n            \"word_locations\": \"word_locations.json\",\n        },\n    }\n    metadata_file = base_path / \"metadata.json\"\n    with metadata_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(metadata, f, indent=4)\n\n    print(f\"Processed data saved in: {base_path}\")\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.serialize_entity_annotations_to_json","title":"<code>serialize_entity_annotations_to_json(annotations)</code>","text":"<p>Serializes a nested list of EntityAnnotation objects into a JSON-compatible format using Base64 encoding.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>List[List[EntityAnnotation]]</code> <p>The nested list of EntityAnnotation objects.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The serialized data in JSON format as a string.</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def serialize_entity_annotations_to_json(\n    annotations: List[List[EntityAnnotation]],\n) -&gt; str:\n    \"\"\"\n    Serializes a nested list of EntityAnnotation objects into a JSON-compatible format using Base64 encoding.\n\n    Parameters:\n        annotations (List[List[EntityAnnotation]]): The nested list of EntityAnnotation objects.\n\n    Returns:\n        str: The serialized data in JSON format as a string.\n    \"\"\"\n    serialized_data = []\n    for page_annotations in annotations:\n        serialized_page = [\n            base64.b64encode(annotation.SerializeToString()).decode(\"utf-8\")\n            for annotation in page_annotations\n        ]\n        serialized_data.append(serialized_page)\n\n    # Convert to a JSON string\n    return json.dumps(serialized_data, indent=4)\n</code></pre>"},{"location":"api/#tnh_scholar.ocr_processing.ocr_processing.start_image_annotator_client","title":"<code>start_image_annotator_client(credentials_file=None, api_endpoint='vision.googleapis.com', timeout=(10, 30), enable_logging=False)</code>","text":"<p>Starts and returns a Google Vision API ImageAnnotatorClient with optional configuration.</p> <p>Parameters:</p> Name Type Description Default <code>credentials_file</code> <code>str</code> <p>Path to the credentials JSON file. If None, uses the default environment variable.</p> <code>None</code> <code>api_endpoint</code> <code>str</code> <p>Custom API endpoint for the Vision API. Default is the global endpoint.</p> <code>'vision.googleapis.com'</code> <code>timeout</code> <code>Tuple[int, int]</code> <p>Connection and read timeouts in seconds. Default is (10, 30).</p> <code>(10, 30)</code> <code>enable_logging</code> <code>bool</code> <p>Enable detailed logging for debugging. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ImageAnnotatorClient</code> <p>vision.ImageAnnotatorClient: Configured Vision API client.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified credentials file is not found.</p> <code>Exception</code> <p>For unexpected errors during client setup.</p> Example <p>client = start_image_annotator_client(     credentials_file=\"/path/to/credentials.json\",     api_endpoint=\"vision.googleapis.com\",     timeout=(10, 30),     enable_logging=True ) print(\"Google Vision API client initialized.\")</p> Source code in <code>src/tnh_scholar/ocr_processing/ocr_processing.py</code> <pre><code>def start_image_annotator_client(\n    credentials_file: str = None,\n    api_endpoint: str = \"vision.googleapis.com\",\n    timeout: Tuple[int, int] = (10, 30),\n    enable_logging: bool = False,\n) -&gt; vision.ImageAnnotatorClient:\n    \"\"\"\n    Starts and returns a Google Vision API ImageAnnotatorClient with optional configuration.\n\n    Parameters:\n        credentials_file (str): Path to the credentials JSON file. If None, uses the default environment variable.\n        api_endpoint (str): Custom API endpoint for the Vision API. Default is the global endpoint.\n        timeout (Tuple[int, int]): Connection and read timeouts in seconds. Default is (10, 30).\n        enable_logging (bool): Enable detailed logging for debugging. Default is False.\n\n    Returns:\n        vision.ImageAnnotatorClient: Configured Vision API client.\n\n    Raises:\n        FileNotFoundError: If the specified credentials file is not found.\n        Exception: For unexpected errors during client setup.\n\n    Example:\n        &gt;&gt;&gt; client = start_image_annotator_client(\n        &gt;&gt;&gt;     credentials_file=\"/path/to/credentials.json\",\n        &gt;&gt;&gt;     api_endpoint=\"vision.googleapis.com\",\n        &gt;&gt;&gt;     timeout=(10, 30),\n        &gt;&gt;&gt;     enable_logging=True\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; print(\"Google Vision API client initialized.\")\n    \"\"\"\n    try:\n        # Set up credentials\n        if credentials_file:\n            if not os.path.exists(credentials_file):\n                raise FileNotFoundError(\n                    f\"Credentials file '{credentials_file}' not found.\"\n                )\n            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_file\n\n        # Configure client options\n        client_options = {\"api_endpoint\": api_endpoint}\n        client = vision.ImageAnnotatorClient(client_options=client_options)\n\n        # Optionally enable logging\n        if enable_logging:\n            print(f\"Vision API Client started with endpoint: {api_endpoint}\")\n            print(f\"Timeout settings: Connect={timeout[0]}s, Read={timeout[1]}s\")\n\n        return client\n\n    except Exception as e:\n        raise Exception(f\"Failed to initialize ImageAnnotatorClient: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system","title":"<code>prompt_system</code>","text":"<p>Prompt system package scaffolding per ADR-PT04.</p> <p>Modules will provide object-service compliant prompt catalog, rendering, and validation.</p>"},{"location":"api/#tnh_scholar.prompt_system.adapters","title":"<code>adapters</code>","text":"<p>Prompt catalog adapters.</p>"},{"location":"api/#tnh_scholar.prompt_system.adapters.filesystem_catalog_adapter","title":"<code>filesystem_catalog_adapter</code>","text":"<p>Filesystem-backed prompt catalog adapter.</p>"},{"location":"api/#tnh_scholar.prompt_system.adapters.filesystem_catalog_adapter.FilesystemPromptCatalog","title":"<code>FilesystemPromptCatalog</code>","text":"<p>               Bases: <code>PromptCatalogPort</code></p> <p>Filesystem-backed catalog for offline/packaged distributions.</p> Source code in <code>src/tnh_scholar/prompt_system/adapters/filesystem_catalog_adapter.py</code> <pre><code>class FilesystemPromptCatalog(PromptCatalogPort):\n    \"\"\"Filesystem-backed catalog for offline/packaged distributions.\"\"\"\n\n    def __init__(\n        self,\n        config: PromptCatalogConfig,\n        mapper: PromptMapper,\n        loader: PromptLoader,\n        cache: CacheTransport[Prompt] | None = None,\n        transport: FilesystemTransport | None = None,\n    ):\n        self._config = config\n        self._mapper = mapper\n        self._loader = loader\n        self._cache = cache or InMemoryCacheTransport(default_ttl_s=config.cache_ttl_s)\n        self._transport = transport or FilesystemTransport(mapper)\n\n    def get(self, key: str) -&gt; Prompt:\n        cache_key = self._make_cache_key(key)\n        cached = self._cache.get(cache_key)\n        if cached:\n            return cached\n\n        file_path = self._mapper.to_file_request(key, self._config.repository_path)\n        request = PromptFileRequest(path=file_path, commit_sha=None)\n        file_resp = self._transport.read_file(request)\n        prompt = self._mapper.to_domain_prompt(file_resp.content)\n\n        if self._config.validation_on_load:\n            validation = self._loader.validate(prompt)\n            if not validation.succeeded():\n                raise ValueError(f\"Invalid prompt: {validation.errors}\")\n\n        self._cache.set(cache_key, prompt, ttl_s=self._config.cache_ttl_s)\n        return prompt\n\n    def list(self) -&gt; list[PromptMetadata]:\n        files = self._transport.list_files(self._config.repository_path, pattern=\"**/*.md\")\n        prompts = []\n        for path in files:\n            key = self._path_to_key(path)\n            prompts.append(self.get(key))\n        return [p.metadata for p in prompts]\n\n    def _make_cache_key(self, prompt_key: str) -&gt; str:\n        return f\"{prompt_key}@filesystem\"\n\n    def _path_to_key(self, path: Path) -&gt; str:\n        return path.stem\n</code></pre> <code>__init__(config, mapper, loader, cache=None, transport=None)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/adapters/filesystem_catalog_adapter.py</code> <pre><code>def __init__(\n    self,\n    config: PromptCatalogConfig,\n    mapper: PromptMapper,\n    loader: PromptLoader,\n    cache: CacheTransport[Prompt] | None = None,\n    transport: FilesystemTransport | None = None,\n):\n    self._config = config\n    self._mapper = mapper\n    self._loader = loader\n    self._cache = cache or InMemoryCacheTransport(default_ttl_s=config.cache_ttl_s)\n    self._transport = transport or FilesystemTransport(mapper)\n</code></pre> <code>get(key)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/adapters/filesystem_catalog_adapter.py</code> <pre><code>def get(self, key: str) -&gt; Prompt:\n    cache_key = self._make_cache_key(key)\n    cached = self._cache.get(cache_key)\n    if cached:\n        return cached\n\n    file_path = self._mapper.to_file_request(key, self._config.repository_path)\n    request = PromptFileRequest(path=file_path, commit_sha=None)\n    file_resp = self._transport.read_file(request)\n    prompt = self._mapper.to_domain_prompt(file_resp.content)\n\n    if self._config.validation_on_load:\n        validation = self._loader.validate(prompt)\n        if not validation.succeeded():\n            raise ValueError(f\"Invalid prompt: {validation.errors}\")\n\n    self._cache.set(cache_key, prompt, ttl_s=self._config.cache_ttl_s)\n    return prompt\n</code></pre> <code>list()</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/adapters/filesystem_catalog_adapter.py</code> <pre><code>def list(self) -&gt; list[PromptMetadata]:\n    files = self._transport.list_files(self._config.repository_path, pattern=\"**/*.md\")\n    prompts = []\n    for path in files:\n        key = self._path_to_key(path)\n        prompts.append(self.get(key))\n    return [p.metadata for p in prompts]\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.adapters.git_catalog_adapter","title":"<code>git_catalog_adapter</code>","text":"<p>Git-backed prompt catalog adapter.</p>"},{"location":"api/#tnh_scholar.prompt_system.adapters.git_catalog_adapter.GitPromptCatalog","title":"<code>GitPromptCatalog</code>","text":"<p>               Bases: <code>PromptCatalogPort</code></p> <p>Git-backed prompt catalog adapter (implements PromptCatalogPort).</p> Source code in <code>src/tnh_scholar/prompt_system/adapters/git_catalog_adapter.py</code> <pre><code>class GitPromptCatalog(PromptCatalogPort):\n    \"\"\"Git-backed prompt catalog adapter (implements PromptCatalogPort).\"\"\"\n\n    def __init__(\n        self,\n        config: PromptCatalogConfig,\n        transport: GitTransportClient,\n        loader: PromptLoader,\n        mapper: PromptMapper | None = None,\n        cache: CacheTransport[Prompt] | None = None,\n    ):\n        self._config = config\n        self._transport = transport\n        self._loader = loader\n        self._cache = cache or InMemoryCacheTransport(default_ttl_s=config.cache_ttl_s)\n        self._mapper = mapper or PromptMapper()\n\n    def get(self, key: str) -&gt; Prompt:\n        cache_key = self._make_cache_key(key)\n        cached = self._cache.get(cache_key)\n        if cached:\n            return cached\n\n        file_req = PromptFileRequest(\n            path=self._mapper.to_file_request(key, self._config.repository_path),\n            commit_sha=None,\n        )\n        file_resp = self._transport.read_file_at_commit(file_req)\n        prompt = self._mapper.to_domain_prompt(file_resp.content)\n\n        if self._config.validation_on_load and self._loader is not None:\n            validation = self._loader.validate(prompt)\n            if not validation.succeeded():\n                raise ValueError(f\"Invalid prompt: {validation.errors}\")\n\n        self._cache.set(cache_key, prompt, ttl_s=self._config.cache_ttl_s)\n        return prompt\n\n    def list(self) -&gt; list[PromptMetadata]:\n        files = self._transport.list_files(pattern=\"**/*.md\")\n        prompts = []\n        for path in files:\n            key = self._path_to_key(path)\n            prompts.append(self.get(key))\n        return [p.metadata for p in prompts]\n\n    def refresh(self) -&gt; None:\n        refresh_resp = self._transport.pull_latest()\n        for changed in refresh_resp.changed_files:\n            key = self._path_to_key(Path(changed))\n            self._cache.invalidate(self._make_cache_key(key))\n\n    def _make_cache_key(self, prompt_key: str) -&gt; str:\n        commit = self._transport.get_current_commit()\n        return f\"{prompt_key}@{commit[:8]}\"\n\n    def _path_to_key(self, path: Path) -&gt; str:\n        return path.stem\n</code></pre> <code>__init__(config, transport, loader, mapper=None, cache=None)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/adapters/git_catalog_adapter.py</code> <pre><code>def __init__(\n    self,\n    config: PromptCatalogConfig,\n    transport: GitTransportClient,\n    loader: PromptLoader,\n    mapper: PromptMapper | None = None,\n    cache: CacheTransport[Prompt] | None = None,\n):\n    self._config = config\n    self._transport = transport\n    self._loader = loader\n    self._cache = cache or InMemoryCacheTransport(default_ttl_s=config.cache_ttl_s)\n    self._mapper = mapper or PromptMapper()\n</code></pre> <code>get(key)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/adapters/git_catalog_adapter.py</code> <pre><code>def get(self, key: str) -&gt; Prompt:\n    cache_key = self._make_cache_key(key)\n    cached = self._cache.get(cache_key)\n    if cached:\n        return cached\n\n    file_req = PromptFileRequest(\n        path=self._mapper.to_file_request(key, self._config.repository_path),\n        commit_sha=None,\n    )\n    file_resp = self._transport.read_file_at_commit(file_req)\n    prompt = self._mapper.to_domain_prompt(file_resp.content)\n\n    if self._config.validation_on_load and self._loader is not None:\n        validation = self._loader.validate(prompt)\n        if not validation.succeeded():\n            raise ValueError(f\"Invalid prompt: {validation.errors}\")\n\n    self._cache.set(cache_key, prompt, ttl_s=self._config.cache_ttl_s)\n    return prompt\n</code></pre> <code>list()</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/adapters/git_catalog_adapter.py</code> <pre><code>def list(self) -&gt; list[PromptMetadata]:\n    files = self._transport.list_files(pattern=\"**/*.md\")\n    prompts = []\n    for path in files:\n        key = self._path_to_key(path)\n        prompts.append(self.get(key))\n    return [p.metadata for p in prompts]\n</code></pre> <code>refresh()</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/adapters/git_catalog_adapter.py</code> <pre><code>def refresh(self) -&gt; None:\n    refresh_resp = self._transport.pull_latest()\n    for changed in refresh_resp.changed_files:\n        key = self._path_to_key(Path(changed))\n        self._cache.invalidate(self._make_cache_key(key))\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.config","title":"<code>config</code>","text":"<p>Configuration models and policies for the prompt system.</p>"},{"location":"api/#tnh_scholar.prompt_system.config.policy","title":"<code>policy</code>","text":"<p>Behavior policies controlling prompt rendering and validation.</p>"},{"location":"api/#tnh_scholar.prompt_system.config.policy.PromptRenderPolicy","title":"<code>PromptRenderPolicy</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Policy for prompt rendering precedence and behavior.</p> Source code in <code>src/tnh_scholar/prompt_system/config/policy.py</code> <pre><code>class PromptRenderPolicy(BaseModel):\n    \"\"\"Policy for prompt rendering precedence and behavior.\"\"\"\n\n    policy_version: str = \"1.0\"\n    precedence_order: list[str] = [\n        \"caller_context\",\n        \"frontmatter_defaults\",\n        \"settings_defaults\",\n    ]\n    allow_undefined_vars: bool = False\n    merge_strategy: Literal[\"override\", \"merge_deep\"] = \"override\"\n</code></pre> <code>allow_undefined_vars = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>merge_strategy = 'override'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>policy_version = '1.0'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>precedence_order = ['caller_context', 'frontmatter_defaults', 'settings_defaults']</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.config.policy.ValidationPolicy","title":"<code>ValidationPolicy</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Policy controlling validation strictness.</p> Source code in <code>src/tnh_scholar/prompt_system/config/policy.py</code> <pre><code>class ValidationPolicy(BaseModel):\n    \"\"\"Policy controlling validation strictness.\"\"\"\n\n    policy_version: str = \"1.0\"\n    mode: Literal[\"strict\", \"warn\", \"permissive\"] = \"strict\"\n    fail_on_missing_required: bool = True\n    allow_extra_variables: bool = False\n</code></pre> <code>allow_extra_variables = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>fail_on_missing_required = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>mode = 'strict'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>policy_version = '1.0'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.config.prompt_catalog_config","title":"<code>prompt_catalog_config</code>","text":"<p>Construction-time configuration models for prompt catalog and transports.</p>"},{"location":"api/#tnh_scholar.prompt_system.config.prompt_catalog_config.GitTransportConfig","title":"<code>GitTransportConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Git transport layer configuration.</p> Source code in <code>src/tnh_scholar/prompt_system/config/prompt_catalog_config.py</code> <pre><code>class GitTransportConfig(BaseModel):\n    \"\"\"Git transport layer configuration.\"\"\"\n\n    repository_path: Path\n    auto_pull: bool = False\n    pull_timeout_s: float = 30.0\n    default_branch: str = \"main\"\n</code></pre> <code>auto_pull = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>default_branch = 'main'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>pull_timeout_s = 30.0</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>repository_path</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.config.prompt_catalog_config.PromptCatalogConfig","title":"<code>PromptCatalogConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for building a prompt catalog.</p> Source code in <code>src/tnh_scholar/prompt_system/config/prompt_catalog_config.py</code> <pre><code>class PromptCatalogConfig(BaseModel):\n    \"\"\"Configuration for building a prompt catalog.\"\"\"\n\n    repository_path: Path\n    enable_git_refresh: bool = True\n    cache_ttl_s: int = 300\n    validation_on_load: bool = True\n</code></pre> <code>cache_ttl_s = 300</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>enable_git_refresh = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>repository_path</code> <code>instance-attribute</code> \u00b6 <code>validation_on_load = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.config.settings","title":"<code>settings</code>","text":"<p>Environment-backed settings for the prompt system.</p>"},{"location":"api/#tnh_scholar.prompt_system.config.settings.PromptSystemSettings","title":"<code>PromptSystemSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Application-wide prompt system settings loaded from environment.</p> Source code in <code>src/tnh_scholar/prompt_system/config/settings.py</code> <pre><code>class PromptSystemSettings(BaseSettings):\n    \"\"\"Application-wide prompt system settings loaded from environment.\"\"\"\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        extra=\"ignore\",\n    )\n\n    tnh_prompt_dir: Path = Path(\"prompts/\")\n    default_validation_mode: str = \"strict\"\n    cache_enabled: bool = True\n    cache_ttl_seconds: int = 300\n    enable_safety_validation: bool = True\n\n    @classmethod\n    def from_env(cls) -&gt; \"PromptSystemSettings\":\n        \"\"\"Factory for consistency with other settings objects.\"\"\"\n        return cls()\n</code></pre> <code>cache_enabled = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>cache_ttl_seconds = 300</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>default_validation_mode = 'strict'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>enable_safety_validation = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', extra='ignore')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>tnh_prompt_dir = Path('prompts/')</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>from_env()</code> <code>classmethod</code> \u00b6 <p>Factory for consistency with other settings objects.</p> Source code in <code>src/tnh_scholar/prompt_system/config/settings.py</code> <pre><code>@classmethod\ndef from_env(cls) -&gt; \"PromptSystemSettings\":\n    \"\"\"Factory for consistency with other settings objects.\"\"\"\n    return cls()\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.domain","title":"<code>domain</code>","text":"<p>Domain models and protocols for prompt handling.</p>"},{"location":"api/#tnh_scholar.prompt_system.domain.models","title":"<code>models</code>","text":"<p>Domain models for the prompt system.</p>"},{"location":"api/#tnh_scholar.prompt_system.domain.models.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single message in a conversation.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/models.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"Single message in a conversation.\"\"\"\n\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n</code></pre> <code>content</code> <code>instance-attribute</code> \u00b6 <code>role</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.domain.models.Prompt","title":"<code>Prompt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Prompt domain model.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/models.py</code> <pre><code>class Prompt(BaseModel):\n    \"\"\"Prompt domain model.\"\"\"\n\n    name: str\n    version: str\n    template: str\n    metadata: PromptMetadata\n</code></pre> <code>metadata</code> <code>instance-attribute</code> \u00b6 <code>name</code> <code>instance-attribute</code> \u00b6 <code>template</code> <code>instance-attribute</code> \u00b6 <code>version</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.domain.models.PromptMetadata","title":"<code>PromptMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Prompt front matter metadata.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/models.py</code> <pre><code>class PromptMetadata(BaseModel):\n    \"\"\"Prompt front matter metadata.\"\"\"\n\n    key: str\n    name: str\n    version: str\n    description: str\n    task_type: str\n    required_variables: list[str]\n    optional_variables: list[str] = Field(default_factory=list)\n    default_variables: dict[str, Any] = Field(default_factory=dict)\n    tags: list[str] = Field(default_factory=list)\n    default_model: str | None = None\n    output_mode: Literal[\"text\", \"json\", \"structured\"] | None = None\n    safety_level: Literal[\"safe\", \"moderate\", \"sensitive\"] | None = None\n    pii_handling: Literal[\"none\", \"anonymize\", \"explicit_consent\"] | None = None\n    content_flags: list[str] = Field(default_factory=list)\n    schema_version: str = \"1.0\"\n    created_at: str | None = None\n    updated_at: str | None = None\n</code></pre> <code>content_flags = Field(default_factory=list)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>created_at = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>default_model = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>default_variables = Field(default_factory=dict)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>description</code> <code>instance-attribute</code> \u00b6 <code>key</code> <code>instance-attribute</code> \u00b6 <code>name</code> <code>instance-attribute</code> \u00b6 <code>optional_variables = Field(default_factory=list)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>output_mode = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>pii_handling = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>required_variables</code> <code>instance-attribute</code> \u00b6 <code>safety_level = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>schema_version = '1.0'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>tags = Field(default_factory=list)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>task_type</code> <code>instance-attribute</code> \u00b6 <code>updated_at = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>version</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.domain.models.PromptValidationResult","title":"<code>PromptValidationResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of prompt validation.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/models.py</code> <pre><code>class PromptValidationResult(BaseModel):\n    \"\"\"Result of prompt validation.\"\"\"\n\n    valid: bool\n    errors: list[ValidationIssue] = Field(default_factory=list)\n    warnings: list[ValidationIssue] = Field(default_factory=list)\n    fingerprint_data: dict[str, Any] = Field(default_factory=dict)\n\n    def succeeded(self) -&gt; bool:\n        return self.valid and len(self.errors) == 0\n</code></pre> <code>errors = Field(default_factory=list)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>fingerprint_data = Field(default_factory=dict)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>valid</code> <code>instance-attribute</code> \u00b6 <code>warnings = Field(default_factory=list)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>succeeded()</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/domain/models.py</code> <pre><code>def succeeded(self) -&gt; bool:\n    return self.valid and len(self.errors) == 0\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.domain.models.RenderParams","title":"<code>RenderParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Per-call rendering parameters.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/models.py</code> <pre><code>class RenderParams(BaseModel):\n    \"\"\"Per-call rendering parameters.\"\"\"\n\n    variables: dict[str, Any] = Field(default_factory=dict)\n    strict_undefined: bool = True\n    preserve_whitespace: bool = False\n    user_input: str | None = None\n</code></pre> <code>preserve_whitespace = False</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>strict_undefined = True</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>user_input = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>variables = Field(default_factory=dict)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.domain.models.RenderedPrompt","title":"<code>RenderedPrompt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rendered prompt ready for the provider.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/models.py</code> <pre><code>class RenderedPrompt(BaseModel):\n    \"\"\"Rendered prompt ready for the provider.\"\"\"\n\n    system: str | None = None\n    messages: list[Message] = Field(default_factory=list)\n</code></pre> <code>messages = Field(default_factory=list)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>system = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.domain.models.ValidationIssue","title":"<code>ValidationIssue</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single validation issue.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/models.py</code> <pre><code>class ValidationIssue(BaseModel):\n    \"\"\"Single validation issue.\"\"\"\n\n    level: Literal[\"error\", \"warning\", \"info\"]\n    code: str\n    message: str\n    field: str | None = None\n    line: int | None = None\n</code></pre> <code>code</code> <code>instance-attribute</code> \u00b6 <code>field = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>level</code> <code>instance-attribute</code> \u00b6 <code>line = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>message</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.domain.protocols","title":"<code>protocols</code>","text":"<p>Protocols defining prompt system behavior contracts.</p>"},{"location":"api/#tnh_scholar.prompt_system.domain.protocols.PromptCatalogPort","title":"<code>PromptCatalogPort</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Repository interface for prompt storage and retrieval.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/protocols.py</code> <pre><code>class PromptCatalogPort(Protocol):\n    \"\"\"Repository interface for prompt storage and retrieval.\"\"\"\n\n    def get(self, key: str) -&gt; Prompt:\n        \"\"\"Retrieve prompt by key.\"\"\"\n        ...\n\n    def list(self) -&gt; list[PromptMetadata]:\n        \"\"\"List available prompts.\"\"\"\n        ...\n</code></pre> <code>get(key)</code> \u00b6 <p>Retrieve prompt by key.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/protocols.py</code> <pre><code>def get(self, key: str) -&gt; Prompt:\n    \"\"\"Retrieve prompt by key.\"\"\"\n    ...\n</code></pre> <code>list()</code> \u00b6 <p>List available prompts.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/protocols.py</code> <pre><code>def list(self) -&gt; list[PromptMetadata]:\n    \"\"\"List available prompts.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.domain.protocols.PromptRendererPort","title":"<code>PromptRendererPort</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Renders prompts with variable substitution.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/protocols.py</code> <pre><code>class PromptRendererPort(Protocol):\n    \"\"\"Renders prompts with variable substitution.\"\"\"\n\n    def render(self, prompt: Prompt, params: RenderParams) -&gt; RenderedPrompt:\n        \"\"\"Render prompt with templating.\"\"\"\n        ...\n</code></pre> <code>render(prompt, params)</code> \u00b6 <p>Render prompt with templating.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/protocols.py</code> <pre><code>def render(self, prompt: Prompt, params: RenderParams) -&gt; RenderedPrompt:\n    \"\"\"Render prompt with templating.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.domain.protocols.PromptValidatorPort","title":"<code>PromptValidatorPort</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Validates prompt schema and render parameters.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/protocols.py</code> <pre><code>class PromptValidatorPort(Protocol):\n    \"\"\"Validates prompt schema and render parameters.\"\"\"\n\n    def validate(self, prompt: Prompt) -&gt; PromptValidationResult:\n        \"\"\"Validate prompt metadata and template.\"\"\"\n        ...\n\n    def validate_render(self, prompt: Prompt, params: RenderParams) -&gt; PromptValidationResult:\n        \"\"\"Validate render inputs against prompt requirements.\"\"\"\n        ...\n</code></pre> <code>validate(prompt)</code> \u00b6 <p>Validate prompt metadata and template.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/protocols.py</code> <pre><code>def validate(self, prompt: Prompt) -&gt; PromptValidationResult:\n    \"\"\"Validate prompt metadata and template.\"\"\"\n    ...\n</code></pre> <code>validate_render(prompt, params)</code> \u00b6 <p>Validate render inputs against prompt requirements.</p> Source code in <code>src/tnh_scholar/prompt_system/domain/protocols.py</code> <pre><code>def validate_render(self, prompt: Prompt, params: RenderParams) -&gt; PromptValidationResult:\n    \"\"\"Validate render inputs against prompt requirements.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.mappers","title":"<code>mappers</code>","text":"<p>Mappers for translating transport data to domain models and back.</p>"},{"location":"api/#tnh_scholar.prompt_system.mappers.prompt_mapper","title":"<code>prompt_mapper</code>","text":"<p>Mapper for translating prompt files to domain models.</p>"},{"location":"api/#tnh_scholar.prompt_system.mappers.prompt_mapper.PromptMapper","title":"<code>PromptMapper</code>","text":"<p>Maps transport-layer prompt data into domain objects.</p> Source code in <code>src/tnh_scholar/prompt_system/mappers/prompt_mapper.py</code> <pre><code>class PromptMapper:\n    \"\"\"Maps transport-layer prompt data into domain objects.\"\"\"\n\n    def to_file_request(self, key: str, base_path: Path) -&gt; Path:\n        \"\"\"Map prompt key to a filesystem path for transport.\"\"\"\n        return base_path / f\"{key}.md\"\n\n    def to_domain_prompt(self, file_content: str) -&gt; Prompt:\n        \"\"\"Map raw file content (including front matter) to a Prompt.\"\"\"\n        metadata_raw, body = self._split_frontmatter(file_content)\n        metadata = PromptMetadata.model_validate(metadata_raw)\n        return Prompt(\n            name=metadata.name,\n            version=metadata.version,\n            template=body,\n            metadata=metadata,\n        )\n\n    def _split_frontmatter(self, content: str) -&gt; tuple[dict[str, Any], str]:\n        \"\"\"Split YAML front matter from markdown content using shared Frontmatter helper.\"\"\"\n        cleaned = content.lstrip(\"\\ufeff\")\n        metadata_obj, body = Frontmatter.extract(cleaned)\n        metadata_raw = metadata_obj.to_dict() if metadata_obj else {}\n        if not metadata_raw:\n            raise ValueError(\"Prompt file missing or invalid YAML front matter.\")\n        return metadata_raw, body.lstrip()\n</code></pre> <code>to_domain_prompt(file_content)</code> \u00b6 <p>Map raw file content (including front matter) to a Prompt.</p> Source code in <code>src/tnh_scholar/prompt_system/mappers/prompt_mapper.py</code> <pre><code>def to_domain_prompt(self, file_content: str) -&gt; Prompt:\n    \"\"\"Map raw file content (including front matter) to a Prompt.\"\"\"\n    metadata_raw, body = self._split_frontmatter(file_content)\n    metadata = PromptMetadata.model_validate(metadata_raw)\n    return Prompt(\n        name=metadata.name,\n        version=metadata.version,\n        template=body,\n        metadata=metadata,\n    )\n</code></pre> <code>to_file_request(key, base_path)</code> \u00b6 <p>Map prompt key to a filesystem path for transport.</p> Source code in <code>src/tnh_scholar/prompt_system/mappers/prompt_mapper.py</code> <pre><code>def to_file_request(self, key: str, base_path: Path) -&gt; Path:\n    \"\"\"Map prompt key to a filesystem path for transport.\"\"\"\n    return base_path / f\"{key}.md\"\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.service","title":"<code>service</code>","text":"<p>Prompt system services (rendering, validation, loading).</p>"},{"location":"api/#tnh_scholar.prompt_system.service.loader","title":"<code>loader</code>","text":"<p>Prompt loader orchestration service.</p>"},{"location":"api/#tnh_scholar.prompt_system.service.loader.PromptLoader","title":"<code>PromptLoader</code>","text":"<p>Responsible for preparing prompts (parse + validate).</p> Source code in <code>src/tnh_scholar/prompt_system/service/loader.py</code> <pre><code>class PromptLoader:\n    \"\"\"Responsible for preparing prompts (parse + validate).\"\"\"\n\n    def __init__(self, validator: PromptValidatorPort):\n        self._validator = validator\n\n    def validate(self, prompt: Prompt) -&gt; PromptValidationResult:\n        \"\"\"Validate prompt using configured validator.\"\"\"\n        return self._validator.validate(prompt)\n</code></pre> <code>__init__(validator)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/service/loader.py</code> <pre><code>def __init__(self, validator: PromptValidatorPort):\n    self._validator = validator\n</code></pre> <code>validate(prompt)</code> \u00b6 <p>Validate prompt using configured validator.</p> Source code in <code>src/tnh_scholar/prompt_system/service/loader.py</code> <pre><code>def validate(self, prompt: Prompt) -&gt; PromptValidationResult:\n    \"\"\"Validate prompt using configured validator.\"\"\"\n    return self._validator.validate(prompt)\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.service.renderer","title":"<code>renderer</code>","text":"<p>Prompt rendering service.</p>"},{"location":"api/#tnh_scholar.prompt_system.service.renderer.PromptRenderer","title":"<code>PromptRenderer</code>","text":"<p>               Bases: <code>PromptRendererPort</code></p> <p>Renders prompts using configured policy.</p> Source code in <code>src/tnh_scholar/prompt_system/service/renderer.py</code> <pre><code>class PromptRenderer(PromptRendererPort):\n    \"\"\"Renders prompts using configured policy.\"\"\"\n\n    def __init__(\n        self,\n        policy: PromptRenderPolicy,\n        settings_defaults: dict[str, Any] | None = None,\n    ):\n        self._policy = policy\n        self._settings_defaults = settings_defaults or {}\n\n    def render(self, prompt: Prompt, params: RenderParams) -&gt; RenderedPrompt:\n        \"\"\"Render prompt with templating and precedence rules.\"\"\"\n        merged_vars = self._merge_variables(prompt, params)\n        env = Environment(\n            undefined=StrictUndefined if params.strict_undefined else Undefined,\n            trim_blocks=not params.preserve_whitespace,\n            lstrip_blocks=not params.preserve_whitespace,\n        )\n\n        try:\n            template = env.from_string(prompt.template)\n            system_content = template.render(**merged_vars)\n        except TemplateSyntaxError as exc:\n            raise ValueError(f\"Invalid prompt template: {exc}\") from exc\n\n        messages: list[Message] = []\n        # ADR-A12 expects a user message even when user_input is empty to preserve shape.\n        messages.append(Message(role=\"user\", content=params.user_input or \"\"))\n\n        return RenderedPrompt(system=system_content, messages=messages)\n\n    def _merge_variables(self, prompt: Prompt, params: RenderParams) -&gt; dict[str, Any]:\n        \"\"\"Merge variables according to policy precedence.\"\"\"\n        sources: dict[str, dict[str, Any]] = {\n            \"settings_defaults\": dict(self._settings_defaults),\n            \"frontmatter_defaults\": self._extract_frontmatter_defaults(prompt),\n            \"caller_context\": dict(params.variables),\n        }\n\n        merged: dict[str, Any] = {}\n        for source_name in self._policy.precedence_order:\n            merged.update(sources.get(source_name, {}))\n        return merged\n\n    def _extract_frontmatter_defaults(self, prompt: Prompt) -&gt; dict[str, Any]:\n        \"\"\"Extract default variable values from prompt metadata if provided.\"\"\"\n        metadata_dict = prompt.metadata.model_dump(exclude_none=True)\n        defaults = metadata_dict.get(\"default_variables\", {})\n        return defaults if isinstance(defaults, dict) else {}\n</code></pre> <code>__init__(policy, settings_defaults=None)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/service/renderer.py</code> <pre><code>def __init__(\n    self,\n    policy: PromptRenderPolicy,\n    settings_defaults: dict[str, Any] | None = None,\n):\n    self._policy = policy\n    self._settings_defaults = settings_defaults or {}\n</code></pre> <code>render(prompt, params)</code> \u00b6 <p>Render prompt with templating and precedence rules.</p> Source code in <code>src/tnh_scholar/prompt_system/service/renderer.py</code> <pre><code>def render(self, prompt: Prompt, params: RenderParams) -&gt; RenderedPrompt:\n    \"\"\"Render prompt with templating and precedence rules.\"\"\"\n    merged_vars = self._merge_variables(prompt, params)\n    env = Environment(\n        undefined=StrictUndefined if params.strict_undefined else Undefined,\n        trim_blocks=not params.preserve_whitespace,\n        lstrip_blocks=not params.preserve_whitespace,\n    )\n\n    try:\n        template = env.from_string(prompt.template)\n        system_content = template.render(**merged_vars)\n    except TemplateSyntaxError as exc:\n        raise ValueError(f\"Invalid prompt template: {exc}\") from exc\n\n    messages: list[Message] = []\n    # ADR-A12 expects a user message even when user_input is empty to preserve shape.\n    messages.append(Message(role=\"user\", content=params.user_input or \"\"))\n\n    return RenderedPrompt(system=system_content, messages=messages)\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.service.validator","title":"<code>validator</code>","text":"<p>Prompt validation service.</p>"},{"location":"api/#tnh_scholar.prompt_system.service.validator.PromptValidator","title":"<code>PromptValidator</code>","text":"<p>               Bases: <code>PromptValidatorPort</code></p> <p>Validates prompt metadata and render parameters.</p> Source code in <code>src/tnh_scholar/prompt_system/service/validator.py</code> <pre><code>class PromptValidator(PromptValidatorPort):\n    \"\"\"Validates prompt metadata and render parameters.\"\"\"\n\n    def __init__(self, policy: ValidationPolicy):\n        self._policy = policy\n\n    def validate(self, prompt: Prompt) -&gt; PromptValidationResult:\n        \"\"\"Validate prompt metadata and template syntax.\"\"\"\n        errors: list[ValidationIssue] = []\n        warnings: list[ValidationIssue] = []\n\n        self._validate_required_fields(prompt, errors)\n        self._validate_version(prompt, errors)\n        self._validate_template(prompt, errors)\n\n        valid = len(errors) == 0\n        return PromptValidationResult(valid=valid, errors=errors, warnings=warnings)\n\n    def validate_render(\n        self, prompt: Prompt, params: RenderParams\n    ) -&gt; PromptValidationResult:\n        \"\"\"Validate render inputs against prompt requirements.\"\"\"\n        errors: list[ValidationIssue] = []\n        warnings: list[ValidationIssue] = []\n\n        self._validate_required_variables(prompt, params, errors)\n        self._validate_extra_variables(prompt, params, errors, warnings)\n\n        valid = len(errors) == 0\n        return PromptValidationResult(valid=valid, errors=errors, warnings=warnings)\n\n    def _validate_required_fields(\n        self, prompt: Prompt, errors: list[ValidationIssue]\n    ) -&gt; None:\n        if not prompt.metadata.name:\n            errors.append(\n                ValidationIssue(\n                    level=\"error\",\n                    code=\"MISSING_NAME\",\n                    message=\"Prompt name is required\",\n                    field=\"name\",\n                )\n            )\n        if not prompt.metadata.version:\n            errors.append(\n                ValidationIssue(\n                    level=\"error\",\n                    code=\"MISSING_VERSION\",\n                    message=\"Prompt version is required\",\n                    field=\"version\",\n                )\n            )\n        if not prompt.template:\n            errors.append(\n                ValidationIssue(\n                    level=\"error\",\n                    code=\"MISSING_TEMPLATE\",\n                    message=\"Prompt template content is required\",\n                    field=\"template\",\n                )\n            )\n\n    def _validate_version(self, prompt: Prompt, errors: list[ValidationIssue]) -&gt; None:\n        if prompt.metadata.version and not _SEMVER_PATTERN.match(\n            prompt.metadata.version\n        ):\n            errors.append(\n                ValidationIssue(\n                    level=\"error\",\n                    code=\"INVALID_VERSION\",\n                    message=\"Version must be semver format (e.g., 1.0.0)\",\n                    field=\"version\",\n                )\n            )\n\n    def _validate_template(self, prompt: Prompt, errors: list[ValidationIssue]) -&gt; None:\n        env = Environment(undefined=StrictUndefined, trim_blocks=True, lstrip_blocks=True)\n        try:\n            env.parse(prompt.template)\n        except TemplateSyntaxError as exc:\n            errors.append(\n                ValidationIssue(\n                    level=\"error\",\n                    code=\"INVALID_TEMPLATE\",\n                    message=str(exc),\n                    field=\"template\",\n                )\n            )\n\n    def _validate_required_variables(\n        self, prompt: Prompt, params: RenderParams, errors: list[ValidationIssue]\n    ) -&gt; None:\n        missing = set(prompt.metadata.required_variables) - set(\n            params.variables.keys()\n        )\n        if missing and self._policy.fail_on_missing_required:\n            errors.append(\n                ValidationIssue(\n                    level=\"error\",\n                    code=\"MISSING_REQUIRED_VARS\",\n                    message=f\"Missing required variables: {sorted(missing)}\",\n                    field=\"variables\",\n                )\n            )\n\n    def _validate_extra_variables(\n        self,\n        prompt: Prompt,\n        params: RenderParams,\n        errors: list[ValidationIssue],\n        warnings: list[ValidationIssue],\n    ) -&gt; None:\n        if self._policy.allow_extra_variables:\n            return\n\n        allowed = set(prompt.metadata.required_variables) | set(\n            prompt.metadata.optional_variables\n        )\n        extra = set(params.variables.keys()) - allowed\n        if not extra:\n            return\n\n        issue = ValidationIssue(\n            level=\"warning\" if self._policy.mode == \"warn\" else \"error\",\n            code=\"EXTRA_VARIABLES\",\n            message=f\"Unexpected variables: {sorted(extra)}\",\n            field=\"variables\",\n        )\n\n        if self._policy.mode == \"warn\":\n            warnings.append(issue)\n        else:\n            errors.append(issue)\n</code></pre> <code>__init__(policy)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/service/validator.py</code> <pre><code>def __init__(self, policy: ValidationPolicy):\n    self._policy = policy\n</code></pre> <code>validate(prompt)</code> \u00b6 <p>Validate prompt metadata and template syntax.</p> Source code in <code>src/tnh_scholar/prompt_system/service/validator.py</code> <pre><code>def validate(self, prompt: Prompt) -&gt; PromptValidationResult:\n    \"\"\"Validate prompt metadata and template syntax.\"\"\"\n    errors: list[ValidationIssue] = []\n    warnings: list[ValidationIssue] = []\n\n    self._validate_required_fields(prompt, errors)\n    self._validate_version(prompt, errors)\n    self._validate_template(prompt, errors)\n\n    valid = len(errors) == 0\n    return PromptValidationResult(valid=valid, errors=errors, warnings=warnings)\n</code></pre> <code>validate_render(prompt, params)</code> \u00b6 <p>Validate render inputs against prompt requirements.</p> Source code in <code>src/tnh_scholar/prompt_system/service/validator.py</code> <pre><code>def validate_render(\n    self, prompt: Prompt, params: RenderParams\n) -&gt; PromptValidationResult:\n    \"\"\"Validate render inputs against prompt requirements.\"\"\"\n    errors: list[ValidationIssue] = []\n    warnings: list[ValidationIssue] = []\n\n    self._validate_required_variables(prompt, params, errors)\n    self._validate_extra_variables(prompt, params, errors, warnings)\n\n    valid = len(errors) == 0\n    return PromptValidationResult(valid=valid, errors=errors, warnings=warnings)\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.transport","title":"<code>transport</code>","text":"<p>Transport layer for prompt system (git/filesystem/cache).</p>"},{"location":"api/#tnh_scholar.prompt_system.transport.cache","title":"<code>cache</code>","text":"<p>Cache transport abstractions.</p>"},{"location":"api/#tnh_scholar.prompt_system.transport.cache.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.prompt_system.transport.cache.CacheTransport","title":"<code>CacheTransport</code>","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[T]</code></p> <p>Abstract cache transport.</p> Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>class CacheTransport(Protocol, Generic[T]):\n    \"\"\"Abstract cache transport.\"\"\"\n\n    def get(self, key: str) -&gt; T | None:\n        ...\n\n    def set(self, key: str, value: T, ttl_s: int | None = None) -&gt; None:\n        ...\n\n    def invalidate(self, key: str) -&gt; None:\n        ...\n\n    def clear(self) -&gt; None:\n        ...\n</code></pre> <code>clear()</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>def clear(self) -&gt; None:\n    ...\n</code></pre> <code>get(key)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>def get(self, key: str) -&gt; T | None:\n    ...\n</code></pre> <code>invalidate(key)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>def invalidate(self, key: str) -&gt; None:\n    ...\n</code></pre> <code>set(key, value, ttl_s=None)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>def set(self, key: str, value: T, ttl_s: int | None = None) -&gt; None:\n    ...\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.transport.cache.InMemoryCacheTransport","title":"<code>InMemoryCacheTransport</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>In-memory cache implementation with TTL.</p> Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>class InMemoryCacheTransport(Generic[T]):\n    \"\"\"In-memory cache implementation with TTL.\"\"\"\n\n    def __init__(self, default_ttl_s: int = 300):\n        self._cache: dict[str, tuple[T, float]] = {}\n        self._default_ttl = default_ttl_s\n\n    def get(self, key: str) -&gt; T | None:\n        import time\n\n        if key not in self._cache:\n            return None\n        value, expires_at = self._cache[key]\n        if time.time() &gt; expires_at:\n            del self._cache[key]\n            return None\n        return value\n\n    def set(self, key: str, value: T, ttl_s: int | None = None) -&gt; None:\n        import time\n\n        ttl = ttl_s if ttl_s is not None else self._default_ttl\n        expires_at = time.time() + ttl\n        self._cache[key] = (value, expires_at)\n\n    def invalidate(self, key: str) -&gt; None:\n        self._cache.pop(key, None)\n\n    def clear(self) -&gt; None:\n        self._cache.clear()\n</code></pre> <code>__init__(default_ttl_s=300)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>def __init__(self, default_ttl_s: int = 300):\n    self._cache: dict[str, tuple[T, float]] = {}\n    self._default_ttl = default_ttl_s\n</code></pre> <code>clear()</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>def clear(self) -&gt; None:\n    self._cache.clear()\n</code></pre> <code>get(key)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>def get(self, key: str) -&gt; T | None:\n    import time\n\n    if key not in self._cache:\n        return None\n    value, expires_at = self._cache[key]\n    if time.time() &gt; expires_at:\n        del self._cache[key]\n        return None\n    return value\n</code></pre> <code>invalidate(key)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>def invalidate(self, key: str) -&gt; None:\n    self._cache.pop(key, None)\n</code></pre> <code>set(key, value, ttl_s=None)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/cache.py</code> <pre><code>def set(self, key: str, value: T, ttl_s: int | None = None) -&gt; None:\n    import time\n\n    ttl = ttl_s if ttl_s is not None else self._default_ttl\n    expires_at = time.time() + ttl\n    self._cache[key] = (value, expires_at)\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.transport.filesystem","title":"<code>filesystem</code>","text":"<p>Filesystem transport for prompt files.</p>"},{"location":"api/#tnh_scholar.prompt_system.transport.filesystem.FilesystemTransport","title":"<code>FilesystemTransport</code>","text":"<p>Reads prompt files from the filesystem.</p> Source code in <code>src/tnh_scholar/prompt_system/transport/filesystem.py</code> <pre><code>class FilesystemTransport:\n    \"\"\"Reads prompt files from the filesystem.\"\"\"\n\n    def __init__(self, mapper: PromptMapper):\n        self._mapper = mapper\n\n    def read_file(self, request: PromptFileRequest) -&gt; PromptFileResponse:\n        \"\"\"Read a prompt file from disk.\"\"\"\n        content = request.path.read_text(encoding=\"utf-8\")\n        metadata_raw, _ = self._mapper._split_frontmatter(content)\n        return PromptFileResponse(\n            content=content,\n            metadata_raw=metadata_raw,\n            file_hash=self._hash_content(content),\n            loaded_at=self._now_iso(),\n        )\n\n    def list_files(self, base_path: Path, pattern: str = \"**/*.md\") -&gt; list[Path]:\n        \"\"\"List prompt files under base path.\"\"\"\n        return sorted(base_path.glob(pattern))\n\n    def _hash_content(self, content: str) -&gt; str:\n        import hashlib\n\n        return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n\n    def _now_iso(self) -&gt; str:\n        import datetime as dt\n\n        return dt.datetime.now(dt.timezone.utc).isoformat()\n</code></pre> <code>__init__(mapper)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/filesystem.py</code> <pre><code>def __init__(self, mapper: PromptMapper):\n    self._mapper = mapper\n</code></pre> <code>list_files(base_path, pattern='**/*.md')</code> \u00b6 <p>List prompt files under base path.</p> Source code in <code>src/tnh_scholar/prompt_system/transport/filesystem.py</code> <pre><code>def list_files(self, base_path: Path, pattern: str = \"**/*.md\") -&gt; list[Path]:\n    \"\"\"List prompt files under base path.\"\"\"\n    return sorted(base_path.glob(pattern))\n</code></pre> <code>read_file(request)</code> \u00b6 <p>Read a prompt file from disk.</p> Source code in <code>src/tnh_scholar/prompt_system/transport/filesystem.py</code> <pre><code>def read_file(self, request: PromptFileRequest) -&gt; PromptFileResponse:\n    \"\"\"Read a prompt file from disk.\"\"\"\n    content = request.path.read_text(encoding=\"utf-8\")\n    metadata_raw, _ = self._mapper._split_frontmatter(content)\n    return PromptFileResponse(\n        content=content,\n        metadata_raw=metadata_raw,\n        file_hash=self._hash_content(content),\n        loaded_at=self._now_iso(),\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.transport.git_client","title":"<code>git_client</code>","text":"<p>Git transport client for prompt files.</p>"},{"location":"api/#tnh_scholar.prompt_system.transport.git_client.GitTransportClient","title":"<code>GitTransportClient</code>","text":"<p>Minimal git transport operations.</p> Source code in <code>src/tnh_scholar/prompt_system/transport/git_client.py</code> <pre><code>class GitTransportClient:\n    \"\"\"Minimal git transport operations.\"\"\"\n\n    def __init__(self, config: GitTransportConfig, mapper: PromptMapper):\n        self._config = config\n        self._mapper = mapper\n\n    def get_current_commit(self) -&gt; str:\n        return (\n            self._run_git([\"rev-parse\", \"HEAD\"], cwd=self._config.repository_path)\n            .strip()\n        )\n\n    def pull_latest(self) -&gt; GitRefreshResponse:\n        if not self._config.auto_pull:\n            return GitRefreshResponse(\n                current_commit=self.get_current_commit(),\n                branch=self._current_branch(),\n                changed_files=[],\n                refreshed_at=self._now_iso(),\n            )\n        self._run_git(\n            [\"pull\"],\n            cwd=self._config.repository_path,\n            timeout=self._config.pull_timeout_s,\n        )\n        return GitRefreshResponse(\n            current_commit=self.get_current_commit(),\n            branch=self._current_branch(),\n            changed_files=self._changed_files(),\n            refreshed_at=self._now_iso(),\n        )\n\n    def read_file_at_commit(self, request: PromptFileRequest) -&gt; PromptFileResponse:\n        if request.commit_sha:\n            spec = f\"{request.commit_sha}:{request.path}\"\n            content = self._run_git(\n                [\"show\", spec], cwd=self._config.repository_path\n            )\n        else:\n            content = request.path.read_text(encoding=\"utf-8\")\n\n        metadata_raw, _ = self._mapper._split_frontmatter(content)\n        return PromptFileResponse(\n            content=content,\n            metadata_raw=metadata_raw,\n            file_hash=self._hash_content(content),\n            loaded_at=self._now_iso(),\n        )\n\n    def list_files(self, pattern: str = \"**/*.md\") -&gt; list[Path]:\n        return sorted(self._config.repository_path.glob(pattern))\n\n    def _current_branch(self) -&gt; str:\n        return (\n            self._run_git([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=self._config.repository_path)\n            .strip()\n        )\n\n    def _changed_files(self) -&gt; list[str]:\n        try:\n            output = self._run_git(\n                [\"diff\", \"--name-only\", \"HEAD@{1}\", \"HEAD\"],\n                cwd=self._config.repository_path,\n            )\n            return [line for line in output.splitlines() if line]\n        except RuntimeError:\n            return []\n\n    def _run_git(self, args: list[str], cwd: Path, timeout: float | None = None) -&gt; str:\n        result = subprocess.run(\n            [\"git\", *args],\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            check=False,\n            timeout=timeout,\n            text=True,\n        )\n        if result.returncode != 0:\n            raise RuntimeError(f\"git {' '.join(args)} failed: {result.stderr.strip()}\")\n        return result.stdout\n\n    def _hash_content(self, content: str) -&gt; str:\n        import hashlib\n\n        return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n\n    def _now_iso(self) -&gt; str:\n        import datetime as dt\n\n        return dt.datetime.now(dt.timezone.utc).isoformat()\n</code></pre> <code>__init__(config, mapper)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/git_client.py</code> <pre><code>def __init__(self, config: GitTransportConfig, mapper: PromptMapper):\n    self._config = config\n    self._mapper = mapper\n</code></pre> <code>get_current_commit()</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/git_client.py</code> <pre><code>def get_current_commit(self) -&gt; str:\n    return (\n        self._run_git([\"rev-parse\", \"HEAD\"], cwd=self._config.repository_path)\n        .strip()\n    )\n</code></pre> <code>list_files(pattern='**/*.md')</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/git_client.py</code> <pre><code>def list_files(self, pattern: str = \"**/*.md\") -&gt; list[Path]:\n    return sorted(self._config.repository_path.glob(pattern))\n</code></pre> <code>pull_latest()</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/git_client.py</code> <pre><code>def pull_latest(self) -&gt; GitRefreshResponse:\n    if not self._config.auto_pull:\n        return GitRefreshResponse(\n            current_commit=self.get_current_commit(),\n            branch=self._current_branch(),\n            changed_files=[],\n            refreshed_at=self._now_iso(),\n        )\n    self._run_git(\n        [\"pull\"],\n        cwd=self._config.repository_path,\n        timeout=self._config.pull_timeout_s,\n    )\n    return GitRefreshResponse(\n        current_commit=self.get_current_commit(),\n        branch=self._current_branch(),\n        changed_files=self._changed_files(),\n        refreshed_at=self._now_iso(),\n    )\n</code></pre> <code>read_file_at_commit(request)</code> \u00b6 Source code in <code>src/tnh_scholar/prompt_system/transport/git_client.py</code> <pre><code>def read_file_at_commit(self, request: PromptFileRequest) -&gt; PromptFileResponse:\n    if request.commit_sha:\n        spec = f\"{request.commit_sha}:{request.path}\"\n        content = self._run_git(\n            [\"show\", spec], cwd=self._config.repository_path\n        )\n    else:\n        content = request.path.read_text(encoding=\"utf-8\")\n\n    metadata_raw, _ = self._mapper._split_frontmatter(content)\n    return PromptFileResponse(\n        content=content,\n        metadata_raw=metadata_raw,\n        file_hash=self._hash_content(content),\n        loaded_at=self._now_iso(),\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.prompt_system.transport.models","title":"<code>models</code>","text":"<p>Transport models for prompt system I/O.</p>"},{"location":"api/#tnh_scholar.prompt_system.transport.models.GitRefreshRequest","title":"<code>GitRefreshRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request to refresh git repository.</p> Source code in <code>src/tnh_scholar/prompt_system/transport/models.py</code> <pre><code>class GitRefreshRequest(BaseModel):\n    \"\"\"Request to refresh git repository.\"\"\"\n\n    repository_path: Path\n    target_ref: str | None = None\n</code></pre> <code>repository_path</code> <code>instance-attribute</code> \u00b6 <code>target_ref = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.transport.models.GitRefreshResponse","title":"<code>GitRefreshResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Git refresh operation result.</p> Source code in <code>src/tnh_scholar/prompt_system/transport/models.py</code> <pre><code>class GitRefreshResponse(BaseModel):\n    \"\"\"Git refresh operation result.\"\"\"\n\n    current_commit: str\n    branch: str\n    changed_files: list[str]\n    refreshed_at: str\n</code></pre> <code>branch</code> <code>instance-attribute</code> \u00b6 <code>changed_files</code> <code>instance-attribute</code> \u00b6 <code>current_commit</code> <code>instance-attribute</code> \u00b6 <code>refreshed_at</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.transport.models.PromptFileRequest","title":"<code>PromptFileRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Transport-level request to load a prompt file.</p> Source code in <code>src/tnh_scholar/prompt_system/transport/models.py</code> <pre><code>class PromptFileRequest(BaseModel):\n    \"\"\"Transport-level request to load a prompt file.\"\"\"\n\n    path: Path\n    commit_sha: str | None = None\n</code></pre> <code>commit_sha = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>path</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.prompt_system.transport.models.PromptFileResponse","title":"<code>PromptFileResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Transport-level prompt file data.</p> Source code in <code>src/tnh_scholar/prompt_system/transport/models.py</code> <pre><code>class PromptFileResponse(BaseModel):\n    \"\"\"Transport-level prompt file data.\"\"\"\n\n    content: str\n    metadata_raw: dict\n    file_hash: str\n    loaded_at: str\n</code></pre> <code>content</code> <code>instance-attribute</code> \u00b6 <code>file_hash</code> <code>instance-attribute</code> \u00b6 <code>loaded_at</code> <code>instance-attribute</code> \u00b6 <code>metadata_raw</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.text_processing","title":"<code>text_processing</code>","text":""},{"location":"api/#tnh_scholar.text_processing.__all__","title":"<code>__all__ = ['bracket_lines', 'unbracket_lines', 'lines_from_bracketed_text', 'NumberedText', 'normalize_newlines', 'clean_text']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText","title":"<code>NumberedText</code>","text":"<p>Represents a text document with numbered lines for easy reference and manipulation.</p> <p>Provides utilities for working with line-numbered text including reading, writing, accessing lines by number, and iterating over numbered lines.</p> <p>Attributes:</p> Name Type Description <code>lines</code> <code>List[str]</code> <p>List of text lines</p> <code>start</code> <code>int</code> <p>Starting line number (default: 1)</p> <code>separator</code> <code>str</code> <p>Separator between line number and content (default: \": \")</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text = \"First line\\nSecond line\\n\\nFourth line\"\n&gt;&gt;&gt; doc = NumberedText(text)\n&gt;&gt;&gt; print(doc)\n1: First line\n2: Second line\n3:\n4: Fourth line\n</code></pre> <pre><code>&gt;&gt;&gt; print(doc.get_line(2))\nSecond line\n</code></pre> <pre><code>&gt;&gt;&gt; for num, line in doc:\n...     print(f\"Line {num}: {len(line)} chars\")\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>class NumberedText:\n    \"\"\"\n    Represents a text document with numbered lines for easy reference and manipulation.\n\n    Provides utilities for working with line-numbered text including reading,\n    writing, accessing lines by number, and iterating over numbered lines.\n\n    Attributes:\n        lines (List[str]): List of text lines\n        start (int): Starting line number (default: 1)\n        separator (str): Separator between line number and content (default: \": \")\n\n    Examples:\n        &gt;&gt;&gt; text = \"First line\\\\nSecond line\\\\n\\\\nFourth line\"\n        &gt;&gt;&gt; doc = NumberedText(text)\n        &gt;&gt;&gt; print(doc)\n        1: First line\n        2: Second line\n        3:\n        4: Fourth line\n\n        &gt;&gt;&gt; print(doc.get_line(2))\n        Second line\n\n        &gt;&gt;&gt; for num, line in doc:\n        ...     print(f\"Line {num}: {len(line)} chars\")\n    \"\"\"\n\n    @dataclass\n    class LineSegment:\n        \"\"\"\n        Represents a segment of lines with start and end indices in 1-based indexing.\n\n        The segment follows Python range conventions where start is inclusive and\n        end is exclusive. However, indexing is 1-based to match NumberedText.\n\n        Attributes:\n            start: Starting line number (inclusive, 1-based)\n            end: Ending line number (exclusive, 1-based)\n        \"\"\"\n\n        start: int\n        end: int\n\n        def __iter__(self):\n            \"\"\"Allow unpacking into start, end pairs.\"\"\"\n            yield self.start\n            yield self.end\n\n    class SegmentIterator:\n        \"\"\"\n        Iterator for generating line segments of specified size.\n\n        Produces segments of lines with start/end indices following 1-based indexing.\n        The final segment may be smaller than the specified segment size.\n\n        Attributes:\n            total_lines: Total number of lines in text\n            segment_size: Number of lines per segment\n            start_line: Starting line number (1-based)\n            min_segment_size: Minimum size for the final segment\n        \"\"\"\n\n        def __init__(\n            self,\n            total_lines: int,\n            segment_size: int,\n            start_line: int = 1,\n            min_segment_size: Optional[int] = None,\n        ):\n            \"\"\"\n            Initialize the segment iterator.\n\n            Args:\n                total_lines: Total number of lines to iterate over\n                segment_size: Desired size of each segment\n                start_line: First line number (default: 1)\n                min_segment_size: Minimum size for final segment (default: None)\n                    If specified, the last segment will be merged with the previous one\n                    if it would be smaller than this size.\n\n            Raises:\n                ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n                ValueError: If start_line &lt; 1 (must use 1-based indexing)\n                ValueError: If min_segment_size &gt;= segment_size\n            \"\"\"\n            if segment_size &lt; 1:\n                raise ValueError(\"Segment size must be at least 1\")\n            if total_lines &lt; 1:\n                raise ValueError(\"Total lines must be at least 1\")\n            if start_line &lt; 1:\n                raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n            if min_segment_size is not None and min_segment_size &gt;= segment_size:\n                raise ValueError(\"Minimum segment size must be less than segment size\")\n\n            self.total_lines = total_lines\n            self.segment_size = segment_size\n            self.start_line = start_line\n            self.min_segment_size = min_segment_size\n\n            # Calculate number of segments\n            remaining_lines = total_lines - start_line + 1\n            self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n\n        def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n            \"\"\"\n            Iterate over line segments.\n\n            Yields:\n                LineSegment containing start (inclusive) and end (exclusive) indices\n            \"\"\"\n            current = self.start_line\n\n            for i in range(self.num_segments):\n                is_last_segment = i == self.num_segments - 1\n                segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n                # Handle minimum segment size for last segment\n                if (\n                    is_last_segment\n                    and self.min_segment_size is not None\n                    and segment_end - current &lt; self.min_segment_size\n                    and i &gt; 0\n                ):\n                    # Merge with previous segment by not yielding\n                    break\n\n                yield NumberedText.LineSegment(current, segment_end)\n                current = segment_end\n\n    def __init__(\n        self, content: Optional[str] = None, start: int = 1, separator: str = \":\"\n    ) -&gt; None:\n        \"\"\"\n        Initialize a numbered text document, \n        detecting and preserving existing numbering.\n\n        Valid numbered text must have:\n        - Sequential line numbers\n        - Consistent separator character(s)\n        - Every non-empty line must follow the numbering pattern\n\n        Args:\n            content: Initial text content, if any\n            start: Starting line number (used only if content isn't already numbered)\n            separator: Separator between line numbers and content (only if content isn't numbered)\n\n        Examples:\n            &gt;&gt;&gt; # Custom separators\n            &gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\\\n2\u2192Second line\")\n            &gt;&gt;&gt; doc.separator == \"\u2192\"\n            True\n\n            &gt;&gt;&gt; # Preserves starting number\n            &gt;&gt;&gt; doc = NumberedText(\"5#First\\\\n6#Second\")\n            &gt;&gt;&gt; doc.start == 5\n            True\n\n            &gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n            &gt;&gt;&gt; doc = NumberedText(\"1. First item\\\\n2. Second item\")\n            &gt;&gt;&gt; doc.numbered_lines\n            ['1: 1. First item', '2: 2. Second item']\n        \"\"\"\n\n        self.lines: List[str] = []  # Declare lines here\n        self.start: int = start  # Declare start with its type\n        self.separator: str = separator  # and separator\n\n        if not isinstance(content, str):\n            raise ValueError(\"NumberedText requires string input.\")\n\n        if start &lt; 1:  # enforce 1 based indexing.\n            raise IndexError(\n                \"NumberedText: Numbered lines must begin on \"\n                \"an integer great or equal to 1.\"\n            )\n\n        if not content:\n            return\n\n        # Analyze the text format\n        is_numbered, detected_sep, start_num = get_numbered_format(content)\n\n        format_info = get_numbered_format(content)\n\n        if format_info.is_numbered:\n            self.start = format_info.start_num  # type: ignore\n            self.separator = format_info.separator  # type: ignore\n\n            # Extract content by removing number and separator\n            pattern = re.compile(rf\"^\\d+{re.escape(detected_sep)}\") # type: ignore\n            self.lines = []\n\n            for line in content.splitlines():\n                if line.strip():\n                    self.lines.append(pattern.sub(\"\", line))\n                else:\n                    self.lines.append(line)\n        else:\n            self.lines = content.splitlines()\n            self.start = start\n            self.separator = separator\n\n    @classmethod\n    def from_file(cls, path: Path, **kwargs) -&gt; \"NumberedText\":\n        \"\"\"Create a NumberedText instance from a file.\"\"\"\n        return cls(read_str_from_file(Path(path)), **kwargs)\n\n    def _format_line(self, line_num: int, line: str) -&gt; str:\n        return f\"{line_num}{self.separator}{line}\"\n\n    def _to_internal_index(self, idx: int) -&gt; int:\n        \"\"\"return the index into the lines object in Python 0-based indexing.\"\"\"\n        if idx &gt; 0:\n            return idx - self.start\n        elif idx &lt; 0:  # allow negative indexing to index from end\n            if abs(idx) &gt; self.size:\n                raise IndexError(f\"NumberedText: negative index out of range: {idx}\")\n            return self.end + idx  # convert to logical positive location for reference.\n        else:\n            raise IndexError(\"NumberedText: Index cannot be zero in 1-based indexing.\")\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the numbered text representation.\"\"\"\n        return \"\\n\".join(\n            self._format_line(i, line) for i, line in enumerate(self.lines, self.start)\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of lines.\"\"\"\n        return len(self.lines)\n\n    def __iter__(self) -&gt; Iterator[tuple[int, str]]:\n        \"\"\"Iterate over (line_number, line_content) pairs.\"\"\"\n        return iter((i, line) for i, line in enumerate(self.lines, self.start))\n\n    def __getitem__(self, index: int) -&gt; str:\n        \"\"\"Get line content by line number (1-based indexing).\"\"\"\n        return self.lines[self._to_internal_index(index)]\n\n    def get_line(self, line_num: int) -&gt; str:\n        \"\"\"Get content of specified line number.\"\"\"\n        return self[line_num]\n\n    def _to_line_index(self, internal_index: int) -&gt; int:\n        return self.start + self._to_internal_index(internal_index)\n\n    def get_numbered_line(self, line_num: int) -&gt; str:\n        \"\"\"Get specified line with line number.\"\"\"\n        idx = self._to_line_index(line_num)\n        return self._format_line(idx, self[idx])\n\n    def get_lines(self, start: int, end: int) -&gt; List[str]:\n        \"\"\"Get content of line range, not inclusive of end line.\"\"\"\n        return self.lines[self._to_internal_index(start) : self._to_internal_index(end)]\n\n    def get_numbered_lines(self, start: int, end: int) -&gt; List[str]:\n        return [\n            self._format_line(i + self._to_internal_index(start) + 1, line)\n            for i, line in enumerate(self.get_lines(start, end))\n        ]\n    def get_segment(self, start: int, end: int) -&gt; str:\n        \"\"\"return the segment from start line (inclusive) up to end line (exclusive)\"\"\"\n        if start &lt; self.start:\n            raise IndexError(f\"Start index {start} is before first line {self.start}\")\n        if end &gt; len(self) + 1:\n            raise IndexError(f\"End index {end} is past last line {len(self)}\")\n        if start &gt;= end:\n            raise IndexError(f\"Start index {start} must be less than end index {end}\")\n        return \"\\n\".join(self.get_lines(start, end))\n\n    def iter_segments(\n        self, segment_size: int, min_segment_size: Optional[int] = None\n    ) -&gt; Iterator[LineSegment]:\n        \"\"\"\n        Iterate over segments of the text with specified size.\n\n        Args:\n            segment_size: Number of lines per segment\n            min_segment_size: Optional minimum size for final segment.\n                If specified, last segment will be merged with previous one\n                if it would be smaller than this size.\n\n        Yields:\n            LineSegment objects containing start and end line numbers\n\n        Example:\n            &gt;&gt;&gt; text = NumberedText(\"line1\\\\nline2\\\\nline3\\\\nline4\\\\nline5\")\n            &gt;&gt;&gt; for segment in text.iter_segments(2):\n            ...     print(f\"Lines {segment.start}-{segment.end}\")\n            Lines 1-3\n            Lines 3-5\n            Lines 5-6\n        \"\"\"\n        iterator = self.SegmentIterator(\n            len(self), segment_size, self.start, min_segment_size\n        )\n        return iter(iterator)\n\n    def get_numbered_segment(self, start: int, end: int) -&gt; str:\n        return \"\\n\".join(self.get_numbered_lines(start, end))\n\n    def save(self, path: Path, numbered: bool = True) -&gt; None:\n        \"\"\"\n        Save document to file.\n\n        Args:\n            path: Output file path\n            numbered: Whether to save with line numbers (default: True)\n        \"\"\"\n        content = str(self) if numbered else \"\\n\".join(self.lines)\n        write_str_to_file(path, content)\n\n    def append(self, text: str) -&gt; None:\n        \"\"\"Append text, splitting into lines if needed.\"\"\"\n        self.lines.extend(text.splitlines())\n\n    def insert(self, line_num: int, text: str) -&gt; None:\n        \"\"\"Insert text at specified line number. Assumes text is not empty.\"\"\"\n        new_lines = text.splitlines()\n        internal_idx = self._to_internal_index(line_num)\n        self.lines[internal_idx:internal_idx] = new_lines\n\n    def reset_numbering(self):\n        self.start = 1\n\n    def remove_whitespace(self) -&gt; None:\n        \"\"\"Remove leading and trailing whitespace from all lines.\"\"\"\n        self.lines = [line.strip() for line in self.lines]\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Get original text without line numbers.\"\"\"\n        return \"\\n\".join(self.lines)\n\n    @property\n    def numbered_content(self) -&gt; str:\n        \"\"\"Get text with line numbers as a string. Equivalent to str(self)\"\"\"\n        return str(self)\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Get the number of lines.\"\"\"\n        return len(self.lines)\n\n    @property\n    def numbered_lines(self) -&gt; List[str]:\n        \"\"\"\n        Get list of lines with line numbers included.\n\n        Returns:\n            List[str]: Lines with numbers and separator prefixed\n\n        Examples:\n            &gt;&gt;&gt; doc = NumberedText(\"First line\\\\nSecond line\")\n            &gt;&gt;&gt; doc.numbered_lines\n            ['1: First line', '2: Second line']\n\n        Note:\n            - Unlike str(self), this returns a list rather than joined string\n            - Maintains consistent formatting with separator\n            - Useful for processing or displaying individual numbered lines\n        \"\"\"\n        return [\n            f\"{i}{self.separator}{line}\"\n            for i, line in enumerate(self.lines, self.start)\n        ]\n\n    @property\n    def end(self) -&gt; int:\n        return self.start + len(self.lines) - 1\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.content","title":"<code>content</code>  <code>property</code>","text":"<p>Get original text without line numbers.</p>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.end","title":"<code>end</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.lines","title":"<code>lines = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.numbered_content","title":"<code>numbered_content</code>  <code>property</code>","text":"<p>Get text with line numbers as a string. Equivalent to str(self)</p>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.numbered_lines","title":"<code>numbered_lines</code>  <code>property</code>","text":"<p>Get list of lines with line numbers included.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Lines with numbers and separator prefixed</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; doc = NumberedText(\"First line\\nSecond line\")\n&gt;&gt;&gt; doc.numbered_lines\n['1: First line', '2: Second line']\n</code></pre> Note <ul> <li>Unlike str(self), this returns a list rather than joined string</li> <li>Maintains consistent formatting with separator</li> <li>Useful for processing or displaying individual numbered lines</li> </ul>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.separator","title":"<code>separator = separator</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.size","title":"<code>size</code>  <code>property</code>","text":"<p>Get the number of lines.</p>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.start","title":"<code>start = start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.LineSegment","title":"<code>LineSegment</code>  <code>dataclass</code>","text":"<p>Represents a segment of lines with start and end indices in 1-based indexing.</p> <p>The segment follows Python range conventions where start is inclusive and end is exclusive. However, indexing is 1-based to match NumberedText.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>int</code> <p>Starting line number (inclusive, 1-based)</p> <code>end</code> <code>int</code> <p>Ending line number (exclusive, 1-based)</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>@dataclass\nclass LineSegment:\n    \"\"\"\n    Represents a segment of lines with start and end indices in 1-based indexing.\n\n    The segment follows Python range conventions where start is inclusive and\n    end is exclusive. However, indexing is 1-based to match NumberedText.\n\n    Attributes:\n        start: Starting line number (inclusive, 1-based)\n        end: Ending line number (exclusive, 1-based)\n    \"\"\"\n\n    start: int\n    end: int\n\n    def __iter__(self):\n        \"\"\"Allow unpacking into start, end pairs.\"\"\"\n        yield self.start\n        yield self.end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.LineSegment.end","title":"<code>end</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.LineSegment.start","title":"<code>start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.LineSegment.__init__","title":"<code>__init__(start, end)</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.LineSegment.__iter__","title":"<code>__iter__()</code>","text":"<p>Allow unpacking into start, end pairs.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self):\n    \"\"\"Allow unpacking into start, end pairs.\"\"\"\n    yield self.start\n    yield self.end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator","title":"<code>SegmentIterator</code>","text":"<p>Iterator for generating line segments of specified size.</p> <p>Produces segments of lines with start/end indices following 1-based indexing. The final segment may be smaller than the specified segment size.</p> <p>Attributes:</p> Name Type Description <code>total_lines</code> <p>Total number of lines in text</p> <code>segment_size</code> <p>Number of lines per segment</p> <code>start_line</code> <p>Starting line number (1-based)</p> <code>min_segment_size</code> <p>Minimum size for the final segment</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>class SegmentIterator:\n    \"\"\"\n    Iterator for generating line segments of specified size.\n\n    Produces segments of lines with start/end indices following 1-based indexing.\n    The final segment may be smaller than the specified segment size.\n\n    Attributes:\n        total_lines: Total number of lines in text\n        segment_size: Number of lines per segment\n        start_line: Starting line number (1-based)\n        min_segment_size: Minimum size for the final segment\n    \"\"\"\n\n    def __init__(\n        self,\n        total_lines: int,\n        segment_size: int,\n        start_line: int = 1,\n        min_segment_size: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize the segment iterator.\n\n        Args:\n            total_lines: Total number of lines to iterate over\n            segment_size: Desired size of each segment\n            start_line: First line number (default: 1)\n            min_segment_size: Minimum size for final segment (default: None)\n                If specified, the last segment will be merged with the previous one\n                if it would be smaller than this size.\n\n        Raises:\n            ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n            ValueError: If start_line &lt; 1 (must use 1-based indexing)\n            ValueError: If min_segment_size &gt;= segment_size\n        \"\"\"\n        if segment_size &lt; 1:\n            raise ValueError(\"Segment size must be at least 1\")\n        if total_lines &lt; 1:\n            raise ValueError(\"Total lines must be at least 1\")\n        if start_line &lt; 1:\n            raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n        if min_segment_size is not None and min_segment_size &gt;= segment_size:\n            raise ValueError(\"Minimum segment size must be less than segment size\")\n\n        self.total_lines = total_lines\n        self.segment_size = segment_size\n        self.start_line = start_line\n        self.min_segment_size = min_segment_size\n\n        # Calculate number of segments\n        remaining_lines = total_lines - start_line + 1\n        self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n\n    def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n        \"\"\"\n        Iterate over line segments.\n\n        Yields:\n            LineSegment containing start (inclusive) and end (exclusive) indices\n        \"\"\"\n        current = self.start_line\n\n        for i in range(self.num_segments):\n            is_last_segment = i == self.num_segments - 1\n            segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n            # Handle minimum segment size for last segment\n            if (\n                is_last_segment\n                and self.min_segment_size is not None\n                and segment_end - current &lt; self.min_segment_size\n                and i &gt; 0\n            ):\n                # Merge with previous segment by not yielding\n                break\n\n            yield NumberedText.LineSegment(current, segment_end)\n            current = segment_end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.min_segment_size","title":"<code>min_segment_size = min_segment_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.num_segments","title":"<code>num_segments = (remaining_lines + segment_size - 1) // segment_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.segment_size","title":"<code>segment_size = segment_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.start_line","title":"<code>start_line = start_line</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.total_lines","title":"<code>total_lines = total_lines</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.__init__","title":"<code>__init__(total_lines, segment_size, start_line=1, min_segment_size=None)</code>","text":"<p>Initialize the segment iterator.</p> <p>Parameters:</p> Name Type Description Default <code>total_lines</code> <code>int</code> <p>Total number of lines to iterate over</p> required <code>segment_size</code> <code>int</code> <p>Desired size of each segment</p> required <code>start_line</code> <code>int</code> <p>First line number (default: 1)</p> <code>1</code> <code>min_segment_size</code> <code>Optional[int]</code> <p>Minimum size for final segment (default: None) If specified, the last segment will be merged with the previous one if it would be smaller than this size.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If segment_size &lt; 1 or total_lines &lt; 1</p> <code>ValueError</code> <p>If start_line &lt; 1 (must use 1-based indexing)</p> <code>ValueError</code> <p>If min_segment_size &gt;= segment_size</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __init__(\n    self,\n    total_lines: int,\n    segment_size: int,\n    start_line: int = 1,\n    min_segment_size: Optional[int] = None,\n):\n    \"\"\"\n    Initialize the segment iterator.\n\n    Args:\n        total_lines: Total number of lines to iterate over\n        segment_size: Desired size of each segment\n        start_line: First line number (default: 1)\n        min_segment_size: Minimum size for final segment (default: None)\n            If specified, the last segment will be merged with the previous one\n            if it would be smaller than this size.\n\n    Raises:\n        ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n        ValueError: If start_line &lt; 1 (must use 1-based indexing)\n        ValueError: If min_segment_size &gt;= segment_size\n    \"\"\"\n    if segment_size &lt; 1:\n        raise ValueError(\"Segment size must be at least 1\")\n    if total_lines &lt; 1:\n        raise ValueError(\"Total lines must be at least 1\")\n    if start_line &lt; 1:\n        raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n    if min_segment_size is not None and min_segment_size &gt;= segment_size:\n        raise ValueError(\"Minimum segment size must be less than segment size\")\n\n    self.total_lines = total_lines\n    self.segment_size = segment_size\n    self.start_line = start_line\n    self.min_segment_size = min_segment_size\n\n    # Calculate number of segments\n    remaining_lines = total_lines - start_line + 1\n    self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.SegmentIterator.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over line segments.</p> <p>Yields:</p> Type Description <code>LineSegment</code> <p>LineSegment containing start (inclusive) and end (exclusive) indices</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n    \"\"\"\n    Iterate over line segments.\n\n    Yields:\n        LineSegment containing start (inclusive) and end (exclusive) indices\n    \"\"\"\n    current = self.start_line\n\n    for i in range(self.num_segments):\n        is_last_segment = i == self.num_segments - 1\n        segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n        # Handle minimum segment size for last segment\n        if (\n            is_last_segment\n            and self.min_segment_size is not None\n            and segment_end - current &lt; self.min_segment_size\n            and i &gt; 0\n        ):\n            # Merge with previous segment by not yielding\n            break\n\n        yield NumberedText.LineSegment(current, segment_end)\n        current = segment_end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get line content by line number (1-based indexing).</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get line content by line number (1-based indexing).\"\"\"\n    return self.lines[self._to_internal_index(index)]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.__init__","title":"<code>__init__(content=None, start=1, separator=':')</code>","text":"<p>Initialize a numbered text document,  detecting and preserving existing numbering.</p> <p>Valid numbered text must have: - Sequential line numbers - Consistent separator character(s) - Every non-empty line must follow the numbering pattern</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Optional[str]</code> <p>Initial text content, if any</p> <code>None</code> <code>start</code> <code>int</code> <p>Starting line number (used only if content isn't already numbered)</p> <code>1</code> <code>separator</code> <code>str</code> <p>Separator between line numbers and content (only if content isn't numbered)</p> <code>':'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Custom separators\n&gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\n2\u2192Second line\")\n&gt;&gt;&gt; doc.separator == \"\u2192\"\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Preserves starting number\n&gt;&gt;&gt; doc = NumberedText(\"5#First\\n6#Second\")\n&gt;&gt;&gt; doc.start == 5\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n&gt;&gt;&gt; doc = NumberedText(\"1. First item\\n2. Second item\")\n&gt;&gt;&gt; doc.numbered_lines\n['1: 1. First item', '2: 2. Second item']\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __init__(\n    self, content: Optional[str] = None, start: int = 1, separator: str = \":\"\n) -&gt; None:\n    \"\"\"\n    Initialize a numbered text document, \n    detecting and preserving existing numbering.\n\n    Valid numbered text must have:\n    - Sequential line numbers\n    - Consistent separator character(s)\n    - Every non-empty line must follow the numbering pattern\n\n    Args:\n        content: Initial text content, if any\n        start: Starting line number (used only if content isn't already numbered)\n        separator: Separator between line numbers and content (only if content isn't numbered)\n\n    Examples:\n        &gt;&gt;&gt; # Custom separators\n        &gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\\\n2\u2192Second line\")\n        &gt;&gt;&gt; doc.separator == \"\u2192\"\n        True\n\n        &gt;&gt;&gt; # Preserves starting number\n        &gt;&gt;&gt; doc = NumberedText(\"5#First\\\\n6#Second\")\n        &gt;&gt;&gt; doc.start == 5\n        True\n\n        &gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n        &gt;&gt;&gt; doc = NumberedText(\"1. First item\\\\n2. Second item\")\n        &gt;&gt;&gt; doc.numbered_lines\n        ['1: 1. First item', '2: 2. Second item']\n    \"\"\"\n\n    self.lines: List[str] = []  # Declare lines here\n    self.start: int = start  # Declare start with its type\n    self.separator: str = separator  # and separator\n\n    if not isinstance(content, str):\n        raise ValueError(\"NumberedText requires string input.\")\n\n    if start &lt; 1:  # enforce 1 based indexing.\n        raise IndexError(\n            \"NumberedText: Numbered lines must begin on \"\n            \"an integer great or equal to 1.\"\n        )\n\n    if not content:\n        return\n\n    # Analyze the text format\n    is_numbered, detected_sep, start_num = get_numbered_format(content)\n\n    format_info = get_numbered_format(content)\n\n    if format_info.is_numbered:\n        self.start = format_info.start_num  # type: ignore\n        self.separator = format_info.separator  # type: ignore\n\n        # Extract content by removing number and separator\n        pattern = re.compile(rf\"^\\d+{re.escape(detected_sep)}\") # type: ignore\n        self.lines = []\n\n        for line in content.splitlines():\n            if line.strip():\n                self.lines.append(pattern.sub(\"\", line))\n            else:\n                self.lines.append(line)\n    else:\n        self.lines = content.splitlines()\n        self.start = start\n        self.separator = separator\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over (line_number, line_content) pairs.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self) -&gt; Iterator[tuple[int, str]]:\n    \"\"\"Iterate over (line_number, line_content) pairs.\"\"\"\n    return iter((i, line) for i, line in enumerate(self.lines, self.start))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of lines.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of lines.\"\"\"\n    return len(self.lines)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.__str__","title":"<code>__str__()</code>","text":"<p>Return the numbered text representation.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the numbered text representation.\"\"\"\n    return \"\\n\".join(\n        self._format_line(i, line) for i, line in enumerate(self.lines, self.start)\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.append","title":"<code>append(text)</code>","text":"<p>Append text, splitting into lines if needed.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def append(self, text: str) -&gt; None:\n    \"\"\"Append text, splitting into lines if needed.\"\"\"\n    self.lines.extend(text.splitlines())\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.from_file","title":"<code>from_file(path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a NumberedText instance from a file.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>@classmethod\ndef from_file(cls, path: Path, **kwargs) -&gt; \"NumberedText\":\n    \"\"\"Create a NumberedText instance from a file.\"\"\"\n    return cls(read_str_from_file(Path(path)), **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_line","title":"<code>get_line(line_num)</code>","text":"<p>Get content of specified line number.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_line(self, line_num: int) -&gt; str:\n    \"\"\"Get content of specified line number.\"\"\"\n    return self[line_num]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_lines","title":"<code>get_lines(start, end)</code>","text":"<p>Get content of line range, not inclusive of end line.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_lines(self, start: int, end: int) -&gt; List[str]:\n    \"\"\"Get content of line range, not inclusive of end line.\"\"\"\n    return self.lines[self._to_internal_index(start) : self._to_internal_index(end)]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_numbered_line","title":"<code>get_numbered_line(line_num)</code>","text":"<p>Get specified line with line number.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_line(self, line_num: int) -&gt; str:\n    \"\"\"Get specified line with line number.\"\"\"\n    idx = self._to_line_index(line_num)\n    return self._format_line(idx, self[idx])\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_numbered_lines","title":"<code>get_numbered_lines(start, end)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_lines(self, start: int, end: int) -&gt; List[str]:\n    return [\n        self._format_line(i + self._to_internal_index(start) + 1, line)\n        for i, line in enumerate(self.get_lines(start, end))\n    ]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_numbered_segment","title":"<code>get_numbered_segment(start, end)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_segment(self, start: int, end: int) -&gt; str:\n    return \"\\n\".join(self.get_numbered_lines(start, end))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.get_segment","title":"<code>get_segment(start, end)</code>","text":"<p>return the segment from start line (inclusive) up to end line (exclusive)</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_segment(self, start: int, end: int) -&gt; str:\n    \"\"\"return the segment from start line (inclusive) up to end line (exclusive)\"\"\"\n    if start &lt; self.start:\n        raise IndexError(f\"Start index {start} is before first line {self.start}\")\n    if end &gt; len(self) + 1:\n        raise IndexError(f\"End index {end} is past last line {len(self)}\")\n    if start &gt;= end:\n        raise IndexError(f\"Start index {start} must be less than end index {end}\")\n    return \"\\n\".join(self.get_lines(start, end))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.insert","title":"<code>insert(line_num, text)</code>","text":"<p>Insert text at specified line number. Assumes text is not empty.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def insert(self, line_num: int, text: str) -&gt; None:\n    \"\"\"Insert text at specified line number. Assumes text is not empty.\"\"\"\n    new_lines = text.splitlines()\n    internal_idx = self._to_internal_index(line_num)\n    self.lines[internal_idx:internal_idx] = new_lines\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.iter_segments","title":"<code>iter_segments(segment_size, min_segment_size=None)</code>","text":"<p>Iterate over segments of the text with specified size.</p> <p>Parameters:</p> Name Type Description Default <code>segment_size</code> <code>int</code> <p>Number of lines per segment</p> required <code>min_segment_size</code> <code>Optional[int]</code> <p>Optional minimum size for final segment. If specified, last segment will be merged with previous one if it would be smaller than this size.</p> <code>None</code> <p>Yields:</p> Type Description <code>LineSegment</code> <p>LineSegment objects containing start and end line numbers</p> Example <p>text = NumberedText(\"line1\\nline2\\nline3\\nline4\\nline5\") for segment in text.iter_segments(2): ...     print(f\"Lines {segment.start}-{segment.end}\") Lines 1-3 Lines 3-5 Lines 5-6</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def iter_segments(\n    self, segment_size: int, min_segment_size: Optional[int] = None\n) -&gt; Iterator[LineSegment]:\n    \"\"\"\n    Iterate over segments of the text with specified size.\n\n    Args:\n        segment_size: Number of lines per segment\n        min_segment_size: Optional minimum size for final segment.\n            If specified, last segment will be merged with previous one\n            if it would be smaller than this size.\n\n    Yields:\n        LineSegment objects containing start and end line numbers\n\n    Example:\n        &gt;&gt;&gt; text = NumberedText(\"line1\\\\nline2\\\\nline3\\\\nline4\\\\nline5\")\n        &gt;&gt;&gt; for segment in text.iter_segments(2):\n        ...     print(f\"Lines {segment.start}-{segment.end}\")\n        Lines 1-3\n        Lines 3-5\n        Lines 5-6\n    \"\"\"\n    iterator = self.SegmentIterator(\n        len(self), segment_size, self.start, min_segment_size\n    )\n    return iter(iterator)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.remove_whitespace","title":"<code>remove_whitespace()</code>","text":"<p>Remove leading and trailing whitespace from all lines.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def remove_whitespace(self) -&gt; None:\n    \"\"\"Remove leading and trailing whitespace from all lines.\"\"\"\n    self.lines = [line.strip() for line in self.lines]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.reset_numbering","title":"<code>reset_numbering()</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def reset_numbering(self):\n    self.start = 1\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.NumberedText.save","title":"<code>save(path, numbered=True)</code>","text":"<p>Save document to file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output file path</p> required <code>numbered</code> <code>bool</code> <p>Whether to save with line numbers (default: True)</p> <code>True</code> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def save(self, path: Path, numbered: bool = True) -&gt; None:\n    \"\"\"\n    Save document to file.\n\n    Args:\n        path: Output file path\n        numbered: Whether to save with line numbers (default: True)\n    \"\"\"\n    content = str(self) if numbered else \"\\n\".join(self.lines)\n    write_str_to_file(path, content)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket_lines","title":"<code>bracket_lines(text, number=False)</code>","text":"<pre><code>Encloses each line of the input text with angle brackets.\nIf number is True, adds a line number followed by a colon `:` and then the line.\n\nArgs:\n    text (str): The input string containing lines separated by '\n</code></pre> <p>'.         number (bool): Whether to prepend line numbers to each line.</p> <pre><code>Returns:\n    str: A string where each line is enclosed in angle brackets.\n\nExamples:\n    &gt;&gt;&gt; bracket_lines(\"This is a string with\n</code></pre> <p>two lines.\")         ' &lt;   two lines.&gt;' <pre><code>    &gt;&gt;&gt; bracket_lines(\"This is a string with\n</code></pre> <p>two lines.\", number=True)         '&lt;1:This is a string with&gt; &lt;2:   two lines.&gt;'</p> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def bracket_lines(text: str, number: bool = False) -&gt; str:\n    \"\"\"\n    Encloses each line of the input text with angle brackets.\n    If number is True, adds a line number followed by a colon `:` and then the line.\n\n    Args:\n        text (str): The input string containing lines separated by '\\n'.\n        number (bool): Whether to prepend line numbers to each line.\n\n    Returns:\n        str: A string where each line is enclosed in angle brackets.\n\n    Examples:\n        &gt;&gt;&gt; bracket_lines(\"This is a string with\\n   two lines.\")\n        '&lt;This is a string with&gt;\\n&lt;   two lines.&gt;'\n\n        &gt;&gt;&gt; bracket_lines(\"This is a string with\\n   two lines.\", number=True)\n        '&lt;1:This is a string with&gt;\\n&lt;2:   two lines.&gt;'\n    \"\"\"\n    return \"\\n\".join(\n        f\"&lt;{f'{i+1}:{line}' if number else line}&gt;\"\n        for i, line in enumerate(text.split(\"\\n\"))\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.clean_text","title":"<code>clean_text(text, newline=False)</code>","text":"<p>Cleans a given text by replacing specific unwanted characters such as tab, and non-breaking spaces with regular spaces.</p> <p>This function takes a string as input and applies replacements based on a predefined mapping of characters to replace.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be cleaned.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text with unwanted characters replaced by spaces.</p> Example <p>text = \"This is\\n an example\\ttext with\\xa0extra spaces.\" clean_text(text) 'This is an example text with extra spaces.'</p> Source code in <code>src/tnh_scholar/text_processing/text_processing.py</code> <pre><code>def clean_text(text: str, newline: bool = False) -&gt; str:\n    \"\"\"\n    Cleans a given text by replacing specific unwanted characters such as\n    tab, and non-breaking spaces with regular spaces.\n\n    This function takes a string as input and applies replacements\n    based on a predefined mapping of characters to replace.\n\n    Args:\n        text (str): The text to be cleaned.\n\n    Returns:\n        str: The cleaned text with unwanted characters replaced by spaces.\n\n    Example:\n        &gt;&gt;&gt; text = \"This is\\\\n an example\\\\ttext with\\\\xa0extra spaces.\"\n        &gt;&gt;&gt; clean_text(text)\n        'This is an example text with extra spaces.'\n\n    \"\"\"\n    # Define a mapping of characters to replace\n    replace_map = {\n        \"\\t\": \" \",  # Replace tabs with space\n        \"\\xa0\": \" \",  # Replace non-breaking space with regular space\n        # Add more replacements as needed\n    }\n\n    if newline:\n        replace_map[\"\\n\"] = \"\"  # remove newlines\n\n    # Loop through the replace map and replace each character\n    for old_char, new_char in replace_map.items():\n        text = text.replace(old_char, new_char)\n\n    return text.strip()  # Ensure any leading/trailing spaces are removed\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.lines_from_bracketed_text","title":"<code>lines_from_bracketed_text(text, start, end, keep_brackets=False)</code>","text":"<pre><code>Extracts lines from bracketed text between the start and end indices, inclusive.\nHandles both numbered and non-numbered cases.\n\nArgs:\n    text (str): The input bracketed text containing lines like &lt;...&gt;.\n    start (int): The starting line number (1-based).\n    end (int): The ending line number (1-based).\n\nReturns:\n    list[str]: The lines from start to end inclusive, with angle brackets removed.\n\nRaises:\n    FormattingError: If the text contains improperly formatted lines (missing angle brackets).\n    ValueError: If start or end indices are invalid or out of bounds.\n\nExamples:\n    &gt;&gt;&gt; text = \"&lt;1:Line 1&gt;\n</code></pre> <p>&lt;2:Line 2&gt; &lt;3:Line 3&gt;\"         &gt;&gt;&gt; lines_from_bracketed_text(text, 1, 2)         ['Line 1', 'Line 2']</p> <pre><code>    &gt;&gt;&gt; text = \"&lt;Line 1&gt;\n</code></pre> <p> \"         &gt;&gt;&gt; lines_from_bracketed_text(text, 2, 3)         ['Line 2', 'Line 3'] Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def lines_from_bracketed_text(\n    text: str, start: int, end: int, keep_brackets=False\n) -&gt; list[str]:\n    \"\"\"\n    Extracts lines from bracketed text between the start and end indices, inclusive.\n    Handles both numbered and non-numbered cases.\n\n    Args:\n        text (str): The input bracketed text containing lines like &lt;...&gt;.\n        start (int): The starting line number (1-based).\n        end (int): The ending line number (1-based).\n\n    Returns:\n        list[str]: The lines from start to end inclusive, with angle brackets removed.\n\n    Raises:\n        FormattingError: If the text contains improperly formatted lines (missing angle brackets).\n        ValueError: If start or end indices are invalid or out of bounds.\n\n    Examples:\n        &gt;&gt;&gt; text = \"&lt;1:Line 1&gt;\\n&lt;2:Line 2&gt;\\n&lt;3:Line 3&gt;\"\n        &gt;&gt;&gt; lines_from_bracketed_text(text, 1, 2)\n        ['Line 1', 'Line 2']\n\n        &gt;&gt;&gt; text = \"&lt;Line 1&gt;\\n&lt;Line 2&gt;\\n&lt;Line 3&gt;\"\n        &gt;&gt;&gt; lines_from_bracketed_text(text, 2, 3)\n        ['Line 2', 'Line 3']\n    \"\"\"\n    # Split the text into lines\n    lines = text.splitlines()\n\n    # Validate indices\n    if start &lt; 1 or end &lt; 1 or start &gt; end or end &gt; len(lines):\n        raise ValueError(\n            \"Invalid start or end indices for the given text: start:{start}, end: {end}\"\n        )\n\n    # Extract lines and validate formatting\n    result = []\n    for i, line in enumerate(lines, start=1):\n        if start &lt;= i &lt;= end:\n            # Check for proper bracketing and extract the content\n            match = re.match(r\"&lt;(\\d+:)?(.*?)&gt;\", line)\n            if not match:\n                raise FormattingError(f\"Invalid format for line {i}: '{line}'\")\n            # Add the extracted content (group 2) to the result\n            if keep_brackets:\n                result.append(line)\n            else:\n                result.append(match[2].strip())\n\n    return \"\\n\".join(result)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.normalize_newlines","title":"<code>normalize_newlines(text, spacing=2)</code>","text":"<pre><code>Normalize newline blocks in the input text by reducing consecutive newlines\nto the specified number of newlines for consistent readability and formatting.\n\nParameters:\n----------\ntext : str\n    The input text containing inconsistent newline spacing.\nspacing : int, optional\n    The number of newlines to insert between lines. Defaults to 2.\n\nReturns:\n-------\nstr\n    The text with consecutive newlines reduced to the specified number of newlines.\n\nExample:\n--------\n&gt;&gt;&gt; raw_text = \"Heading\n</code></pre> <p>Paragraph text 1 Paragraph text 2</p> <p>\"     &gt;&gt;&gt; normalize_newlines(raw_text, spacing=2)     'Heading</p> <p>Paragraph text 1</p> <p>Paragraph text 2</p> <p>'</p> Source code in <code>src/tnh_scholar/text_processing/text_processing.py</code> <pre><code>def normalize_newlines(text: str, spacing: int = 2) -&gt; str:\n    \"\"\"\n    Normalize newline blocks in the input text by reducing consecutive newlines\n    to the specified number of newlines for consistent readability and formatting.\n\n    Parameters:\n    ----------\n    text : str\n        The input text containing inconsistent newline spacing.\n    spacing : int, optional\n        The number of newlines to insert between lines. Defaults to 2.\n\n    Returns:\n    -------\n    str\n        The text with consecutive newlines reduced to the specified number of newlines.\n\n    Example:\n    --------\n    &gt;&gt;&gt; raw_text = \"Heading\\n\\n\\nParagraph text 1\\nParagraph text 2\\n\\n\\n\"\n    &gt;&gt;&gt; normalize_newlines(raw_text, spacing=2)\n    'Heading\\n\\nParagraph text 1\\n\\nParagraph text 2\\n\\n'\n    \"\"\"\n    # Replace one or more newlines with the desired number of newlines\n    newlines = \"\\n\" * spacing\n    return re.sub(r\"\\n{1,}\", newlines, text)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.unbracket_lines","title":"<code>unbracket_lines(text, number=False)</code>","text":"<pre><code>Removes angle brackets (&lt; &gt;) from encapsulated lines and optionally removes line numbers.\n\nArgs:\n    text (str): The input string with encapsulated lines.\n    number (bool): If True, removes line numbers in the format 'digit:'.\n                   Raises a ValueError if `number=True` and a line does not start with a digit followed by a colon.\n\nReturns:\n    str: A newline-separated string with the encapsulation removed, and line numbers stripped if specified.\n\nExamples:\n    &gt;&gt;&gt; unbracket_lines(\"&lt;1:Line 1&gt;\n</code></pre> <p>&lt;2:Line 2&gt;\", number=True)         'Line 1 Line 2'</p> <pre><code>    &gt;&gt;&gt; unbracket_lines(\"&lt;Line 1&gt;\n</code></pre> <p>\")         'Line 1 Line 2' <pre><code>    &gt;&gt;&gt; unbracket_lines(\"&lt;1Line 1&gt;\", number=True)\n    ValueError: Line does not start with a valid number: '1Line 1'\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def unbracket_lines(text: str, number: bool = False) -&gt; str:\n    \"\"\"\n    Removes angle brackets (&lt; &gt;) from encapsulated lines and optionally removes line numbers.\n\n    Args:\n        text (str): The input string with encapsulated lines.\n        number (bool): If True, removes line numbers in the format 'digit:'.\n                       Raises a ValueError if `number=True` and a line does not start with a digit followed by a colon.\n\n    Returns:\n        str: A newline-separated string with the encapsulation removed, and line numbers stripped if specified.\n\n    Examples:\n        &gt;&gt;&gt; unbracket_lines(\"&lt;1:Line 1&gt;\\n&lt;2:Line 2&gt;\", number=True)\n        'Line 1\\nLine 2'\n\n        &gt;&gt;&gt; unbracket_lines(\"&lt;Line 1&gt;\\n&lt;Line 2&gt;\")\n        'Line 1\\nLine 2'\n\n        &gt;&gt;&gt; unbracket_lines(\"&lt;1Line 1&gt;\", number=True)\n        ValueError: Line does not start with a valid number: '1Line 1'\n    \"\"\"\n    unbracketed_lines = []\n\n    for line in text.splitlines():\n        match = (\n            re.match(r\"&lt;(\\d+):(.*?)&gt;\", line) if number else re.match(r\"&lt;(.*?)&gt;\", line)\n        )\n        if match:\n            content = match[2].strip() if number else match[1].strip()\n            unbracketed_lines.append(content)\n        elif number:\n            raise FormattingError(f\"Line does not start with a valid number: '{line}'\")\n        else:\n            raise FormattingError(f\"Line does not follow the expected format: '{line}'\")\n\n    return \"\\n\".join(unbracketed_lines)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket","title":"<code>bracket</code>","text":""},{"location":"api/#tnh_scholar.text_processing.bracket.FormattingError","title":"<code>FormattingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception raised for formatting-related errors.</p> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>class FormattingError(Exception):\n    \"\"\"\n    Custom exception raised for formatting-related errors.\n    \"\"\"\n\n    def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n        super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.FormattingError.__init__","title":"<code>__init__(message='An error occurred due to invalid formatting.')</code>","text":"Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n    super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.bracket_all_lines","title":"<code>bracket_all_lines(pages)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def bracket_all_lines(pages):\n    return [bracket_lines(page) for page in pages]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.bracket_lines","title":"<code>bracket_lines(text, number=False)</code>","text":"<pre><code>Encloses each line of the input text with angle brackets.\nIf number is True, adds a line number followed by a colon `:` and then the line.\n\nArgs:\n    text (str): The input string containing lines separated by '\n</code></pre> <p>'.         number (bool): Whether to prepend line numbers to each line.</p> <pre><code>Returns:\n    str: A string where each line is enclosed in angle brackets.\n\nExamples:\n    &gt;&gt;&gt; bracket_lines(\"This is a string with\n</code></pre> <p>two lines.\")         ' &lt;   two lines.&gt;' <pre><code>    &gt;&gt;&gt; bracket_lines(\"This is a string with\n</code></pre> <p>two lines.\", number=True)         '&lt;1:This is a string with&gt; &lt;2:   two lines.&gt;'</p> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def bracket_lines(text: str, number: bool = False) -&gt; str:\n    \"\"\"\n    Encloses each line of the input text with angle brackets.\n    If number is True, adds a line number followed by a colon `:` and then the line.\n\n    Args:\n        text (str): The input string containing lines separated by '\\n'.\n        number (bool): Whether to prepend line numbers to each line.\n\n    Returns:\n        str: A string where each line is enclosed in angle brackets.\n\n    Examples:\n        &gt;&gt;&gt; bracket_lines(\"This is a string with\\n   two lines.\")\n        '&lt;This is a string with&gt;\\n&lt;   two lines.&gt;'\n\n        &gt;&gt;&gt; bracket_lines(\"This is a string with\\n   two lines.\", number=True)\n        '&lt;1:This is a string with&gt;\\n&lt;2:   two lines.&gt;'\n    \"\"\"\n    return \"\\n\".join(\n        f\"&lt;{f'{i+1}:{line}' if number else line}&gt;\"\n        for i, line in enumerate(text.split(\"\\n\"))\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.lines_from_bracketed_text","title":"<code>lines_from_bracketed_text(text, start, end, keep_brackets=False)</code>","text":"<pre><code>Extracts lines from bracketed text between the start and end indices, inclusive.\nHandles both numbered and non-numbered cases.\n\nArgs:\n    text (str): The input bracketed text containing lines like &lt;...&gt;.\n    start (int): The starting line number (1-based).\n    end (int): The ending line number (1-based).\n\nReturns:\n    list[str]: The lines from start to end inclusive, with angle brackets removed.\n\nRaises:\n    FormattingError: If the text contains improperly formatted lines (missing angle brackets).\n    ValueError: If start or end indices are invalid or out of bounds.\n\nExamples:\n    &gt;&gt;&gt; text = \"&lt;1:Line 1&gt;\n</code></pre> <p>&lt;2:Line 2&gt; &lt;3:Line 3&gt;\"         &gt;&gt;&gt; lines_from_bracketed_text(text, 1, 2)         ['Line 1', 'Line 2']</p> <pre><code>    &gt;&gt;&gt; text = \"&lt;Line 1&gt;\n</code></pre> <p> \"         &gt;&gt;&gt; lines_from_bracketed_text(text, 2, 3)         ['Line 2', 'Line 3'] Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def lines_from_bracketed_text(\n    text: str, start: int, end: int, keep_brackets=False\n) -&gt; list[str]:\n    \"\"\"\n    Extracts lines from bracketed text between the start and end indices, inclusive.\n    Handles both numbered and non-numbered cases.\n\n    Args:\n        text (str): The input bracketed text containing lines like &lt;...&gt;.\n        start (int): The starting line number (1-based).\n        end (int): The ending line number (1-based).\n\n    Returns:\n        list[str]: The lines from start to end inclusive, with angle brackets removed.\n\n    Raises:\n        FormattingError: If the text contains improperly formatted lines (missing angle brackets).\n        ValueError: If start or end indices are invalid or out of bounds.\n\n    Examples:\n        &gt;&gt;&gt; text = \"&lt;1:Line 1&gt;\\n&lt;2:Line 2&gt;\\n&lt;3:Line 3&gt;\"\n        &gt;&gt;&gt; lines_from_bracketed_text(text, 1, 2)\n        ['Line 1', 'Line 2']\n\n        &gt;&gt;&gt; text = \"&lt;Line 1&gt;\\n&lt;Line 2&gt;\\n&lt;Line 3&gt;\"\n        &gt;&gt;&gt; lines_from_bracketed_text(text, 2, 3)\n        ['Line 2', 'Line 3']\n    \"\"\"\n    # Split the text into lines\n    lines = text.splitlines()\n\n    # Validate indices\n    if start &lt; 1 or end &lt; 1 or start &gt; end or end &gt; len(lines):\n        raise ValueError(\n            \"Invalid start or end indices for the given text: start:{start}, end: {end}\"\n        )\n\n    # Extract lines and validate formatting\n    result = []\n    for i, line in enumerate(lines, start=1):\n        if start &lt;= i &lt;= end:\n            # Check for proper bracketing and extract the content\n            match = re.match(r\"&lt;(\\d+:)?(.*?)&gt;\", line)\n            if not match:\n                raise FormattingError(f\"Invalid format for line {i}: '{line}'\")\n            # Add the extracted content (group 2) to the result\n            if keep_brackets:\n                result.append(line)\n            else:\n                result.append(match[2].strip())\n\n    return \"\\n\".join(result)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.number_lines","title":"<code>number_lines(text, start=1, separator=': ')</code>","text":"<p>Numbers each line of text with a readable format, including empty lines.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to be numbered. Can be multi-line.</p> required <code>start</code> <code>int</code> <p>Starting line number. Defaults to 1.</p> <code>1</code> <code>separator</code> <code>str</code> <p>Separator between line number and content. Defaults to \": \".</p> <code>': '</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Numbered text where each line starts with \"{number}: \".</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text = \"First line\\nSecond line\\n\\nFourth line\"\n&gt;&gt;&gt; print(number_lines(text))\n1: First line\n2: Second line\n3:\n4: Fourth line\n</code></pre> <pre><code>&gt;&gt;&gt; print(number_lines(text, start=5, separator=\" | \"))\n5 | First line\n6 | Second line\n7 |\n8 | Fourth line\n</code></pre> Notes <ul> <li>All lines are numbered, including empty lines, to maintain text structure</li> <li>Line numbers are aligned through natural string formatting</li> <li>Customizable separator allows for different formatting needs</li> <li>Can start from any line number for flexibility in text processing</li> </ul> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def number_lines(text: str, start: int = 1, separator: str = \": \") -&gt; str:\n    \"\"\"\n    Numbers each line of text with a readable format, including empty lines.\n\n    Args:\n        text (str): Input text to be numbered. Can be multi-line.\n        start (int, optional): Starting line number. Defaults to 1.\n        separator (str, optional): Separator between line number and content.\n            Defaults to \": \".\n\n    Returns:\n        str: Numbered text where each line starts with \"{number}: \".\n\n    Examples:\n        &gt;&gt;&gt; text = \"First line\\\\nSecond line\\\\n\\\\nFourth line\"\n        &gt;&gt;&gt; print(number_lines(text))\n        1: First line\n        2: Second line\n        3:\n        4: Fourth line\n\n        &gt;&gt;&gt; print(number_lines(text, start=5, separator=\" | \"))\n        5 | First line\n        6 | Second line\n        7 |\n        8 | Fourth line\n\n    Notes:\n        - All lines are numbered, including empty lines, to maintain text structure\n        - Line numbers are aligned through natural string formatting\n        - Customizable separator allows for different formatting needs\n        - Can start from any line number for flexibility in text processing\n    \"\"\"\n    lines = text.splitlines()\n    return \"\\n\".join(f\"{i}{separator}{line}\" for i, line in enumerate(lines, start))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.unbracket_all_lines","title":"<code>unbracket_all_lines(pages)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def unbracket_all_lines(pages):\n    result = []\n    for page in pages:\n        if page == \"blank page\":\n            result.append(page)\n        else:\n            result.append(unbracket_lines(page))\n    return result\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.bracket.unbracket_lines","title":"<code>unbracket_lines(text, number=False)</code>","text":"<pre><code>Removes angle brackets (&lt; &gt;) from encapsulated lines and optionally removes line numbers.\n\nArgs:\n    text (str): The input string with encapsulated lines.\n    number (bool): If True, removes line numbers in the format 'digit:'.\n                   Raises a ValueError if `number=True` and a line does not start with a digit followed by a colon.\n\nReturns:\n    str: A newline-separated string with the encapsulation removed, and line numbers stripped if specified.\n\nExamples:\n    &gt;&gt;&gt; unbracket_lines(\"&lt;1:Line 1&gt;\n</code></pre> <p>&lt;2:Line 2&gt;\", number=True)         'Line 1 Line 2'</p> <pre><code>    &gt;&gt;&gt; unbracket_lines(\"&lt;Line 1&gt;\n</code></pre> <p>\")         'Line 1 Line 2' <pre><code>    &gt;&gt;&gt; unbracket_lines(\"&lt;1Line 1&gt;\", number=True)\n    ValueError: Line does not start with a valid number: '1Line 1'\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/bracket.py</code> <pre><code>def unbracket_lines(text: str, number: bool = False) -&gt; str:\n    \"\"\"\n    Removes angle brackets (&lt; &gt;) from encapsulated lines and optionally removes line numbers.\n\n    Args:\n        text (str): The input string with encapsulated lines.\n        number (bool): If True, removes line numbers in the format 'digit:'.\n                       Raises a ValueError if `number=True` and a line does not start with a digit followed by a colon.\n\n    Returns:\n        str: A newline-separated string with the encapsulation removed, and line numbers stripped if specified.\n\n    Examples:\n        &gt;&gt;&gt; unbracket_lines(\"&lt;1:Line 1&gt;\\n&lt;2:Line 2&gt;\", number=True)\n        'Line 1\\nLine 2'\n\n        &gt;&gt;&gt; unbracket_lines(\"&lt;Line 1&gt;\\n&lt;Line 2&gt;\")\n        'Line 1\\nLine 2'\n\n        &gt;&gt;&gt; unbracket_lines(\"&lt;1Line 1&gt;\", number=True)\n        ValueError: Line does not start with a valid number: '1Line 1'\n    \"\"\"\n    unbracketed_lines = []\n\n    for line in text.splitlines():\n        match = (\n            re.match(r\"&lt;(\\d+):(.*?)&gt;\", line) if number else re.match(r\"&lt;(.*?)&gt;\", line)\n        )\n        if match:\n            content = match[2].strip() if number else match[1].strip()\n            unbracketed_lines.append(content)\n        elif number:\n            raise FormattingError(f\"Line does not start with a valid number: '{line}'\")\n        else:\n            raise FormattingError(f\"Line does not follow the expected format: '{line}'\")\n\n    return \"\\n\".join(unbracketed_lines)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.match_section","title":"<code>match_section</code>","text":""},{"location":"api/#tnh_scholar.text_processing.match_section.MatchObject","title":"<code>MatchObject</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Basic Match Object definition.</p> Source code in <code>src/tnh_scholar/text_processing/match_section.py</code> <pre><code>class MatchObject(BaseModel):\n    \"\"\"Basic Match Object definition.\"\"\"\n    type: str\n    level: Optional[int] = None\n    words: Optional[List[str]] = None\n    case_sensitive: Optional[bool] = False\n    decorator: Optional[str] = None\n    pattern: Optional[str] = None\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.match_section.MatchObject.case_sensitive","title":"<code>case_sensitive = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.match_section.MatchObject.decorator","title":"<code>decorator = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.match_section.MatchObject.level","title":"<code>level = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.match_section.MatchObject.pattern","title":"<code>pattern = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.match_section.MatchObject.type","title":"<code>type</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.match_section.MatchObject.words","title":"<code>words = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.match_section.SectionConfig","title":"<code>SectionConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for section detection.</p> Source code in <code>src/tnh_scholar/text_processing/match_section.py</code> <pre><code>class SectionConfig(BaseModel):\n    \"\"\"Configuration for section detection.\"\"\"\n    name: str\n    description: Optional[str] = None\n    patterns: List[MatchObject]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.match_section.SectionConfig.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.match_section.SectionConfig.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.match_section.SectionConfig.patterns","title":"<code>patterns</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.match_section.find_keyword","title":"<code>find_keyword(line, words, case_sensitive, decorator)</code>","text":"<p>Check if line matches keyword pattern.</p> Source code in <code>src/tnh_scholar/text_processing/match_section.py</code> <pre><code>def find_keyword(\n    line: str, \n    words: List[str], \n    case_sensitive: bool, \n    decorator: Optional[str]\n    ) -&gt; bool:\n    \"\"\"Check if line matches keyword pattern.\"\"\"\n    if not case_sensitive:\n        line = line.lower()\n        words = [w.lower() for w in words]\n\n    # Check if line starts with any keyword\n    if not any(line.lstrip().startswith(word) for word in words):\n        return False\n\n    # If decorator specified, check if it appears in line\n    return not decorator or decorator in line\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.match_section.find_markdown_header","title":"<code>find_markdown_header(line, level)</code>","text":"<p>Check if line matches markdown header pattern.</p> Source code in <code>src/tnh_scholar/text_processing/match_section.py</code> <pre><code>def find_markdown_header(line: str, level: int) -&gt; bool:\n    \"\"\"Check if line matches markdown header pattern.\"\"\"\n    stripped = line.lstrip()\n    return stripped.startswith('#' * level + ' ')\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.match_section.find_regex","title":"<code>find_regex(line, pattern)</code>","text":"<p>Check if line matches regex pattern.</p> Source code in <code>src/tnh_scholar/text_processing/match_section.py</code> <pre><code>def find_regex(line: str, pattern: str) -&gt; bool:\n    \"\"\"Check if line matches regex pattern.\"\"\"\n    return bool(re.match(pattern, line))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.match_section.find_section_boundaries","title":"<code>find_section_boundaries(text, config)</code>","text":"<p>Find all section boundary line numbers.</p> Source code in <code>src/tnh_scholar/text_processing/match_section.py</code> <pre><code>def find_section_boundaries(text: str, config: SectionConfig) -&gt; List[int]:\n    \"\"\"Find all section boundary line numbers.\"\"\"\n    boundaries = []\n\n    for i, line in enumerate(text.splitlines(), 1):\n        for pattern in config.patterns:\n            matched = False\n\n            if pattern.type == \"markdown_header\" and pattern.level:\n                matched = find_markdown_header(line, pattern.level)\n\n            elif pattern.type == \"keyword\" and pattern.words:\n                matched = find_keyword(\n                    line, \n                    pattern.words,\n                    pattern.case_sensitive or False,\n                    pattern.decorator\n                )\n\n            elif pattern.type == \"regex\" and pattern.pattern:\n                matched = find_regex(line, pattern.pattern)\n\n            if matched:\n                boundaries.append(i)\n                break  # Stop checking patterns if we found a match\n\n    return boundaries\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text","title":"<code>numbered_text</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedFormat","title":"<code>NumberedFormat</code>","text":"<p>               Bases: <code>NamedTuple</code></p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>class NumberedFormat(NamedTuple):\n    is_numbered: bool\n    separator: Optional[str] = None\n    start_num: Optional[int] = None\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedFormat.is_numbered","title":"<code>is_numbered</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedFormat.separator","title":"<code>separator = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedFormat.start_num","title":"<code>start_num = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText","title":"<code>NumberedText</code>","text":"<p>Represents a text document with numbered lines for easy reference and manipulation.</p> <p>Provides utilities for working with line-numbered text including reading, writing, accessing lines by number, and iterating over numbered lines.</p> <p>Attributes:</p> Name Type Description <code>lines</code> <code>List[str]</code> <p>List of text lines</p> <code>start</code> <code>int</code> <p>Starting line number (default: 1)</p> <code>separator</code> <code>str</code> <p>Separator between line number and content (default: \": \")</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text = \"First line\\nSecond line\\n\\nFourth line\"\n&gt;&gt;&gt; doc = NumberedText(text)\n&gt;&gt;&gt; print(doc)\n1: First line\n2: Second line\n3:\n4: Fourth line\n</code></pre> <pre><code>&gt;&gt;&gt; print(doc.get_line(2))\nSecond line\n</code></pre> <pre><code>&gt;&gt;&gt; for num, line in doc:\n...     print(f\"Line {num}: {len(line)} chars\")\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>class NumberedText:\n    \"\"\"\n    Represents a text document with numbered lines for easy reference and manipulation.\n\n    Provides utilities for working with line-numbered text including reading,\n    writing, accessing lines by number, and iterating over numbered lines.\n\n    Attributes:\n        lines (List[str]): List of text lines\n        start (int): Starting line number (default: 1)\n        separator (str): Separator between line number and content (default: \": \")\n\n    Examples:\n        &gt;&gt;&gt; text = \"First line\\\\nSecond line\\\\n\\\\nFourth line\"\n        &gt;&gt;&gt; doc = NumberedText(text)\n        &gt;&gt;&gt; print(doc)\n        1: First line\n        2: Second line\n        3:\n        4: Fourth line\n\n        &gt;&gt;&gt; print(doc.get_line(2))\n        Second line\n\n        &gt;&gt;&gt; for num, line in doc:\n        ...     print(f\"Line {num}: {len(line)} chars\")\n    \"\"\"\n\n    @dataclass\n    class LineSegment:\n        \"\"\"\n        Represents a segment of lines with start and end indices in 1-based indexing.\n\n        The segment follows Python range conventions where start is inclusive and\n        end is exclusive. However, indexing is 1-based to match NumberedText.\n\n        Attributes:\n            start: Starting line number (inclusive, 1-based)\n            end: Ending line number (exclusive, 1-based)\n        \"\"\"\n\n        start: int\n        end: int\n\n        def __iter__(self):\n            \"\"\"Allow unpacking into start, end pairs.\"\"\"\n            yield self.start\n            yield self.end\n\n    class SegmentIterator:\n        \"\"\"\n        Iterator for generating line segments of specified size.\n\n        Produces segments of lines with start/end indices following 1-based indexing.\n        The final segment may be smaller than the specified segment size.\n\n        Attributes:\n            total_lines: Total number of lines in text\n            segment_size: Number of lines per segment\n            start_line: Starting line number (1-based)\n            min_segment_size: Minimum size for the final segment\n        \"\"\"\n\n        def __init__(\n            self,\n            total_lines: int,\n            segment_size: int,\n            start_line: int = 1,\n            min_segment_size: Optional[int] = None,\n        ):\n            \"\"\"\n            Initialize the segment iterator.\n\n            Args:\n                total_lines: Total number of lines to iterate over\n                segment_size: Desired size of each segment\n                start_line: First line number (default: 1)\n                min_segment_size: Minimum size for final segment (default: None)\n                    If specified, the last segment will be merged with the previous one\n                    if it would be smaller than this size.\n\n            Raises:\n                ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n                ValueError: If start_line &lt; 1 (must use 1-based indexing)\n                ValueError: If min_segment_size &gt;= segment_size\n            \"\"\"\n            if segment_size &lt; 1:\n                raise ValueError(\"Segment size must be at least 1\")\n            if total_lines &lt; 1:\n                raise ValueError(\"Total lines must be at least 1\")\n            if start_line &lt; 1:\n                raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n            if min_segment_size is not None and min_segment_size &gt;= segment_size:\n                raise ValueError(\"Minimum segment size must be less than segment size\")\n\n            self.total_lines = total_lines\n            self.segment_size = segment_size\n            self.start_line = start_line\n            self.min_segment_size = min_segment_size\n\n            # Calculate number of segments\n            remaining_lines = total_lines - start_line + 1\n            self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n\n        def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n            \"\"\"\n            Iterate over line segments.\n\n            Yields:\n                LineSegment containing start (inclusive) and end (exclusive) indices\n            \"\"\"\n            current = self.start_line\n\n            for i in range(self.num_segments):\n                is_last_segment = i == self.num_segments - 1\n                segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n                # Handle minimum segment size for last segment\n                if (\n                    is_last_segment\n                    and self.min_segment_size is not None\n                    and segment_end - current &lt; self.min_segment_size\n                    and i &gt; 0\n                ):\n                    # Merge with previous segment by not yielding\n                    break\n\n                yield NumberedText.LineSegment(current, segment_end)\n                current = segment_end\n\n    def __init__(\n        self, content: Optional[str] = None, start: int = 1, separator: str = \":\"\n    ) -&gt; None:\n        \"\"\"\n        Initialize a numbered text document, \n        detecting and preserving existing numbering.\n\n        Valid numbered text must have:\n        - Sequential line numbers\n        - Consistent separator character(s)\n        - Every non-empty line must follow the numbering pattern\n\n        Args:\n            content: Initial text content, if any\n            start: Starting line number (used only if content isn't already numbered)\n            separator: Separator between line numbers and content (only if content isn't numbered)\n\n        Examples:\n            &gt;&gt;&gt; # Custom separators\n            &gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\\\n2\u2192Second line\")\n            &gt;&gt;&gt; doc.separator == \"\u2192\"\n            True\n\n            &gt;&gt;&gt; # Preserves starting number\n            &gt;&gt;&gt; doc = NumberedText(\"5#First\\\\n6#Second\")\n            &gt;&gt;&gt; doc.start == 5\n            True\n\n            &gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n            &gt;&gt;&gt; doc = NumberedText(\"1. First item\\\\n2. Second item\")\n            &gt;&gt;&gt; doc.numbered_lines\n            ['1: 1. First item', '2: 2. Second item']\n        \"\"\"\n\n        self.lines: List[str] = []  # Declare lines here\n        self.start: int = start  # Declare start with its type\n        self.separator: str = separator  # and separator\n\n        if not isinstance(content, str):\n            raise ValueError(\"NumberedText requires string input.\")\n\n        if start &lt; 1:  # enforce 1 based indexing.\n            raise IndexError(\n                \"NumberedText: Numbered lines must begin on \"\n                \"an integer great or equal to 1.\"\n            )\n\n        if not content:\n            return\n\n        # Analyze the text format\n        is_numbered, detected_sep, start_num = get_numbered_format(content)\n\n        format_info = get_numbered_format(content)\n\n        if format_info.is_numbered:\n            self.start = format_info.start_num  # type: ignore\n            self.separator = format_info.separator  # type: ignore\n\n            # Extract content by removing number and separator\n            pattern = re.compile(rf\"^\\d+{re.escape(detected_sep)}\") # type: ignore\n            self.lines = []\n\n            for line in content.splitlines():\n                if line.strip():\n                    self.lines.append(pattern.sub(\"\", line))\n                else:\n                    self.lines.append(line)\n        else:\n            self.lines = content.splitlines()\n            self.start = start\n            self.separator = separator\n\n    @classmethod\n    def from_file(cls, path: Path, **kwargs) -&gt; \"NumberedText\":\n        \"\"\"Create a NumberedText instance from a file.\"\"\"\n        return cls(read_str_from_file(Path(path)), **kwargs)\n\n    def _format_line(self, line_num: int, line: str) -&gt; str:\n        return f\"{line_num}{self.separator}{line}\"\n\n    def _to_internal_index(self, idx: int) -&gt; int:\n        \"\"\"return the index into the lines object in Python 0-based indexing.\"\"\"\n        if idx &gt; 0:\n            return idx - self.start\n        elif idx &lt; 0:  # allow negative indexing to index from end\n            if abs(idx) &gt; self.size:\n                raise IndexError(f\"NumberedText: negative index out of range: {idx}\")\n            return self.end + idx  # convert to logical positive location for reference.\n        else:\n            raise IndexError(\"NumberedText: Index cannot be zero in 1-based indexing.\")\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the numbered text representation.\"\"\"\n        return \"\\n\".join(\n            self._format_line(i, line) for i, line in enumerate(self.lines, self.start)\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of lines.\"\"\"\n        return len(self.lines)\n\n    def __iter__(self) -&gt; Iterator[tuple[int, str]]:\n        \"\"\"Iterate over (line_number, line_content) pairs.\"\"\"\n        return iter((i, line) for i, line in enumerate(self.lines, self.start))\n\n    def __getitem__(self, index: int) -&gt; str:\n        \"\"\"Get line content by line number (1-based indexing).\"\"\"\n        return self.lines[self._to_internal_index(index)]\n\n    def get_line(self, line_num: int) -&gt; str:\n        \"\"\"Get content of specified line number.\"\"\"\n        return self[line_num]\n\n    def _to_line_index(self, internal_index: int) -&gt; int:\n        return self.start + self._to_internal_index(internal_index)\n\n    def get_numbered_line(self, line_num: int) -&gt; str:\n        \"\"\"Get specified line with line number.\"\"\"\n        idx = self._to_line_index(line_num)\n        return self._format_line(idx, self[idx])\n\n    def get_lines(self, start: int, end: int) -&gt; List[str]:\n        \"\"\"Get content of line range, not inclusive of end line.\"\"\"\n        return self.lines[self._to_internal_index(start) : self._to_internal_index(end)]\n\n    def get_numbered_lines(self, start: int, end: int) -&gt; List[str]:\n        return [\n            self._format_line(i + self._to_internal_index(start) + 1, line)\n            for i, line in enumerate(self.get_lines(start, end))\n        ]\n    def get_segment(self, start: int, end: int) -&gt; str:\n        \"\"\"return the segment from start line (inclusive) up to end line (exclusive)\"\"\"\n        if start &lt; self.start:\n            raise IndexError(f\"Start index {start} is before first line {self.start}\")\n        if end &gt; len(self) + 1:\n            raise IndexError(f\"End index {end} is past last line {len(self)}\")\n        if start &gt;= end:\n            raise IndexError(f\"Start index {start} must be less than end index {end}\")\n        return \"\\n\".join(self.get_lines(start, end))\n\n    def iter_segments(\n        self, segment_size: int, min_segment_size: Optional[int] = None\n    ) -&gt; Iterator[LineSegment]:\n        \"\"\"\n        Iterate over segments of the text with specified size.\n\n        Args:\n            segment_size: Number of lines per segment\n            min_segment_size: Optional minimum size for final segment.\n                If specified, last segment will be merged with previous one\n                if it would be smaller than this size.\n\n        Yields:\n            LineSegment objects containing start and end line numbers\n\n        Example:\n            &gt;&gt;&gt; text = NumberedText(\"line1\\\\nline2\\\\nline3\\\\nline4\\\\nline5\")\n            &gt;&gt;&gt; for segment in text.iter_segments(2):\n            ...     print(f\"Lines {segment.start}-{segment.end}\")\n            Lines 1-3\n            Lines 3-5\n            Lines 5-6\n        \"\"\"\n        iterator = self.SegmentIterator(\n            len(self), segment_size, self.start, min_segment_size\n        )\n        return iter(iterator)\n\n    def get_numbered_segment(self, start: int, end: int) -&gt; str:\n        return \"\\n\".join(self.get_numbered_lines(start, end))\n\n    def save(self, path: Path, numbered: bool = True) -&gt; None:\n        \"\"\"\n        Save document to file.\n\n        Args:\n            path: Output file path\n            numbered: Whether to save with line numbers (default: True)\n        \"\"\"\n        content = str(self) if numbered else \"\\n\".join(self.lines)\n        write_str_to_file(path, content)\n\n    def append(self, text: str) -&gt; None:\n        \"\"\"Append text, splitting into lines if needed.\"\"\"\n        self.lines.extend(text.splitlines())\n\n    def insert(self, line_num: int, text: str) -&gt; None:\n        \"\"\"Insert text at specified line number. Assumes text is not empty.\"\"\"\n        new_lines = text.splitlines()\n        internal_idx = self._to_internal_index(line_num)\n        self.lines[internal_idx:internal_idx] = new_lines\n\n    def reset_numbering(self):\n        self.start = 1\n\n    def remove_whitespace(self) -&gt; None:\n        \"\"\"Remove leading and trailing whitespace from all lines.\"\"\"\n        self.lines = [line.strip() for line in self.lines]\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Get original text without line numbers.\"\"\"\n        return \"\\n\".join(self.lines)\n\n    @property\n    def numbered_content(self) -&gt; str:\n        \"\"\"Get text with line numbers as a string. Equivalent to str(self)\"\"\"\n        return str(self)\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Get the number of lines.\"\"\"\n        return len(self.lines)\n\n    @property\n    def numbered_lines(self) -&gt; List[str]:\n        \"\"\"\n        Get list of lines with line numbers included.\n\n        Returns:\n            List[str]: Lines with numbers and separator prefixed\n\n        Examples:\n            &gt;&gt;&gt; doc = NumberedText(\"First line\\\\nSecond line\")\n            &gt;&gt;&gt; doc.numbered_lines\n            ['1: First line', '2: Second line']\n\n        Note:\n            - Unlike str(self), this returns a list rather than joined string\n            - Maintains consistent formatting with separator\n            - Useful for processing or displaying individual numbered lines\n        \"\"\"\n        return [\n            f\"{i}{self.separator}{line}\"\n            for i, line in enumerate(self.lines, self.start)\n        ]\n\n    @property\n    def end(self) -&gt; int:\n        return self.start + len(self.lines) - 1\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.content","title":"<code>content</code>  <code>property</code>","text":"<p>Get original text without line numbers.</p>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.end","title":"<code>end</code>  <code>property</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.lines","title":"<code>lines = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.numbered_content","title":"<code>numbered_content</code>  <code>property</code>","text":"<p>Get text with line numbers as a string. Equivalent to str(self)</p>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.numbered_lines","title":"<code>numbered_lines</code>  <code>property</code>","text":"<p>Get list of lines with line numbers included.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Lines with numbers and separator prefixed</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; doc = NumberedText(\"First line\\nSecond line\")\n&gt;&gt;&gt; doc.numbered_lines\n['1: First line', '2: Second line']\n</code></pre> Note <ul> <li>Unlike str(self), this returns a list rather than joined string</li> <li>Maintains consistent formatting with separator</li> <li>Useful for processing or displaying individual numbered lines</li> </ul>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.separator","title":"<code>separator = separator</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.size","title":"<code>size</code>  <code>property</code>","text":"<p>Get the number of lines.</p>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.start","title":"<code>start = start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.LineSegment","title":"<code>LineSegment</code>  <code>dataclass</code>","text":"<p>Represents a segment of lines with start and end indices in 1-based indexing.</p> <p>The segment follows Python range conventions where start is inclusive and end is exclusive. However, indexing is 1-based to match NumberedText.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>int</code> <p>Starting line number (inclusive, 1-based)</p> <code>end</code> <code>int</code> <p>Ending line number (exclusive, 1-based)</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>@dataclass\nclass LineSegment:\n    \"\"\"\n    Represents a segment of lines with start and end indices in 1-based indexing.\n\n    The segment follows Python range conventions where start is inclusive and\n    end is exclusive. However, indexing is 1-based to match NumberedText.\n\n    Attributes:\n        start: Starting line number (inclusive, 1-based)\n        end: Ending line number (exclusive, 1-based)\n    \"\"\"\n\n    start: int\n    end: int\n\n    def __iter__(self):\n        \"\"\"Allow unpacking into start, end pairs.\"\"\"\n        yield self.start\n        yield self.end\n</code></pre> <code>end</code> <code>instance-attribute</code> \u00b6 <code>start</code> <code>instance-attribute</code> \u00b6 <code>__init__(start, end)</code> \u00b6 <code>__iter__()</code> \u00b6 <p>Allow unpacking into start, end pairs.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self):\n    \"\"\"Allow unpacking into start, end pairs.\"\"\"\n    yield self.start\n    yield self.end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.SegmentIterator","title":"<code>SegmentIterator</code>","text":"<p>Iterator for generating line segments of specified size.</p> <p>Produces segments of lines with start/end indices following 1-based indexing. The final segment may be smaller than the specified segment size.</p> <p>Attributes:</p> Name Type Description <code>total_lines</code> <p>Total number of lines in text</p> <code>segment_size</code> <p>Number of lines per segment</p> <code>start_line</code> <p>Starting line number (1-based)</p> <code>min_segment_size</code> <p>Minimum size for the final segment</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>class SegmentIterator:\n    \"\"\"\n    Iterator for generating line segments of specified size.\n\n    Produces segments of lines with start/end indices following 1-based indexing.\n    The final segment may be smaller than the specified segment size.\n\n    Attributes:\n        total_lines: Total number of lines in text\n        segment_size: Number of lines per segment\n        start_line: Starting line number (1-based)\n        min_segment_size: Minimum size for the final segment\n    \"\"\"\n\n    def __init__(\n        self,\n        total_lines: int,\n        segment_size: int,\n        start_line: int = 1,\n        min_segment_size: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize the segment iterator.\n\n        Args:\n            total_lines: Total number of lines to iterate over\n            segment_size: Desired size of each segment\n            start_line: First line number (default: 1)\n            min_segment_size: Minimum size for final segment (default: None)\n                If specified, the last segment will be merged with the previous one\n                if it would be smaller than this size.\n\n        Raises:\n            ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n            ValueError: If start_line &lt; 1 (must use 1-based indexing)\n            ValueError: If min_segment_size &gt;= segment_size\n        \"\"\"\n        if segment_size &lt; 1:\n            raise ValueError(\"Segment size must be at least 1\")\n        if total_lines &lt; 1:\n            raise ValueError(\"Total lines must be at least 1\")\n        if start_line &lt; 1:\n            raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n        if min_segment_size is not None and min_segment_size &gt;= segment_size:\n            raise ValueError(\"Minimum segment size must be less than segment size\")\n\n        self.total_lines = total_lines\n        self.segment_size = segment_size\n        self.start_line = start_line\n        self.min_segment_size = min_segment_size\n\n        # Calculate number of segments\n        remaining_lines = total_lines - start_line + 1\n        self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n\n    def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n        \"\"\"\n        Iterate over line segments.\n\n        Yields:\n            LineSegment containing start (inclusive) and end (exclusive) indices\n        \"\"\"\n        current = self.start_line\n\n        for i in range(self.num_segments):\n            is_last_segment = i == self.num_segments - 1\n            segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n            # Handle minimum segment size for last segment\n            if (\n                is_last_segment\n                and self.min_segment_size is not None\n                and segment_end - current &lt; self.min_segment_size\n                and i &gt; 0\n            ):\n                # Merge with previous segment by not yielding\n                break\n\n            yield NumberedText.LineSegment(current, segment_end)\n            current = segment_end\n</code></pre> <code>min_segment_size = min_segment_size</code> <code>instance-attribute</code> \u00b6 <code>num_segments = (remaining_lines + segment_size - 1) // segment_size</code> <code>instance-attribute</code> \u00b6 <code>segment_size = segment_size</code> <code>instance-attribute</code> \u00b6 <code>start_line = start_line</code> <code>instance-attribute</code> \u00b6 <code>total_lines = total_lines</code> <code>instance-attribute</code> \u00b6 <code>__init__(total_lines, segment_size, start_line=1, min_segment_size=None)</code> \u00b6 <p>Initialize the segment iterator.</p> <p>Parameters:</p> Name Type Description Default <code>total_lines</code> <code>int</code> <p>Total number of lines to iterate over</p> required <code>segment_size</code> <code>int</code> <p>Desired size of each segment</p> required <code>start_line</code> <code>int</code> <p>First line number (default: 1)</p> <code>1</code> <code>min_segment_size</code> <code>Optional[int]</code> <p>Minimum size for final segment (default: None) If specified, the last segment will be merged with the previous one if it would be smaller than this size.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If segment_size &lt; 1 or total_lines &lt; 1</p> <code>ValueError</code> <p>If start_line &lt; 1 (must use 1-based indexing)</p> <code>ValueError</code> <p>If min_segment_size &gt;= segment_size</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __init__(\n    self,\n    total_lines: int,\n    segment_size: int,\n    start_line: int = 1,\n    min_segment_size: Optional[int] = None,\n):\n    \"\"\"\n    Initialize the segment iterator.\n\n    Args:\n        total_lines: Total number of lines to iterate over\n        segment_size: Desired size of each segment\n        start_line: First line number (default: 1)\n        min_segment_size: Minimum size for final segment (default: None)\n            If specified, the last segment will be merged with the previous one\n            if it would be smaller than this size.\n\n    Raises:\n        ValueError: If segment_size &lt; 1 or total_lines &lt; 1\n        ValueError: If start_line &lt; 1 (must use 1-based indexing)\n        ValueError: If min_segment_size &gt;= segment_size\n    \"\"\"\n    if segment_size &lt; 1:\n        raise ValueError(\"Segment size must be at least 1\")\n    if total_lines &lt; 1:\n        raise ValueError(\"Total lines must be at least 1\")\n    if start_line &lt; 1:\n        raise ValueError(\"Start line must be at least 1 (1-based indexing)\")\n    if min_segment_size is not None and min_segment_size &gt;= segment_size:\n        raise ValueError(\"Minimum segment size must be less than segment size\")\n\n    self.total_lines = total_lines\n    self.segment_size = segment_size\n    self.start_line = start_line\n    self.min_segment_size = min_segment_size\n\n    # Calculate number of segments\n    remaining_lines = total_lines - start_line + 1\n    self.num_segments = (remaining_lines + segment_size - 1) // segment_size\n</code></pre> <code>__iter__()</code> \u00b6 <p>Iterate over line segments.</p> <p>Yields:</p> Type Description <code>LineSegment</code> <p>LineSegment containing start (inclusive) and end (exclusive) indices</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self) -&gt; Iterator[\"NumberedText.LineSegment\"]:\n    \"\"\"\n    Iterate over line segments.\n\n    Yields:\n        LineSegment containing start (inclusive) and end (exclusive) indices\n    \"\"\"\n    current = self.start_line\n\n    for i in range(self.num_segments):\n        is_last_segment = i == self.num_segments - 1\n        segment_end = min(current + self.segment_size, self.total_lines + 1)\n\n        # Handle minimum segment size for last segment\n        if (\n            is_last_segment\n            and self.min_segment_size is not None\n            and segment_end - current &lt; self.min_segment_size\n            and i &gt; 0\n        ):\n            # Merge with previous segment by not yielding\n            break\n\n        yield NumberedText.LineSegment(current, segment_end)\n        current = segment_end\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get line content by line number (1-based indexing).</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get line content by line number (1-based indexing).\"\"\"\n    return self.lines[self._to_internal_index(index)]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.__init__","title":"<code>__init__(content=None, start=1, separator=':')</code>","text":"<p>Initialize a numbered text document,  detecting and preserving existing numbering.</p> <p>Valid numbered text must have: - Sequential line numbers - Consistent separator character(s) - Every non-empty line must follow the numbering pattern</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Optional[str]</code> <p>Initial text content, if any</p> <code>None</code> <code>start</code> <code>int</code> <p>Starting line number (used only if content isn't already numbered)</p> <code>1</code> <code>separator</code> <code>str</code> <p>Separator between line numbers and content (only if content isn't numbered)</p> <code>':'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Custom separators\n&gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\n2\u2192Second line\")\n&gt;&gt;&gt; doc.separator == \"\u2192\"\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Preserves starting number\n&gt;&gt;&gt; doc = NumberedText(\"5#First\\n6#Second\")\n&gt;&gt;&gt; doc.start == 5\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n&gt;&gt;&gt; doc = NumberedText(\"1. First item\\n2. Second item\")\n&gt;&gt;&gt; doc.numbered_lines\n['1: 1. First item', '2: 2. Second item']\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __init__(\n    self, content: Optional[str] = None, start: int = 1, separator: str = \":\"\n) -&gt; None:\n    \"\"\"\n    Initialize a numbered text document, \n    detecting and preserving existing numbering.\n\n    Valid numbered text must have:\n    - Sequential line numbers\n    - Consistent separator character(s)\n    - Every non-empty line must follow the numbering pattern\n\n    Args:\n        content: Initial text content, if any\n        start: Starting line number (used only if content isn't already numbered)\n        separator: Separator between line numbers and content (only if content isn't numbered)\n\n    Examples:\n        &gt;&gt;&gt; # Custom separators\n        &gt;&gt;&gt; doc = NumberedText(\"1\u2192First line\\\\n2\u2192Second line\")\n        &gt;&gt;&gt; doc.separator == \"\u2192\"\n        True\n\n        &gt;&gt;&gt; # Preserves starting number\n        &gt;&gt;&gt; doc = NumberedText(\"5#First\\\\n6#Second\")\n        &gt;&gt;&gt; doc.start == 5\n        True\n\n        &gt;&gt;&gt; # Regular numbered list isn't treated as line numbers\n        &gt;&gt;&gt; doc = NumberedText(\"1. First item\\\\n2. Second item\")\n        &gt;&gt;&gt; doc.numbered_lines\n        ['1: 1. First item', '2: 2. Second item']\n    \"\"\"\n\n    self.lines: List[str] = []  # Declare lines here\n    self.start: int = start  # Declare start with its type\n    self.separator: str = separator  # and separator\n\n    if not isinstance(content, str):\n        raise ValueError(\"NumberedText requires string input.\")\n\n    if start &lt; 1:  # enforce 1 based indexing.\n        raise IndexError(\n            \"NumberedText: Numbered lines must begin on \"\n            \"an integer great or equal to 1.\"\n        )\n\n    if not content:\n        return\n\n    # Analyze the text format\n    is_numbered, detected_sep, start_num = get_numbered_format(content)\n\n    format_info = get_numbered_format(content)\n\n    if format_info.is_numbered:\n        self.start = format_info.start_num  # type: ignore\n        self.separator = format_info.separator  # type: ignore\n\n        # Extract content by removing number and separator\n        pattern = re.compile(rf\"^\\d+{re.escape(detected_sep)}\") # type: ignore\n        self.lines = []\n\n        for line in content.splitlines():\n            if line.strip():\n                self.lines.append(pattern.sub(\"\", line))\n            else:\n                self.lines.append(line)\n    else:\n        self.lines = content.splitlines()\n        self.start = start\n        self.separator = separator\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over (line_number, line_content) pairs.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __iter__(self) -&gt; Iterator[tuple[int, str]]:\n    \"\"\"Iterate over (line_number, line_content) pairs.\"\"\"\n    return iter((i, line) for i, line in enumerate(self.lines, self.start))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of lines.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of lines.\"\"\"\n    return len(self.lines)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.__str__","title":"<code>__str__()</code>","text":"<p>Return the numbered text representation.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the numbered text representation.\"\"\"\n    return \"\\n\".join(\n        self._format_line(i, line) for i, line in enumerate(self.lines, self.start)\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.append","title":"<code>append(text)</code>","text":"<p>Append text, splitting into lines if needed.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def append(self, text: str) -&gt; None:\n    \"\"\"Append text, splitting into lines if needed.\"\"\"\n    self.lines.extend(text.splitlines())\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.from_file","title":"<code>from_file(path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a NumberedText instance from a file.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>@classmethod\ndef from_file(cls, path: Path, **kwargs) -&gt; \"NumberedText\":\n    \"\"\"Create a NumberedText instance from a file.\"\"\"\n    return cls(read_str_from_file(Path(path)), **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_line","title":"<code>get_line(line_num)</code>","text":"<p>Get content of specified line number.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_line(self, line_num: int) -&gt; str:\n    \"\"\"Get content of specified line number.\"\"\"\n    return self[line_num]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_lines","title":"<code>get_lines(start, end)</code>","text":"<p>Get content of line range, not inclusive of end line.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_lines(self, start: int, end: int) -&gt; List[str]:\n    \"\"\"Get content of line range, not inclusive of end line.\"\"\"\n    return self.lines[self._to_internal_index(start) : self._to_internal_index(end)]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_numbered_line","title":"<code>get_numbered_line(line_num)</code>","text":"<p>Get specified line with line number.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_line(self, line_num: int) -&gt; str:\n    \"\"\"Get specified line with line number.\"\"\"\n    idx = self._to_line_index(line_num)\n    return self._format_line(idx, self[idx])\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_numbered_lines","title":"<code>get_numbered_lines(start, end)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_lines(self, start: int, end: int) -&gt; List[str]:\n    return [\n        self._format_line(i + self._to_internal_index(start) + 1, line)\n        for i, line in enumerate(self.get_lines(start, end))\n    ]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_numbered_segment","title":"<code>get_numbered_segment(start, end)</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_segment(self, start: int, end: int) -&gt; str:\n    return \"\\n\".join(self.get_numbered_lines(start, end))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.get_segment","title":"<code>get_segment(start, end)</code>","text":"<p>return the segment from start line (inclusive) up to end line (exclusive)</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_segment(self, start: int, end: int) -&gt; str:\n    \"\"\"return the segment from start line (inclusive) up to end line (exclusive)\"\"\"\n    if start &lt; self.start:\n        raise IndexError(f\"Start index {start} is before first line {self.start}\")\n    if end &gt; len(self) + 1:\n        raise IndexError(f\"End index {end} is past last line {len(self)}\")\n    if start &gt;= end:\n        raise IndexError(f\"Start index {start} must be less than end index {end}\")\n    return \"\\n\".join(self.get_lines(start, end))\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.insert","title":"<code>insert(line_num, text)</code>","text":"<p>Insert text at specified line number. Assumes text is not empty.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def insert(self, line_num: int, text: str) -&gt; None:\n    \"\"\"Insert text at specified line number. Assumes text is not empty.\"\"\"\n    new_lines = text.splitlines()\n    internal_idx = self._to_internal_index(line_num)\n    self.lines[internal_idx:internal_idx] = new_lines\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.iter_segments","title":"<code>iter_segments(segment_size, min_segment_size=None)</code>","text":"<p>Iterate over segments of the text with specified size.</p> <p>Parameters:</p> Name Type Description Default <code>segment_size</code> <code>int</code> <p>Number of lines per segment</p> required <code>min_segment_size</code> <code>Optional[int]</code> <p>Optional minimum size for final segment. If specified, last segment will be merged with previous one if it would be smaller than this size.</p> <code>None</code> <p>Yields:</p> Type Description <code>LineSegment</code> <p>LineSegment objects containing start and end line numbers</p> Example <p>text = NumberedText(\"line1\\nline2\\nline3\\nline4\\nline5\") for segment in text.iter_segments(2): ...     print(f\"Lines {segment.start}-{segment.end}\") Lines 1-3 Lines 3-5 Lines 5-6</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def iter_segments(\n    self, segment_size: int, min_segment_size: Optional[int] = None\n) -&gt; Iterator[LineSegment]:\n    \"\"\"\n    Iterate over segments of the text with specified size.\n\n    Args:\n        segment_size: Number of lines per segment\n        min_segment_size: Optional minimum size for final segment.\n            If specified, last segment will be merged with previous one\n            if it would be smaller than this size.\n\n    Yields:\n        LineSegment objects containing start and end line numbers\n\n    Example:\n        &gt;&gt;&gt; text = NumberedText(\"line1\\\\nline2\\\\nline3\\\\nline4\\\\nline5\")\n        &gt;&gt;&gt; for segment in text.iter_segments(2):\n        ...     print(f\"Lines {segment.start}-{segment.end}\")\n        Lines 1-3\n        Lines 3-5\n        Lines 5-6\n    \"\"\"\n    iterator = self.SegmentIterator(\n        len(self), segment_size, self.start, min_segment_size\n    )\n    return iter(iterator)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.remove_whitespace","title":"<code>remove_whitespace()</code>","text":"<p>Remove leading and trailing whitespace from all lines.</p> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def remove_whitespace(self) -&gt; None:\n    \"\"\"Remove leading and trailing whitespace from all lines.\"\"\"\n    self.lines = [line.strip() for line in self.lines]\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.reset_numbering","title":"<code>reset_numbering()</code>","text":"Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def reset_numbering(self):\n    self.start = 1\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.NumberedText.save","title":"<code>save(path, numbered=True)</code>","text":"<p>Save document to file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output file path</p> required <code>numbered</code> <code>bool</code> <p>Whether to save with line numbers (default: True)</p> <code>True</code> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def save(self, path: Path, numbered: bool = True) -&gt; None:\n    \"\"\"\n    Save document to file.\n\n    Args:\n        path: Output file path\n        numbered: Whether to save with line numbers (default: True)\n    \"\"\"\n    content = str(self) if numbered else \"\\n\".join(self.lines)\n    write_str_to_file(path, content)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.numbered_text.get_numbered_format","title":"<code>get_numbered_format(text)</code>","text":"<p>Analyze text to determine if it follows a consistent line numbering format.</p> <p>Valid formats have: - Sequential numbers starting from some value - Consistent separator character(s) - Every line must follow the format</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to analyze</p> required <p>Returns:</p> Type Description <code>NumberedFormat</code> <p>Tuple of (is_numbered, separator, start_number)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; _analyze_numbered_format(\"1\u2192First\\n2\u2192Second\")\n(True, \"\u2192\", 1)\n&gt;&gt;&gt; _analyze_numbered_format(\"1. First\")  # Numbered list format\n(False, None, None)\n&gt;&gt;&gt; _analyze_numbered_format(\"5#Line\\n6#Other\")\n(True, \"#\", 5)\n</code></pre> Source code in <code>src/tnh_scholar/text_processing/numbered_text.py</code> <pre><code>def get_numbered_format(text: str) -&gt; NumberedFormat:\n    \"\"\"\n    Analyze text to determine if it follows a consistent line numbering format.\n\n    Valid formats have:\n    - Sequential numbers starting from some value\n    - Consistent separator character(s)\n    - Every line must follow the format\n\n    Args:\n        text: Text to analyze\n\n    Returns:\n        Tuple of (is_numbered, separator, start_number)\n\n    Examples:\n        &gt;&gt;&gt; _analyze_numbered_format(\"1\u2192First\\\\n2\u2192Second\")\n        (True, \"\u2192\", 1)\n        &gt;&gt;&gt; _analyze_numbered_format(\"1. First\")  # Numbered list format\n        (False, None, None)\n        &gt;&gt;&gt; _analyze_numbered_format(\"5#Line\\\\n6#Other\")\n        (True, \"#\", 5)\n    \"\"\"\n    if not text.strip():\n        return NumberedFormat(False)\n\n    lines = [line for line in text.splitlines() if line.strip()]\n    if not lines:\n        return NumberedFormat(False)\n\n    # Try to detect pattern from first line\n    SEPARATOR_PATTERN = r\"[^\\w\\s.]\"  # not (word char or whitespace or period)\n    first_match = re.match(rf\"^(\\d+)({SEPARATOR_PATTERN})(.*?)$\", lines[0])\n\n    if not first_match:\n        return NumberedFormat(False)\n    try:\n        return _check_line_structure(first_match, lines)\n    except (ValueError, AttributeError):\n        return NumberedFormat(False)\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_processing","title":"<code>text_processing</code>","text":""},{"location":"api/#tnh_scholar.text_processing.text_processing.clean_text","title":"<code>clean_text(text, newline=False)</code>","text":"<p>Cleans a given text by replacing specific unwanted characters such as tab, and non-breaking spaces with regular spaces.</p> <p>This function takes a string as input and applies replacements based on a predefined mapping of characters to replace.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be cleaned.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text with unwanted characters replaced by spaces.</p> Example <p>text = \"This is\\n an example\\ttext with\\xa0extra spaces.\" clean_text(text) 'This is an example text with extra spaces.'</p> Source code in <code>src/tnh_scholar/text_processing/text_processing.py</code> <pre><code>def clean_text(text: str, newline: bool = False) -&gt; str:\n    \"\"\"\n    Cleans a given text by replacing specific unwanted characters such as\n    tab, and non-breaking spaces with regular spaces.\n\n    This function takes a string as input and applies replacements\n    based on a predefined mapping of characters to replace.\n\n    Args:\n        text (str): The text to be cleaned.\n\n    Returns:\n        str: The cleaned text with unwanted characters replaced by spaces.\n\n    Example:\n        &gt;&gt;&gt; text = \"This is\\\\n an example\\\\ttext with\\\\xa0extra spaces.\"\n        &gt;&gt;&gt; clean_text(text)\n        'This is an example text with extra spaces.'\n\n    \"\"\"\n    # Define a mapping of characters to replace\n    replace_map = {\n        \"\\t\": \" \",  # Replace tabs with space\n        \"\\xa0\": \" \",  # Replace non-breaking space with regular space\n        # Add more replacements as needed\n    }\n\n    if newline:\n        replace_map[\"\\n\"] = \"\"  # remove newlines\n\n    # Loop through the replace map and replace each character\n    for old_char, new_char in replace_map.items():\n        text = text.replace(old_char, new_char)\n\n    return text.strip()  # Ensure any leading/trailing spaces are removed\n</code></pre>"},{"location":"api/#tnh_scholar.text_processing.text_processing.normalize_newlines","title":"<code>normalize_newlines(text, spacing=2)</code>","text":"<pre><code>Normalize newline blocks in the input text by reducing consecutive newlines\nto the specified number of newlines for consistent readability and formatting.\n\nParameters:\n----------\ntext : str\n    The input text containing inconsistent newline spacing.\nspacing : int, optional\n    The number of newlines to insert between lines. Defaults to 2.\n\nReturns:\n-------\nstr\n    The text with consecutive newlines reduced to the specified number of newlines.\n\nExample:\n--------\n&gt;&gt;&gt; raw_text = \"Heading\n</code></pre> <p>Paragraph text 1 Paragraph text 2</p> <p>\"     &gt;&gt;&gt; normalize_newlines(raw_text, spacing=2)     'Heading</p> <p>Paragraph text 1</p> <p>Paragraph text 2</p> <p>'</p> Source code in <code>src/tnh_scholar/text_processing/text_processing.py</code> <pre><code>def normalize_newlines(text: str, spacing: int = 2) -&gt; str:\n    \"\"\"\n    Normalize newline blocks in the input text by reducing consecutive newlines\n    to the specified number of newlines for consistent readability and formatting.\n\n    Parameters:\n    ----------\n    text : str\n        The input text containing inconsistent newline spacing.\n    spacing : int, optional\n        The number of newlines to insert between lines. Defaults to 2.\n\n    Returns:\n    -------\n    str\n        The text with consecutive newlines reduced to the specified number of newlines.\n\n    Example:\n    --------\n    &gt;&gt;&gt; raw_text = \"Heading\\n\\n\\nParagraph text 1\\nParagraph text 2\\n\\n\\n\"\n    &gt;&gt;&gt; normalize_newlines(raw_text, spacing=2)\n    'Heading\\n\\nParagraph text 1\\n\\nParagraph text 2\\n\\n'\n    \"\"\"\n    # Replace one or more newlines with the desired number of newlines\n    newlines = \"\\n\" * spacing\n    return re.sub(r\"\\n{1,}\", newlines, text)\n</code></pre>"},{"location":"api/#tnh_scholar.tools","title":"<code>tools</code>","text":"<p>Internal helper utilities for dev workflows.</p>"},{"location":"api/#tnh_scholar.tools.notebook_prep","title":"<code>notebook_prep</code>","text":"<p>Utilities for maintaining paired *_local.ipynb notebooks.</p>"},{"location":"api/#tnh_scholar.tools.notebook_prep.EXCLUDED_PARTS","title":"<code>EXCLUDED_PARTS = {'.ipynb_checkpoints'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.tools.notebook_prep.prep_notebooks","title":"<code>prep_notebooks(directory, dry_run=True)</code>","text":"<p>Create *_local notebooks and strip outputs from originals.</p>"},{"location":"api/#tnh_scholar.tools.notebook_prep.prep_notebooks--parameters","title":"Parameters","text":"<p>directory:     Directory whose notebooks will be processed. dry_run:     When <code>True</code> only report pending work without copying files or invoking     <code>nbconvert</code>.</p> Source code in <code>src/tnh_scholar/tools/notebook_prep.py</code> <pre><code>def prep_notebooks(directory: Path | str, dry_run: bool = True) -&gt; bool:\n    \"\"\"Create *_local notebooks and strip outputs from originals.\n\n    Parameters\n    ----------\n    directory:\n        Directory whose notebooks will be processed.\n    dry_run:\n        When ``True`` only report pending work without copying files or invoking\n        ``nbconvert``.\n    \"\"\"\n\n    directory = Path(directory).expanduser()\n    if not directory.exists():\n        print(f\"Directory not found: {directory}\")\n        return False\n\n    notebooks = list(_iter_source_notebooks(directory))\n    print(\n        f\"Found {len(notebooks)} notebooks to process in {directory}. \"\n        \"Ignoring all checkpoint and *_local notebooks.\"\n    )\n\n    for nb_path in notebooks:\n        local_path = nb_path.parent / f\"{nb_path.stem}_local{nb_path.suffix}\"\n\n        if local_path.exists():\n            print(f\"No action required: local copy of notebook exists: {local_path}\")\n            continue\n        if dry_run:\n            print(f\"Would copy: {nb_path} -&gt; {local_path}\")\n        else:\n            print(f\"Copying: {nb_path} -&gt; {local_path}\")\n            shutil.copy2(nb_path, local_path)\n\n        if dry_run:\n            print(f\"Would strip outputs from: {nb_path}\")\n            continue\n\n        print(f\"Stripping outputs from: {nb_path}\")\n        subprocess.run(\n            [\n                \"jupyter\",\n                \"nbconvert\",\n                \"--ClearOutputPreprocessor.enabled=True\",\n                \"--inplace\",\n                str(nb_path),\n            ],\n            check=True,\n        )\n\n    return True\n</code></pre>"},{"location":"api/#tnh_scholar.tools.tree_builder","title":"<code>tree_builder</code>","text":"<p>Helpers for generating directory-tree text files.</p>"},{"location":"api/#tnh_scholar.tools.tree_builder.build_tree","title":"<code>build_tree(root_dir, src_dir=None)</code>","text":"<p>Generate directory trees for the project and optionally its source directory.</p> Source code in <code>src/tnh_scholar/tools/tree_builder.py</code> <pre><code>def build_tree(root_dir: Path, src_dir: Optional[Path] = None) -&gt; None:\n    \"\"\"Generate directory trees for the project and optionally its source directory.\"\"\"\n    if shutil.which(\"tree\") is None:\n        raise FileNotFoundError(\n            \"The 'tree' command is not found in the system PATH. Please install it first.\"\n        )\n\n    if not root_dir.exists() or not root_dir.is_dir():\n        raise FileNotFoundError(\n            f\"The root directory '{root_dir}' does not exist or is not a directory.\"\n        )\n\n    project_tree_output = root_dir / \"project_directory_tree.txt\"\n    # Run from the root so the tree uses relative paths (stable across environments).\n    subprocess.run(\n        [\"tree\", \"--gitignore\", \".\", \"-o\", str(project_tree_output)],\n        cwd=root_dir,\n        check=True,\n    )\n\n    if src_dir:\n        if not src_dir.exists() or not src_dir.is_dir():\n            raise FileNotFoundError(\n                f\"The source directory '{src_dir}' does not exist or is not a directory.\"\n            )\n        src_tree_output = root_dir / \"src_directory_tree.txt\"\n        subprocess.run(\n            [\"tree\", \"--gitignore\", \".\", \"-o\", str(src_tree_output)],\n            cwd=src_dir,\n            check=True,\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.utils","title":"<code>utils</code>","text":""},{"location":"api/#tnh_scholar.utils.__all__","title":"<code>__all__ = ['copy_files_with_regex', 'ensure_directory_exists', 'ensure_directory_writable', 'iterate_subdir', 'path_as_str', 'read_str_from_file', 'sanitize_filename', 'to_slug', 'write_str_to_file', 'load_json_into_model', 'load_jsonl_to_dict', 'save_model_to_json', 'get_language_code_from_text', 'get_language_from_code', 'get_language_name_from_text', 'ExpectedTimeTQDM', 'TimeProgress', 'TimeMs', 'TNHAudioSegment', 'convert_ms_to_sec', 'convert_sec_to_ms', 'get_user_confirmation', 'check_ocr_env', 'check_openai_env']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.ExpectedTimeTQDM","title":"<code>ExpectedTimeTQDM</code>","text":"<p>A context manager for a time-based tqdm progress bar with optional delay.</p> <ul> <li>'expected_time': number of seconds we anticipate the task might take.</li> <li>'display_interval': how often (seconds) to refresh the bar.</li> <li>'desc': a short description for the bar.</li> <li>'delay_start': how many seconds to wait (sleep) before we even create/start the bar.</li> </ul> <p>If the task finishes before 'delay_start' has elapsed, the bar may never appear.</p> Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>class ExpectedTimeTQDM:\n    \"\"\"\n    A context manager for a time-based tqdm progress bar with optional delay.\n\n    - 'expected_time': number of seconds we anticipate the task might take.\n    - 'display_interval': how often (seconds) to refresh the bar.\n    - 'desc': a short description for the bar.\n    - 'delay_start': how many seconds to wait (sleep) before we even create/start the bar.\n\n    If the task finishes before 'delay_start' has elapsed, the bar may never appear.\n    \"\"\"\n\n    def __init__(\n        self,\n        expected_time: float,\n        display_interval: float = 0.5,\n        desc: str = \"Time-based Progress\",\n        delay_start: float = 1.0,\n    ) -&gt; None:\n        self.expected_time = round(expected_time)  # use nearest second.\n        self.display_interval = display_interval\n        self.desc = desc\n        self.delay_start = delay_start\n\n        self._stop_event = threading.Event()\n        self._pbar = None  # We won't create the bar until after 'delay_start'\n        self._start_time = None\n\n    def __enter__(self):\n        # Record the start time for reference\n        self._start_time = time.time()\n\n        # Spawn the background thread; it will handle waiting and then creating/updating the bar\n        self._thread = threading.Thread(target=self._update_bar, daemon=True)\n        self._thread.start()\n\n        return self\n\n    def _update_bar(self):\n        # 1) Delay so warnings/logs can appear before the bar\n        if self.delay_start &gt; 0:\n            time.sleep(self.delay_start)\n\n        # 2) Create the tqdm bar (only now does it appear)\n        self._pbar = tqdm(\n            total=self.expected_time, desc=self.desc, unit=\"sec\", bar_format=BAR_FORMAT\n        )\n\n        # 3) Update until told to stop\n        while not self._stop_event.is_set():\n            elapsed = time.time() - self._start_time\n            current_value = min(elapsed, self.expected_time)\n            if self._pbar:\n                self._pbar.n = round(current_value)\n                self._pbar.refresh()\n            time.sleep(self.display_interval)\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # Signal the thread to stop\n        self._stop_event.set()\n        self._thread.join()\n\n        # If the bar was actually created (i.e., we didn't finish too quickly),\n        # do a final update and close\n        if self._pbar:\n            elapsed = time.time() - self._start_time\n            self._pbar.n = round(min(elapsed, self.expected_time))\n            self._pbar.refresh()\n            self._pbar.close()\n\n    import time\n</code></pre>"},{"location":"api/#tnh_scholar.utils.ExpectedTimeTQDM.delay_start","title":"<code>delay_start = delay_start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.ExpectedTimeTQDM.desc","title":"<code>desc = desc</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.ExpectedTimeTQDM.display_interval","title":"<code>display_interval = display_interval</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.ExpectedTimeTQDM.expected_time","title":"<code>expected_time = round(expected_time)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.ExpectedTimeTQDM.__enter__","title":"<code>__enter__()</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __enter__(self):\n    # Record the start time for reference\n    self._start_time = time.time()\n\n    # Spawn the background thread; it will handle waiting and then creating/updating the bar\n    self._thread = threading.Thread(target=self._update_bar, daemon=True)\n    self._thread.start()\n\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.utils.ExpectedTimeTQDM.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    # Signal the thread to stop\n    self._stop_event.set()\n    self._thread.join()\n\n    # If the bar was actually created (i.e., we didn't finish too quickly),\n    # do a final update and close\n    if self._pbar:\n        elapsed = time.time() - self._start_time\n        self._pbar.n = round(min(elapsed, self.expected_time))\n        self._pbar.refresh()\n        self._pbar.close()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.ExpectedTimeTQDM.__init__","title":"<code>__init__(expected_time, display_interval=0.5, desc='Time-based Progress', delay_start=1.0)</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __init__(\n    self,\n    expected_time: float,\n    display_interval: float = 0.5,\n    desc: str = \"Time-based Progress\",\n    delay_start: float = 1.0,\n) -&gt; None:\n    self.expected_time = round(expected_time)  # use nearest second.\n    self.display_interval = display_interval\n    self.desc = desc\n    self.delay_start = delay_start\n\n    self._stop_event = threading.Event()\n    self._pbar = None  # We won't create the bar until after 'delay_start'\n    self._start_time = None\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment","title":"<code>TNHAudioSegment</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>class TNHAudioSegment:\n    def __init__(self, segment: _AudioSegment):\n        self._segment = segment\n\n    @staticmethod\n    def from_file(\n        file: str | Path | BytesIO,\n        format: str | None = None,\n        **kwargs: Any,\n    ) -&gt; \"TNHAudioSegment\":\n        \"\"\"\n        Wrapper: Load an audio file into a TNHAudioSegment.\n\n        Args:\n            file: Path to the audio file.\n            format: Optional audio format (e.g., 'mp3', 'wav'). If None, pydub will attempt to infer it.\n            **kwargs: Additional keyword arguments passed to pydub.AudioSegment.from_file.\n\n        Returns:\n            TNHAudioSegment instance containing the loaded audio.\n        \"\"\"\n        return TNHAudioSegment(_AudioSegment.from_file(file, format=format, **kwargs))\n\n    def export(self, out_f: str | BinaryIO, format: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Wrapper: Export the audio segment to a file-like object or file path.\n\n        Args:\n            out_f: File path or file-like object to write the audio data to.\n            format: Audio format (e.g., 'mp3', 'wav').\n            **kwargs: Additional keyword arguments passed to pydub.AudioSegment.export.\n        \"\"\"\n        self._segment.export(out_f, format=format, **kwargs)\n\n    @staticmethod\n    def silent(duration: int) -&gt; \"TNHAudioSegment\":\n        return TNHAudioSegment(_AudioSegment.silent(duration=duration))\n\n    @staticmethod\n    def empty() -&gt; \"TNHAudioSegment\":\n        return TNHAudioSegment(_AudioSegment.empty())\n\n    def __getitem__(self, key: int | slice) -&gt; \"TNHAudioSegment\":\n        return TNHAudioSegment(self._segment[key]) # type: ignore\n\n    def __add__(self, other: \"TNHAudioSegment\") -&gt; \"TNHAudioSegment\":\n        return TNHAudioSegment(self._segment + other._segment)\n\n    def __iadd__(self, other: \"TNHAudioSegment\") -&gt; \"TNHAudioSegment\":\n        self._segment = self._segment + other._segment\n        return self\n\n    def __len__(self) -&gt; int:\n        return len(self._segment)\n\n    # Add more methods as needed, e.g., export, from_file, etc.\n\n    @property\n    def raw(self) -&gt; _AudioSegment:\n        \"\"\"Access the underlying pydub.AudioSegment if needed.\"\"\"\n        return self._segment\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment.raw","title":"<code>raw</code>  <code>property</code>","text":"<p>Access the underlying pydub.AudioSegment if needed.</p>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment.__add__","title":"<code>__add__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def __add__(self, other: \"TNHAudioSegment\") -&gt; \"TNHAudioSegment\":\n    return TNHAudioSegment(self._segment + other._segment)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment.__getitem__","title":"<code>__getitem__(key)</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def __getitem__(self, key: int | slice) -&gt; \"TNHAudioSegment\":\n    return TNHAudioSegment(self._segment[key]) # type: ignore\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment.__iadd__","title":"<code>__iadd__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def __iadd__(self, other: \"TNHAudioSegment\") -&gt; \"TNHAudioSegment\":\n    self._segment = self._segment + other._segment\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment.__init__","title":"<code>__init__(segment)</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def __init__(self, segment: _AudioSegment):\n    self._segment = segment\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment.__len__","title":"<code>__len__()</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._segment)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment.empty","title":"<code>empty()</code>  <code>staticmethod</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>@staticmethod\ndef empty() -&gt; \"TNHAudioSegment\":\n    return TNHAudioSegment(_AudioSegment.empty())\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment.export","title":"<code>export(out_f, format, **kwargs)</code>","text":"<p>Wrapper: Export the audio segment to a file-like object or file path.</p> <p>Parameters:</p> Name Type Description Default <code>out_f</code> <code>str | BinaryIO</code> <p>File path or file-like object to write the audio data to.</p> required <code>format</code> <code>str</code> <p>Audio format (e.g., 'mp3', 'wav').</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to pydub.AudioSegment.export.</p> <code>{}</code> Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def export(self, out_f: str | BinaryIO, format: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Wrapper: Export the audio segment to a file-like object or file path.\n\n    Args:\n        out_f: File path or file-like object to write the audio data to.\n        format: Audio format (e.g., 'mp3', 'wav').\n        **kwargs: Additional keyword arguments passed to pydub.AudioSegment.export.\n    \"\"\"\n    self._segment.export(out_f, format=format, **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment.from_file","title":"<code>from_file(file, format=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Wrapper: Load an audio file into a TNHAudioSegment.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | Path | BytesIO</code> <p>Path to the audio file.</p> required <code>format</code> <code>str | None</code> <p>Optional audio format (e.g., 'mp3', 'wav'). If None, pydub will attempt to infer it.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to pydub.AudioSegment.from_file.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TNHAudioSegment</code> <p>TNHAudioSegment instance containing the loaded audio.</p> Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>@staticmethod\ndef from_file(\n    file: str | Path | BytesIO,\n    format: str | None = None,\n    **kwargs: Any,\n) -&gt; \"TNHAudioSegment\":\n    \"\"\"\n    Wrapper: Load an audio file into a TNHAudioSegment.\n\n    Args:\n        file: Path to the audio file.\n        format: Optional audio format (e.g., 'mp3', 'wav'). If None, pydub will attempt to infer it.\n        **kwargs: Additional keyword arguments passed to pydub.AudioSegment.from_file.\n\n    Returns:\n        TNHAudioSegment instance containing the loaded audio.\n    \"\"\"\n    return TNHAudioSegment(_AudioSegment.from_file(file, format=format, **kwargs))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TNHAudioSegment.silent","title":"<code>silent(duration)</code>  <code>staticmethod</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>@staticmethod\ndef silent(duration: int) -&gt; \"TNHAudioSegment\":\n    return TNHAudioSegment(_AudioSegment.silent(duration=duration))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs","title":"<code>TimeMs</code>","text":"<p>               Bases: <code>int</code></p> <p>Lightweight representation of a time interval or timestamp in milliseconds. Allows negative values.</p> Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>class TimeMs(int):\n    \"\"\"\n    Lightweight representation of a time interval or timestamp in milliseconds.\n    Allows negative values.\n    \"\"\"\n\n    def __new__(cls, ms: Union[int, float, \"TimeMs\"]):\n        if isinstance(ms, TimeMs):\n            value = int(ms)\n        elif isinstance(ms, (int, float)):\n            if not math.isfinite(ms):\n                raise ValueError(\"ms must be a finite number\")\n            value = round(ms)\n        else:\n            raise TypeError(f\"ms must be a number or TimeMs, got {type(ms).__name__}\")\n        return int.__new__(cls, value)\n\n    @classmethod\n    def from_seconds(cls, seconds: int | float) -&gt; \"TimeMs\":\n        return cls(round(seconds * 1000))\n\n    def to_ms(self) -&gt; int:\n        return int(self)\n\n    def to_seconds(self) -&gt; float:\n        return float(self) / 1000\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source_type, handler: GetCoreSchemaHandler):\n        return core_schema.with_info_plain_validator_function(\n            cls._validate,\n            serialization=core_schema.plain_serializer_function_ser_schema(lambda v: int(v)),\n        )\n\n    @classmethod\n    def _validate(cls, value, info):\n        \"\"\"\n        Pydantic core validator for TimeMs.\n\n        Args:\n            value: The value to validate.\n            info: Pydantic core schema info (unused).\n\n        Returns:\n            TimeMs: Validated TimeMs instance.\n        \"\"\"\n        return cls(value)\n\n    def __add__(self, other):\n        return TimeMs(int(self) + int(other))\n\n    def __radd__(self, other):\n        return TimeMs(int(other) + int(self))\n\n    def __sub__(self, other):\n        return TimeMs(int(self) - int(other))\n\n    def __rsub__(self, other):\n        return TimeMs(int(self) - int(other))\n\n    def __repr__(self) -&gt; str:\n        return f\"TimeMs({self.to_seconds():.3f}s)\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs.__add__","title":"<code>__add__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __add__(self, other):\n    return TimeMs(int(self) + int(other))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs.__get_pydantic_core_schema__","title":"<code>__get_pydantic_core_schema__(source_type, handler)</code>  <code>classmethod</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(cls, source_type, handler: GetCoreSchemaHandler):\n    return core_schema.with_info_plain_validator_function(\n        cls._validate,\n        serialization=core_schema.plain_serializer_function_ser_schema(lambda v: int(v)),\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs.__new__","title":"<code>__new__(ms)</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __new__(cls, ms: Union[int, float, \"TimeMs\"]):\n    if isinstance(ms, TimeMs):\n        value = int(ms)\n    elif isinstance(ms, (int, float)):\n        if not math.isfinite(ms):\n            raise ValueError(\"ms must be a finite number\")\n        value = round(ms)\n    else:\n        raise TypeError(f\"ms must be a number or TimeMs, got {type(ms).__name__}\")\n    return int.__new__(cls, value)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs.__radd__","title":"<code>__radd__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __radd__(self, other):\n    return TimeMs(int(other) + int(self))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs.__repr__","title":"<code>__repr__()</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"TimeMs({self.to_seconds():.3f}s)\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs.__rsub__","title":"<code>__rsub__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __rsub__(self, other):\n    return TimeMs(int(self) - int(other))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs.__sub__","title":"<code>__sub__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __sub__(self, other):\n    return TimeMs(int(self) - int(other))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs.from_seconds","title":"<code>from_seconds(seconds)</code>  <code>classmethod</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>@classmethod\ndef from_seconds(cls, seconds: int | float) -&gt; \"TimeMs\":\n    return cls(round(seconds * 1000))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs.to_ms","title":"<code>to_ms()</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def to_ms(self) -&gt; int:\n    return int(self)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeMs.to_seconds","title":"<code>to_seconds()</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def to_seconds(self) -&gt; float:\n    return float(self) / 1000\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeProgress","title":"<code>TimeProgress</code>","text":"<p>A context manager for a time-based progress display using dots.</p> <p>The display updates once per second, printing a dot and showing: - Expected time (if provided) - Elapsed time (always displayed)</p> <p>Example:</p> <p>import time with ExpectedTimeProgress(expected_time=60, desc=\"Transcribing...\"): ...     time.sleep(5)  # Simulate work [Expected Time: 1:00, Elapsed Time: 0:05] .....</p> <p>Parameters:</p> Name Type Description Default <code>expected_time</code> <code>Optional[float]</code> <p>Expected time in seconds. Optional.</p> <code>None</code> <code>display_interval</code> <code>float</code> <p>How often to print a dot (seconds).</p> <code>1.0</code> <code>desc</code> <code>str</code> <p>Description to display alongside the progress.</p> <code>''</code> Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>class TimeProgress:\n    \"\"\"\n    A context manager for a time-based progress display using dots.\n\n    The display updates once per second, printing a dot and showing:\n    - Expected time (if provided)\n    - Elapsed time (always displayed)\n\n    Example:\n    &gt;&gt;&gt; import time\n    &gt;&gt;&gt; with ExpectedTimeProgress(expected_time=60, desc=\"Transcribing...\"):\n    ...     time.sleep(5)  # Simulate work\n    [Expected Time: 1:00, Elapsed Time: 0:05] .....\n\n    Args:\n        expected_time (Optional[float]): Expected time in seconds. Optional.\n        display_interval (float): How often to print a dot (seconds).\n        desc (str): Description to display alongside the progress.\n    \"\"\"\n\n    def __init__(\n        self,\n        expected_time: Optional[float] = None,\n        display_interval: float = 1.0,\n        desc: str = \"\",\n    ):\n        self.expected_time = expected_time\n        self.display_interval = display_interval\n        self._stop_event = threading.Event()\n        self._start_time = None\n        self._thread = None\n        self.desc = desc\n        self._last_length = 0  # To keep track of the last printed line length\n\n    def __enter__(self):\n        # Record the start time\n        self._start_time = time.time()\n\n        # Spawn the background thread\n        self._thread = threading.Thread(target=self._print_progress, daemon=True)\n        self._thread.start()\n\n        return self\n\n    def _print_progress(self):\n        \"\"\"\n        Continuously prints progress alternating between | and \u2014 along with elapsed/expected time.\n        \"\"\"\n        symbols = [\"|\", \"/\", \"\u2014\", \"\\\\\"]  # Symbols to alternate between\n        symbol_index = 0  # Keep track of the current symbol\n\n        while not self._stop_event.is_set():\n            elapsed = time.time() - self._start_time\n\n            # Format elapsed time as mm:ss\n            elapsed_str = self._format_time(elapsed)\n\n            # Format expected time if provided\n            if self.expected_time is not None:\n                expected_str = self._format_time(self.expected_time)\n                header = f\"{self.desc} [Expected Time: {expected_str}, Elapsed Time: {elapsed_str}]\"\n            else:\n                header = f\"{self.desc} [Elapsed Time: {elapsed_str}]\"\n\n            # Get the current symbol for the spinner\n            spinner = symbols[symbol_index]\n\n            # Construct the line with the spinner\n            line = f\"\\r{header} {spinner}\"\n\n            # Write to stdout\n            sys.stdout.write(line)\n            sys.stdout.flush()\n\n            # Update the symbol index to alternate\n            symbol_index = (symbol_index + 1) % len(symbols)\n\n            # Sleep before next update\n            time.sleep(self.display_interval)\n\n        # Clear the spinner after finishing\n        sys.stdout.write(\"\\r\" + \" \" * len(line) + \"\\r\")\n        sys.stdout.flush()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # Signal the thread to stop\n        self._stop_event.set()\n        self._thread.join()\n\n        # Final elapsed time\n        elapsed = time.time() - self._start_time\n        elapsed_str = self._format_time(elapsed)\n\n        # Construct the final line\n        if self.expected_time is not None:\n            expected_str = self._format_time(self.expected_time)\n            final_header = f\"{self.desc} [Expected Time: {expected_str}, Elapsed Time: {elapsed_str}]\"\n        else:\n            final_header = f\"{self.desc} [Elapsed Time: {elapsed_str}]\"\n\n        # Final dots\n        final_line = f\"\\r{final_header}\"\n\n        # Clear the line and move to the next line\n        padding = \" \" * max(self._last_length - len(final_line), 0)\n        sys.stdout.write(final_line + padding + \"\\n\")\n        sys.stdout.flush()\n\n    @staticmethod\n    def _format_time(seconds: float) -&gt; str:\n        \"\"\"\n        Converts seconds to a formatted string (mm:ss).\n        \"\"\"\n        minutes = int(seconds // 60)\n        seconds = int(seconds % 60)\n        return f\"{minutes}:{seconds:02}\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeProgress.desc","title":"<code>desc = desc</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.TimeProgress.display_interval","title":"<code>display_interval = display_interval</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.TimeProgress.expected_time","title":"<code>expected_time = expected_time</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.TimeProgress.__enter__","title":"<code>__enter__()</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __enter__(self):\n    # Record the start time\n    self._start_time = time.time()\n\n    # Spawn the background thread\n    self._thread = threading.Thread(target=self._print_progress, daemon=True)\n    self._thread.start()\n\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeProgress.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    # Signal the thread to stop\n    self._stop_event.set()\n    self._thread.join()\n\n    # Final elapsed time\n    elapsed = time.time() - self._start_time\n    elapsed_str = self._format_time(elapsed)\n\n    # Construct the final line\n    if self.expected_time is not None:\n        expected_str = self._format_time(self.expected_time)\n        final_header = f\"{self.desc} [Expected Time: {expected_str}, Elapsed Time: {elapsed_str}]\"\n    else:\n        final_header = f\"{self.desc} [Elapsed Time: {elapsed_str}]\"\n\n    # Final dots\n    final_line = f\"\\r{final_header}\"\n\n    # Clear the line and move to the next line\n    padding = \" \" * max(self._last_length - len(final_line), 0)\n    sys.stdout.write(final_line + padding + \"\\n\")\n    sys.stdout.flush()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.TimeProgress.__init__","title":"<code>__init__(expected_time=None, display_interval=1.0, desc='')</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __init__(\n    self,\n    expected_time: Optional[float] = None,\n    display_interval: float = 1.0,\n    desc: str = \"\",\n):\n    self.expected_time = expected_time\n    self.display_interval = display_interval\n    self._stop_event = threading.Event()\n    self._start_time = None\n    self._thread = None\n    self.desc = desc\n    self._last_length = 0  # To keep track of the last printed line length\n</code></pre>"},{"location":"api/#tnh_scholar.utils.check_ocr_env","title":"<code>check_ocr_env(output=True)</code>","text":"<p>Check OCR processing requirements.</p> Source code in <code>src/tnh_scholar/utils/validate.py</code> <pre><code>def check_ocr_env(output: bool = True) -&gt; bool:\n    \"\"\"Check OCR processing requirements.\"\"\"\n    return check_env(OCR_ENV_VARS, \"OCR processing\", output=output)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.check_openai_env","title":"<code>check_openai_env(output=True)</code>","text":"<p>Check OpenAI API requirements.</p> Source code in <code>src/tnh_scholar/utils/validate.py</code> <pre><code>def check_openai_env(output: bool = True) -&gt; bool:\n    \"\"\"Check OpenAI API requirements.\"\"\"\n    return check_env(OPENAI_ENV_VARS, \"OpenAI API access\", output=output)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.convert_ms_to_sec","title":"<code>convert_ms_to_sec(ms)</code>","text":"<p>Convert time from milliseconds (int) to seconds (float).</p> Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def convert_ms_to_sec(ms: int) -&gt; float:\n    \"\"\"Convert time from milliseconds (int) to seconds (float).\"\"\"\n    return float(ms / 1000)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.convert_sec_to_ms","title":"<code>convert_sec_to_ms(val)</code>","text":"<p>Convert seconds to milliseconds, rounding to the nearest integer.</p> Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def convert_sec_to_ms(val: float) -&gt; int:\n    \"\"\" \n    Convert seconds to milliseconds, rounding to the nearest integer.\n    \"\"\"\n    return round(val * 1000)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.copy_files_with_regex","title":"<code>copy_files_with_regex(source_dir, destination_dir, regex_patterns, preserve_structure=True)</code>","text":"<p>Copies files from subdirectories one level down in the source directory to  the destination directory if they match any regex pattern. Optionally preserves the  directory structure.</p> <p>Parameters:</p> Name Type Description Default <code>source_dir</code> <code>Path</code> <p>Path to the source directory to search files in.</p> required <code>destination_dir</code> <code>Path</code> <p>Path to the destination directory where files will be  copied.</p> required <code>regex_patterns</code> <code>list[str]</code> <p>List of regex patterns to match file names.</p> required <code>preserve_structure</code> <code>bool</code> <p>Whether to preserve the directory structure.  Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source directory does not exist or is not a directory.</p> Example <p>copy_files_with_regex( ...     source_dir=Path(\"/path/to/source\"), ...     destination_dir=Path(\"/path/to/destination\"), ...     regex_patterns=[r'.*.txt\\(', r'.*\\.log\\)'], ...     preserve_structure=True ... )</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def copy_files_with_regex(\n    source_dir: Path,\n    destination_dir: Path,\n    regex_patterns: list[str],\n    preserve_structure: bool = True,\n) -&gt; None:\n    \"\"\"\n    Copies files from subdirectories one level down in the source directory to \n    the destination directory if they match any regex pattern. Optionally preserves the \n    directory structure.\n\n    Args:\n        source_dir (Path): Path to the source directory to search files in.\n        destination_dir (Path): Path to the destination directory where files will be \n            copied.\n        regex_patterns (list[str]): List of regex patterns to match file names.\n        preserve_structure (bool): Whether to preserve the directory structure. \n            Defaults to True.\n\n    Raises:\n        ValueError: If the source directory does not exist or is not a directory.\n\n    Example:\n        &gt;&gt;&gt; copy_files_with_regex(\n        ...     source_dir=Path(\"/path/to/source\"),\n        ...     destination_dir=Path(\"/path/to/destination\"),\n        ...     regex_patterns=[r'.*\\\\.txt$', r'.*\\\\.log$'],\n        ...     preserve_structure=True\n        ... )\n    \"\"\"\n    if not source_dir.is_dir():\n        raise ValueError(\n            f\"The source directory {source_dir} does not exist or is not a directory.\"\n        )\n\n    if not destination_dir.exists():\n        destination_dir.mkdir(parents=True, exist_ok=True)\n\n    # Compile regex patterns for efficiency\n    compiled_patterns = [re.compile(pattern) for pattern in regex_patterns]\n\n    # Process only one level down\n    for subdir in source_dir.iterdir():\n        if subdir.is_dir():  # Only process subdirectories\n            print(f\"processing {subdir}:\")\n            for file_path in subdir.iterdir():  # Only files in this subdirectory\n                if file_path.is_file():\n                    print(f\"checking file: {file_path.name}\")\n                    # Check if the file matches any of the regex patterns\n                    if any(\n                        pattern.match(file_path.name) for pattern in compiled_patterns\n                    ):\n                        if preserve_structure:\n                            # Construct the target path, preserving relative structure\n                            relative_path = (\n                                subdir.relative_to(source_dir) / file_path.name\n                            )\n                            target_path = destination_dir / relative_path\n                            target_path.parent.mkdir(parents=True, exist_ok=True)\n                        else:\n                            # Put directly in destination without subdirectory structure\n                            target_path = destination_dir / file_path.name\n\n                        shutil.copy2(file_path, target_path)\n                        print(f\"Copied: {file_path} -&gt; {target_path}\")\n</code></pre>"},{"location":"api/#tnh_scholar.utils.ensure_directory_exists","title":"<code>ensure_directory_exists(dir_path)</code>","text":"<p>Create directory if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Path</code> <p>Directory path to ensure exists.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the directory exists or was created successfully, False otherwise.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def ensure_directory_exists(dir_path: Path) -&gt; bool:\n    \"\"\"\n    Create directory if it doesn't exist.\n\n    Args:\n        dir_path (Path): Directory path to ensure exists.\n\n    Returns:\n        bool: True if the directory exists or was created successfully, False otherwise.\n    \"\"\"\n    # No exception handling here. \n    # If exceptions occur let them propagate. \n    # Prototype code.\n\n    dir_path.mkdir(parents=True, exist_ok=True)\n    return True\n</code></pre>"},{"location":"api/#tnh_scholar.utils.ensure_directory_writable","title":"<code>ensure_directory_writable(dir_path)</code>","text":"<p>Ensure the directory exists and is writable. Creates the directory if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Path</code> <p>Directory to verify or create.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the directory cannot be created or is not writable.</p> <code>TypeError</code> <p>If the provided path is not a Path instance.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def ensure_directory_writable(dir_path: Path) -&gt; None:\n    \"\"\"\n    Ensure the directory exists and is writable.\n    Creates the directory if it does not exist.\n\n    Args:\n        dir_path (Path): Directory to verify or create.\n\n    Raises:\n        ValueError: If the directory cannot be created or is not writable.\n        TypeError: If the provided path is not a Path instance.\n    \"\"\"\n    if not isinstance(dir_path, Path):\n        raise TypeError(\"dir_path must be a pathlib.Path instance\")\n\n    # Ensure directory exists first\n    ensure_directory_exists(dir_path)\n\n    # Check writability safely using NamedTemporaryFile\n    try:\n        with tempfile.NamedTemporaryFile(dir=dir_path, prefix=\".writability_check_\", delete=True) as tmp:\n            tmp.write(b\"test\")\n            tmp.flush()\n    except Exception as e:\n        raise ValueError(f\"Directory is not writable: {dir_path}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.utils.get_language_code_from_text","title":"<code>get_language_code_from_text(text)</code>","text":"<p>Detect the language of the provided text using langdetect.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to analyze</p> <pre><code>      code or 'name' for full English language name\n</code></pre> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>return result 'code' ISO 639-1 for detected language.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text is empty or invalid</p> Source code in <code>src/tnh_scholar/utils/lang.py</code> <pre><code>def get_language_code_from_text(text: str) -&gt; str:\n    \"\"\"\n    Detect the language of the provided text using langdetect.\n\n    Args:\n        text: Text to analyze\n\n                      code or 'name' for full English language name\n\n    Returns:\n        str: return result 'code' ISO 639-1 for detected language.\n\n    Raises:\n        ValueError: If text is empty or invalid\n    \"\"\"\n\n    if not text or text.isspace():\n        raise ValueError(\"Input text cannot be empty\")\n\n    sample = _get_sample_text(text)\n\n    try:\n        return detect(sample)\n    except LangDetectException:\n        logger.warning(\"Language could not be detected in get_language().\")\n        return \"un\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.get_language_from_code","title":"<code>get_language_from_code(code)</code>","text":"Source code in <code>src/tnh_scholar/utils/lang.py</code> <pre><code>def get_language_from_code(code: str):\n    if language := pycountry.languages.get(alpha_2=code):\n        return language.name\n    logger.warning(f\"No language name found for code: {code}\")\n    return \"Unknown\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.get_language_name_from_text","title":"<code>get_language_name_from_text(text)</code>","text":"Source code in <code>src/tnh_scholar/utils/lang.py</code> <pre><code>def get_language_name_from_text(text: str) -&gt; str:\n    return get_language_from_code(get_language_code_from_text(text))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.get_user_confirmation","title":"<code>get_user_confirmation(prompt, default=True)</code>","text":"<p>Prompt the user for a yes/no confirmation with single-character input. Cross-platform implementation. Returns True if 'y' is entered, and False if 'n' Allows for default value if return is entered.</p> <p>Example usage     if get_user_confirmation(\"Do you want to continue\"):         print(\"Continuing...\")     else:         print(\"Exiting...\")</p> Source code in <code>src/tnh_scholar/utils/user_io_utils.py</code> <pre><code>def get_user_confirmation(prompt: str, default: bool = True) -&gt; bool:\n    \"\"\"\n    Prompt the user for a yes/no confirmation with single-character input.\n    Cross-platform implementation. Returns True if 'y' is entered, and False if 'n'\n    Allows for default value if return is entered.\n\n    Example usage\n        if get_user_confirmation(\"Do you want to continue\"):\n            print(\"Continuing...\")\n        else:\n            print(\"Exiting...\")\n    \"\"\"\n    print(f\"{prompt} \", end=\"\", flush=True)\n\n    while True:\n        char = get_single_char().lower()\n        if char == \"y\":\n            print(char)  # Echo the choice\n            return True\n        elif char == \"n\":\n            print(char)\n            return False\n        elif char in (\"\\r\", \"\\n\"):  # Enter key (use default)\n            print()  # Add a newline\n            return default\n        else:\n            print(\n                f\"\\nInvalid input: {char}. Please type 'y' or 'n': \", end=\"\", flush=True\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.utils.iterate_subdir","title":"<code>iterate_subdir(directory, recursive=False)</code>","text":"<p>Iterates through subdirectories in the given directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>The root directory to start the iteration.</p> required <code>recursive</code> <code>bool</code> <p>If True, iterates recursively through all subdirectories.               If False, iterates only over the immediate subdirectories.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Paths to each subdirectory.</p> Example <p>for subdir in iterate_subdir(Path('/root'), recursive=False): ...     print(subdir)</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def iterate_subdir(\n    directory: Path, recursive: bool = False\n) -&gt; Generator[Path, None, None]:\n    \"\"\"\n    Iterates through subdirectories in the given directory.\n\n    Args:\n        directory (Path): The root directory to start the iteration.\n        recursive (bool): If True, iterates recursively through all subdirectories.\n                          If False, iterates only over the immediate subdirectories.\n\n    Yields:\n        Path: Paths to each subdirectory.\n\n    Example:\n        &gt;&gt;&gt; for subdir in iterate_subdir(Path('/root'), recursive=False):\n        ...     print(subdir)\n    \"\"\"\n    if recursive:\n        for subdirectory in directory.rglob(\"*\"):\n            if subdirectory.is_dir():\n                yield subdirectory\n    else:\n        for subdirectory in directory.iterdir():\n            if subdirectory.is_dir():\n                yield subdirectory\n</code></pre>"},{"location":"api/#tnh_scholar.utils.load_json_into_model","title":"<code>load_json_into_model(file, model)</code>","text":"<p>Loads a JSON file and validates it against a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the JSON file.</p> required <code>model</code> <code>type[BaseModel]</code> <p>The Pydantic model to validate against.</p> required <p>Returns:</p> Name Type Description <code>BaseModel</code> <code>BaseModel</code> <p>An instance of the validated Pydantic model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file content is invalid JSON or does not match the model.</p> <p>Example:     class ExampleModel(BaseModel):     name: str     age: int     city: str</p> <pre><code>if __name__ == \"__main__\":\n    json_file = Path(\"example.json\")\n    try:\n        data = load_json_into_model(json_file, ExampleModel)\n        print(data)\n    except ValueError as e:\n        print(e)\n</code></pre> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def load_json_into_model(file: Path, model: type[BaseModel]) -&gt; BaseModel:\n    \"\"\"\n    Loads a JSON file and validates it against a Pydantic model.\n\n    Args:\n        file (Path): Path to the JSON file.\n        model (type[BaseModel]): The Pydantic model to validate against.\n\n    Returns:\n        BaseModel: An instance of the validated Pydantic model.\n\n    Raises:\n        ValueError: If the file content is invalid JSON or does not match the model.\n    Example:\n        class ExampleModel(BaseModel):\n        name: str\n        age: int\n        city: str\n\n        if __name__ == \"__main__\":\n            json_file = Path(\"example.json\")\n            try:\n                data = load_json_into_model(json_file, ExampleModel)\n                print(data)\n            except ValueError as e:\n                print(e)\n    \"\"\"\n    try:\n        with file.open(\"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return model(**data)\n    except (json.JSONDecodeError, ValidationError) as e:\n        raise ValueError(f\"Error loading or validating JSON file '{file}': {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.utils.load_jsonl_to_dict","title":"<code>load_jsonl_to_dict(file_path)</code>","text":"<p>Load a JSONL file into a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the JSONL file.</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: A list of dictionaries, each representing a line in the JSONL file.</p> Example <p>from pathlib import Path file_path = Path(\"data.jsonl\") data = load_jsonl_to_dict(file_path) print(data) [{'key1': 'value1'}, {'key2': 'value2'}]</p> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def load_jsonl_to_dict(file_path: Path) -&gt; List[Dict]:\n    \"\"\"\n    Load a JSONL file into a list of dictionaries.\n\n    Args:\n        file_path (Path): Path to the JSONL file.\n\n    Returns:\n        List[Dict]: A list of dictionaries, each representing a line in the JSONL file.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; file_path = Path(\"data.jsonl\")\n        &gt;&gt;&gt; data = load_jsonl_to_dict(file_path)\n        &gt;&gt;&gt; print(data)\n        [{'key1': 'value1'}, {'key2': 'value2'}]\n    \"\"\"\n    with file_path.open(\"r\", encoding=\"utf-8\") as file:\n        return [json.loads(line.strip()) for line in file if line.strip()]\n</code></pre>"},{"location":"api/#tnh_scholar.utils.path_as_str","title":"<code>path_as_str(path)</code>","text":"Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def path_as_str(path: Path) -&gt; str:\n    return str(path.resolve())\n</code></pre>"},{"location":"api/#tnh_scholar.utils.read_str_from_file","title":"<code>read_str_from_file(file_path)</code>","text":"<p>Reads the entire content of a text file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the text file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The content of the text file as a single string.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def read_str_from_file(file_path: Path) -&gt; str:\n    \"\"\"Reads the entire content of a text file.\n\n    Args:\n        file_path: The path to the text file.\n\n    Returns:\n        The content of the text file as a single string.\n    \"\"\"\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        return file.read()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.sanitize_filename","title":"<code>sanitize_filename(filename, max_length=DEFAULT_MAX_FILENAME_LENGTH)</code>","text":"<p>Sanitize filename for use unix use.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def sanitize_filename(\n    filename: str, \n    max_length: int = DEFAULT_MAX_FILENAME_LENGTH\n    ) -&gt; str:  \n    \"\"\"Sanitize filename for use unix use.\"\"\"\n\n    # Normalize Unicode to remove accents and convert to ASCII\n    clean = (\n        unicodedata.normalize(\n            \"NFKD\", \n            filename).encode(\n                \"ascii\", \n                \"ignore\").decode(\"ascii\")\n    )\n\n    clean = clean.lower()\n    clean = re.sub(r\"[^a-z0-9\\s]\", \" \", clean.strip())\n    clean = clean.strip()\n\n    # shorten\n    clean = clean[:max_length].strip() \n\n    # convert spaces to _\n    clean = re.sub(r\"\\s+\", \"_\", clean)\n\n    return clean\n</code></pre>"},{"location":"api/#tnh_scholar.utils.save_model_to_json","title":"<code>save_model_to_json(file, model, indent=4, ensure_ascii=False)</code>","text":"<p>Saves a Pydantic model to a JSON file, formatted with indentation for readability.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the JSON file where the model will be saved.</p> required <code>model</code> <code>BaseModel</code> <p>The Pydantic model instance to save.</p> required <code>indent</code> <code>int</code> <p>Number of spaces for JSON indentation. Defaults to 4.</p> <code>4</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to escape non-ASCII characters. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model cannot be serialized to JSON.</p> <code>IOError</code> <p>If there is an issue writing to the file.</p> Example <p>class ExampleModel(BaseModel):     name: str     age: int</p> <p>if name == \"main\":     model_instance = ExampleModel(name=\"John\", age=30)     json_file = Path(\"example.json\")     try:         save_model_to_json(json_file, model_instance)         print(f\"Model saved to {json_file}\")     except (ValueError, IOError) as e:         print(e)</p> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def save_model_to_json(\n    file: Path, model: BaseModel, indent: int = 4, ensure_ascii: bool = False\n) -&gt; None:\n    \"\"\"\n    Saves a Pydantic model to a JSON file, formatted with indentation for readability.\n\n    Args:\n        file (Path): Path to the JSON file where the model will be saved.\n        model (BaseModel): The Pydantic model instance to save.\n        indent (int): Number of spaces for JSON indentation. Defaults to 4.\n        ensure_ascii (bool): Whether to escape non-ASCII characters. Defaults to False.\n\n    Raises:\n        ValueError: If the model cannot be serialized to JSON.\n        IOError: If there is an issue writing to the file.\n\n    Example:\n        class ExampleModel(BaseModel):\n            name: str\n            age: int\n\n        if __name__ == \"__main__\":\n            model_instance = ExampleModel(name=\"John\", age=30)\n            json_file = Path(\"example.json\")\n            try:\n                save_model_to_json(json_file, model_instance)\n                print(f\"Model saved to {json_file}\")\n            except (ValueError, IOError) as e:\n                print(e)\n    \"\"\"\n    try:\n        # Serialize model to JSON string\n        model_dict = model.model_dump()\n    except TypeError as e:\n        raise ValueError(f\"Error serializing model to JSON: {e}\") from e\n\n    # Write the JSON string to the file\n    write_data_to_json_file(file, model_dict, indent=indent, ensure_ascii=ensure_ascii)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.to_slug","title":"<code>to_slug(string)</code>","text":"<p>Slugify a Unicode string.</p> <p>Converts a string to a strict URL-friendly slug format, allowing only lowercase letters, digits, and hyphens.</p> Example <p>slugify(\"H\u00e9ll\u00f8_W\u00f6rld!\") 'hello-world'</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def to_slug(string: str) -&gt; str:\n    \"\"\"\n    Slugify a Unicode string.\n\n    Converts a string to a strict URL-friendly slug format,\n    allowing only lowercase letters, digits, and hyphens.\n\n    Example:\n        &gt;&gt;&gt; slugify(\"H\u00e9ll\u00f8_W\u00f6rld!\")\n        'hello-world'\n    \"\"\"\n    # Normalize Unicode to remove accents and convert to ASCII\n    string = (\n        unicodedata.normalize(\"NFKD\", string).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    )\n\n    # Replace all non-alphanumeric characters with spaces (only keep a-z and 0-9)\n    string = re.sub(r\"[^a-z0-9\\s]\", \" \", string.lower().strip())\n\n    # Replace any sequence of spaces with a single hyphen\n    return re.sub(r\"\\s+\", \"-\", string)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.write_str_to_file","title":"<code>write_str_to_file(file_path, text, overwrite=False)</code>","text":"<p>Writes text to a file with file locking.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>PathLike</code> <p>The path to the file to write.</p> required <code>text</code> <code>str</code> <p>The text to write to the file.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it exists.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file exists and overwrite is False.</p> <code>OSError</code> <p>If there's an issue with file locking or writing.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def write_str_to_file(file_path: PathLike, text: str, overwrite: bool = False):\n    \"\"\"Writes text to a file with file locking.\n\n    Args:\n        file_path: The path to the file to write.\n        text: The text to write to the file.\n        overwrite: Whether to overwrite the file if it exists.\n\n    Raises:\n        FileExistsError: If the file exists and overwrite is False.\n        OSError: If there's an issue with file locking or writing.\n    \"\"\"\n    file_path = Path(file_path)\n\n    if file_path.exists() and not overwrite:\n        raise FileExistsError(f\"File already exists: {file_path}\")\n\n    try:\n        with file_path.open(\"w\", encoding=\"utf-8\") as f:\n            fcntl.flock(f, fcntl.LOCK_EX)\n            f.write(text)\n            fcntl.flock(f, fcntl.LOCK_UN)  # Release lock\n    except OSError as e:\n        raise OSError(f\"Error writing to or locking file {file_path}: {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils","title":"<code>file_utils</code>","text":""},{"location":"api/#tnh_scholar.utils.file_utils.DEFAULT_MAX_FILENAME_LENGTH","title":"<code>DEFAULT_MAX_FILENAME_LENGTH = 25</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.file_utils.PathLike","title":"<code>PathLike = Union[str, Path]</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.file_utils.__all__","title":"<code>__all__ = ['DEFAULT_MAX_FILENAME_LENGTH', 'FileExistsWarning', 'ensure_directory_exists', 'ensure_directory_writable', 'iterate_subdir', 'path_source_str', 'copy_files_with_regex', 'read_str_from_file', 'write_str_to_file', 'sanitize_filename', 'to_slug', 'path_as_str']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.file_utils.FileExistsWarning","title":"<code>FileExistsWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>class FileExistsWarning(UserWarning):\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.copy_files_with_regex","title":"<code>copy_files_with_regex(source_dir, destination_dir, regex_patterns, preserve_structure=True)</code>","text":"<p>Copies files from subdirectories one level down in the source directory to  the destination directory if they match any regex pattern. Optionally preserves the  directory structure.</p> <p>Parameters:</p> Name Type Description Default <code>source_dir</code> <code>Path</code> <p>Path to the source directory to search files in.</p> required <code>destination_dir</code> <code>Path</code> <p>Path to the destination directory where files will be  copied.</p> required <code>regex_patterns</code> <code>list[str]</code> <p>List of regex patterns to match file names.</p> required <code>preserve_structure</code> <code>bool</code> <p>Whether to preserve the directory structure.  Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source directory does not exist or is not a directory.</p> Example <p>copy_files_with_regex( ...     source_dir=Path(\"/path/to/source\"), ...     destination_dir=Path(\"/path/to/destination\"), ...     regex_patterns=[r'.*.txt\\(', r'.*\\.log\\)'], ...     preserve_structure=True ... )</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def copy_files_with_regex(\n    source_dir: Path,\n    destination_dir: Path,\n    regex_patterns: list[str],\n    preserve_structure: bool = True,\n) -&gt; None:\n    \"\"\"\n    Copies files from subdirectories one level down in the source directory to \n    the destination directory if they match any regex pattern. Optionally preserves the \n    directory structure.\n\n    Args:\n        source_dir (Path): Path to the source directory to search files in.\n        destination_dir (Path): Path to the destination directory where files will be \n            copied.\n        regex_patterns (list[str]): List of regex patterns to match file names.\n        preserve_structure (bool): Whether to preserve the directory structure. \n            Defaults to True.\n\n    Raises:\n        ValueError: If the source directory does not exist or is not a directory.\n\n    Example:\n        &gt;&gt;&gt; copy_files_with_regex(\n        ...     source_dir=Path(\"/path/to/source\"),\n        ...     destination_dir=Path(\"/path/to/destination\"),\n        ...     regex_patterns=[r'.*\\\\.txt$', r'.*\\\\.log$'],\n        ...     preserve_structure=True\n        ... )\n    \"\"\"\n    if not source_dir.is_dir():\n        raise ValueError(\n            f\"The source directory {source_dir} does not exist or is not a directory.\"\n        )\n\n    if not destination_dir.exists():\n        destination_dir.mkdir(parents=True, exist_ok=True)\n\n    # Compile regex patterns for efficiency\n    compiled_patterns = [re.compile(pattern) for pattern in regex_patterns]\n\n    # Process only one level down\n    for subdir in source_dir.iterdir():\n        if subdir.is_dir():  # Only process subdirectories\n            print(f\"processing {subdir}:\")\n            for file_path in subdir.iterdir():  # Only files in this subdirectory\n                if file_path.is_file():\n                    print(f\"checking file: {file_path.name}\")\n                    # Check if the file matches any of the regex patterns\n                    if any(\n                        pattern.match(file_path.name) for pattern in compiled_patterns\n                    ):\n                        if preserve_structure:\n                            # Construct the target path, preserving relative structure\n                            relative_path = (\n                                subdir.relative_to(source_dir) / file_path.name\n                            )\n                            target_path = destination_dir / relative_path\n                            target_path.parent.mkdir(parents=True, exist_ok=True)\n                        else:\n                            # Put directly in destination without subdirectory structure\n                            target_path = destination_dir / file_path.name\n\n                        shutil.copy2(file_path, target_path)\n                        print(f\"Copied: {file_path} -&gt; {target_path}\")\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.ensure_directory_exists","title":"<code>ensure_directory_exists(dir_path)</code>","text":"<p>Create directory if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Path</code> <p>Directory path to ensure exists.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the directory exists or was created successfully, False otherwise.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def ensure_directory_exists(dir_path: Path) -&gt; bool:\n    \"\"\"\n    Create directory if it doesn't exist.\n\n    Args:\n        dir_path (Path): Directory path to ensure exists.\n\n    Returns:\n        bool: True if the directory exists or was created successfully, False otherwise.\n    \"\"\"\n    # No exception handling here. \n    # If exceptions occur let them propagate. \n    # Prototype code.\n\n    dir_path.mkdir(parents=True, exist_ok=True)\n    return True\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.ensure_directory_writable","title":"<code>ensure_directory_writable(dir_path)</code>","text":"<p>Ensure the directory exists and is writable. Creates the directory if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Path</code> <p>Directory to verify or create.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the directory cannot be created or is not writable.</p> <code>TypeError</code> <p>If the provided path is not a Path instance.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def ensure_directory_writable(dir_path: Path) -&gt; None:\n    \"\"\"\n    Ensure the directory exists and is writable.\n    Creates the directory if it does not exist.\n\n    Args:\n        dir_path (Path): Directory to verify or create.\n\n    Raises:\n        ValueError: If the directory cannot be created or is not writable.\n        TypeError: If the provided path is not a Path instance.\n    \"\"\"\n    if not isinstance(dir_path, Path):\n        raise TypeError(\"dir_path must be a pathlib.Path instance\")\n\n    # Ensure directory exists first\n    ensure_directory_exists(dir_path)\n\n    # Check writability safely using NamedTemporaryFile\n    try:\n        with tempfile.NamedTemporaryFile(dir=dir_path, prefix=\".writability_check_\", delete=True) as tmp:\n            tmp.write(b\"test\")\n            tmp.flush()\n    except Exception as e:\n        raise ValueError(f\"Directory is not writable: {dir_path}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.iterate_subdir","title":"<code>iterate_subdir(directory, recursive=False)</code>","text":"<p>Iterates through subdirectories in the given directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>The root directory to start the iteration.</p> required <code>recursive</code> <code>bool</code> <p>If True, iterates recursively through all subdirectories.               If False, iterates only over the immediate subdirectories.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Paths to each subdirectory.</p> Example <p>for subdir in iterate_subdir(Path('/root'), recursive=False): ...     print(subdir)</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def iterate_subdir(\n    directory: Path, recursive: bool = False\n) -&gt; Generator[Path, None, None]:\n    \"\"\"\n    Iterates through subdirectories in the given directory.\n\n    Args:\n        directory (Path): The root directory to start the iteration.\n        recursive (bool): If True, iterates recursively through all subdirectories.\n                          If False, iterates only over the immediate subdirectories.\n\n    Yields:\n        Path: Paths to each subdirectory.\n\n    Example:\n        &gt;&gt;&gt; for subdir in iterate_subdir(Path('/root'), recursive=False):\n        ...     print(subdir)\n    \"\"\"\n    if recursive:\n        for subdirectory in directory.rglob(\"*\"):\n            if subdirectory.is_dir():\n                yield subdirectory\n    else:\n        for subdirectory in directory.iterdir():\n            if subdirectory.is_dir():\n                yield subdirectory\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.path_as_str","title":"<code>path_as_str(path)</code>","text":"Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def path_as_str(path: Path) -&gt; str:\n    return str(path.resolve())\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.path_source_str","title":"<code>path_source_str(path)</code>","text":"Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def path_source_str(path: Path):\n    return str(path.resolve())\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.read_str_from_file","title":"<code>read_str_from_file(file_path)</code>","text":"<p>Reads the entire content of a text file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the text file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The content of the text file as a single string.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def read_str_from_file(file_path: Path) -&gt; str:\n    \"\"\"Reads the entire content of a text file.\n\n    Args:\n        file_path: The path to the text file.\n\n    Returns:\n        The content of the text file as a single string.\n    \"\"\"\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        return file.read()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.sanitize_filename","title":"<code>sanitize_filename(filename, max_length=DEFAULT_MAX_FILENAME_LENGTH)</code>","text":"<p>Sanitize filename for use unix use.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def sanitize_filename(\n    filename: str, \n    max_length: int = DEFAULT_MAX_FILENAME_LENGTH\n    ) -&gt; str:  \n    \"\"\"Sanitize filename for use unix use.\"\"\"\n\n    # Normalize Unicode to remove accents and convert to ASCII\n    clean = (\n        unicodedata.normalize(\n            \"NFKD\", \n            filename).encode(\n                \"ascii\", \n                \"ignore\").decode(\"ascii\")\n    )\n\n    clean = clean.lower()\n    clean = re.sub(r\"[^a-z0-9\\s]\", \" \", clean.strip())\n    clean = clean.strip()\n\n    # shorten\n    clean = clean[:max_length].strip() \n\n    # convert spaces to _\n    clean = re.sub(r\"\\s+\", \"_\", clean)\n\n    return clean\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.to_slug","title":"<code>to_slug(string)</code>","text":"<p>Slugify a Unicode string.</p> <p>Converts a string to a strict URL-friendly slug format, allowing only lowercase letters, digits, and hyphens.</p> Example <p>slugify(\"H\u00e9ll\u00f8_W\u00f6rld!\") 'hello-world'</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def to_slug(string: str) -&gt; str:\n    \"\"\"\n    Slugify a Unicode string.\n\n    Converts a string to a strict URL-friendly slug format,\n    allowing only lowercase letters, digits, and hyphens.\n\n    Example:\n        &gt;&gt;&gt; slugify(\"H\u00e9ll\u00f8_W\u00f6rld!\")\n        'hello-world'\n    \"\"\"\n    # Normalize Unicode to remove accents and convert to ASCII\n    string = (\n        unicodedata.normalize(\"NFKD\", string).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    )\n\n    # Replace all non-alphanumeric characters with spaces (only keep a-z and 0-9)\n    string = re.sub(r\"[^a-z0-9\\s]\", \" \", string.lower().strip())\n\n    # Replace any sequence of spaces with a single hyphen\n    return re.sub(r\"\\s+\", \"-\", string)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.file_utils.write_str_to_file","title":"<code>write_str_to_file(file_path, text, overwrite=False)</code>","text":"<p>Writes text to a file with file locking.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>PathLike</code> <p>The path to the file to write.</p> required <code>text</code> <code>str</code> <p>The text to write to the file.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it exists.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file exists and overwrite is False.</p> <code>OSError</code> <p>If there's an issue with file locking or writing.</p> Source code in <code>src/tnh_scholar/utils/file_utils.py</code> <pre><code>def write_str_to_file(file_path: PathLike, text: str, overwrite: bool = False):\n    \"\"\"Writes text to a file with file locking.\n\n    Args:\n        file_path: The path to the file to write.\n        text: The text to write to the file.\n        overwrite: Whether to overwrite the file if it exists.\n\n    Raises:\n        FileExistsError: If the file exists and overwrite is False.\n        OSError: If there's an issue with file locking or writing.\n    \"\"\"\n    file_path = Path(file_path)\n\n    if file_path.exists() and not overwrite:\n        raise FileExistsError(f\"File already exists: {file_path}\")\n\n    try:\n        with file_path.open(\"w\", encoding=\"utf-8\") as f:\n            fcntl.flock(f, fcntl.LOCK_EX)\n            f.write(text)\n            fcntl.flock(f, fcntl.LOCK_UN)  # Release lock\n    except OSError as e:\n        raise OSError(f\"Error writing to or locking file {file_path}: {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.utils.json_utils","title":"<code>json_utils</code>","text":""},{"location":"api/#tnh_scholar.utils.json_utils.format_json","title":"<code>format_json(file)</code>","text":"<p>Formats a JSON file with line breaks and indentation for readability.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the JSON file to be formatted.</p> required Example <p>format_json(Path(\"data.json\"))</p> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def format_json(file: Path) -&gt; None:\n    \"\"\"\n    Formats a JSON file with line breaks and indentation for readability.\n\n    Args:\n        file (Path): Path to the JSON file to be formatted.\n\n    Example:\n        format_json(Path(\"data.json\"))\n    \"\"\"\n    with file.open(\"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    with file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=4, ensure_ascii=False)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.json_utils.load_json_into_model","title":"<code>load_json_into_model(file, model)</code>","text":"<p>Loads a JSON file and validates it against a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the JSON file.</p> required <code>model</code> <code>type[BaseModel]</code> <p>The Pydantic model to validate against.</p> required <p>Returns:</p> Name Type Description <code>BaseModel</code> <code>BaseModel</code> <p>An instance of the validated Pydantic model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file content is invalid JSON or does not match the model.</p> <p>Example:     class ExampleModel(BaseModel):     name: str     age: int     city: str</p> <pre><code>if __name__ == \"__main__\":\n    json_file = Path(\"example.json\")\n    try:\n        data = load_json_into_model(json_file, ExampleModel)\n        print(data)\n    except ValueError as e:\n        print(e)\n</code></pre> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def load_json_into_model(file: Path, model: type[BaseModel]) -&gt; BaseModel:\n    \"\"\"\n    Loads a JSON file and validates it against a Pydantic model.\n\n    Args:\n        file (Path): Path to the JSON file.\n        model (type[BaseModel]): The Pydantic model to validate against.\n\n    Returns:\n        BaseModel: An instance of the validated Pydantic model.\n\n    Raises:\n        ValueError: If the file content is invalid JSON or does not match the model.\n    Example:\n        class ExampleModel(BaseModel):\n        name: str\n        age: int\n        city: str\n\n        if __name__ == \"__main__\":\n            json_file = Path(\"example.json\")\n            try:\n                data = load_json_into_model(json_file, ExampleModel)\n                print(data)\n            except ValueError as e:\n                print(e)\n    \"\"\"\n    try:\n        with file.open(\"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return model(**data)\n    except (json.JSONDecodeError, ValidationError) as e:\n        raise ValueError(f\"Error loading or validating JSON file '{file}': {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.utils.json_utils.load_jsonl_to_dict","title":"<code>load_jsonl_to_dict(file_path)</code>","text":"<p>Load a JSONL file into a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the JSONL file.</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: A list of dictionaries, each representing a line in the JSONL file.</p> Example <p>from pathlib import Path file_path = Path(\"data.jsonl\") data = load_jsonl_to_dict(file_path) print(data) [{'key1': 'value1'}, {'key2': 'value2'}]</p> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def load_jsonl_to_dict(file_path: Path) -&gt; List[Dict]:\n    \"\"\"\n    Load a JSONL file into a list of dictionaries.\n\n    Args:\n        file_path (Path): Path to the JSONL file.\n\n    Returns:\n        List[Dict]: A list of dictionaries, each representing a line in the JSONL file.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; file_path = Path(\"data.jsonl\")\n        &gt;&gt;&gt; data = load_jsonl_to_dict(file_path)\n        &gt;&gt;&gt; print(data)\n        [{'key1': 'value1'}, {'key2': 'value2'}]\n    \"\"\"\n    with file_path.open(\"r\", encoding=\"utf-8\") as file:\n        return [json.loads(line.strip()) for line in file if line.strip()]\n</code></pre>"},{"location":"api/#tnh_scholar.utils.json_utils.save_model_to_json","title":"<code>save_model_to_json(file, model, indent=4, ensure_ascii=False)</code>","text":"<p>Saves a Pydantic model to a JSON file, formatted with indentation for readability.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the JSON file where the model will be saved.</p> required <code>model</code> <code>BaseModel</code> <p>The Pydantic model instance to save.</p> required <code>indent</code> <code>int</code> <p>Number of spaces for JSON indentation. Defaults to 4.</p> <code>4</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to escape non-ASCII characters. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model cannot be serialized to JSON.</p> <code>IOError</code> <p>If there is an issue writing to the file.</p> Example <p>class ExampleModel(BaseModel):     name: str     age: int</p> <p>if name == \"main\":     model_instance = ExampleModel(name=\"John\", age=30)     json_file = Path(\"example.json\")     try:         save_model_to_json(json_file, model_instance)         print(f\"Model saved to {json_file}\")     except (ValueError, IOError) as e:         print(e)</p> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def save_model_to_json(\n    file: Path, model: BaseModel, indent: int = 4, ensure_ascii: bool = False\n) -&gt; None:\n    \"\"\"\n    Saves a Pydantic model to a JSON file, formatted with indentation for readability.\n\n    Args:\n        file (Path): Path to the JSON file where the model will be saved.\n        model (BaseModel): The Pydantic model instance to save.\n        indent (int): Number of spaces for JSON indentation. Defaults to 4.\n        ensure_ascii (bool): Whether to escape non-ASCII characters. Defaults to False.\n\n    Raises:\n        ValueError: If the model cannot be serialized to JSON.\n        IOError: If there is an issue writing to the file.\n\n    Example:\n        class ExampleModel(BaseModel):\n            name: str\n            age: int\n\n        if __name__ == \"__main__\":\n            model_instance = ExampleModel(name=\"John\", age=30)\n            json_file = Path(\"example.json\")\n            try:\n                save_model_to_json(json_file, model_instance)\n                print(f\"Model saved to {json_file}\")\n            except (ValueError, IOError) as e:\n                print(e)\n    \"\"\"\n    try:\n        # Serialize model to JSON string\n        model_dict = model.model_dump()\n    except TypeError as e:\n        raise ValueError(f\"Error serializing model to JSON: {e}\") from e\n\n    # Write the JSON string to the file\n    write_data_to_json_file(file, model_dict, indent=indent, ensure_ascii=ensure_ascii)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.json_utils.write_data_to_json_file","title":"<code>write_data_to_json_file(file, data, indent=4, ensure_ascii=False)</code>","text":"<p>Writes a dictionary or list as a JSON string to a file,  ensuring the parent directory exists, and supports formatting with indentation and ASCII control.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the JSON file where the data will be written.</p> required <code>data</code> <code>Union[dict, list]</code> <p>The data to write to the file. Typically a dict or list.</p> required <code>indent</code> <code>int</code> <p>Number of spaces for JSON indentation. Defaults to 4.</p> <code>4</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to escape non-ASCII characters. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data cannot be serialized to JSON.</p> <code>IOError</code> <p>If there is an issue writing to the file.</p> Example <p>from pathlib import Path data = {\"key\": \"value\"} write_json_str_to_file(Path(\"output.json\"), data, indent=2, ensure_ascii=True)</p> Source code in <code>src/tnh_scholar/utils/json_utils.py</code> <pre><code>def write_data_to_json_file(\n    file: Path, data: Union[dict, list], indent: int = 4, ensure_ascii: bool = False\n) -&gt; None:\n    \"\"\"\n    Writes a dictionary or list as a JSON string to a file, \n    ensuring the parent directory exists,\n    and supports formatting with indentation and ASCII control.\n\n    Args:\n        file (Path): Path to the JSON file where the data will be written.\n        data (Union[dict, list]): The data to write to the file. Typically a dict or list.\n        indent (int): Number of spaces for JSON indentation. Defaults to 4.\n        ensure_ascii (bool): Whether to escape non-ASCII characters. Defaults to False.\n\n    Raises:\n        ValueError: If the data cannot be serialized to JSON.\n        IOError: If there is an issue writing to the file.\n\n    Example:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; data = {\"key\": \"value\"}\n        &gt;&gt;&gt; write_json_str_to_file(Path(\"output.json\"), data, indent=2, ensure_ascii=True)\n    \"\"\"\n    try:\n        # Convert the data to a formatted JSON string\n        json_str = json.dumps(data, indent=indent, ensure_ascii=ensure_ascii)\n    except TypeError as e:\n        raise ValueError(f\"Error serializing data to JSON: {e}\") from e\n\n    try:\n        # Ensure the parent directory exists\n        file.parent.mkdir(parents=True, exist_ok=True)\n\n        # Write the JSON string to the file\n        with file.open(\"w\", encoding=\"utf-8\") as f:\n            f.write(json_str)\n    except IOError as e:\n        raise IOError(f\"Error writing JSON string to file '{file}': {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.utils.lang","title":"<code>lang</code>","text":""},{"location":"api/#tnh_scholar.utils.lang.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.lang.get_language_code_from_text","title":"<code>get_language_code_from_text(text)</code>","text":"<p>Detect the language of the provided text using langdetect.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to analyze</p> <pre><code>      code or 'name' for full English language name\n</code></pre> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>return result 'code' ISO 639-1 for detected language.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text is empty or invalid</p> Source code in <code>src/tnh_scholar/utils/lang.py</code> <pre><code>def get_language_code_from_text(text: str) -&gt; str:\n    \"\"\"\n    Detect the language of the provided text using langdetect.\n\n    Args:\n        text: Text to analyze\n\n                      code or 'name' for full English language name\n\n    Returns:\n        str: return result 'code' ISO 639-1 for detected language.\n\n    Raises:\n        ValueError: If text is empty or invalid\n    \"\"\"\n\n    if not text or text.isspace():\n        raise ValueError(\"Input text cannot be empty\")\n\n    sample = _get_sample_text(text)\n\n    try:\n        return detect(sample)\n    except LangDetectException:\n        logger.warning(\"Language could not be detected in get_language().\")\n        return \"un\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.lang.get_language_from_code","title":"<code>get_language_from_code(code)</code>","text":"Source code in <code>src/tnh_scholar/utils/lang.py</code> <pre><code>def get_language_from_code(code: str):\n    if language := pycountry.languages.get(alpha_2=code):\n        return language.name\n    logger.warning(f\"No language name found for code: {code}\")\n    return \"Unknown\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.lang.get_language_name_from_text","title":"<code>get_language_name_from_text(text)</code>","text":"Source code in <code>src/tnh_scholar/utils/lang.py</code> <pre><code>def get_language_name_from_text(text: str) -&gt; str:\n    return get_language_from_code(get_language_code_from_text(text))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils","title":"<code>progress_utils</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.BAR_FORMAT","title":"<code>BAR_FORMAT = '{desc}: {percentage:3.0f}%|{bar}| Total: {total_fmt} sec. [elapsed: {elapsed}]'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM","title":"<code>ExpectedTimeTQDM</code>","text":"<p>A context manager for a time-based tqdm progress bar with optional delay.</p> <ul> <li>'expected_time': number of seconds we anticipate the task might take.</li> <li>'display_interval': how often (seconds) to refresh the bar.</li> <li>'desc': a short description for the bar.</li> <li>'delay_start': how many seconds to wait (sleep) before we even create/start the bar.</li> </ul> <p>If the task finishes before 'delay_start' has elapsed, the bar may never appear.</p> Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>class ExpectedTimeTQDM:\n    \"\"\"\n    A context manager for a time-based tqdm progress bar with optional delay.\n\n    - 'expected_time': number of seconds we anticipate the task might take.\n    - 'display_interval': how often (seconds) to refresh the bar.\n    - 'desc': a short description for the bar.\n    - 'delay_start': how many seconds to wait (sleep) before we even create/start the bar.\n\n    If the task finishes before 'delay_start' has elapsed, the bar may never appear.\n    \"\"\"\n\n    def __init__(\n        self,\n        expected_time: float,\n        display_interval: float = 0.5,\n        desc: str = \"Time-based Progress\",\n        delay_start: float = 1.0,\n    ) -&gt; None:\n        self.expected_time = round(expected_time)  # use nearest second.\n        self.display_interval = display_interval\n        self.desc = desc\n        self.delay_start = delay_start\n\n        self._stop_event = threading.Event()\n        self._pbar = None  # We won't create the bar until after 'delay_start'\n        self._start_time = None\n\n    def __enter__(self):\n        # Record the start time for reference\n        self._start_time = time.time()\n\n        # Spawn the background thread; it will handle waiting and then creating/updating the bar\n        self._thread = threading.Thread(target=self._update_bar, daemon=True)\n        self._thread.start()\n\n        return self\n\n    def _update_bar(self):\n        # 1) Delay so warnings/logs can appear before the bar\n        if self.delay_start &gt; 0:\n            time.sleep(self.delay_start)\n\n        # 2) Create the tqdm bar (only now does it appear)\n        self._pbar = tqdm(\n            total=self.expected_time, desc=self.desc, unit=\"sec\", bar_format=BAR_FORMAT\n        )\n\n        # 3) Update until told to stop\n        while not self._stop_event.is_set():\n            elapsed = time.time() - self._start_time\n            current_value = min(elapsed, self.expected_time)\n            if self._pbar:\n                self._pbar.n = round(current_value)\n                self._pbar.refresh()\n            time.sleep(self.display_interval)\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # Signal the thread to stop\n        self._stop_event.set()\n        self._thread.join()\n\n        # If the bar was actually created (i.e., we didn't finish too quickly),\n        # do a final update and close\n        if self._pbar:\n            elapsed = time.time() - self._start_time\n            self._pbar.n = round(min(elapsed, self.expected_time))\n            self._pbar.refresh()\n            self._pbar.close()\n\n    import time\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.delay_start","title":"<code>delay_start = delay_start</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.desc","title":"<code>desc = desc</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.display_interval","title":"<code>display_interval = display_interval</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.expected_time","title":"<code>expected_time = round(expected_time)</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.__enter__","title":"<code>__enter__()</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __enter__(self):\n    # Record the start time for reference\n    self._start_time = time.time()\n\n    # Spawn the background thread; it will handle waiting and then creating/updating the bar\n    self._thread = threading.Thread(target=self._update_bar, daemon=True)\n    self._thread.start()\n\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    # Signal the thread to stop\n    self._stop_event.set()\n    self._thread.join()\n\n    # If the bar was actually created (i.e., we didn't finish too quickly),\n    # do a final update and close\n    if self._pbar:\n        elapsed = time.time() - self._start_time\n        self._pbar.n = round(min(elapsed, self.expected_time))\n        self._pbar.refresh()\n        self._pbar.close()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.ExpectedTimeTQDM.__init__","title":"<code>__init__(expected_time, display_interval=0.5, desc='Time-based Progress', delay_start=1.0)</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __init__(\n    self,\n    expected_time: float,\n    display_interval: float = 0.5,\n    desc: str = \"Time-based Progress\",\n    delay_start: float = 1.0,\n) -&gt; None:\n    self.expected_time = round(expected_time)  # use nearest second.\n    self.display_interval = display_interval\n    self.desc = desc\n    self.delay_start = delay_start\n\n    self._stop_event = threading.Event()\n    self._pbar = None  # We won't create the bar until after 'delay_start'\n    self._start_time = None\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress","title":"<code>TimeProgress</code>","text":"<p>A context manager for a time-based progress display using dots.</p> <p>The display updates once per second, printing a dot and showing: - Expected time (if provided) - Elapsed time (always displayed)</p> <p>Example:</p> <p>import time with ExpectedTimeProgress(expected_time=60, desc=\"Transcribing...\"): ...     time.sleep(5)  # Simulate work [Expected Time: 1:00, Elapsed Time: 0:05] .....</p> <p>Parameters:</p> Name Type Description Default <code>expected_time</code> <code>Optional[float]</code> <p>Expected time in seconds. Optional.</p> <code>None</code> <code>display_interval</code> <code>float</code> <p>How often to print a dot (seconds).</p> <code>1.0</code> <code>desc</code> <code>str</code> <p>Description to display alongside the progress.</p> <code>''</code> Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>class TimeProgress:\n    \"\"\"\n    A context manager for a time-based progress display using dots.\n\n    The display updates once per second, printing a dot and showing:\n    - Expected time (if provided)\n    - Elapsed time (always displayed)\n\n    Example:\n    &gt;&gt;&gt; import time\n    &gt;&gt;&gt; with ExpectedTimeProgress(expected_time=60, desc=\"Transcribing...\"):\n    ...     time.sleep(5)  # Simulate work\n    [Expected Time: 1:00, Elapsed Time: 0:05] .....\n\n    Args:\n        expected_time (Optional[float]): Expected time in seconds. Optional.\n        display_interval (float): How often to print a dot (seconds).\n        desc (str): Description to display alongside the progress.\n    \"\"\"\n\n    def __init__(\n        self,\n        expected_time: Optional[float] = None,\n        display_interval: float = 1.0,\n        desc: str = \"\",\n    ):\n        self.expected_time = expected_time\n        self.display_interval = display_interval\n        self._stop_event = threading.Event()\n        self._start_time = None\n        self._thread = None\n        self.desc = desc\n        self._last_length = 0  # To keep track of the last printed line length\n\n    def __enter__(self):\n        # Record the start time\n        self._start_time = time.time()\n\n        # Spawn the background thread\n        self._thread = threading.Thread(target=self._print_progress, daemon=True)\n        self._thread.start()\n\n        return self\n\n    def _print_progress(self):\n        \"\"\"\n        Continuously prints progress alternating between | and \u2014 along with elapsed/expected time.\n        \"\"\"\n        symbols = [\"|\", \"/\", \"\u2014\", \"\\\\\"]  # Symbols to alternate between\n        symbol_index = 0  # Keep track of the current symbol\n\n        while not self._stop_event.is_set():\n            elapsed = time.time() - self._start_time\n\n            # Format elapsed time as mm:ss\n            elapsed_str = self._format_time(elapsed)\n\n            # Format expected time if provided\n            if self.expected_time is not None:\n                expected_str = self._format_time(self.expected_time)\n                header = f\"{self.desc} [Expected Time: {expected_str}, Elapsed Time: {elapsed_str}]\"\n            else:\n                header = f\"{self.desc} [Elapsed Time: {elapsed_str}]\"\n\n            # Get the current symbol for the spinner\n            spinner = symbols[symbol_index]\n\n            # Construct the line with the spinner\n            line = f\"\\r{header} {spinner}\"\n\n            # Write to stdout\n            sys.stdout.write(line)\n            sys.stdout.flush()\n\n            # Update the symbol index to alternate\n            symbol_index = (symbol_index + 1) % len(symbols)\n\n            # Sleep before next update\n            time.sleep(self.display_interval)\n\n        # Clear the spinner after finishing\n        sys.stdout.write(\"\\r\" + \" \" * len(line) + \"\\r\")\n        sys.stdout.flush()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # Signal the thread to stop\n        self._stop_event.set()\n        self._thread.join()\n\n        # Final elapsed time\n        elapsed = time.time() - self._start_time\n        elapsed_str = self._format_time(elapsed)\n\n        # Construct the final line\n        if self.expected_time is not None:\n            expected_str = self._format_time(self.expected_time)\n            final_header = f\"{self.desc} [Expected Time: {expected_str}, Elapsed Time: {elapsed_str}]\"\n        else:\n            final_header = f\"{self.desc} [Elapsed Time: {elapsed_str}]\"\n\n        # Final dots\n        final_line = f\"\\r{final_header}\"\n\n        # Clear the line and move to the next line\n        padding = \" \" * max(self._last_length - len(final_line), 0)\n        sys.stdout.write(final_line + padding + \"\\n\")\n        sys.stdout.flush()\n\n    @staticmethod\n    def _format_time(seconds: float) -&gt; str:\n        \"\"\"\n        Converts seconds to a formatted string (mm:ss).\n        \"\"\"\n        minutes = int(seconds // 60)\n        seconds = int(seconds % 60)\n        return f\"{minutes}:{seconds:02}\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.desc","title":"<code>desc = desc</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.display_interval","title":"<code>display_interval = display_interval</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.expected_time","title":"<code>expected_time = expected_time</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.__enter__","title":"<code>__enter__()</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __enter__(self):\n    # Record the start time\n    self._start_time = time.time()\n\n    # Spawn the background thread\n    self._thread = threading.Thread(target=self._print_progress, daemon=True)\n    self._thread.start()\n\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    # Signal the thread to stop\n    self._stop_event.set()\n    self._thread.join()\n\n    # Final elapsed time\n    elapsed = time.time() - self._start_time\n    elapsed_str = self._format_time(elapsed)\n\n    # Construct the final line\n    if self.expected_time is not None:\n        expected_str = self._format_time(self.expected_time)\n        final_header = f\"{self.desc} [Expected Time: {expected_str}, Elapsed Time: {elapsed_str}]\"\n    else:\n        final_header = f\"{self.desc} [Elapsed Time: {elapsed_str}]\"\n\n    # Final dots\n    final_line = f\"\\r{final_header}\"\n\n    # Clear the line and move to the next line\n    padding = \" \" * max(self._last_length - len(final_line), 0)\n    sys.stdout.write(final_line + padding + \"\\n\")\n    sys.stdout.flush()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.progress_utils.TimeProgress.__init__","title":"<code>__init__(expected_time=None, display_interval=1.0, desc='')</code>","text":"Source code in <code>src/tnh_scholar/utils/progress_utils.py</code> <pre><code>def __init__(\n    self,\n    expected_time: Optional[float] = None,\n    display_interval: float = 1.0,\n    desc: str = \"\",\n):\n    self.expected_time = expected_time\n    self.display_interval = display_interval\n    self._stop_event = threading.Event()\n    self._start_time = None\n    self._thread = None\n    self.desc = desc\n    self._last_length = 0  # To keep track of the last printed line length\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils","title":"<code>timing_utils</code>","text":""},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs","title":"<code>TimeMs</code>","text":"<p>               Bases: <code>int</code></p> <p>Lightweight representation of a time interval or timestamp in milliseconds. Allows negative values.</p> Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>class TimeMs(int):\n    \"\"\"\n    Lightweight representation of a time interval or timestamp in milliseconds.\n    Allows negative values.\n    \"\"\"\n\n    def __new__(cls, ms: Union[int, float, \"TimeMs\"]):\n        if isinstance(ms, TimeMs):\n            value = int(ms)\n        elif isinstance(ms, (int, float)):\n            if not math.isfinite(ms):\n                raise ValueError(\"ms must be a finite number\")\n            value = round(ms)\n        else:\n            raise TypeError(f\"ms must be a number or TimeMs, got {type(ms).__name__}\")\n        return int.__new__(cls, value)\n\n    @classmethod\n    def from_seconds(cls, seconds: int | float) -&gt; \"TimeMs\":\n        return cls(round(seconds * 1000))\n\n    def to_ms(self) -&gt; int:\n        return int(self)\n\n    def to_seconds(self) -&gt; float:\n        return float(self) / 1000\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source_type, handler: GetCoreSchemaHandler):\n        return core_schema.with_info_plain_validator_function(\n            cls._validate,\n            serialization=core_schema.plain_serializer_function_ser_schema(lambda v: int(v)),\n        )\n\n    @classmethod\n    def _validate(cls, value, info):\n        \"\"\"\n        Pydantic core validator for TimeMs.\n\n        Args:\n            value: The value to validate.\n            info: Pydantic core schema info (unused).\n\n        Returns:\n            TimeMs: Validated TimeMs instance.\n        \"\"\"\n        return cls(value)\n\n    def __add__(self, other):\n        return TimeMs(int(self) + int(other))\n\n    def __radd__(self, other):\n        return TimeMs(int(other) + int(self))\n\n    def __sub__(self, other):\n        return TimeMs(int(self) - int(other))\n\n    def __rsub__(self, other):\n        return TimeMs(int(self) - int(other))\n\n    def __repr__(self) -&gt; str:\n        return f\"TimeMs({self.to_seconds():.3f}s)\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs.__add__","title":"<code>__add__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __add__(self, other):\n    return TimeMs(int(self) + int(other))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs.__get_pydantic_core_schema__","title":"<code>__get_pydantic_core_schema__(source_type, handler)</code>  <code>classmethod</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(cls, source_type, handler: GetCoreSchemaHandler):\n    return core_schema.with_info_plain_validator_function(\n        cls._validate,\n        serialization=core_schema.plain_serializer_function_ser_schema(lambda v: int(v)),\n    )\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs.__new__","title":"<code>__new__(ms)</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __new__(cls, ms: Union[int, float, \"TimeMs\"]):\n    if isinstance(ms, TimeMs):\n        value = int(ms)\n    elif isinstance(ms, (int, float)):\n        if not math.isfinite(ms):\n            raise ValueError(\"ms must be a finite number\")\n        value = round(ms)\n    else:\n        raise TypeError(f\"ms must be a number or TimeMs, got {type(ms).__name__}\")\n    return int.__new__(cls, value)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs.__radd__","title":"<code>__radd__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __radd__(self, other):\n    return TimeMs(int(other) + int(self))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs.__repr__","title":"<code>__repr__()</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"TimeMs({self.to_seconds():.3f}s)\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs.__rsub__","title":"<code>__rsub__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __rsub__(self, other):\n    return TimeMs(int(self) - int(other))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs.__sub__","title":"<code>__sub__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def __sub__(self, other):\n    return TimeMs(int(self) - int(other))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs.from_seconds","title":"<code>from_seconds(seconds)</code>  <code>classmethod</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>@classmethod\ndef from_seconds(cls, seconds: int | float) -&gt; \"TimeMs\":\n    return cls(round(seconds * 1000))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs.to_ms","title":"<code>to_ms()</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def to_ms(self) -&gt; int:\n    return int(self)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.TimeMs.to_seconds","title":"<code>to_seconds()</code>","text":"Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def to_seconds(self) -&gt; float:\n    return float(self) / 1000\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.convert_ms_to_sec","title":"<code>convert_ms_to_sec(ms)</code>","text":"<p>Convert time from milliseconds (int) to seconds (float).</p> Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def convert_ms_to_sec(ms: int) -&gt; float:\n    \"\"\"Convert time from milliseconds (int) to seconds (float).\"\"\"\n    return float(ms / 1000)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.timing_utils.convert_sec_to_ms","title":"<code>convert_sec_to_ms(val)</code>","text":"<p>Convert seconds to milliseconds, rounding to the nearest integer.</p> Source code in <code>src/tnh_scholar/utils/timing_utils.py</code> <pre><code>def convert_sec_to_ms(val: float) -&gt; int:\n    \"\"\" \n    Convert seconds to milliseconds, rounding to the nearest integer.\n    \"\"\"\n    return round(val * 1000)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment","title":"<code>tnh_audio_segment</code>","text":"<p>TNHAudioSegment: A typed, minimal wrapper for pydub.AudioSegment.</p> <p>This class provides a type-safe interface for working with audio segments using pydub, enabling easier composition, slicing, and manipulation of audio data. It exposes common operations such as concatenation, slicing, and length retrieval, while hiding the underlying pydub implementation.</p> Key features <ul> <li>Type-annotated methods for static analysis and IDE support</li> <li>Static constructors for silent and empty segments</li> <li>Operator overloads for concatenation and slicing</li> <li>Access to the underlying pydub.AudioSegment via the <code>raw</code> property</li> </ul> <p>Extend this class with additional methods as needed for your audio processing workflows.</p>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment","title":"<code>TNHAudioSegment</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>class TNHAudioSegment:\n    def __init__(self, segment: _AudioSegment):\n        self._segment = segment\n\n    @staticmethod\n    def from_file(\n        file: str | Path | BytesIO,\n        format: str | None = None,\n        **kwargs: Any,\n    ) -&gt; \"TNHAudioSegment\":\n        \"\"\"\n        Wrapper: Load an audio file into a TNHAudioSegment.\n\n        Args:\n            file: Path to the audio file.\n            format: Optional audio format (e.g., 'mp3', 'wav'). If None, pydub will attempt to infer it.\n            **kwargs: Additional keyword arguments passed to pydub.AudioSegment.from_file.\n\n        Returns:\n            TNHAudioSegment instance containing the loaded audio.\n        \"\"\"\n        return TNHAudioSegment(_AudioSegment.from_file(file, format=format, **kwargs))\n\n    def export(self, out_f: str | BinaryIO, format: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Wrapper: Export the audio segment to a file-like object or file path.\n\n        Args:\n            out_f: File path or file-like object to write the audio data to.\n            format: Audio format (e.g., 'mp3', 'wav').\n            **kwargs: Additional keyword arguments passed to pydub.AudioSegment.export.\n        \"\"\"\n        self._segment.export(out_f, format=format, **kwargs)\n\n    @staticmethod\n    def silent(duration: int) -&gt; \"TNHAudioSegment\":\n        return TNHAudioSegment(_AudioSegment.silent(duration=duration))\n\n    @staticmethod\n    def empty() -&gt; \"TNHAudioSegment\":\n        return TNHAudioSegment(_AudioSegment.empty())\n\n    def __getitem__(self, key: int | slice) -&gt; \"TNHAudioSegment\":\n        return TNHAudioSegment(self._segment[key]) # type: ignore\n\n    def __add__(self, other: \"TNHAudioSegment\") -&gt; \"TNHAudioSegment\":\n        return TNHAudioSegment(self._segment + other._segment)\n\n    def __iadd__(self, other: \"TNHAudioSegment\") -&gt; \"TNHAudioSegment\":\n        self._segment = self._segment + other._segment\n        return self\n\n    def __len__(self) -&gt; int:\n        return len(self._segment)\n\n    # Add more methods as needed, e.g., export, from_file, etc.\n\n    @property\n    def raw(self) -&gt; _AudioSegment:\n        \"\"\"Access the underlying pydub.AudioSegment if needed.\"\"\"\n        return self._segment\n</code></pre>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment.raw","title":"<code>raw</code>  <code>property</code>","text":"<p>Access the underlying pydub.AudioSegment if needed.</p>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment.__add__","title":"<code>__add__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def __add__(self, other: \"TNHAudioSegment\") -&gt; \"TNHAudioSegment\":\n    return TNHAudioSegment(self._segment + other._segment)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment.__getitem__","title":"<code>__getitem__(key)</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def __getitem__(self, key: int | slice) -&gt; \"TNHAudioSegment\":\n    return TNHAudioSegment(self._segment[key]) # type: ignore\n</code></pre>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment.__iadd__","title":"<code>__iadd__(other)</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def __iadd__(self, other: \"TNHAudioSegment\") -&gt; \"TNHAudioSegment\":\n    self._segment = self._segment + other._segment\n    return self\n</code></pre>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment.__init__","title":"<code>__init__(segment)</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def __init__(self, segment: _AudioSegment):\n    self._segment = segment\n</code></pre>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment.__len__","title":"<code>__len__()</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._segment)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment.empty","title":"<code>empty()</code>  <code>staticmethod</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>@staticmethod\ndef empty() -&gt; \"TNHAudioSegment\":\n    return TNHAudioSegment(_AudioSegment.empty())\n</code></pre>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment.export","title":"<code>export(out_f, format, **kwargs)</code>","text":"<p>Wrapper: Export the audio segment to a file-like object or file path.</p> <p>Parameters:</p> Name Type Description Default <code>out_f</code> <code>str | BinaryIO</code> <p>File path or file-like object to write the audio data to.</p> required <code>format</code> <code>str</code> <p>Audio format (e.g., 'mp3', 'wav').</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to pydub.AudioSegment.export.</p> <code>{}</code> Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>def export(self, out_f: str | BinaryIO, format: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Wrapper: Export the audio segment to a file-like object or file path.\n\n    Args:\n        out_f: File path or file-like object to write the audio data to.\n        format: Audio format (e.g., 'mp3', 'wav').\n        **kwargs: Additional keyword arguments passed to pydub.AudioSegment.export.\n    \"\"\"\n    self._segment.export(out_f, format=format, **kwargs)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment.from_file","title":"<code>from_file(file, format=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Wrapper: Load an audio file into a TNHAudioSegment.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | Path | BytesIO</code> <p>Path to the audio file.</p> required <code>format</code> <code>str | None</code> <p>Optional audio format (e.g., 'mp3', 'wav'). If None, pydub will attempt to infer it.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to pydub.AudioSegment.from_file.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TNHAudioSegment</code> <p>TNHAudioSegment instance containing the loaded audio.</p> Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>@staticmethod\ndef from_file(\n    file: str | Path | BytesIO,\n    format: str | None = None,\n    **kwargs: Any,\n) -&gt; \"TNHAudioSegment\":\n    \"\"\"\n    Wrapper: Load an audio file into a TNHAudioSegment.\n\n    Args:\n        file: Path to the audio file.\n        format: Optional audio format (e.g., 'mp3', 'wav'). If None, pydub will attempt to infer it.\n        **kwargs: Additional keyword arguments passed to pydub.AudioSegment.from_file.\n\n    Returns:\n        TNHAudioSegment instance containing the loaded audio.\n    \"\"\"\n    return TNHAudioSegment(_AudioSegment.from_file(file, format=format, **kwargs))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.tnh_audio_segment.TNHAudioSegment.silent","title":"<code>silent(duration)</code>  <code>staticmethod</code>","text":"Source code in <code>src/tnh_scholar/utils/tnh_audio_segment.py</code> <pre><code>@staticmethod\ndef silent(duration: int) -&gt; \"TNHAudioSegment\":\n    return TNHAudioSegment(_AudioSegment.silent(duration=duration))\n</code></pre>"},{"location":"api/#tnh_scholar.utils.user_io_utils","title":"<code>user_io_utils</code>","text":""},{"location":"api/#tnh_scholar.utils.user_io_utils.get_single_char","title":"<code>get_single_char(prompt=None)</code>","text":"<p>Get a single character from input, adapting to the execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[str]</code> <p>Optional prompt to display before getting input</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A single character string from user input</p> Note <ul> <li>In terminal environments, uses raw input mode without requiring Enter</li> <li>In Jupyter/IPython, falls back to regular input with message about Enter</li> </ul> Source code in <code>src/tnh_scholar/utils/user_io_utils.py</code> <pre><code>def get_single_char(prompt: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Get a single character from input, adapting to the execution environment.\n\n    Args:\n        prompt: Optional prompt to display before getting input\n\n    Returns:\n        A single character string from user input\n\n    Note:\n        - In terminal environments, uses raw input mode without requiring Enter\n        - In Jupyter/IPython, falls back to regular input with message about Enter\n    \"\"\"\n    # Check if we're in IPython/Jupyter\n    is_notebook = hasattr(sys, 'ps1') or bool(sys.flags.interactive)\n\n    if prompt:\n        print(prompt, end='', flush=True)\n\n    if is_notebook:\n        # Jupyter/IPython environment - use regular input\n        entry = input(\"Single character input required \")\n        return entry[0] if entry else \"\\n\" # Use newline if no entry\n\n    # Terminal environment\n    if os.name == \"nt\":  # Windows\n        import msvcrt\n        return msvcrt.getch().decode(\"utf-8\")\n    else:  # Unix-like\n        import termios\n        import tty\n\n        try:\n            fd = sys.stdin.fileno()\n            old_settings = termios.tcgetattr(fd)\n            try:\n                tty.setraw(fd)\n                char = sys.stdin.read(1)\n            finally:\n                termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n            return char\n        except termios.error:\n            # Fallback if terminal handling fails\n            return input(\"Single character input required \")[0]\n</code></pre>"},{"location":"api/#tnh_scholar.utils.user_io_utils.get_user_confirmation","title":"<code>get_user_confirmation(prompt, default=True)</code>","text":"<p>Prompt the user for a yes/no confirmation with single-character input. Cross-platform implementation. Returns True if 'y' is entered, and False if 'n' Allows for default value if return is entered.</p> <p>Example usage     if get_user_confirmation(\"Do you want to continue\"):         print(\"Continuing...\")     else:         print(\"Exiting...\")</p> Source code in <code>src/tnh_scholar/utils/user_io_utils.py</code> <pre><code>def get_user_confirmation(prompt: str, default: bool = True) -&gt; bool:\n    \"\"\"\n    Prompt the user for a yes/no confirmation with single-character input.\n    Cross-platform implementation. Returns True if 'y' is entered, and False if 'n'\n    Allows for default value if return is entered.\n\n    Example usage\n        if get_user_confirmation(\"Do you want to continue\"):\n            print(\"Continuing...\")\n        else:\n            print(\"Exiting...\")\n    \"\"\"\n    print(f\"{prompt} \", end=\"\", flush=True)\n\n    while True:\n        char = get_single_char().lower()\n        if char == \"y\":\n            print(char)  # Echo the choice\n            return True\n        elif char == \"n\":\n            print(char)\n            return False\n        elif char in (\"\\r\", \"\\n\"):  # Enter key (use default)\n            print()  # Add a newline\n            return default\n        else:\n            print(\n                f\"\\nInvalid input: {char}. Please type 'y' or 'n': \", end=\"\", flush=True\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.utils.validate","title":"<code>validate</code>","text":""},{"location":"api/#tnh_scholar.utils.validate.OCR_ENV_VARS","title":"<code>OCR_ENV_VARS = {'GOOGLE_APPLICATION_CREDENTIALS'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.validate.OPENAI_ENV_VARS","title":"<code>OPENAI_ENV_VARS = {'OPENAI_API_KEY'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.validate.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.validate.check_env","title":"<code>check_env(required_vars, feature='this feature', output=True)</code>","text":"<p>Check environment variables and provide user-friendly error messages.</p> <p>Parameters:</p> Name Type Description Default <code>required_vars</code> <code>Set[str]</code> <p>Set of environment variable names to check</p> required <code>feature</code> <code>str</code> <p>Description of feature requiring these variables</p> <code>'this feature'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all required variables are set</p> Source code in <code>src/tnh_scholar/utils/validate.py</code> <pre><code>def check_env(required_vars: Set[str], feature: str = \"this feature\", output: bool = True) -&gt; bool:\n    \"\"\"\n    Check environment variables and provide user-friendly error messages.\n\n    Args:\n        required_vars: Set of environment variable names to check\n        feature: Description of feature requiring these variables\n\n    Returns:\n        bool: True if all required variables are set\n    \"\"\"\n    if missing := [var for var in required_vars if not os.getenv(var)]:\n        if output:\n            message = get_env_message(missing, feature)\n            logger.error(f\"Missing environment variables: {', '.join(missing)}\")\n            print(message, file=sys.stderr)\n        return False\n    return True\n</code></pre>"},{"location":"api/#tnh_scholar.utils.validate.check_ocr_env","title":"<code>check_ocr_env(output=True)</code>","text":"<p>Check OCR processing requirements.</p> Source code in <code>src/tnh_scholar/utils/validate.py</code> <pre><code>def check_ocr_env(output: bool = True) -&gt; bool:\n    \"\"\"Check OCR processing requirements.\"\"\"\n    return check_env(OCR_ENV_VARS, \"OCR processing\", output=output)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.validate.check_openai_env","title":"<code>check_openai_env(output=True)</code>","text":"<p>Check OpenAI API requirements.</p> Source code in <code>src/tnh_scholar/utils/validate.py</code> <pre><code>def check_openai_env(output: bool = True) -&gt; bool:\n    \"\"\"Check OpenAI API requirements.\"\"\"\n    return check_env(OPENAI_ENV_VARS, \"OpenAI API access\", output=output)\n</code></pre>"},{"location":"api/#tnh_scholar.utils.validate.get_env_message","title":"<code>get_env_message(missing_vars, feature='this feature')</code>","text":"<p>Generate user-friendly environment setup message.</p> <p>Parameters:</p> Name Type Description Default <code>missing_vars</code> <code>List[str]</code> <p>List of missing environment variable names</p> required <code>feature</code> <code>str</code> <p>Name of feature requiring the variables</p> <code>'this feature'</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted error message with setup instructions</p> Source code in <code>src/tnh_scholar/utils/validate.py</code> <pre><code>def get_env_message(missing_vars: List[str], feature: str = \"this feature\") -&gt; str:\n    \"\"\"Generate user-friendly environment setup message.\n\n    Args:\n        missing_vars: List of missing environment variable names\n        feature: Name of feature requiring the variables\n\n    Returns:\n        Formatted error message with setup instructions\n    \"\"\"\n    export_cmds = \" \".join(f\"{var}=your_{var.lower()}_here\" for var in missing_vars)\n\n    return \"\\n\".join([\n        f\"\\nEnvironment Error: Missing required variables for {feature}:\",\n        \", \".join(missing_vars),\n        \"\\nSet variables in your shell:\",\n        f\"export {export_cmds}\",\n        \"\\nSee documentation for details.\",\n        \"\\nFor development: Add to .env file in project root.\\n\"\n    ])\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check","title":"<code>version_check</code>","text":"<p>Version checker package for monitoring package version compatibility.</p>"},{"location":"api/#tnh_scholar.utils.version_check.__all__","title":"<code>__all__ = ['PackageVersionChecker', 'VersionCheckerConfig', 'VersionStrategy', 'Result', 'PackageInfo']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.PackageInfo","title":"<code>PackageInfo</code>  <code>dataclass</code>","text":"<p>Information about a package and its versions.</p> Source code in <code>src/tnh_scholar/utils/version_check/models.py</code> <pre><code>@dataclass\nclass PackageInfo:\n    \"\"\"Information about a package and its versions.\"\"\"\n\n    name: str\n    installed_version: Optional[str] = None\n    latest_version: Optional[str] = None\n    required_version: Optional[str] = None\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.PackageInfo.installed_version","title":"<code>installed_version = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.PackageInfo.latest_version","title":"<code>latest_version = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.PackageInfo.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.PackageInfo.required_version","title":"<code>required_version = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.PackageInfo.__init__","title":"<code>__init__(name, installed_version=None, latest_version=None, required_version=None)</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.PackageVersionChecker","title":"<code>PackageVersionChecker</code>","text":"<p>Main class for checking package versions against requirements.</p> Source code in <code>src/tnh_scholar/utils/version_check/checker.py</code> <pre><code>class PackageVersionChecker:\n    \"\"\"Main class for checking package versions against requirements.\"\"\"\n\n    def __init__(self, \n                 provider: Optional[VersionProvider] = None,\n                 cache: Optional[VersionCache] = None):\n        self.provider = provider or StandardVersionProvider()\n        self.cache = cache or VersionCache()\n\n    # TODO make this method more modular and extract out complexity  \n    # also check why parse_vdiff is not being used.\n    def check_version(self, \n                      package_name: str, \n                      config: Optional[VersionCheckerConfig] = None) -&gt; Result:\n        \"\"\"Check if package meets version requirements based on config.\"\"\"\n        config = config or VersionCheckerConfig()\n\n        try:\n            # Get versions\n            installed = self.provider.get_installed_version(package_name)\n            latest = self.provider.get_latest_version(package_name)\n\n            # Default values\n            is_compatible = True\n            needs_update = installed &lt; latest\n            warning_level = None\n            diff_details = None\n\n            # Check based on strategy\n            if config.strategy == VersionStrategy.MINIMUM:\n                is_compatible = check_minimum_version(installed, config.get_required_version())\n\n            elif config.strategy == VersionStrategy.EXACT:\n                is_compatible = check_exact_version(installed, config.get_required_version())\n\n            elif config.strategy == VersionStrategy.VERSION_DIFF:\n                # Check warning threshold\n                if config.vdiff_warn_matrix:\n                    warn_within_limits, diff_details = check_version_diff(\n                        installed, latest, config.vdiff_warn_matrix)\n                    if not warn_within_limits:\n                        # Determine warning level based on which component exceeded threshold\n                        if diff_details and \"major\" in diff_details and diff_details[\"major\"] &gt; 0:\n                            warning_level = \"MAJOR\"\n                        elif diff_details and \"minor\" in diff_details and diff_details[\"minor\"] &gt; 0:\n                            warning_level = \"MINOR\"\n                        else:\n                            warning_level = \"MICRO\"\n\n                # Check failure threshold\n                if config.vdiff_fail_matrix:\n                    fail_within_limits, diff_details = check_version_diff(\n                        installed, latest, config.vdiff_fail_matrix)\n                    is_compatible = fail_within_limits\n\n            # Create package info\n            package_info = PackageInfo(\n                name=package_name,\n                installed_version=str(installed),\n                latest_version=str(latest),\n                required_version=str(config.get_required_version()) if config.requirement else None\n            )\n\n            # Create and return result\n            return Result(\n                is_compatible=is_compatible,\n                needs_update=needs_update,\n                package_info=package_info,\n                warning_level=warning_level,\n                diff_details=diff_details\n            )\n\n        except Exception as e:\n            # Handle errors based on configuration\n            if config.fail_on_error:\n                raise\n            return Result(\n                is_compatible=False,\n                needs_update=False,\n                package_info=PackageInfo(name=package_name),\n                error=str(e)\n            )\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.PackageVersionChecker.cache","title":"<code>cache = cache or VersionCache()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.PackageVersionChecker.provider","title":"<code>provider = provider or StandardVersionProvider()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.PackageVersionChecker.__init__","title":"<code>__init__(provider=None, cache=None)</code>","text":"Source code in <code>src/tnh_scholar/utils/version_check/checker.py</code> <pre><code>def __init__(self, \n             provider: Optional[VersionProvider] = None,\n             cache: Optional[VersionCache] = None):\n    self.provider = provider or StandardVersionProvider()\n    self.cache = cache or VersionCache()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.PackageVersionChecker.check_version","title":"<code>check_version(package_name, config=None)</code>","text":"<p>Check if package meets version requirements based on config.</p> Source code in <code>src/tnh_scholar/utils/version_check/checker.py</code> <pre><code>def check_version(self, \n                  package_name: str, \n                  config: Optional[VersionCheckerConfig] = None) -&gt; Result:\n    \"\"\"Check if package meets version requirements based on config.\"\"\"\n    config = config or VersionCheckerConfig()\n\n    try:\n        # Get versions\n        installed = self.provider.get_installed_version(package_name)\n        latest = self.provider.get_latest_version(package_name)\n\n        # Default values\n        is_compatible = True\n        needs_update = installed &lt; latest\n        warning_level = None\n        diff_details = None\n\n        # Check based on strategy\n        if config.strategy == VersionStrategy.MINIMUM:\n            is_compatible = check_minimum_version(installed, config.get_required_version())\n\n        elif config.strategy == VersionStrategy.EXACT:\n            is_compatible = check_exact_version(installed, config.get_required_version())\n\n        elif config.strategy == VersionStrategy.VERSION_DIFF:\n            # Check warning threshold\n            if config.vdiff_warn_matrix:\n                warn_within_limits, diff_details = check_version_diff(\n                    installed, latest, config.vdiff_warn_matrix)\n                if not warn_within_limits:\n                    # Determine warning level based on which component exceeded threshold\n                    if diff_details and \"major\" in diff_details and diff_details[\"major\"] &gt; 0:\n                        warning_level = \"MAJOR\"\n                    elif diff_details and \"minor\" in diff_details and diff_details[\"minor\"] &gt; 0:\n                        warning_level = \"MINOR\"\n                    else:\n                        warning_level = \"MICRO\"\n\n            # Check failure threshold\n            if config.vdiff_fail_matrix:\n                fail_within_limits, diff_details = check_version_diff(\n                    installed, latest, config.vdiff_fail_matrix)\n                is_compatible = fail_within_limits\n\n        # Create package info\n        package_info = PackageInfo(\n            name=package_name,\n            installed_version=str(installed),\n            latest_version=str(latest),\n            required_version=str(config.get_required_version()) if config.requirement else None\n        )\n\n        # Create and return result\n        return Result(\n            is_compatible=is_compatible,\n            needs_update=needs_update,\n            package_info=package_info,\n            warning_level=warning_level,\n            diff_details=diff_details\n        )\n\n    except Exception as e:\n        # Handle errors based on configuration\n        if config.fail_on_error:\n            raise\n        return Result(\n            is_compatible=False,\n            needs_update=False,\n            package_info=PackageInfo(name=package_name),\n            error=str(e)\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.Result","title":"<code>Result</code>  <code>dataclass</code>","text":"<p>Result of a version check operation.</p> Source code in <code>src/tnh_scholar/utils/version_check/models.py</code> <pre><code>@dataclass\nclass Result:\n    \"\"\"Result of a version check operation.\"\"\"\n\n    is_compatible: bool\n    needs_update: bool\n    package_info: PackageInfo\n    error: Optional[str] = None\n    warning_level: Optional[str] = None\n    diff_details: Optional[Dict[str, int]] = None\n\n    def get_upgrade_command(self) -&gt; str:\n        \"\"\"Return pip command to upgrade package.\"\"\"\n        if not self.package_info or not self.package_info.name:\n            return \"\"\n\n        if self.package_info.latest_version:\n            return f\"pip install --upgrade {self.package_info.name}=={self.package_info.latest_version}\"\n        else:\n            return f\"pip install --upgrade {self.package_info.name}\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.Result.diff_details","title":"<code>diff_details = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.Result.error","title":"<code>error = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.Result.is_compatible","title":"<code>is_compatible</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.Result.needs_update","title":"<code>needs_update</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.Result.package_info","title":"<code>package_info</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.Result.warning_level","title":"<code>warning_level = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.Result.__init__","title":"<code>__init__(is_compatible, needs_update, package_info, error=None, warning_level=None, diff_details=None)</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.Result.get_upgrade_command","title":"<code>get_upgrade_command()</code>","text":"<p>Return pip command to upgrade package.</p> Source code in <code>src/tnh_scholar/utils/version_check/models.py</code> <pre><code>def get_upgrade_command(self) -&gt; str:\n    \"\"\"Return pip command to upgrade package.\"\"\"\n    if not self.package_info or not self.package_info.name:\n        return \"\"\n\n    if self.package_info.latest_version:\n        return f\"pip install --upgrade {self.package_info.name}=={self.package_info.latest_version}\"\n    else:\n        return f\"pip install --upgrade {self.package_info.name}\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.VersionCheckerConfig","title":"<code>VersionCheckerConfig</code>","text":"<p>Configuration for version checking behavior.</p> Source code in <code>src/tnh_scholar/utils/version_check/config.py</code> <pre><code>class VersionCheckerConfig:\n    \"\"\"Configuration for version checking behavior.\"\"\"\n\n    def __init__(self,\n                 strategy: VersionStrategy = VersionStrategy.MINIMUM,\n                 requirement: str = \"\",\n                 fail_on_error: bool = False,\n                 cache_duration: int = 3600,  # 1 hour\n                 network_timeout: int = 5,    # seconds\n                 vdiff_warn_matrix: Optional[str] = None,\n                 vdiff_fail_matrix: Optional[str] = None):\n        \"\"\"Initialize version checker configuration.\"\"\"\n        self.strategy = strategy\n        self.requirement = requirement\n        self.fail_on_error = fail_on_error\n        self.cache_duration = cache_duration\n        self.network_timeout = network_timeout\n        self.vdiff_warn_matrix = vdiff_warn_matrix\n        self.vdiff_fail_matrix = vdiff_fail_matrix\n\n    def get_required_version(self) -&gt; Optional[Version]:\n        \"\"\"Get required version as a Version object.\"\"\"\n        return Version(self.requirement) if self.requirement else None\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.VersionCheckerConfig.cache_duration","title":"<code>cache_duration = cache_duration</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionCheckerConfig.fail_on_error","title":"<code>fail_on_error = fail_on_error</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionCheckerConfig.network_timeout","title":"<code>network_timeout = network_timeout</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionCheckerConfig.requirement","title":"<code>requirement = requirement</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionCheckerConfig.strategy","title":"<code>strategy = strategy</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionCheckerConfig.vdiff_fail_matrix","title":"<code>vdiff_fail_matrix = vdiff_fail_matrix</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionCheckerConfig.vdiff_warn_matrix","title":"<code>vdiff_warn_matrix = vdiff_warn_matrix</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionCheckerConfig.__init__","title":"<code>__init__(strategy=VersionStrategy.MINIMUM, requirement='', fail_on_error=False, cache_duration=3600, network_timeout=5, vdiff_warn_matrix=None, vdiff_fail_matrix=None)</code>","text":"<p>Initialize version checker configuration.</p> Source code in <code>src/tnh_scholar/utils/version_check/config.py</code> <pre><code>def __init__(self,\n             strategy: VersionStrategy = VersionStrategy.MINIMUM,\n             requirement: str = \"\",\n             fail_on_error: bool = False,\n             cache_duration: int = 3600,  # 1 hour\n             network_timeout: int = 5,    # seconds\n             vdiff_warn_matrix: Optional[str] = None,\n             vdiff_fail_matrix: Optional[str] = None):\n    \"\"\"Initialize version checker configuration.\"\"\"\n    self.strategy = strategy\n    self.requirement = requirement\n    self.fail_on_error = fail_on_error\n    self.cache_duration = cache_duration\n    self.network_timeout = network_timeout\n    self.vdiff_warn_matrix = vdiff_warn_matrix\n    self.vdiff_fail_matrix = vdiff_fail_matrix\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.VersionCheckerConfig.get_required_version","title":"<code>get_required_version()</code>","text":"<p>Get required version as a Version object.</p> Source code in <code>src/tnh_scholar/utils/version_check/config.py</code> <pre><code>def get_required_version(self) -&gt; Optional[Version]:\n    \"\"\"Get required version as a Version object.\"\"\"\n    return Version(self.requirement) if self.requirement else None\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.VersionStrategy","title":"<code>VersionStrategy</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of version checking strategies.</p> Source code in <code>src/tnh_scholar/utils/version_check/config.py</code> <pre><code>class VersionStrategy(Enum):\n    \"\"\"Enumeration of version checking strategies.\"\"\"\n    MINIMUM = \"minimum\"    # Package version must be &gt;= requirement\n    EXACT = \"exact\"        # Package version must be == requirement\n    LATEST = \"latest\"      # Package version should be the latest available\n    RANGE = \"range\"        # Package version must be within a specified range\n    VERSION_DIFF = \"vdiff\" # Check version difference against thresholds\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.VersionStrategy.EXACT","title":"<code>EXACT = 'exact'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionStrategy.LATEST","title":"<code>LATEST = 'latest'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionStrategy.MINIMUM","title":"<code>MINIMUM = 'minimum'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionStrategy.RANGE","title":"<code>RANGE = 'range'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.VersionStrategy.VERSION_DIFF","title":"<code>VERSION_DIFF = 'vdiff'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.version_check.cache","title":"<code>cache</code>","text":"<p>Simple caching mechanism for version information.</p>"},{"location":"api/#tnh_scholar.utils.version_check.cache.VersionCache","title":"<code>VersionCache</code>","text":"<p>Simple time-based cache for version information.</p> Source code in <code>src/tnh_scholar/utils/version_check/cache.py</code> <pre><code>class VersionCache:\n    \"\"\"Simple time-based cache for version information.\"\"\"\n\n    def __init__(self, cache_duration: int = 3600):\n        \"\"\"Initialize cache with specified expiration time in seconds.\"\"\"\n        self.cache: Dict[str, Version] = {}\n        self.timestamps: Dict[str, float] = {}\n        self.cache_duration = cache_duration\n\n    def get(self, key: str) -&gt; Optional[Version]:\n        \"\"\"Get cached version if still valid.\"\"\"\n        return self.cache.get(key) if self.is_valid(key) else None\n\n    def set(self, key: str, value: Version) -&gt; None:\n        \"\"\"Cache version with current timestamp.\"\"\"\n        self.cache[key] = value\n        self.timestamps[key] = time.time()\n\n    def is_valid(self, key: str) -&gt; bool:\n        \"\"\"Check if cached value is still valid.\"\"\"\n        if key not in self.timestamps:\n            return False\n        age = time.time() - self.timestamps[key]\n        return age &lt; self.cache_duration\n</code></pre> <code>cache = {}</code> <code>instance-attribute</code> \u00b6 <code>cache_duration = cache_duration</code> <code>instance-attribute</code> \u00b6 <code>timestamps = {}</code> <code>instance-attribute</code> \u00b6 <code>__init__(cache_duration=3600)</code> \u00b6 <p>Initialize cache with specified expiration time in seconds.</p> Source code in <code>src/tnh_scholar/utils/version_check/cache.py</code> <pre><code>def __init__(self, cache_duration: int = 3600):\n    \"\"\"Initialize cache with specified expiration time in seconds.\"\"\"\n    self.cache: Dict[str, Version] = {}\n    self.timestamps: Dict[str, float] = {}\n    self.cache_duration = cache_duration\n</code></pre> <code>get(key)</code> \u00b6 <p>Get cached version if still valid.</p> Source code in <code>src/tnh_scholar/utils/version_check/cache.py</code> <pre><code>def get(self, key: str) -&gt; Optional[Version]:\n    \"\"\"Get cached version if still valid.\"\"\"\n    return self.cache.get(key) if self.is_valid(key) else None\n</code></pre> <code>is_valid(key)</code> \u00b6 <p>Check if cached value is still valid.</p> Source code in <code>src/tnh_scholar/utils/version_check/cache.py</code> <pre><code>def is_valid(self, key: str) -&gt; bool:\n    \"\"\"Check if cached value is still valid.\"\"\"\n    if key not in self.timestamps:\n        return False\n    age = time.time() - self.timestamps[key]\n    return age &lt; self.cache_duration\n</code></pre> <code>set(key, value)</code> \u00b6 <p>Cache version with current timestamp.</p> Source code in <code>src/tnh_scholar/utils/version_check/cache.py</code> <pre><code>def set(self, key: str, value: Version) -&gt; None:\n    \"\"\"Cache version with current timestamp.\"\"\"\n    self.cache[key] = value\n    self.timestamps[key] = time.time()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.checker","title":"<code>checker</code>","text":"<p>Main version checker implementation.</p>"},{"location":"api/#tnh_scholar.utils.version_check.checker.PackageVersionChecker","title":"<code>PackageVersionChecker</code>","text":"<p>Main class for checking package versions against requirements.</p> Source code in <code>src/tnh_scholar/utils/version_check/checker.py</code> <pre><code>class PackageVersionChecker:\n    \"\"\"Main class for checking package versions against requirements.\"\"\"\n\n    def __init__(self, \n                 provider: Optional[VersionProvider] = None,\n                 cache: Optional[VersionCache] = None):\n        self.provider = provider or StandardVersionProvider()\n        self.cache = cache or VersionCache()\n\n    # TODO make this method more modular and extract out complexity  \n    # also check why parse_vdiff is not being used.\n    def check_version(self, \n                      package_name: str, \n                      config: Optional[VersionCheckerConfig] = None) -&gt; Result:\n        \"\"\"Check if package meets version requirements based on config.\"\"\"\n        config = config or VersionCheckerConfig()\n\n        try:\n            # Get versions\n            installed = self.provider.get_installed_version(package_name)\n            latest = self.provider.get_latest_version(package_name)\n\n            # Default values\n            is_compatible = True\n            needs_update = installed &lt; latest\n            warning_level = None\n            diff_details = None\n\n            # Check based on strategy\n            if config.strategy == VersionStrategy.MINIMUM:\n                is_compatible = check_minimum_version(installed, config.get_required_version())\n\n            elif config.strategy == VersionStrategy.EXACT:\n                is_compatible = check_exact_version(installed, config.get_required_version())\n\n            elif config.strategy == VersionStrategy.VERSION_DIFF:\n                # Check warning threshold\n                if config.vdiff_warn_matrix:\n                    warn_within_limits, diff_details = check_version_diff(\n                        installed, latest, config.vdiff_warn_matrix)\n                    if not warn_within_limits:\n                        # Determine warning level based on which component exceeded threshold\n                        if diff_details and \"major\" in diff_details and diff_details[\"major\"] &gt; 0:\n                            warning_level = \"MAJOR\"\n                        elif diff_details and \"minor\" in diff_details and diff_details[\"minor\"] &gt; 0:\n                            warning_level = \"MINOR\"\n                        else:\n                            warning_level = \"MICRO\"\n\n                # Check failure threshold\n                if config.vdiff_fail_matrix:\n                    fail_within_limits, diff_details = check_version_diff(\n                        installed, latest, config.vdiff_fail_matrix)\n                    is_compatible = fail_within_limits\n\n            # Create package info\n            package_info = PackageInfo(\n                name=package_name,\n                installed_version=str(installed),\n                latest_version=str(latest),\n                required_version=str(config.get_required_version()) if config.requirement else None\n            )\n\n            # Create and return result\n            return Result(\n                is_compatible=is_compatible,\n                needs_update=needs_update,\n                package_info=package_info,\n                warning_level=warning_level,\n                diff_details=diff_details\n            )\n\n        except Exception as e:\n            # Handle errors based on configuration\n            if config.fail_on_error:\n                raise\n            return Result(\n                is_compatible=False,\n                needs_update=False,\n                package_info=PackageInfo(name=package_name),\n                error=str(e)\n            )\n</code></pre> <code>cache = cache or VersionCache()</code> <code>instance-attribute</code> \u00b6 <code>provider = provider or StandardVersionProvider()</code> <code>instance-attribute</code> \u00b6 <code>__init__(provider=None, cache=None)</code> \u00b6 Source code in <code>src/tnh_scholar/utils/version_check/checker.py</code> <pre><code>def __init__(self, \n             provider: Optional[VersionProvider] = None,\n             cache: Optional[VersionCache] = None):\n    self.provider = provider or StandardVersionProvider()\n    self.cache = cache or VersionCache()\n</code></pre> <code>check_version(package_name, config=None)</code> \u00b6 <p>Check if package meets version requirements based on config.</p> Source code in <code>src/tnh_scholar/utils/version_check/checker.py</code> <pre><code>def check_version(self, \n                  package_name: str, \n                  config: Optional[VersionCheckerConfig] = None) -&gt; Result:\n    \"\"\"Check if package meets version requirements based on config.\"\"\"\n    config = config or VersionCheckerConfig()\n\n    try:\n        # Get versions\n        installed = self.provider.get_installed_version(package_name)\n        latest = self.provider.get_latest_version(package_name)\n\n        # Default values\n        is_compatible = True\n        needs_update = installed &lt; latest\n        warning_level = None\n        diff_details = None\n\n        # Check based on strategy\n        if config.strategy == VersionStrategy.MINIMUM:\n            is_compatible = check_minimum_version(installed, config.get_required_version())\n\n        elif config.strategy == VersionStrategy.EXACT:\n            is_compatible = check_exact_version(installed, config.get_required_version())\n\n        elif config.strategy == VersionStrategy.VERSION_DIFF:\n            # Check warning threshold\n            if config.vdiff_warn_matrix:\n                warn_within_limits, diff_details = check_version_diff(\n                    installed, latest, config.vdiff_warn_matrix)\n                if not warn_within_limits:\n                    # Determine warning level based on which component exceeded threshold\n                    if diff_details and \"major\" in diff_details and diff_details[\"major\"] &gt; 0:\n                        warning_level = \"MAJOR\"\n                    elif diff_details and \"minor\" in diff_details and diff_details[\"minor\"] &gt; 0:\n                        warning_level = \"MINOR\"\n                    else:\n                        warning_level = \"MICRO\"\n\n            # Check failure threshold\n            if config.vdiff_fail_matrix:\n                fail_within_limits, diff_details = check_version_diff(\n                    installed, latest, config.vdiff_fail_matrix)\n                is_compatible = fail_within_limits\n\n        # Create package info\n        package_info = PackageInfo(\n            name=package_name,\n            installed_version=str(installed),\n            latest_version=str(latest),\n            required_version=str(config.get_required_version()) if config.requirement else None\n        )\n\n        # Create and return result\n        return Result(\n            is_compatible=is_compatible,\n            needs_update=needs_update,\n            package_info=package_info,\n            warning_level=warning_level,\n            diff_details=diff_details\n        )\n\n    except Exception as e:\n        # Handle errors based on configuration\n        if config.fail_on_error:\n            raise\n        return Result(\n            is_compatible=False,\n            needs_update=False,\n            package_info=PackageInfo(name=package_name),\n            error=str(e)\n        )\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.cli","title":"<code>cli</code>","text":"<p>Command-line interface for version checking (stub for future implementation).</p>"},{"location":"api/#tnh_scholar.utils.version_check.cli.main","title":"<code>main()</code>","text":"<p>Command-line interface for version checking.</p> Source code in <code>src/tnh_scholar/utils/version_check/cli.py</code> <pre><code>def main():\n    \"\"\"Command-line interface for version checking.\"\"\"\n    raise NotImplementedError(\"CLI functionality is not yet implemented\")\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.config","title":"<code>config</code>","text":"<p>Configuration classes for version checking.</p>"},{"location":"api/#tnh_scholar.utils.version_check.config.VersionCheckerConfig","title":"<code>VersionCheckerConfig</code>","text":"<p>Configuration for version checking behavior.</p> Source code in <code>src/tnh_scholar/utils/version_check/config.py</code> <pre><code>class VersionCheckerConfig:\n    \"\"\"Configuration for version checking behavior.\"\"\"\n\n    def __init__(self,\n                 strategy: VersionStrategy = VersionStrategy.MINIMUM,\n                 requirement: str = \"\",\n                 fail_on_error: bool = False,\n                 cache_duration: int = 3600,  # 1 hour\n                 network_timeout: int = 5,    # seconds\n                 vdiff_warn_matrix: Optional[str] = None,\n                 vdiff_fail_matrix: Optional[str] = None):\n        \"\"\"Initialize version checker configuration.\"\"\"\n        self.strategy = strategy\n        self.requirement = requirement\n        self.fail_on_error = fail_on_error\n        self.cache_duration = cache_duration\n        self.network_timeout = network_timeout\n        self.vdiff_warn_matrix = vdiff_warn_matrix\n        self.vdiff_fail_matrix = vdiff_fail_matrix\n\n    def get_required_version(self) -&gt; Optional[Version]:\n        \"\"\"Get required version as a Version object.\"\"\"\n        return Version(self.requirement) if self.requirement else None\n</code></pre> <code>cache_duration = cache_duration</code> <code>instance-attribute</code> \u00b6 <code>fail_on_error = fail_on_error</code> <code>instance-attribute</code> \u00b6 <code>network_timeout = network_timeout</code> <code>instance-attribute</code> \u00b6 <code>requirement = requirement</code> <code>instance-attribute</code> \u00b6 <code>strategy = strategy</code> <code>instance-attribute</code> \u00b6 <code>vdiff_fail_matrix = vdiff_fail_matrix</code> <code>instance-attribute</code> \u00b6 <code>vdiff_warn_matrix = vdiff_warn_matrix</code> <code>instance-attribute</code> \u00b6 <code>__init__(strategy=VersionStrategy.MINIMUM, requirement='', fail_on_error=False, cache_duration=3600, network_timeout=5, vdiff_warn_matrix=None, vdiff_fail_matrix=None)</code> \u00b6 <p>Initialize version checker configuration.</p> Source code in <code>src/tnh_scholar/utils/version_check/config.py</code> <pre><code>def __init__(self,\n             strategy: VersionStrategy = VersionStrategy.MINIMUM,\n             requirement: str = \"\",\n             fail_on_error: bool = False,\n             cache_duration: int = 3600,  # 1 hour\n             network_timeout: int = 5,    # seconds\n             vdiff_warn_matrix: Optional[str] = None,\n             vdiff_fail_matrix: Optional[str] = None):\n    \"\"\"Initialize version checker configuration.\"\"\"\n    self.strategy = strategy\n    self.requirement = requirement\n    self.fail_on_error = fail_on_error\n    self.cache_duration = cache_duration\n    self.network_timeout = network_timeout\n    self.vdiff_warn_matrix = vdiff_warn_matrix\n    self.vdiff_fail_matrix = vdiff_fail_matrix\n</code></pre> <code>get_required_version()</code> \u00b6 <p>Get required version as a Version object.</p> Source code in <code>src/tnh_scholar/utils/version_check/config.py</code> <pre><code>def get_required_version(self) -&gt; Optional[Version]:\n    \"\"\"Get required version as a Version object.\"\"\"\n    return Version(self.requirement) if self.requirement else None\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.config.VersionStrategy","title":"<code>VersionStrategy</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of version checking strategies.</p> Source code in <code>src/tnh_scholar/utils/version_check/config.py</code> <pre><code>class VersionStrategy(Enum):\n    \"\"\"Enumeration of version checking strategies.\"\"\"\n    MINIMUM = \"minimum\"    # Package version must be &gt;= requirement\n    EXACT = \"exact\"        # Package version must be == requirement\n    LATEST = \"latest\"      # Package version should be the latest available\n    RANGE = \"range\"        # Package version must be within a specified range\n    VERSION_DIFF = \"vdiff\" # Check version difference against thresholds\n</code></pre> <code>EXACT = 'exact'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>LATEST = 'latest'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>MINIMUM = 'minimum'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>RANGE = 'range'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>VERSION_DIFF = 'vdiff'</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"api/#tnh_scholar.utils.version_check.models","title":"<code>models</code>","text":"<p>Data models for version checking results.</p>"},{"location":"api/#tnh_scholar.utils.version_check.models.PackageInfo","title":"<code>PackageInfo</code>  <code>dataclass</code>","text":"<p>Information about a package and its versions.</p> Source code in <code>src/tnh_scholar/utils/version_check/models.py</code> <pre><code>@dataclass\nclass PackageInfo:\n    \"\"\"Information about a package and its versions.\"\"\"\n\n    name: str\n    installed_version: Optional[str] = None\n    latest_version: Optional[str] = None\n    required_version: Optional[str] = None\n</code></pre> <code>installed_version = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>latest_version = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>name</code> <code>instance-attribute</code> \u00b6 <code>required_version = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>__init__(name, installed_version=None, latest_version=None, required_version=None)</code> \u00b6"},{"location":"api/#tnh_scholar.utils.version_check.models.Result","title":"<code>Result</code>  <code>dataclass</code>","text":"<p>Result of a version check operation.</p> Source code in <code>src/tnh_scholar/utils/version_check/models.py</code> <pre><code>@dataclass\nclass Result:\n    \"\"\"Result of a version check operation.\"\"\"\n\n    is_compatible: bool\n    needs_update: bool\n    package_info: PackageInfo\n    error: Optional[str] = None\n    warning_level: Optional[str] = None\n    diff_details: Optional[Dict[str, int]] = None\n\n    def get_upgrade_command(self) -&gt; str:\n        \"\"\"Return pip command to upgrade package.\"\"\"\n        if not self.package_info or not self.package_info.name:\n            return \"\"\n\n        if self.package_info.latest_version:\n            return f\"pip install --upgrade {self.package_info.name}=={self.package_info.latest_version}\"\n        else:\n            return f\"pip install --upgrade {self.package_info.name}\"\n</code></pre> <code>diff_details = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>error = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>is_compatible</code> <code>instance-attribute</code> \u00b6 <code>needs_update</code> <code>instance-attribute</code> \u00b6 <code>package_info</code> <code>instance-attribute</code> \u00b6 <code>warning_level = None</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>__init__(is_compatible, needs_update, package_info, error=None, warning_level=None, diff_details=None)</code> \u00b6 <code>get_upgrade_command()</code> \u00b6 <p>Return pip command to upgrade package.</p> Source code in <code>src/tnh_scholar/utils/version_check/models.py</code> <pre><code>def get_upgrade_command(self) -&gt; str:\n    \"\"\"Return pip command to upgrade package.\"\"\"\n    if not self.package_info or not self.package_info.name:\n        return \"\"\n\n    if self.package_info.latest_version:\n        return f\"pip install --upgrade {self.package_info.name}=={self.package_info.latest_version}\"\n    else:\n        return f\"pip install --upgrade {self.package_info.name}\"\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.providers","title":"<code>providers</code>","text":"<p>Version provider implementations for retrieving package versions.</p>"},{"location":"api/#tnh_scholar.utils.version_check.providers.StandardVersionProvider","title":"<code>StandardVersionProvider</code>","text":"<p>               Bases: <code>VersionProvider</code></p> <p>Standard implementation of version provider using importlib and PyPI.</p> Source code in <code>src/tnh_scholar/utils/version_check/providers.py</code> <pre><code>class StandardVersionProvider(VersionProvider):\n    \"\"\"Standard implementation of version provider using importlib and PyPI.\"\"\"\n\n    def __init__(self, cache: Optional[VersionCache] = None, timeout: int = 5):\n        self.cache = cache or VersionCache()\n        self.timeout = timeout\n        self.pypi_url_template = \"https://pypi.org/pypi/{package}/json\"\n\n    def get_installed_version(self, package_name: str) -&gt; Version:\n        \"\"\"Get installed package version.\"\"\"\n        try:\n            if version_str := str(importlib.metadata.version(package_name)):\n                return Version(version_str)\n            else:\n                raise InvalidVersion(f\"{package_name} version string is empty\")\n        except importlib.metadata.PackageNotFoundError as e:\n            raise ImportError(f\"{package_name} is not installed\") from e\n        except InvalidVersion as e:\n            raise InvalidVersion(f\"Invalid version for {package_name}: {e}\") from e\n\n    def get_latest_version(self, package_name: str) -&gt; Version:\n        \"\"\"Get latest available package version from PyPI.\"\"\"\n        # Check cache first\n        if cached_version := self.cache.get(f\"{package_name}_latest\"):\n            return cached_version\n\n        # Fetch from PyPI\n        url = self.pypi_url_template.format(package=package_name)\n        try:\n            return self._send_url_request(url, package_name)\n        except requests.RequestException as e:\n            raise requests.RequestException(\n                f\"Failed to fetch {package_name} version from PyPI: {e}\"\n            ) from e\n\n    def _send_url_request(self, url, package_name):\n        response = requests.get(url, timeout=self.timeout)\n        response.raise_for_status()\n        version_str = response.json()[\"info\"][\"version\"]\n        version = Version(version_str)\n\n        # Cache the result\n        self.cache.set(f\"{package_name}_latest\", version)\n\n        return version\n</code></pre> <code>cache = cache or VersionCache()</code> <code>instance-attribute</code> \u00b6 <code>pypi_url_template = 'https://pypi.org/pypi/{package}/json'</code> <code>instance-attribute</code> \u00b6 <code>timeout = timeout</code> <code>instance-attribute</code> \u00b6 <code>__init__(cache=None, timeout=5)</code> \u00b6 Source code in <code>src/tnh_scholar/utils/version_check/providers.py</code> <pre><code>def __init__(self, cache: Optional[VersionCache] = None, timeout: int = 5):\n    self.cache = cache or VersionCache()\n    self.timeout = timeout\n    self.pypi_url_template = \"https://pypi.org/pypi/{package}/json\"\n</code></pre> <code>get_installed_version(package_name)</code> \u00b6 <p>Get installed package version.</p> Source code in <code>src/tnh_scholar/utils/version_check/providers.py</code> <pre><code>def get_installed_version(self, package_name: str) -&gt; Version:\n    \"\"\"Get installed package version.\"\"\"\n    try:\n        if version_str := str(importlib.metadata.version(package_name)):\n            return Version(version_str)\n        else:\n            raise InvalidVersion(f\"{package_name} version string is empty\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ImportError(f\"{package_name} is not installed\") from e\n    except InvalidVersion as e:\n        raise InvalidVersion(f\"Invalid version for {package_name}: {e}\") from e\n</code></pre> <code>get_latest_version(package_name)</code> \u00b6 <p>Get latest available package version from PyPI.</p> Source code in <code>src/tnh_scholar/utils/version_check/providers.py</code> <pre><code>def get_latest_version(self, package_name: str) -&gt; Version:\n    \"\"\"Get latest available package version from PyPI.\"\"\"\n    # Check cache first\n    if cached_version := self.cache.get(f\"{package_name}_latest\"):\n        return cached_version\n\n    # Fetch from PyPI\n    url = self.pypi_url_template.format(package=package_name)\n    try:\n        return self._send_url_request(url, package_name)\n    except requests.RequestException as e:\n        raise requests.RequestException(\n            f\"Failed to fetch {package_name} version from PyPI: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.providers.VersionProvider","title":"<code>VersionProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for retrieving package version information.</p> Source code in <code>src/tnh_scholar/utils/version_check/providers.py</code> <pre><code>class VersionProvider(ABC):\n    \"\"\"Interface for retrieving package version information.\"\"\"\n\n    @abstractmethod\n    def get_installed_version(self, package_name: str) -&gt; Version:\n        \"\"\"Get installed package version.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_latest_version(self, package_name: str) -&gt; Version:\n        \"\"\"Get latest available package version.\"\"\"\n        pass\n</code></pre> <code>get_installed_version(package_name)</code> <code>abstractmethod</code> \u00b6 <p>Get installed package version.</p> Source code in <code>src/tnh_scholar/utils/version_check/providers.py</code> <pre><code>@abstractmethod\ndef get_installed_version(self, package_name: str) -&gt; Version:\n    \"\"\"Get installed package version.\"\"\"\n    pass\n</code></pre> <code>get_latest_version(package_name)</code> <code>abstractmethod</code> \u00b6 <p>Get latest available package version.</p> Source code in <code>src/tnh_scholar/utils/version_check/providers.py</code> <pre><code>@abstractmethod\ndef get_latest_version(self, package_name: str) -&gt; Version:\n    \"\"\"Get latest available package version.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.strategies","title":"<code>strategies</code>","text":"<p>Version comparison strategies for package version checking.</p>"},{"location":"api/#tnh_scholar.utils.version_check.strategies.check_exact_version","title":"<code>check_exact_version(installed, required)</code>","text":"<p>Check if installed version exactly matches requirement.</p> Source code in <code>src/tnh_scholar/utils/version_check/strategies.py</code> <pre><code>def check_exact_version(installed: Version, required: Optional[Version]) -&gt; bool:\n    \"\"\"Check if installed version exactly matches requirement.\"\"\"\n    return True if required is None else installed == required\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.strategies.check_minimum_version","title":"<code>check_minimum_version(installed, required)</code>","text":"<p>Check if installed version meets minimum requirement.</p> Source code in <code>src/tnh_scholar/utils/version_check/strategies.py</code> <pre><code>def check_minimum_version(installed: Version, required: Optional[Version]) -&gt; bool:\n    \"\"\"Check if installed version meets minimum requirement.\"\"\"\n    return True if required is None else installed &gt;= required\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.strategies.check_version_diff","title":"<code>check_version_diff(installed, reference, vdiff_matrix)</code>","text":"<p>Check if version difference is within specified limits.</p> Source code in <code>src/tnh_scholar/utils/version_check/strategies.py</code> <pre><code>def check_version_diff(\n    installed: Version,\n    reference: Version,\n    vdiff_matrix: str\n) -&gt; Tuple[bool, Dict[str, int]]:\n    \"\"\"Check if version difference is within specified limits.\"\"\"\n    # Calculate actual differences\n    major_diff = abs(reference.major - installed.major)\n    minor_diff = abs(reference.minor - installed.minor) if reference.major == installed.major else 0\n    micro_diff = abs(reference.micro - installed.micro) if (reference.major == installed.major and \n                                                          reference.minor == installed.minor) else 0\n\n    diff_details = {\n        \"major\": major_diff,\n        \"minor\": minor_diff,\n        \"micro\": micro_diff\n    }\n\n    # If no matrix provided, differences are acceptable\n    if not vdiff_matrix:\n        return True, diff_details\n\n    # Parse matrix\n    major_limit, minor_limit, micro_limit = parse_vdiff_matrix(vdiff_matrix)\n\n    # Check limits (None means no limit)\n    if major_limit is not None and major_diff &gt; major_limit:\n        return False, diff_details\n\n    if minor_limit is not None and minor_diff &gt; minor_limit:\n        return False, diff_details\n\n    if micro_limit is not None and micro_diff &gt; micro_limit:\n        return False, diff_details\n\n    return True, diff_details\n</code></pre>"},{"location":"api/#tnh_scholar.utils.version_check.strategies.parse_vdiff_matrix","title":"<code>parse_vdiff_matrix(matrix_str)</code>","text":"<p>Parse a version difference matrix string.</p> Source code in <code>src/tnh_scholar/utils/version_check/strategies.py</code> <pre><code>def parse_vdiff_matrix(matrix_str: str) -&gt; Tuple[Optional[int], Optional[int], Optional[int]]:\n    \"\"\"Parse a version difference matrix string.\"\"\"\n    parts = matrix_str.split(\".\")\n    if len(parts) != 3:\n        raise ValueError(f\"Invalid version difference matrix: {matrix_str}\")\n\n    limits = []\n    for part in parts:\n        if part == \"*\":\n            limits.append(None)  # No limit\n        else:\n            try:\n                limits.append(int(part))\n            except ValueError as e:\n                raise ValueError(f\"Invalid version component: {part}\") from e\n\n    return tuple(limits)  # Tuple[Optional[int], Optional[int], Optional[int]]\n</code></pre>"},{"location":"api/#tnh_scholar.utils.webhook_server","title":"<code>webhook_server</code>","text":""},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer","title":"<code>WebhookServer</code>","text":"<p>A generic webhook server that can receive callbacks from external services.</p> Source code in <code>src/tnh_scholar/utils/webhook_server.py</code> <pre><code>class WebhookServer:\n    \"\"\"A generic webhook server that can receive callbacks from external services.\"\"\"\n\n    def __init__(self, port: int = 5050):\n        \"\"\"\n        Initialize webhook server with configuration.\n\n        Args:\n            port: The port to run the Flask server on\n        \"\"\"\n        self.port = port\n        self.app = self._create_flask_app()\n        self.webhook_received = Condition()\n        self.webhook_data = None\n        self.flask_running = Event()\n        self.flask_server_thread = None\n        self.tunnel_process = None\n\n    def _create_flask_app(self) -&gt; Flask:\n        \"\"\"Create and configure Flask app with webhook endpoint.\"\"\"\n        app = Flask(__name__)\n\n        @app.route('/healthcheck', methods=['GET'])\n        def healthcheck():\n            \"\"\"Simple endpoint to verify the server is running.\"\"\"\n            return jsonify({\n                'status': 'ok',\n                'timestamp': datetime.now().isoformat(),\n                'webhook_received': self.webhook_data is not None\n            })\n\n        @app.route('/webhook', methods=['POST'])\n        def handle_webhook():\n            \"\"\"Receive webhook data from external services.\"\"\"\n            # Get JSON data from the request\n            data = request.json\n\n            # Log webhook receipt\n            print(\"\\n\" + \"=\"*40)\n            print(f\"WEBHOOK RECEIVED at {datetime.now().strftime('%H:%M:%S')}\")\n\n            if data is not None:\n                print(f\"Webhook data status: {data.get('status', 'unknown')}\")\n\n                # Update the shared state with proper synchronization\n                with self.webhook_received:\n                    self.webhook_data = data\n                    self.webhook_received.notify_all()\n                    print(\"Notification sent to waiting threads\")\n            else:\n                print(\"Webhook received with no JSON data\")\n\n            # Always return a success response to acknowledge receipt\n            return jsonify({'status': 'received'}), 200\n\n        @app.route('/shutdown', methods=['POST'])\n        def shutdown():\n            \"\"\"Endpoint to gracefully shut down the Flask server.\"\"\"\n            func = request.environ.get('werkzeug.server.shutdown')\n            if func is None:\n                raise RuntimeError('Not running with the Werkzeug Server')\n            func()\n            return 'Server shutting down...'\n\n        return app\n\n    def start_server(self) -&gt; None:\n        \"\"\"Start Flask server in a separate thread.\"\"\"\n        # Check if server is already running\n        if self.flask_running.is_set() and \\\n            self.flask_server_thread and \\\n                self.flask_server_thread.is_alive():\n            print(f\"Flask server already running on port {self.port}\")\n            return\n\n        # Reset state\n        self.flask_running.clear()\n\n        # Create thread function that sets event when server starts\n        def run_server():\n            print(f\"Starting Flask server on port {self.port}...\")\n            self.flask_running.set()\n            self.app.run(\n                host=\"0.0.0.0\", \n                port=self.port, \n                debug=False, \n                use_reloader=False\n                )\n            self.flask_running.clear()\n            print(\"Flask server has stopped\")\n\n        # Start server in a daemon thread\n        self.flask_server_thread = Thread(target=run_server, daemon=True)\n        self.flask_server_thread.start()\n\n        # Wait for server to start\n        if not self.flask_running.wait(timeout=5):\n            raise RuntimeError(\"Flask server failed to start within timeout period\")\n\n        print(f\"Flask server started successfully on port {self.port}\")\n\n    def shutdown_server(self) -&gt; None:\n        \"\"\"Gracefully shut down the Flask server.\"\"\"\n        if not self.flask_running.is_set():\n            print(\"Flask server is not running\")\n            return\n\n        try:\n            print(\"Shutting down Flask server...\")\n            requests.post(f\"http://localhost:{self.port}/shutdown\")\n\n            # Wait for server to stop\n            if self.flask_server_thread:\n                self.flask_server_thread.join(timeout=5)\n\n            if self.flask_running.is_set():\n                print(\"WARNING: Flask server did not shut down gracefully\")\n            else:\n                print(\"Flask server shut down successfully\")\n        except Exception as e:\n            print(f\"Error shutting down Flask server: {e}\")\n\n    def create_tunnel(self) -&gt; Optional[str]:\n        \"\"\"\n        Create a public webhook URL using py-localtunnel.\n\n        Returns:\n            Optional[str]: The public webhook URL or None if tunnel creation failed\n        \"\"\"\n        # First check if pylt is installed\n        try:\n            subprocess.run([\"pylt\", \"--version\"], check=True, capture_output=True)\n        except (subprocess.SubprocessError, FileNotFoundError) as e:\n            print(\"ERROR: pylt not found. Install with: pip install pylt\")\n            raise RuntimeError(\"Tunnel not started.\") from e\n\n        print(f\"Creating public tunnel to port {self.port}...\")\n\n        # Start the localtunnel process\n        self.tunnel_process = subprocess.Popen(\n            [\"pylt\", \"port\", str(self.port)],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n\n        # Give it time to establish the tunnel\n        time.sleep(3)\n\n        # Check if process started successfully\n        if self.tunnel_process.poll() is not None:\n            if self.tunnel_process.stderr:\n                stderr = self.tunnel_process.stderr.read()\n            else:\n                raise RuntimeError(\"ERROR: Tunnel process failed: no stderr\")\n            if self.tunnel_process.stdout:\n                stdout = self.tunnel_process.stdout.read()\n            else:\n                raise RuntimeError(\"ERROR: Tunnel process failed: no stderr\")\n            raise RuntimeError(\n                f\"ERROR: Tunnel process failed:\\nSTDOUT: {stdout}\\nSTDERR: {stderr}\"\n                )\n\n        # Regular expression to find the URL in the output\n        url_pattern = re.compile(r'https?://[^\\s\\'\"]+')\n\n        # Read output with timeout\n        start_time = time.time()\n        tunnel_url = None\n\n        while time.time() - start_time &lt; 15:  # Wait up to 15 seconds\n            if not self.tunnel_process.stdout:\n                raise RuntimeError(\"ERROR: tunnel process has no stdout.\")\n\n            line = self.tunnel_process.stdout.readline()\n            if not line:\n                time.sleep(0.1)\n                continue\n            print(f\"Tunnel output: {line.strip()}\")\n\n            # Check if the URL pattern is found\n            if match := url_pattern.search(line):\n                tunnel_url = match[0]\n                break\n\n        if not tunnel_url:\n            print(\"ERROR: Could not find tunnel URL in output\")\n            if self.tunnel_process.poll() is None:\n                self.tunnel_process.terminate()\n            return None\n\n        print(f\"Public tunnel created: {tunnel_url}\")\n        webhook_url = f\"{tunnel_url}/webhook\"\n\n        # Verify the tunnel works\n        try:\n            response = requests.get(f\"{tunnel_url}/healthcheck\", timeout=20)\n            if response.status_code == 200:\n                print(\"Tunnel verified: Flask server is accessible\")\n            else:\n                print(f\"Tunnel health check returned status {response.status_code}\")\n                raise RuntimeError(\"Could not verify Tunnel.\") \n        except requests.RequestException as e:\n            raise e\n\n        return webhook_url\n\n    def close_tunnel(self) -&gt; None:\n        \"\"\"Close the tunnel if it's running.\"\"\"\n        if self.tunnel_process and self.tunnel_process.poll() is None:\n            print(\"Closing tunnel...\")\n            self.tunnel_process.terminate()\n            self.tunnel_process.wait(timeout=5)\n            print(\"Tunnel closed\")\n\n    def wait_for_webhook(self, timeout: int = 120) -&gt; Optional[Dict]:\n        \"\"\"\n        Wait for webhook data to be received.\n\n        Args:\n            timeout: Maximum time to wait in seconds\n\n        Returns:\n            Optional[Dict]: The webhook data or None if timed out\n        \"\"\"\n        print(f\"Waiting for webhook callback (timeout: {timeout}s)...\")\n\n        with self.webhook_received:\n            # Wait for notification with timeout\n            webhook_received = self.webhook_received.wait(timeout=timeout)\n\n            if webhook_received and self.webhook_data is not None:\n                print(\"Webhook received with data\")\n                return self.webhook_data\n\n        print(f\"Timed out waiting for webhook after {timeout} seconds\")\n        return None\n\n    def cleanup(self) -&gt; None:\n        \"\"\"Clean up all resources.\"\"\"\n        self.close_tunnel()\n        self.shutdown_server()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.app","title":"<code>app = self._create_flask_app()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.flask_running","title":"<code>flask_running = Event()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.flask_server_thread","title":"<code>flask_server_thread = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.port","title":"<code>port = port</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.tunnel_process","title":"<code>tunnel_process = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.webhook_data","title":"<code>webhook_data = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.webhook_received","title":"<code>webhook_received = Condition()</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.__init__","title":"<code>__init__(port=5050)</code>","text":"<p>Initialize webhook server with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>The port to run the Flask server on</p> <code>5050</code> Source code in <code>src/tnh_scholar/utils/webhook_server.py</code> <pre><code>def __init__(self, port: int = 5050):\n    \"\"\"\n    Initialize webhook server with configuration.\n\n    Args:\n        port: The port to run the Flask server on\n    \"\"\"\n    self.port = port\n    self.app = self._create_flask_app()\n    self.webhook_received = Condition()\n    self.webhook_data = None\n    self.flask_running = Event()\n    self.flask_server_thread = None\n    self.tunnel_process = None\n</code></pre>"},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.cleanup","title":"<code>cleanup()</code>","text":"<p>Clean up all resources.</p> Source code in <code>src/tnh_scholar/utils/webhook_server.py</code> <pre><code>def cleanup(self) -&gt; None:\n    \"\"\"Clean up all resources.\"\"\"\n    self.close_tunnel()\n    self.shutdown_server()\n</code></pre>"},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.close_tunnel","title":"<code>close_tunnel()</code>","text":"<p>Close the tunnel if it's running.</p> Source code in <code>src/tnh_scholar/utils/webhook_server.py</code> <pre><code>def close_tunnel(self) -&gt; None:\n    \"\"\"Close the tunnel if it's running.\"\"\"\n    if self.tunnel_process and self.tunnel_process.poll() is None:\n        print(\"Closing tunnel...\")\n        self.tunnel_process.terminate()\n        self.tunnel_process.wait(timeout=5)\n        print(\"Tunnel closed\")\n</code></pre>"},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.create_tunnel","title":"<code>create_tunnel()</code>","text":"<p>Create a public webhook URL using py-localtunnel.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The public webhook URL or None if tunnel creation failed</p> Source code in <code>src/tnh_scholar/utils/webhook_server.py</code> <pre><code>def create_tunnel(self) -&gt; Optional[str]:\n    \"\"\"\n    Create a public webhook URL using py-localtunnel.\n\n    Returns:\n        Optional[str]: The public webhook URL or None if tunnel creation failed\n    \"\"\"\n    # First check if pylt is installed\n    try:\n        subprocess.run([\"pylt\", \"--version\"], check=True, capture_output=True)\n    except (subprocess.SubprocessError, FileNotFoundError) as e:\n        print(\"ERROR: pylt not found. Install with: pip install pylt\")\n        raise RuntimeError(\"Tunnel not started.\") from e\n\n    print(f\"Creating public tunnel to port {self.port}...\")\n\n    # Start the localtunnel process\n    self.tunnel_process = subprocess.Popen(\n        [\"pylt\", \"port\", str(self.port)],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True\n    )\n\n    # Give it time to establish the tunnel\n    time.sleep(3)\n\n    # Check if process started successfully\n    if self.tunnel_process.poll() is not None:\n        if self.tunnel_process.stderr:\n            stderr = self.tunnel_process.stderr.read()\n        else:\n            raise RuntimeError(\"ERROR: Tunnel process failed: no stderr\")\n        if self.tunnel_process.stdout:\n            stdout = self.tunnel_process.stdout.read()\n        else:\n            raise RuntimeError(\"ERROR: Tunnel process failed: no stderr\")\n        raise RuntimeError(\n            f\"ERROR: Tunnel process failed:\\nSTDOUT: {stdout}\\nSTDERR: {stderr}\"\n            )\n\n    # Regular expression to find the URL in the output\n    url_pattern = re.compile(r'https?://[^\\s\\'\"]+')\n\n    # Read output with timeout\n    start_time = time.time()\n    tunnel_url = None\n\n    while time.time() - start_time &lt; 15:  # Wait up to 15 seconds\n        if not self.tunnel_process.stdout:\n            raise RuntimeError(\"ERROR: tunnel process has no stdout.\")\n\n        line = self.tunnel_process.stdout.readline()\n        if not line:\n            time.sleep(0.1)\n            continue\n        print(f\"Tunnel output: {line.strip()}\")\n\n        # Check if the URL pattern is found\n        if match := url_pattern.search(line):\n            tunnel_url = match[0]\n            break\n\n    if not tunnel_url:\n        print(\"ERROR: Could not find tunnel URL in output\")\n        if self.tunnel_process.poll() is None:\n            self.tunnel_process.terminate()\n        return None\n\n    print(f\"Public tunnel created: {tunnel_url}\")\n    webhook_url = f\"{tunnel_url}/webhook\"\n\n    # Verify the tunnel works\n    try:\n        response = requests.get(f\"{tunnel_url}/healthcheck\", timeout=20)\n        if response.status_code == 200:\n            print(\"Tunnel verified: Flask server is accessible\")\n        else:\n            print(f\"Tunnel health check returned status {response.status_code}\")\n            raise RuntimeError(\"Could not verify Tunnel.\") \n    except requests.RequestException as e:\n        raise e\n\n    return webhook_url\n</code></pre>"},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.shutdown_server","title":"<code>shutdown_server()</code>","text":"<p>Gracefully shut down the Flask server.</p> Source code in <code>src/tnh_scholar/utils/webhook_server.py</code> <pre><code>def shutdown_server(self) -&gt; None:\n    \"\"\"Gracefully shut down the Flask server.\"\"\"\n    if not self.flask_running.is_set():\n        print(\"Flask server is not running\")\n        return\n\n    try:\n        print(\"Shutting down Flask server...\")\n        requests.post(f\"http://localhost:{self.port}/shutdown\")\n\n        # Wait for server to stop\n        if self.flask_server_thread:\n            self.flask_server_thread.join(timeout=5)\n\n        if self.flask_running.is_set():\n            print(\"WARNING: Flask server did not shut down gracefully\")\n        else:\n            print(\"Flask server shut down successfully\")\n    except Exception as e:\n        print(f\"Error shutting down Flask server: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.start_server","title":"<code>start_server()</code>","text":"<p>Start Flask server in a separate thread.</p> Source code in <code>src/tnh_scholar/utils/webhook_server.py</code> <pre><code>def start_server(self) -&gt; None:\n    \"\"\"Start Flask server in a separate thread.\"\"\"\n    # Check if server is already running\n    if self.flask_running.is_set() and \\\n        self.flask_server_thread and \\\n            self.flask_server_thread.is_alive():\n        print(f\"Flask server already running on port {self.port}\")\n        return\n\n    # Reset state\n    self.flask_running.clear()\n\n    # Create thread function that sets event when server starts\n    def run_server():\n        print(f\"Starting Flask server on port {self.port}...\")\n        self.flask_running.set()\n        self.app.run(\n            host=\"0.0.0.0\", \n            port=self.port, \n            debug=False, \n            use_reloader=False\n            )\n        self.flask_running.clear()\n        print(\"Flask server has stopped\")\n\n    # Start server in a daemon thread\n    self.flask_server_thread = Thread(target=run_server, daemon=True)\n    self.flask_server_thread.start()\n\n    # Wait for server to start\n    if not self.flask_running.wait(timeout=5):\n        raise RuntimeError(\"Flask server failed to start within timeout period\")\n\n    print(f\"Flask server started successfully on port {self.port}\")\n</code></pre>"},{"location":"api/#tnh_scholar.utils.webhook_server.WebhookServer.wait_for_webhook","title":"<code>wait_for_webhook(timeout=120)</code>","text":"<p>Wait for webhook data to be received.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>Maximum time to wait in seconds</p> <code>120</code> <p>Returns:</p> Type Description <code>Optional[Dict]</code> <p>Optional[Dict]: The webhook data or None if timed out</p> Source code in <code>src/tnh_scholar/utils/webhook_server.py</code> <pre><code>def wait_for_webhook(self, timeout: int = 120) -&gt; Optional[Dict]:\n    \"\"\"\n    Wait for webhook data to be received.\n\n    Args:\n        timeout: Maximum time to wait in seconds\n\n    Returns:\n        Optional[Dict]: The webhook data or None if timed out\n    \"\"\"\n    print(f\"Waiting for webhook callback (timeout: {timeout}s)...\")\n\n    with self.webhook_received:\n        # Wait for notification with timeout\n        webhook_received = self.webhook_received.wait(timeout=timeout)\n\n        if webhook_received and self.webhook_data is not None:\n            print(\"Webhook received with data\")\n            return self.webhook_data\n\n    print(f\"Timed out waiting for webhook after {timeout} seconds\")\n    return None\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing","title":"<code>video_processing</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing","title":"<code>video_processing</code>","text":"<p>video_processing.py</p>"},{"location":"api/#tnh_scholar.video_processing.video_processing.BASE_YDL_OPTIONS","title":"<code>BASE_YDL_OPTIONS = {'quiet': False, 'no_warnings': True, 'extract_flat': True, 'socket_timeout': 30, 'retries': 3, 'ignoreerrors': True, 'logger': logger}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.DEFAULT_AUDIO_OPTIONS","title":"<code>DEFAULT_AUDIO_OPTIONS = BASE_YDL_OPTIONS | {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'noplaylist': True}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.DEFAULT_METADATA_FIELDS","title":"<code>DEFAULT_METADATA_FIELDS = ['id', 'title', 'description', 'duration', 'upload_date', 'uploader', 'channel_url', 'webpage_url', 'original_url', 'channel', 'language', 'categories', 'tags']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.DEFAULT_METADATA_OPTIONS","title":"<code>DEFAULT_METADATA_OPTIONS = BASE_YDL_OPTIONS | {'skip_download': True}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.DEFAULT_TRANSCRIPT_OPTIONS","title":"<code>DEFAULT_TRANSCRIPT_OPTIONS = BASE_YDL_OPTIONS | {'skip_download': True, 'writesubtitles': True, 'writeautomaticsub': True, 'subtitlesformat': 'ttml'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.DEFAULT_VIDEO_OPTIONS","title":"<code>DEFAULT_VIDEO_OPTIONS = BASE_YDL_OPTIONS | {'format': 'bestvideo+bestaudio/best', 'merge_output_format': 'mp4', 'noplaylist': True}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.TEMP_FILENAME_FORMAT","title":"<code>TEMP_FILENAME_FORMAT = 'temp_%(id)s'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.TEMP_FILENAME_STR","title":"<code>TEMP_FILENAME_STR = 'temp_{id}'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.DLPDownloader","title":"<code>DLPDownloader</code>","text":"<p>               Bases: <code>YTDownloader</code></p> <p>yt-dlp based implementation of YouTube content retrieval.</p> <p>Assures temporary file export is in the form .  where ID is the YouTube video id, and ext is the appropriate extension. <p>Renames the export file to be based on title and ID by default, or moves the export file to the specified output file with appropriate extension.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class DLPDownloader(YTDownloader):\n    \"\"\"\n    yt-dlp based implementation of YouTube content retrieval.\n\n    Assures temporary file export is in the form &lt;ID&gt;.&lt;ext&gt; \n    where ID is the YouTube video id, and ext is the appropriate\n    extension.\n\n    Renames the export file to be based on title and ID by\n    default, or moves the export file to the specified output\n    file with appropriate extension.\n    \"\"\"\n\n    def __init__(self, config: Optional[dict] = None):\n        self.config = config or BASE_YDL_OPTIONS\n\n    def get_metadata(\n        self,\n        url: str,\n    ) -&gt; Metadata:\n        \"\"\"\n        Get metadata for a YouTube video. \n        \"\"\"\n        options = DEFAULT_METADATA_OPTIONS | self.config\n        with yt_dlp.YoutubeDL(options) as ydl:\n            if info := ydl.extract_info(url):\n                return self._extract_metadata(info)\n            logger.error(f\"Unable to download metadata for {url}.\")\n            raise DownloadError(\"No info returned.\")\n\n    def get_transcript(\n        self,\n        url: str,\n        lang: str = \"en\",\n        output_path: Optional[Path] = None,\n    ) -&gt; VideoTranscript:\n        \"\"\"\n        Downloads video transcript in TTML format.\n\n        Args:\n            url: YouTube video URL\n            lang: Language code for transcript (default: \"en\")\n            output_path: Optional output directory (uses current dir if None)\n\n        Returns:\n            TranscriptResource containing TTML file path and metadata\n\n        Raises:\n            TranscriptError: If no transcript found for specified language\n        \"\"\"\n        temp_path = Path.cwd() / TEMP_FILENAME_FORMAT\n        options = DEFAULT_TRANSCRIPT_OPTIONS | self.config | {\n            \"skip_download\": True,\n            \"subtitleslangs\": [lang],\n            \"outtmpl\": str(temp_path),\n        }\n\n        with yt_dlp.YoutubeDL(options) as ydl:\n            if info := ydl.extract_info(url):\n                metadata = self._extract_metadata(info)\n                filepath = Path(ydl.prepare_filename(info)).with_suffix(f\".{lang}.ttml\")\n                filepath = self._convert_filename(filepath, metadata, output_path)\n                return VideoTranscript(metadata=metadata, filepath=filepath)\n            else:\n                logger.error(\"Info not found.\")\n                raise TranscriptError(f\"Transcript not downloaded for {url} in {lang}\")\n\n    def get_audio(\n        self, \n        url: str, \n        start: Optional[str] = None,\n        end: Optional[str] = None,\n        output_path: Optional[Path] = None\n    ) -&gt; VideoAudio:\n        \"\"\"Download audio and get metadata for a YouTube video.\"\"\"\n        temp_path = Path.cwd() / TEMP_FILENAME_FORMAT\n        options = DEFAULT_AUDIO_OPTIONS | self.config | {\n            \"outtmpl\": str(temp_path)\n        }\n\n        self._add_start_stop_times(options, start, end)\n\n        with yt_dlp.YoutubeDL(options) as ydl:\n            if info := ydl.extract_info(url, download=True):\n                metadata = self._extract_metadata(info)\n                filepath = Path(ydl.prepare_filename(info)).with_suffix(\".mp3\")\n                filepath = self._convert_filename(filepath, metadata, output_path)\n                return VideoAudio(metadata=metadata, filepath=filepath)\n            else:\n                logger.error(\"Info not found.\")\n                raise DownloadError(f\"Unable to download {url}.\")\n\n    def get_video(\n        self,\n        url: str,\n        quality: Optional[str] = None,\n        output_path: Optional[Path] = None\n    ) -&gt; VideoFile:\n        \"\"\"\n        Download the full video with associated metadata.\n\n        Args:\n            url: YouTube video URL\n            quality: yt-dlp format string (default: highest available)\n            output_path: Optional output directory\n\n        Returns:\n            VideoFile containing video file path and metadata\n\n        Raises:\n            VideoDownloadError: If download fails\n        \"\"\"\n        temp_path = Path.cwd() / TEMP_FILENAME_FORMAT\n        video_options = DEFAULT_VIDEO_OPTIONS | self.config | {\n            \"outtmpl\": str(temp_path)\n        }\n        if quality:\n            video_options[\"format\"] = quality\n\n        with yt_dlp.YoutubeDL(video_options) as ydl:\n            if info := ydl.extract_info(url, download=True):\n                metadata = self._extract_metadata(info)\n                ext = info.get(\"ext\", \"mp4\")\n                filepath = Path(ydl.prepare_filename(info)).with_suffix(f\".{ext}\")\n                filepath = self._convert_filename(filepath, metadata, output_path)\n                return VideoFile(metadata=metadata, filepath=filepath)\n            else:\n                logger.error(\"Info not found.\")\n                raise VideoDownloadError(f\"Unable to download video for {url}.\")\n\n    #TODO this function is not affecting the start time of processing. \n    # find a fix or new implementation \n    # (pydub postprocessing after yt-dlp? keep yt-dlp minimal?)        \n    def _add_start_stop_times(\n        self, options: dict, start: Optional[str], end: Optional[str]) -&gt; None:\n        \"\"\"\n        Adds -ss and -to arguments for FFmpegExtractAudio via postprocessor_args dict.\n        Modifies options in place.\n        \"\"\"\n        if start or end:\n            ppa_args = []\n            if start:\n                ppa_args.extend([\"-ss\", start])\n                logger.debug(f\"Added start time to postprocessor args: {start}\")\n            if end:\n                ppa_args.extend([\"-to\", end])\n                logger.debug(f\"Added end time to postprocessor args: {end}\")\n\n            postprocessor_args = options.setdefault(\"postprocessor_args\", {})\n\n            postprocessor_args.setdefault(\"ExtractAudio\", []).extend(ppa_args)\n\n        logger.info(f\"Updated options for postprocessor_args: \"\n                    f\"{options.get('postprocessor_args')}\")\n\n    def _extract_metadata(self, info: dict) -&gt; Metadata:\n        \"\"\"Extract standard metadata fields from yt-dlp info.\"\"\"\n        return Metadata.from_fields(info, DEFAULT_METADATA_FIELDS)\n\n    def _show_info(self, info: Metadata) -&gt; None:\n        \"\"\"Debug routine for displaying info.\"\"\"\n        for k in info:\n            if data := str(info[k]):\n                if len(data) &lt; 200:\n                    print(f\"{k}: {data}\")\n                else:\n                    print(f\"{k}: {data[:200]} ...\")\n\n    def get_default_filename_stem(self, metadata: Metadata) -&gt; str:\n        \"\"\"Generate the object download filename.\"\"\"\n        # Expect both id and title in Youtube metadata\n        assert metadata[\"id\"]\n        assert metadata[\"title\"] \n        video_id = str(metadata[\"id\"])\n        sanitized_title = sanitize_filename(str(metadata[\"title\"]))\n        return f\"{sanitized_title}_{video_id}\"\n\n    def get_default_export_name(self, url) -&gt; str:\n        \"\"\"Get default export filename for a URL.\"\"\"\n        metadata = self.get_metadata(url)\n        return self.get_default_filename_stem(metadata)\n\n    def _convert_filename(\n        self, \n        temp_path: Path,\n        metadata: Metadata, \n        output_path: Optional[Path]\n    ) -&gt; Path:\n        \"\"\"\n        Move/rename file from temp_path to output_path if specified.\n        If output_path is not provided, a sanitized title and video ID are \n        used to create the new filename. This function is required because yt-dlp \n        is not consistent in its output file naming (across subtitles, audio, metadata)\n        In this interface implementation we use a temp_path and TEMP_FILENAME_FORMAT to \n        specify the the temporary output to be the video_id followed by the correct \n        extension for all resources. This function then converts the temp_path\n        to the appropriately named resource, using output_path if specified,\n        or a default filename format ({sanitized_title}_{id}).\n        \"\"\"\n        video_id = str(metadata[\"id\"])\n        if video_id not in str(temp_path):\n            raise VideoProcessingError(f\"Temporary path '{temp_path}' \"\n                                       \"does not contain video ID '{video_id}'.\")\n        if not temp_path.suffix:\n            raise VideoProcessingError(f\"Temporary path '{temp_path}' \"\n                                       \"does not have a file extension.\")\n\n        if not output_path:\n            new_filename = self.get_default_filename_stem(metadata)\n            new_path = Path(str(temp_path).replace(\n                TEMP_FILENAME_STR.format(id=video_id), new_filename\n                )\n            )\n            logger.debug(f\"Renaming downloaded YT resource to: {new_path}\")\n            return temp_path.rename(new_path)\n\n        if not output_path.suffix:\n            output_path = output_path.with_suffix(temp_path.suffix)\n            logger.info(f\"Added extension {temp_path.suffix} to output path\")\n        elif output_path.suffix != temp_path.suffix:\n            output_path = output_path.with_suffix(temp_path.suffix)\n            logger.warning(f\"Replaced output extension with {temp_path.suffix}\")\n        return temp_path.rename(output_path)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.DLPDownloader.config","title":"<code>config = config or BASE_YDL_OPTIONS</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.DLPDownloader.__init__","title":"<code>__init__(config=None)</code>","text":"Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def __init__(self, config: Optional[dict] = None):\n    self.config = config or BASE_YDL_OPTIONS\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.DLPDownloader.get_audio","title":"<code>get_audio(url, start=None, end=None, output_path=None)</code>","text":"<p>Download audio and get metadata for a YouTube video.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_audio(\n    self, \n    url: str, \n    start: Optional[str] = None,\n    end: Optional[str] = None,\n    output_path: Optional[Path] = None\n) -&gt; VideoAudio:\n    \"\"\"Download audio and get metadata for a YouTube video.\"\"\"\n    temp_path = Path.cwd() / TEMP_FILENAME_FORMAT\n    options = DEFAULT_AUDIO_OPTIONS | self.config | {\n        \"outtmpl\": str(temp_path)\n    }\n\n    self._add_start_stop_times(options, start, end)\n\n    with yt_dlp.YoutubeDL(options) as ydl:\n        if info := ydl.extract_info(url, download=True):\n            metadata = self._extract_metadata(info)\n            filepath = Path(ydl.prepare_filename(info)).with_suffix(\".mp3\")\n            filepath = self._convert_filename(filepath, metadata, output_path)\n            return VideoAudio(metadata=metadata, filepath=filepath)\n        else:\n            logger.error(\"Info not found.\")\n            raise DownloadError(f\"Unable to download {url}.\")\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.DLPDownloader.get_default_export_name","title":"<code>get_default_export_name(url)</code>","text":"<p>Get default export filename for a URL.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_default_export_name(self, url) -&gt; str:\n    \"\"\"Get default export filename for a URL.\"\"\"\n    metadata = self.get_metadata(url)\n    return self.get_default_filename_stem(metadata)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.DLPDownloader.get_default_filename_stem","title":"<code>get_default_filename_stem(metadata)</code>","text":"<p>Generate the object download filename.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_default_filename_stem(self, metadata: Metadata) -&gt; str:\n    \"\"\"Generate the object download filename.\"\"\"\n    # Expect both id and title in Youtube metadata\n    assert metadata[\"id\"]\n    assert metadata[\"title\"] \n    video_id = str(metadata[\"id\"])\n    sanitized_title = sanitize_filename(str(metadata[\"title\"]))\n    return f\"{sanitized_title}_{video_id}\"\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.DLPDownloader.get_metadata","title":"<code>get_metadata(url)</code>","text":"<p>Get metadata for a YouTube video.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_metadata(\n    self,\n    url: str,\n) -&gt; Metadata:\n    \"\"\"\n    Get metadata for a YouTube video. \n    \"\"\"\n    options = DEFAULT_METADATA_OPTIONS | self.config\n    with yt_dlp.YoutubeDL(options) as ydl:\n        if info := ydl.extract_info(url):\n            return self._extract_metadata(info)\n        logger.error(f\"Unable to download metadata for {url}.\")\n        raise DownloadError(\"No info returned.\")\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.DLPDownloader.get_transcript","title":"<code>get_transcript(url, lang='en', output_path=None)</code>","text":"<p>Downloads video transcript in TTML format.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>YouTube video URL</p> required <code>lang</code> <code>str</code> <p>Language code for transcript (default: \"en\")</p> <code>'en'</code> <code>output_path</code> <code>Optional[Path]</code> <p>Optional output directory (uses current dir if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>VideoTranscript</code> <p>TranscriptResource containing TTML file path and metadata</p> <p>Raises:</p> Type Description <code>TranscriptError</code> <p>If no transcript found for specified language</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_transcript(\n    self,\n    url: str,\n    lang: str = \"en\",\n    output_path: Optional[Path] = None,\n) -&gt; VideoTranscript:\n    \"\"\"\n    Downloads video transcript in TTML format.\n\n    Args:\n        url: YouTube video URL\n        lang: Language code for transcript (default: \"en\")\n        output_path: Optional output directory (uses current dir if None)\n\n    Returns:\n        TranscriptResource containing TTML file path and metadata\n\n    Raises:\n        TranscriptError: If no transcript found for specified language\n    \"\"\"\n    temp_path = Path.cwd() / TEMP_FILENAME_FORMAT\n    options = DEFAULT_TRANSCRIPT_OPTIONS | self.config | {\n        \"skip_download\": True,\n        \"subtitleslangs\": [lang],\n        \"outtmpl\": str(temp_path),\n    }\n\n    with yt_dlp.YoutubeDL(options) as ydl:\n        if info := ydl.extract_info(url):\n            metadata = self._extract_metadata(info)\n            filepath = Path(ydl.prepare_filename(info)).with_suffix(f\".{lang}.ttml\")\n            filepath = self._convert_filename(filepath, metadata, output_path)\n            return VideoTranscript(metadata=metadata, filepath=filepath)\n        else:\n            logger.error(\"Info not found.\")\n            raise TranscriptError(f\"Transcript not downloaded for {url} in {lang}\")\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.DLPDownloader.get_video","title":"<code>get_video(url, quality=None, output_path=None)</code>","text":"<p>Download the full video with associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>YouTube video URL</p> required <code>quality</code> <code>Optional[str]</code> <p>yt-dlp format string (default: highest available)</p> <code>None</code> <code>output_path</code> <code>Optional[Path]</code> <p>Optional output directory</p> <code>None</code> <p>Returns:</p> Type Description <code>VideoFile</code> <p>VideoFile containing video file path and metadata</p> <p>Raises:</p> Type Description <code>VideoDownloadError</code> <p>If download fails</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_video(\n    self,\n    url: str,\n    quality: Optional[str] = None,\n    output_path: Optional[Path] = None\n) -&gt; VideoFile:\n    \"\"\"\n    Download the full video with associated metadata.\n\n    Args:\n        url: YouTube video URL\n        quality: yt-dlp format string (default: highest available)\n        output_path: Optional output directory\n\n    Returns:\n        VideoFile containing video file path and metadata\n\n    Raises:\n        VideoDownloadError: If download fails\n    \"\"\"\n    temp_path = Path.cwd() / TEMP_FILENAME_FORMAT\n    video_options = DEFAULT_VIDEO_OPTIONS | self.config | {\n        \"outtmpl\": str(temp_path)\n    }\n    if quality:\n        video_options[\"format\"] = quality\n\n    with yt_dlp.YoutubeDL(video_options) as ydl:\n        if info := ydl.extract_info(url, download=True):\n            metadata = self._extract_metadata(info)\n            ext = info.get(\"ext\", \"mp4\")\n            filepath = Path(ydl.prepare_filename(info)).with_suffix(f\".{ext}\")\n            filepath = self._convert_filename(filepath, metadata, output_path)\n            return VideoFile(metadata=metadata, filepath=filepath)\n        else:\n            logger.error(\"Info not found.\")\n            raise VideoDownloadError(f\"Unable to download video for {url}.\")\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.DownloadError","title":"<code>DownloadError</code>","text":"<p>               Bases: <code>VideoProcessingError</code></p> <p>Raised for download-related errors.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class DownloadError(VideoProcessingError):\n    \"\"\"Raised for download-related errors.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.TranscriptError","title":"<code>TranscriptError</code>","text":"<p>               Bases: <code>VideoProcessingError</code></p> <p>Raised for transcript-related errors.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class TranscriptError(VideoProcessingError):\n    \"\"\"Raised for transcript-related errors.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoAudio","title":"<code>VideoAudio</code>  <code>dataclass</code>","text":"<p>               Bases: <code>VideoResource</code></p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class VideoAudio(VideoResource): \n    pass \n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoDownloadError","title":"<code>VideoDownloadError</code>","text":"<p>               Bases: <code>VideoProcessingError</code></p> <p>Raised for video download-related errors.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class VideoDownloadError(VideoProcessingError):\n    \"\"\"Raised for video download-related errors.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoFile","title":"<code>VideoFile</code>  <code>dataclass</code>","text":"<p>               Bases: <code>VideoResource</code></p> <p>Represents a downloaded video file and its metadata.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class VideoFile(VideoResource):\n    \"\"\"Represents a downloaded video file and its metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoProcessingError","title":"<code>VideoProcessingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for video processing errors.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class VideoProcessingError(Exception):\n    \"\"\"Base exception for video processing errors.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoResource","title":"<code>VideoResource</code>  <code>dataclass</code>","text":"<p>Base class for all video resources.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>@dataclass \nclass VideoResource:\n    \"\"\"Base class for all video resources.\"\"\"\n    metadata: Metadata\n    filepath: Optional[Path] = None\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoResource.filepath","title":"<code>filepath = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoResource.metadata","title":"<code>metadata</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoResource.__init__","title":"<code>__init__(metadata, filepath=None)</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing.VideoTranscript","title":"<code>VideoTranscript</code>  <code>dataclass</code>","text":"<p>               Bases: <code>VideoResource</code></p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class VideoTranscript(VideoResource): \n    pass\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.YTDownloader","title":"<code>YTDownloader</code>","text":"<p>Abstract base class for YouTube content retrieval.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>class YTDownloader:\n    \"\"\"Abstract base class for YouTube content retrieval.\"\"\"\n\n    def get_transcript(\n        self, \n        url: str, \n        lang: str = \"en\", \n        output_path: Optional[Path] = None\n    ) -&gt; VideoTranscript:\n        \"\"\"Retrieve video transcript with associated metadata.\"\"\"\n        raise NotImplementedError\n\n    def get_audio(\n        self, \n        url: str, \n        start: str,\n        end: str,\n        output_path: Optional[Path]\n    ) -&gt; VideoAudio:\n        \"\"\"Extract audio with associated metadata.\"\"\"\n        raise NotImplementedError\n\n    def get_metadata(\n        self, \n        url: str, \n    ) -&gt; Metadata:\n        \"\"\"Retrieve video metadata only.\"\"\"\n        raise NotImplementedError\n\n    def get_video(\n        self,\n        url: str,\n        quality: Optional[str] = None,\n        output_path: Optional[Path] = None\n    ) -&gt; VideoFile:\n        \"\"\"\n        Download the full video with associated metadata.\n\n        Args:\n            url: YouTube video URL\n            quality: yt-dlp format string (default: highest available)\n            output_path: Optional output directory\n\n        Returns:\n            VideoFile containing video file path and metadata\n\n        Raises:\n            VideoDownloadError: If download fails\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.YTDownloader.get_audio","title":"<code>get_audio(url, start, end, output_path)</code>","text":"<p>Extract audio with associated metadata.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_audio(\n    self, \n    url: str, \n    start: str,\n    end: str,\n    output_path: Optional[Path]\n) -&gt; VideoAudio:\n    \"\"\"Extract audio with associated metadata.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.YTDownloader.get_metadata","title":"<code>get_metadata(url)</code>","text":"<p>Retrieve video metadata only.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_metadata(\n    self, \n    url: str, \n) -&gt; Metadata:\n    \"\"\"Retrieve video metadata only.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.YTDownloader.get_transcript","title":"<code>get_transcript(url, lang='en', output_path=None)</code>","text":"<p>Retrieve video transcript with associated metadata.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_transcript(\n    self, \n    url: str, \n    lang: str = \"en\", \n    output_path: Optional[Path] = None\n) -&gt; VideoTranscript:\n    \"\"\"Retrieve video transcript with associated metadata.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.YTDownloader.get_video","title":"<code>get_video(url, quality=None, output_path=None)</code>","text":"<p>Download the full video with associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>YouTube video URL</p> required <code>quality</code> <code>Optional[str]</code> <p>yt-dlp format string (default: highest available)</p> <code>None</code> <code>output_path</code> <code>Optional[Path]</code> <p>Optional output directory</p> <code>None</code> <p>Returns:</p> Type Description <code>VideoFile</code> <p>VideoFile containing video file path and metadata</p> <p>Raises:</p> Type Description <code>VideoDownloadError</code> <p>If download fails</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_video(\n    self,\n    url: str,\n    quality: Optional[str] = None,\n    output_path: Optional[Path] = None\n) -&gt; VideoFile:\n    \"\"\"\n    Download the full video with associated metadata.\n\n    Args:\n        url: YouTube video URL\n        quality: yt-dlp format string (default: highest available)\n        output_path: Optional output directory\n\n    Returns:\n        VideoFile containing video file path and metadata\n\n    Raises:\n        VideoDownloadError: If download fails\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.extract_text_from_ttml","title":"<code>extract_text_from_ttml(ttml_path)</code>","text":"<p>Extract plain text content from TTML file.</p> <p>Parameters:</p> Name Type Description Default <code>ttml_path</code> <code>Path</code> <p>Path to TTML transcript file</p> required <p>Returns:</p> Type Description <code>str</code> <p>Plain text content with one sentence per line</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file doesn't exist or has invalid content</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def extract_text_from_ttml(ttml_path: Path) -&gt; str:\n    \"\"\"Extract plain text content from TTML file.\n\n    Args:\n        ttml_path: Path to TTML transcript file\n\n    Returns:\n        Plain text content with one sentence per line\n\n    Raises:\n        ValueError: If file doesn't exist or has invalid content\n    \"\"\"\n    if not ttml_path.exists():\n        raise ValueError(f\"TTML file not found: {ttml_path}\")\n\n    ttml_str = ttml_path.read_text()\n    if not ttml_str.strip():\n        return \"\"\n\n    namespaces = {\n        \"tt\": \"http://www.w3.org/ns/ttml\",\n        \"tts\": \"http://www.w3.org/ns/ttml#styling\",\n    }\n\n    try:\n        root = ET.fromstring(ttml_str)\n        text_lines = []\n        for p in root.findall(\".//tt:p\", namespaces):\n            if p.text is not None:\n                text_lines.append(p.text.strip())\n            else:\n                text_lines.append(\"\")\n                logger.debug(\"Found empty paragraph in TTML, preserving as blank line\")\n\n        logger.info(f\"Extracted {len(text_lines)} lines of text from TTML\")\n        return \"\\n\".join(text_lines)\n\n    except ParseError as e:\n        logger.error(f\"Failed to parse XML content: {e}\")\n        raise\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing.get_youtube_urls_from_csv","title":"<code>get_youtube_urls_from_csv(file_path)</code>","text":"<p>Reads a CSV file containing YouTube URLs and titles, logs the titles, and returns a list of URLs.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the CSV file containing YouTube URLs and titles.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of YouTube URLs.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> <code>ValueError</code> <p>If the CSV file is improperly formatted.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing.py</code> <pre><code>def get_youtube_urls_from_csv(file_path: Path) -&gt; List[str]:\n    \"\"\"\n    Reads a CSV file containing YouTube URLs and titles, logs the titles,\n    and returns a list of URLs.\n\n    Args:\n        file_path (Path): Path to the CSV file containing YouTube URLs and titles.\n\n    Returns:\n        List[str]: List of YouTube URLs.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the CSV file is improperly formatted.\n    \"\"\"\n    if not file_path.exists():\n        logger.error(f\"File not found: {file_path}\")\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    urls = []\n\n    try:\n        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n            reader = csv.DictReader(f)\n\n            if (reader.fieldnames is None \n                or \"url\" not in reader.fieldnames \n                or \"title\" not in reader.fieldnames\n            ):\n                logger.error(\"CSV file must contain 'url' and 'title' columns.\")\n                raise ValueError(\"CSV file must contain 'url' and 'title' columns.\")\n\n            for row in reader:\n                url = row[\"url\"]\n                title = row[\"title\"]\n                urls.append(url)\n                logger.info(f\"Found video title: {title}\")\n    except Exception as e:\n        logger.exception(f\"Error processing CSV file: {e}\")\n        raise\n\n    return urls\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old1","title":"<code>video_processing_old1</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.DEFAULT_TRANSCRIPT_DIR","title":"<code>DEFAULT_TRANSCRIPT_DIR = Path.home() / '.yt_dlp_transcripts'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.DEFAULT_TRANSCRIPT_OPTIONS","title":"<code>DEFAULT_TRANSCRIPT_OPTIONS = {'skip_download': True, 'quiet': True, 'no_warnings': True, 'extract_flat': True, 'socket_timeout': 30, 'retries': 3, 'ignoreerrors': True, 'logger': logger}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.SubtitleTrack","title":"<code>SubtitleTrack</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type definition for a subtitle track entry.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old1.py</code> <pre><code>class SubtitleTrack(TypedDict):\n    \"\"\"Type definition for a subtitle track entry.\"\"\"\n\n    url: str\n    ext: str\n    name: str\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.SubtitleTrack.ext","title":"<code>ext</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.SubtitleTrack.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.SubtitleTrack.url","title":"<code>url</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.TranscriptNotFoundError","title":"<code>TranscriptNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when no transcript is available for the requested language.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old1.py</code> <pre><code>class TranscriptNotFoundError(Exception):\n    \"\"\"Raised when no transcript is available for the requested language.\"\"\"\n\n    def __init__(\n        self,\n        video_url: str,\n        language: str,\n    ) -&gt; None:\n        \"\"\"\n        Initialize TranscriptNotFoundError.\n\n        Args:\n            video_url: URL of the video where transcript was not found\n            language: Language code that was requested\n        \"\"\"\n        self.video_url = video_url\n        self.language = language\n\n        message = (\n            f\"No transcript found for {self.video_url} in language {self.language}. \"\n        )\n        super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.TranscriptNotFoundError.language","title":"<code>language = language</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.TranscriptNotFoundError.video_url","title":"<code>video_url = video_url</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.TranscriptNotFoundError.__init__","title":"<code>__init__(video_url, language)</code>","text":"<p>Initialize TranscriptNotFoundError.</p> <p>Parameters:</p> Name Type Description Default <code>video_url</code> <code>str</code> <p>URL of the video where transcript was not found</p> required <code>language</code> <code>str</code> <p>Language code that was requested</p> required Source code in <code>src/tnh_scholar/video_processing/video_processing_old1.py</code> <pre><code>def __init__(\n    self,\n    video_url: str,\n    language: str,\n) -&gt; None:\n    \"\"\"\n    Initialize TranscriptNotFoundError.\n\n    Args:\n        video_url: URL of the video where transcript was not found\n        language: Language code that was requested\n    \"\"\"\n    self.video_url = video_url\n    self.language = language\n\n    message = (\n        f\"No transcript found for {self.video_url} in language {self.language}. \"\n    )\n    super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.VideoInfo","title":"<code>VideoInfo</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type definition for relevant video info fields.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old1.py</code> <pre><code>class VideoInfo(TypedDict):\n    \"\"\"Type definition for relevant video info fields.\"\"\"\n\n    subtitles: Dict[str, List[SubtitleTrack]]\n    automatic_captions: Dict[str, List[SubtitleTrack]]\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.VideoInfo.automatic_captions","title":"<code>automatic_captions</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.VideoInfo.subtitles","title":"<code>subtitles</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.download_audio_yt","title":"<code>download_audio_yt(url, output_dir, start_time=None, prompt_overwrite=True)</code>","text":"<p>Downloads audio from a YouTube video using yt_dlp.YoutubeDL, with an optional start time.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the YouTube video.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the downloaded audio file.</p> required <code>start_time</code> <code>str</code> <p>Optional start time (e.g., '00:01:30' for 1 minute 30 seconds).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the downloaded audio file.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old1.py</code> <pre><code>def download_audio_yt(\n    url: str, output_dir: Path, start_time: Optional[str] = None, prompt_overwrite=True\n) -&gt; Path:\n    \"\"\"\n    Downloads audio from a YouTube video using yt_dlp.YoutubeDL, with an optional start time.\n\n    Args:\n        url (str): URL of the YouTube video.\n        output_dir (Path): Directory to save the downloaded audio file.\n        start_time (str): Optional start time (e.g., '00:01:30' for 1 minute 30 seconds).\n\n    Returns:\n        Path: Path to the downloaded audio file.\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n    ydl_opts = {\n        \"format\": \"bestaudio/best\",\n        \"postprocessors\": [\n            {\n                \"key\": \"FFmpegExtractAudio\",\n                \"preferredcodec\": \"mp3\",\n                \"preferredquality\": \"192\",\n            }\n        ],\n        \"postprocessor_args\": [],\n        \"noplaylist\": True,\n        \"outtmpl\": str(output_dir / \"%(title)s.%(ext)s\"),\n    }\n\n    # Add start time to the FFmpeg postprocessor if provided\n    if start_time:\n        ydl_opts[\"postprocessor_args\"].extend([\"-ss\", start_time])\n        logger.info(f\"Postprocessor start time set to: {start_time}\")\n\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        info = ydl.extract_info(url, download=True)  # Extract metadata and download\n        filename = ydl.prepare_filename(info)\n        return Path(filename).with_suffix(\".mp3\")\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.get_transcript","title":"<code>get_transcript(url, lang='en', download_dir=DEFAULT_TRANSCRIPT_DIR, keep_transcript_file=False)</code>","text":"<p>Downloads and extracts the transcript for a given YouTube video URL.</p> <p>Retrieves the transcript file, extracts the text content, and returns the raw text.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the YouTube video.</p> required <code>lang</code> <code>str</code> <p>The language code for the transcript (default: 'en').</p> <code>'en'</code> <code>download_dir</code> <code>Path</code> <p>The directory to download the transcript to.</p> <code>DEFAULT_TRANSCRIPT_DIR</code> <code>keep_transcript_file</code> <code>bool</code> <p>Whether to keep the downloaded transcript file (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The extracted transcript text.</p> <p>Raises:</p> Type Description <code>TranscriptNotFoundError</code> <p>If no transcript is available in the specified language.</p> <code>DownloadError</code> <p>If video info extraction or download fails.</p> <code>ValueError</code> <p>If the downloaded transcript file is invalid or empty.</p> <code>ParseError</code> <p>If XML parsing of the transcript fails.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old1.py</code> <pre><code>def get_transcript(\n    url: str,\n    lang: str = \"en\",\n    download_dir: Path = DEFAULT_TRANSCRIPT_DIR,\n    keep_transcript_file: bool = False,\n) -&gt; str:\n    \"\"\"Downloads and extracts the transcript for a given YouTube video URL.\n\n    Retrieves the transcript file, extracts the text content, and returns the raw text.\n\n    Args:\n        url: The URL of the YouTube video.\n        lang: The language code for the transcript (default: 'en').\n        download_dir: The directory to download the transcript to.\n        keep_transcript_file: Whether to keep the downloaded transcript file (default: False).\n\n    Returns:\n        The extracted transcript text.\n\n    Raises:\n        TranscriptNotFoundError: If no transcript is available in the specified language.\n        yt_dlp.utils.DownloadError: If video info extraction or download fails.\n        ValueError: If the downloaded transcript file is invalid or empty.\n        ParseError: If XML parsing of the transcript fails.\n    \"\"\"\n\n    transcript_file = _download_yt_ttml(download_dir, url=url, lang=lang)\n\n    text = read_str_from_file(transcript_file)\n\n    if not keep_transcript_file:\n        try:\n            os.remove(transcript_file)\n            logger.debug(f\"Removed temporary transcript file: {transcript_file}\")\n        except OSError as e:\n            logger.warning(\n                f\"Failed to remove temporary transcript file {transcript_file}: {e}\"\n            )\n\n    return _extract_ttml_text(text)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.get_transcript_info","title":"<code>get_transcript_info(video_url, lang='en')</code>","text":"<p>Retrieves the transcript URL for a video in the specified language.</p> <p>Parameters:</p> Name Type Description Default <code>video_url</code> <code>str</code> <p>The URL of the video</p> required <code>lang</code> <code>str</code> <p>The desired language code</p> <code>'en'</code> <p>Returns:</p> Type Description <code>str</code> <p>URL of the transcript</p> <p>Raises:</p> Type Description <code>TranscriptNotFoundError</code> <p>If no transcript is available in the specified language</p> <code>DownloadError</code> <p>If video info extraction fails</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old1.py</code> <pre><code>def get_transcript_info(video_url: str, lang: str = \"en\") -&gt; str:\n    \"\"\"\n    Retrieves the transcript URL for a video in the specified language.\n\n    Args:\n        video_url: The URL of the video\n        lang: The desired language code\n\n    Returns:\n        URL of the transcript\n\n    Raises:\n        TranscriptNotFoundError: If no transcript is available in the specified language\n        yt_dlp.utils.DownloadError: If video info extraction fails\n    \"\"\"\n    options = {\n        \"writesubtitles\": True,\n        \"writeautomaticsub\": True,\n        \"subtitleslangs\": [lang],\n        \"skip_download\": True,\n        #    'verbose': True\n    }\n\n    with yt_dlp.YoutubeDL(options) as ydl:\n        # This may raise yt_dlp.utils.DownloadError which we let propagate\n        info: VideoInfo = ydl.extract_info(video_url, download=False)  # type: ignore\n\n        subtitles = info.get(\"subtitles\", {})\n        auto_subtitles = info.get(\"automatic_captions\", {})\n\n        # Log available subtitle information\n        logger.debug(\"Available subtitles:\")\n        logger.debug(f\"Manual subtitles: {list(subtitles.keys())}\")\n        logger.debug(f\"Auto captions: {list(auto_subtitles.keys())}\")\n\n        if lang in subtitles:\n            return subtitles[lang][0][\"url\"]\n        elif lang in auto_subtitles:\n            return auto_subtitles[lang][0][\"url\"]\n\n        raise TranscriptNotFoundError(video_url=video_url, language=lang)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.get_video_download_path_yt","title":"<code>get_video_download_path_yt(output_dir, url)</code>","text":"<p>Extracts the video title using yt-dlp.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The YouTube URL.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Path</code> <p>The title of the video.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old1.py</code> <pre><code>def get_video_download_path_yt(output_dir: Path, url: str) -&gt; Path:\n    \"\"\"\n    Extracts the video title using yt-dlp.\n\n    Args:\n        url (str): The YouTube URL.\n\n    Returns:\n        str: The title of the video.\n    \"\"\"\n    ydl_opts = {\n        \"quiet\": True,  # Suppress output\n        \"skip_download\": True,  # Don't download, just fetch metadata\n        \"outtmpl\": str(output_dir / \"%(title)s.%(ext)s\"),\n    }\n\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        info = ydl.extract_info(\n            url, download=False\n        )  # Extract metadata without downloading\n        filepath = ydl.prepare_filename(info)\n\n    return Path(filepath).with_suffix(\".mp3\")\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old1.get_youtube_urls_from_csv","title":"<code>get_youtube_urls_from_csv(file_path)</code>","text":"<p>Reads a CSV file containing YouTube URLs and titles, logs the titles, and returns a list of URLs.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the CSV file containing YouTube URLs and titles.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of YouTube URLs.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> <code>ValueError</code> <p>If the CSV file is improperly formatted.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old1.py</code> <pre><code>def get_youtube_urls_from_csv(file_path: Path) -&gt; List[str]:\n    \"\"\"\n    Reads a CSV file containing YouTube URLs and titles, logs the titles,\n    and returns a list of URLs.\n\n    Args:\n        file_path (Path): Path to the CSV file containing YouTube URLs and titles.\n\n    Returns:\n        List[str]: List of YouTube URLs.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the CSV file is improperly formatted.\n    \"\"\"\n    if not file_path.exists():\n        logger.error(f\"File not found: {file_path}\")\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    urls = []\n\n    try:\n        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n            reader = csv.DictReader(f)\n\n            if reader.fieldnames is None or \"url\" not in reader.fieldnames or \"title\" not in reader.fieldnames:\n                logger.error(\"CSV file must contain 'url' and 'title' columns.\")\n                raise ValueError(\"CSV file must contain 'url' and 'title' columns.\")\n\n            for row in reader:\n                url = row[\"url\"]\n                title = row[\"title\"]\n                urls.append(url)\n                logger.info(f\"Found video title: {title}\")\n    except Exception as e:\n        logger.exception(f\"Error processing CSV file: {e}\")\n        raise\n\n    return urls\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2","title":"<code>video_processing_old2</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.AUDIO_DOWNLOAD_OPTIONS","title":"<code>AUDIO_DOWNLOAD_OPTIONS = BASE_YDL_OPTIONS | {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'noplaylist': True}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.BASE_YDL_OPTIONS","title":"<code>BASE_YDL_OPTIONS = {'quiet': True, 'no_warnings': True, 'extract_flat': True, 'socket_timeout': 30, 'retries': 3, 'ignoreerrors': True, 'logger': logger}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.DEFAULT_METADATA_FIELDS","title":"<code>DEFAULT_METADATA_FIELDS = ['id', 'title', 'description', 'duration', 'upload_date', 'uploader', 'channel_url', 'webpage_url', 'original_url', 'channel', 'language', 'categories', 'tags']</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.DEFAULT_TRANSCRIPT_DIR","title":"<code>DEFAULT_TRANSCRIPT_DIR = Path.home() / '.yt_dlp_transcripts'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.TRANSCRIPT_OPTIONS","title":"<code>TRANSCRIPT_OPTIONS = BASE_YDL_OPTIONS | {'writesubtitles': True, 'writeautomaticsub': True, 'subtitlesformat': 'ttml'}</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.logger","title":"<code>logger = get_child_logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.SubtitleTrack","title":"<code>SubtitleTrack</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type definition for a subtitle track entry.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>class SubtitleTrack(TypedDict):\n    \"\"\"Type definition for a subtitle track entry.\"\"\"\n    url: str\n    ext: str\n    name: str\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.SubtitleTrack.ext","title":"<code>ext</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.SubtitleTrack.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.SubtitleTrack.url","title":"<code>url</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.TranscriptNotFoundError","title":"<code>TranscriptNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when no transcript is available for the requested language.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>class TranscriptNotFoundError(Exception):\n    \"\"\"Raised when no transcript is available for the requested language.\"\"\"\n    def __init__(self, video_url: str, language: str) -&gt; None:\n        self.video_url = video_url\n        self.language = language\n        message = f\"No transcript found for {self.video_url} \\\n                    in language {self.language}.\"\n        super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.TranscriptNotFoundError.language","title":"<code>language = language</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.TranscriptNotFoundError.video_url","title":"<code>video_url = video_url</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.TranscriptNotFoundError.__init__","title":"<code>__init__(video_url, language)</code>","text":"Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>def __init__(self, video_url: str, language: str) -&gt; None:\n    self.video_url = video_url\n    self.language = language\n    message = f\"No transcript found for {self.video_url} \\\n                in language {self.language}.\"\n    super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoDownload","title":"<code>VideoDownload</code>  <code>dataclass</code>","text":"<p>               Bases: <code>VideoMetadata</code></p> <p>Result of download operations.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>@dataclass\nclass VideoDownload(VideoMetadata):\n    \"\"\"Result of download operations.\"\"\"\n    filepath: Path\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoDownload.filepath","title":"<code>filepath</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoDownload.__init__","title":"<code>__init__(metadata, filepath)</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoInfo","title":"<code>VideoInfo</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type definition for relevant video info fields.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>class VideoInfo(TypedDict):\n    \"\"\"Type definition for relevant video info fields.\"\"\"\n    subtitles: Dict[str, List[SubtitleTrack]]\n    automatic_captions: Dict[str, List[SubtitleTrack]]\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoInfo.automatic_captions","title":"<code>automatic_captions</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoInfo.subtitles","title":"<code>subtitles</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoMetadata","title":"<code>VideoMetadata</code>  <code>dataclass</code>","text":"<p>Base class for video operations containing common metadata.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>@dataclass\nclass VideoMetadata:\n    \"\"\"Base class for video operations containing common metadata.\"\"\"\n    metadata: Dict[str, Any]\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoMetadata.metadata","title":"<code>metadata</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoMetadata.__init__","title":"<code>__init__(metadata)</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoTranscript","title":"<code>VideoTranscript</code>  <code>dataclass</code>","text":"<p>               Bases: <code>VideoMetadata</code></p> <p>Result of transcript operations.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>@dataclass\nclass VideoTranscript(VideoMetadata):\n    \"\"\"Result of transcript operations.\"\"\"\n    content: str\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoTranscript.content","title":"<code>content</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.VideoTranscript.__init__","title":"<code>__init__(metadata, content)</code>","text":""},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.download_audio_yt","title":"<code>download_audio_yt(url, output_dir, start_time=None)</code>","text":"<p>Downloads audio from YouTube URL with optional start time.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>def download_audio_yt(\n    url: str, \n    output_dir: Path, \n    start_time: Optional[str] = None\n) -&gt; VideoDownload:\n    \"\"\"Downloads audio from YouTube URL with optional start time.\"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    options = AUDIO_DOWNLOAD_OPTIONS | {\n        \"outtmpl\": str(output_dir / \"%(title)s.%(ext)s\"),\n    }\n\n    if start_time:\n        options[\"postprocessor_args\"] = [\"-ss\", start_time]\n        logger.info(f\"Postprocessor start time set to: {start_time}\")\n\n    with yt_dlp.YoutubeDL(options) as ydl:\n        if info := ydl.extract_info(url, download=True):\n            filepath = Path(ydl.prepare_filename(info)).with_suffix(\".mp3\")\n            metadata = _extract_metadata(info)\n        else:\n            logger.error(f\"YT audio download: Unable to get info for {url}.\")\n            raise \n        return VideoDownload(metadata=metadata, filepath=filepath)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.get_transcript","title":"<code>get_transcript(url, lang='en', download_dir=DEFAULT_TRANSCRIPT_DIR, keep_transcript_file=False)</code>","text":"<p>Downloads and extracts transcript with metadata.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>def get_transcript(\n    url: str,\n    lang: str = \"en\",\n    download_dir: Path = DEFAULT_TRANSCRIPT_DIR,\n    keep_transcript_file: bool = False,\n) -&gt; VideoTranscript:\n    \"\"\"Downloads and extracts transcript with metadata.\"\"\"\n    transcript_file = _download_yt_ttml(download_dir, url=url, lang=lang)\n    text = read_str_from_file(transcript_file)\n\n    if not keep_transcript_file:\n        try:\n            os.remove(transcript_file)\n            logger.debug(f\"Removed temporary transcript file: {transcript_file}\")\n        except OSError as e:\n            logger.warning(\n                f\"Failed to remove temporary transcript file {transcript_file}: {e}\"\n                )\n\n    content = _extract_ttml_text(text)\n\n    # Get metadata\n    options = BASE_YDL_OPTIONS | {\"skip_download\": True}\n    with yt_dlp.YoutubeDL(options) as ydl:\n        if info := ydl.extract_info(url, download=False):\n            metadata = _extract_metadata(info)\n        else:\n            logger.error(f\"YT get transcript: unable to get info for {url}.\")\n            raise\n    return VideoTranscript(metadata=metadata, content=content)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.get_video_download_path_yt","title":"<code>get_video_download_path_yt(output_dir, url)</code>","text":"<p>Get video metadata and expected download path.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>def get_video_download_path_yt(output_dir: Path, url: str) -&gt; VideoDownload:\n    \"\"\"Get video metadata and expected download path.\"\"\"\n    options = AUDIO_DOWNLOAD_OPTIONS | {\n        \"skip_download\": True,\n        \"outtmpl\": str(output_dir / \"%(title)s.%(ext)s\"),\n    }\n\n    with yt_dlp.YoutubeDL(options) as ydl:\n        if info := ydl.extract_info(url, download=False):\n            filepath = Path(ydl.prepare_filename(info)).with_suffix(\".mp3\")\n            metadata = _extract_metadata(info)\n        else:\n            logger.error(f\"YT video download: unable to extract info for {url}\")\n            raise\n        return VideoDownload(metadata=metadata, filepath=filepath)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.get_video_metadata","title":"<code>get_video_metadata(url)</code>","text":"<p>Get metadata for a YouTube video without downloading content.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>YouTube video URL</p> required <p>Returns:</p> Type Description <code>VideoResult</code> <p>VideoResult with only metadata field populated</p> <p>Raises:</p> Type Description <code>DownloadError</code> <p>If video info extraction fails</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>def get_video_metadata(url: str) -&gt; VideoResult:\n    \"\"\"Get metadata for a YouTube video without downloading content.\n\n    Args:\n        url: YouTube video URL\n\n    Returns:\n        VideoResult with only metadata field populated\n\n    Raises:\n        yt_dlp.utils.DownloadError: If video info extraction fails\n    \"\"\"\n    options = BASE_YDL_OPTIONS | {\"skip_download\": True}\n\n    with yt_dlp.YoutubeDL(options) as ydl:\n        info = ydl.extract_info(url, download=False)\n        metadata = _extract_metadata(info)\n        return VideoResult(metadata=metadata)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.video_processing_old2.get_youtube_urls_from_csv","title":"<code>get_youtube_urls_from_csv(file_path)</code>","text":"<p>Reads YouTube URLs from a CSV file containing URLs and titles.</p> Source code in <code>src/tnh_scholar/video_processing/video_processing_old2.py</code> <pre><code>def get_youtube_urls_from_csv(file_path: Path) -&gt; List[str]:\n    \"\"\"Reads YouTube URLs from a CSV file containing URLs and titles.\"\"\"\n    if not file_path.exists():\n        logger.error(f\"File not found: {file_path}\")\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    urls = []\n    try:\n        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n            reader = csv.DictReader(f)\n            if reader.fieldnames is None \\\n                or \"url\" not in reader.fieldnames \\\n                or \"title\" not in reader.fieldnames:\n                logger.error(\"CSV file must contain 'url' and 'title' columns.\")\n                raise ValueError(\"CSV file must contain 'url' and 'title' columns.\")\n\n            for row in reader:\n                urls.append(row[\"url\"])\n                logger.info(f\"Found video title: {row['title']}\")\n    except Exception as e:\n        logger.exception(f\"Error processing CSV file: {e}\")\n        raise\n\n    return urls\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.yt_transcribe","title":"<code>yt_transcribe</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.DEFAULT_CHUNK_DURATION_MS","title":"<code>DEFAULT_CHUNK_DURATION_MS = 10 * 60 * 1000</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.DEFAULT_CHUNK_DURATION_S","title":"<code>DEFAULT_CHUNK_DURATION_S = 10 * 60</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.DEFAULT_OUTPUT_DIR","title":"<code>DEFAULT_OUTPUT_DIR = './video_transcriptions'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.DEFAULT_PROMPT","title":"<code>DEFAULT_PROMPT = 'Dharma, Deer Park, Thay, Thich Nhat Hanh, Bodhicitta, Bodhisattva, Mahayana'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.EXPECTED_ENV","title":"<code>EXPECTED_ENV = 'tnh-scholar'</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.group","title":"<code>group = parser.add_mutually_exclusive_group(required=True)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.logger","title":"<code>logger = get_child_logger('yt_transcribe')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.output_directory","title":"<code>output_directory = Path(args.output_dir)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.parser","title":"<code>parser = argparse.ArgumentParser(description='Transcribe YouTube videos from a URL or a file containing URLs.')</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.url_file","title":"<code>url_file = Path(args.file)</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.video_urls","title":"<code>video_urls = []</code>  <code>module-attribute</code>","text":""},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.check_conda_env","title":"<code>check_conda_env()</code>","text":"Source code in <code>src/tnh_scholar/video_processing/yt_transcribe.py</code> <pre><code>def check_conda_env():\n    active_env = os.environ.get(\"CONDA_DEFAULT_ENV\")\n    if active_env != EXPECTED_ENV:\n        logger.warning(\n            f\"WARNING: The active conda environment is '{active_env}', but '{EXPECTED_ENV}' is required. \"\n            \"Please activate the correct environment.\"\n        )\n        # Optionally exit the script\n        sys.exit(1)\n</code></pre>"},{"location":"api/#tnh_scholar.video_processing.yt_transcribe.transcribe_youtube_videos","title":"<code>transcribe_youtube_videos(urls, output_base_dir, max_chunk_duration=DEFAULT_CHUNK_DURATION_S, start=None, translate=False)</code>","text":"<p>Full pipeline for transcribing a list of YouTube videos.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>List of YouTube video URLs.</p> required <code>output_base_dir</code> <code>Path</code> <p>Base directory for storing output.</p> required <code>max_chunk_duration</code> <code>int</code> <p>Maximum duration for audio chunks in seconds (default is 10 minutes).</p> <code>DEFAULT_CHUNK_DURATION_S</code> Source code in <code>src/tnh_scholar/video_processing/yt_transcribe.py</code> <pre><code>def transcribe_youtube_videos(\n    urls: list[str],\n    output_base_dir: Path,\n    max_chunk_duration: int = DEFAULT_CHUNK_DURATION_S,\n    start: str = None,\n    translate=False,\n):\n    \"\"\"\n    Full pipeline for transcribing a list of YouTube videos.\n\n    Args:\n        urls (list[str]): List of YouTube video URLs.\n        output_base_dir (Path): Base directory for storing output.\n        max_chunk_duration (int): Maximum duration for audio chunks in seconds (default is 10 minutes).\n    \"\"\"\n    output_base_dir.mkdir(parents=True, exist_ok=True)\n\n    for url in urls:\n        try:\n            logger.info(f\"Processing video: {url}\")\n\n            # Step 1: Download audio\n            logger.info(\"Downloading audio...\")\n            tmp_audio_file = download_audio_yt(url, output_base_dir, start_time=start)\n            logger.info(f\"Downloaded audio file: {tmp_audio_file}\")\n\n            # Prepare directories for chunks and outputs\n            video_name = (\n                tmp_audio_file.stem\n            )  # Use the stem of the audio file (title without extension)\n            video_output_dir = output_base_dir / video_name\n            chunks_dir = video_output_dir / \"chunks\"\n            chunks_dir.mkdir(parents=True, exist_ok=True)\n\n            # Create the video directory and move the audio file into it\n            video_output_dir.mkdir(parents=True, exist_ok=True)\n            audio_file = video_output_dir / tmp_audio_file.name\n\n            try:\n                tmp_audio_file.rename(\n                    audio_file\n                )  # Move the audio file to the video directory\n                logger.info(f\"Moved audio file to: {audio_file}\")\n            except Exception as e:\n                logger.error(f\"Failed to move audio file to {video_output_dir}: {e}\")\n                # Ensure the code gracefully handles issues here, reassigning to the original tmp path.\n                audio_file = tmp_audio_file\n\n            # Step 2: Detect boundaries\n            logger.info(\"Detecting boundaries...\")\n            boundaries = detect_boundaries(audio_file)\n            logger.info(\"Boundaries generated.\")\n\n            # Step 3: Split audio into chunks\n            logger.info(\"Splitting audio into chunks...\")\n            split_audio_at_boundaries(\n                audio_file=audio_file,\n                boundaries=boundaries,\n                output_dir=chunks_dir,\n                max_duration=max_chunk_duration,\n            )\n            logger.info(f\"Audio chunks saved to: {chunks_dir}\")\n\n            # Step 4: Transcribe audio chunks\n            logger.info(\"Transcribing audio chunks...\")\n            transcript_file = video_output_dir / f\"{video_name}.txt\"\n            jsonl_file = video_output_dir / f\"{video_name}.jsonl\"\n            process_audio_chunks(\n                directory=chunks_dir,\n                output_file=transcript_file,\n                jsonl_file=jsonl_file,\n                prompt=DEFAULT_PROMPT,\n                translate=translate,\n            )\n            logger.info(f\"Transcription completed for {url}\")\n            logger.info(f\"Transcript saved to: {transcript_file}\")\n            logger.info(f\"Raw transcription data saved to: {jsonl_file}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to process video {url}: {e}\")\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing","title":"<code>xml_processing</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.FormattingError","title":"<code>FormattingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception raised for formatting-related errors.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>class FormattingError(Exception):\n    \"\"\"\n    Custom exception raised for formatting-related errors.\n    \"\"\"\n\n    def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n        super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.FormattingError.__init__","title":"<code>__init__(message='An error occurred due to invalid formatting.')</code>","text":"Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n    super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.PagebreakXMLParser","title":"<code>PagebreakXMLParser</code>","text":"<p>Parses XML documents split by  tags, with optional grouping and tag retention. Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>class PagebreakXMLParser:\n    \"\"\"\n    Parses XML documents split by &lt;pagebreak&gt; tags, with optional grouping and tag retention.\n    \"\"\"\n\n    def __init__(self, text: str):\n        if not text or not text.strip():\n            raise ValueError(\"Input XML text is empty or whitespace.\")\n        self.original_text = text\n        self.cleaned_text = \"\"\n        self.pages: List[str] = []\n        self.pagebreak_tags: List[str] = []\n        self._xml_decl_pattern = re.compile(r\"^\\s*&lt;\\?xml[^&gt;]*\\?&gt;\\s*\", re.IGNORECASE)\n        self._document_open_pattern = re.compile(r\"^\\s*&lt;document&gt;\\s*\", re.IGNORECASE)\n        self._document_close_pattern = re.compile(r\"\\s*&lt;/document&gt;\\s*$\", re.IGNORECASE)\n        self._pagebreak_pattern = re.compile(r\"^\\s*&lt;pagebreak\\b[^&gt;]*/&gt;\\s*$\", re.IGNORECASE | re.MULTILINE)\n\n    def _remove_preamble_and_document_tags(self):\n        text = self._xml_decl_pattern.sub(\"\", self.original_text, count=1)\n        text = self._document_open_pattern.sub(\"\", text, count=1)\n        text = self._document_close_pattern.sub(\"\", text, count=1)\n        if not text.strip():\n            raise ValueError(\"No content found between &lt;document&gt; tags.\")\n        self.cleaned_text = text\n\n    def _split_on_pagebreaks(self):\n        self.pages = []\n        self.pagebreak_tags = re.findall(self._pagebreak_pattern, self.cleaned_text)\n        split_lines = re.split(self._pagebreak_pattern, self.cleaned_text)\n        for i, page_content in enumerate(split_lines):\n            page_content = page_content.strip()\n            # skip trailing empty after last pagebreak\n            if not page_content and (i &gt;= len(self.pagebreak_tags)):\n                continue\n            self.pages.append(page_content)\n\n    def _attach_pagebreaks(self, keep_pagebreaks: bool):\n        if not keep_pagebreaks:\n            return\n        for i in range(min(len(self.pages), len(self.pagebreak_tags))):\n            if self.pages[i]:\n                self.pages[i] = f\"{self.pages[i]}\\n{self.pagebreak_tags[i].strip()}\"\n            else:\n                self.pages[i] = self.pagebreak_tags[i].strip()\n\n    def _group_pages(self, page_groups: List[Tuple[int, int]]) -&gt; List[str]:\n        grouped_pages: List[str] = []\n        for start, end in page_groups:\n            if start &lt; 1 or end &lt; start:\n                continue  # skip invalid groups\n            if group := [\n                self.pages[i]\n                for i in range(start - 1, end)\n                if 0 &lt;= i &lt; len(self.pages)\n            ]:\n                grouped_pages.append(\"\\n\".join(group).strip())\n        return grouped_pages\n\n    def parse(\n        self,\n        page_groups: Optional[List[Tuple[int, int]]] = None,\n        keep_pagebreaks: bool = True,\n    ) -&gt; List[str]:\n        \"\"\"\n        Parses the XML and returns a list of page contents, optionally grouped and with pagebreaks retained.\n        \"\"\"\n        self._remove_preamble_and_document_tags()\n        self._split_on_pagebreaks()\n        self._attach_pagebreaks(keep_pagebreaks)\n        # Remove empty pages\n        self.pages = [p for p in self.pages if p]\n        if not self.pages:\n            raise ValueError(\"No pages found in the XML content after splitting on &lt;pagebreak&gt; tags.\")\n        return self._group_pages(page_groups) if page_groups else self.pages\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.PagebreakXMLParser.cleaned_text","title":"<code>cleaned_text = ''</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.PagebreakXMLParser.original_text","title":"<code>original_text = text</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.PagebreakXMLParser.pagebreak_tags","title":"<code>pagebreak_tags = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.PagebreakXMLParser.pages","title":"<code>pages = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.PagebreakXMLParser.__init__","title":"<code>__init__(text)</code>","text":"Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def __init__(self, text: str):\n    if not text or not text.strip():\n        raise ValueError(\"Input XML text is empty or whitespace.\")\n    self.original_text = text\n    self.cleaned_text = \"\"\n    self.pages: List[str] = []\n    self.pagebreak_tags: List[str] = []\n    self._xml_decl_pattern = re.compile(r\"^\\s*&lt;\\?xml[^&gt;]*\\?&gt;\\s*\", re.IGNORECASE)\n    self._document_open_pattern = re.compile(r\"^\\s*&lt;document&gt;\\s*\", re.IGNORECASE)\n    self._document_close_pattern = re.compile(r\"\\s*&lt;/document&gt;\\s*$\", re.IGNORECASE)\n    self._pagebreak_pattern = re.compile(r\"^\\s*&lt;pagebreak\\b[^&gt;]*/&gt;\\s*$\", re.IGNORECASE | re.MULTILINE)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.PagebreakXMLParser.parse","title":"<code>parse(page_groups=None, keep_pagebreaks=True)</code>","text":"<p>Parses the XML and returns a list of page contents, optionally grouped and with pagebreaks retained.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def parse(\n    self,\n    page_groups: Optional[List[Tuple[int, int]]] = None,\n    keep_pagebreaks: bool = True,\n) -&gt; List[str]:\n    \"\"\"\n    Parses the XML and returns a list of page contents, optionally grouped and with pagebreaks retained.\n    \"\"\"\n    self._remove_preamble_and_document_tags()\n    self._split_on_pagebreaks()\n    self._attach_pagebreaks(keep_pagebreaks)\n    # Remove empty pages\n    self.pages = [p for p in self.pages if p]\n    if not self.pages:\n        raise ValueError(\"No pages found in the XML content after splitting on &lt;pagebreak&gt; tags.\")\n    return self._group_pages(page_groups) if page_groups else self.pages\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.join_xml_data_to_doc","title":"<code>join_xml_data_to_doc(file_path, data, overwrite=False)</code>","text":"<p>Joins a list of XML-tagged data with newlines, wraps it with  tags, and writes it to the specified file. Raises an exception if the file exists and overwrite is not set. <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the output file.</p> required <code>data</code> <code>List[str]</code> <p>List of XML-tagged data strings.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it exists.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file exists and overwrite is False.</p> <code>ValueError</code> <p>If the data list is empty.</p> Example <p>join_xml_data_to_doc(Path(\"output.xml\"), [\"Data\"], overwrite=True)</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def join_xml_data_to_doc(\n    file_path: Path, data: List[str], overwrite: bool = False\n) -&gt; None:\n    \"\"\"\n    Joins a list of XML-tagged data with newlines, wraps it with &lt;document&gt; tags,\n    and writes it to the specified file. Raises an exception if the file exists\n    and overwrite is not set.\n\n    Args:\n        file_path (Path): Path to the output file.\n        data (List[str]): List of XML-tagged data strings.\n        overwrite (bool): Whether to overwrite the file if it exists.\n\n    Raises:\n        FileExistsError: If the file exists and overwrite is False.\n        ValueError: If the data list is empty.\n\n    Example:\n        &gt;&gt;&gt; join_xml_data_to_doc(Path(\"output.xml\"), [\"&lt;tag&gt;Data&lt;/tag&gt;\"], overwrite=True)\n    \"\"\"\n    if file_path.exists() and not overwrite:\n        raise FileExistsError(\n            f\"The file {file_path} already exists and overwrite is not set.\"\n        )\n\n    if not data:\n        raise ValueError(\"The data list cannot be empty.\")\n\n    # Create the XML content\n    joined_data = \"\\n\".join(data)  # Joining data with newline\n    xml_content = f\"&lt;document&gt;\\n{joined_data}\\n&lt;/document&gt;\"\n\n    # Write to file\n    file_path.write_text(xml_content, encoding=\"utf-8\")\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.remove_page_tags","title":"<code>remove_page_tags(text)</code>","text":"<p>Removes  and  tags from a text string.</p> <p>Parameters: - text (str): The input text containing  tags. <p>Returns: - str: The text with  tags removed. Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def remove_page_tags(text):\n    \"\"\"\n    Removes &lt;page ...&gt; and &lt;/page&gt; tags from a text string.\n\n    Parameters:\n    - text (str): The input text containing &lt;page&gt; tags.\n\n    Returns:\n    - str: The text with &lt;page&gt; tags removed.\n    \"\"\"\n    # Remove opening &lt;page ...&gt; tags\n    text = re.sub(r\"&lt;page[^&gt;]*&gt;\", \"\", text)\n    # Remove closing &lt;/page&gt; tags\n    text = re.sub(r\"&lt;/page&gt;\", \"\", text)\n    return text\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.save_pages_to_xml","title":"<code>save_pages_to_xml(output_xml_path, text_pages, overwrite=False)</code>","text":"<p>Generates and saves an XML file containing text pages, with a  tag indicating the page ends. <p>Parameters:</p> Name Type Description Default <code>output_xml_path</code> <code>Path</code> <p>The Path object for the file where the XML file will be saved.</p> required <code>text_pages</code> <code>List[str]</code> <p>A list of strings, each representing the text content of a page.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrites the file if it exists. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input list of text_pages is empty or contains invalid types.</p> <code>FileExistsError</code> <p>If the file already exists and overwrite is False.</p> <code>PermissionError</code> <p>If the file cannot be created due to insufficient permissions.</p> <code>OSError</code> <p>For other file I/O-related errors.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def save_pages_to_xml(\n    output_xml_path: Path,\n    text_pages: List[str],\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Generates and saves an XML file containing text pages, with a &lt;pagebreak&gt; tag indicating the page ends.\n\n    Parameters:\n        output_xml_path (Path): The Path object for the file where the XML file will be saved.\n        text_pages (List[str]): A list of strings, each representing the text content of a page.\n        overwrite (bool): If True, overwrites the file if it exists. Default is False.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the input list of text_pages is empty or contains invalid types.\n        FileExistsError: If the file already exists and overwrite is False.\n        PermissionError: If the file cannot be created due to insufficient permissions.\n        OSError: For other file I/O-related errors.\n    \"\"\"\n    if not text_pages:\n        raise ValueError(\"The text_pages list is empty. Cannot generate XML.\")\n\n    # Check if the file exists and handle overwrite behavior\n    if output_xml_path.exists() and not overwrite:\n        raise FileExistsError(\n            f\"The file '{output_xml_path}' already exists. Set overwrite=True to overwrite.\"\n        )\n\n    try:\n        # Ensure the output directory exists\n        output_xml_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Write the XML file\n        with output_xml_path.open(\"w\", encoding=\"utf-8\") as xml_file:\n            # Write XML declaration and root element\n            xml_file.write(\"&lt;?xml version='1.0' encoding='UTF-8'?&gt;\\n\")\n            xml_file.write(\"&lt;document&gt;\\n\")\n\n            # Add each page with its content and &lt;pagebreak&gt; tag\n            for page_number, text in enumerate(text_pages, start=1):\n                if not isinstance(text, str):\n                    raise ValueError(\n                        f\"Invalid page content at index {page_number - 1}: expected a string.\"\n                    )\n\n                content = text.strip()\n                escaped_text = escape(content)\n                xml_file.write(f\"    {escaped_text}\\n\")\n                xml_file.write(f\"    &lt;pagebreak page='{page_number}' /&gt;\\n\")\n\n            # Close the root element\n            xml_file.write(\"&lt;/document&gt;\\n\")\n\n        print(f\"XML file successfully saved at {output_xml_path}\")\n\n    except PermissionError as e:\n        raise PermissionError(\n            f\"Permission denied while writing to {output_xml_path}: {e}\"\n        ) from e\n\n    except OSError as e:\n        raise OSError(\n            f\"An OS-related error occurred while saving XML file at {output_xml_path}: {e}\"\n        ) from e\n\n    except Exception as e:\n        raise RuntimeError(f\"An unexpected error occurred: {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.split_xml_on_pagebreaks","title":"<code>split_xml_on_pagebreaks(text, page_groups=None, keep_pagebreaks=True)</code>","text":"<p>Splits an XML document into individual pages based on  tags. Optionally groups pages together based on page_groups and retains  tags if keep_pagebreaks is True. Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def split_xml_on_pagebreaks(\n    text: str,\n    page_groups: Optional[List[Tuple[int, int]]] = None,\n    keep_pagebreaks: bool = True,\n) -&gt; List[str]:\n    \"\"\"\n    Splits an XML document into individual pages based on &lt;pagebreak&gt; tags.\n    Optionally groups pages together based on page_groups\n    and retains &lt;pagebreak&gt; tags if keep_pagebreaks is True.\n    \"\"\"\n    parser = PagebreakXMLParser(text)\n    return parser.parse(page_groups=page_groups, keep_pagebreaks=keep_pagebreaks)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.split_xml_pages","title":"<code>split_xml_pages(text)</code>","text":"<p>Backwards-compatible helper that returns the page contents without pagebreak tags.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>XML document string.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of page strings.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def split_xml_pages(text: str) -&gt; List[str]:\n    \"\"\"\n    Backwards-compatible helper that returns the page contents without pagebreak tags.\n\n    Args:\n        text: XML document string.\n\n    Returns:\n        List of page strings.\n    \"\"\"\n    return split_xml_on_pagebreaks(text, keep_pagebreaks=False)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.extract_tags","title":"<code>extract_tags</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.extract_tags.extract_unique_tags","title":"<code>extract_unique_tags(xml_file)</code>","text":"<p>Extract all unique tags from an XML file using lxml.</p> <p>Parameters:</p> Name Type Description Default <code>xml_file</code> <code>str</code> <p>Path to the XML file.</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p>Set[str]: A set of unique tags in the XML document.</p> Source code in <code>src/tnh_scholar/xml_processing/extract_tags.py</code> <pre><code>def extract_unique_tags(xml_file: str | Path) -&gt; Set[str]:\n    \"\"\"\n    Extract all unique tags from an XML file using lxml.\n\n    Parameters:\n        xml_file (str): Path to the XML file.\n\n    Returns:\n        Set[str]: A set of unique tags in the XML document.\n    \"\"\"\n    # Parse the XML file\n    tree = etree.parse(xml_file)\n\n    # Find all unique tags and return\n    return {element.tag for element in tree.iter()}\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.extract_tags.main","title":"<code>main()</code>","text":"Source code in <code>src/tnh_scholar/xml_processing/extract_tags.py</code> <pre><code>def main():\n    # Create argument parser\n    parser = argparse.ArgumentParser(\n        description=\"Extract all unique tags from an XML file.\"\n    )\n    parser.add_argument(\"xml_file\", type=str, help=\"Path to the XML file.\")\n\n    # Parse command-line arguments\n    args = parser.parse_args()\n\n    # Extract tags\n    tags = extract_unique_tags(args.xml_file)\n\n    # Print results\n    print(\"Unique Tags Found:\")\n    for tag in sorted(tags):\n        print(tag)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing","title":"<code>xml_processing</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.xml_processing.FormattingError","title":"<code>FormattingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception raised for formatting-related errors.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>class FormattingError(Exception):\n    \"\"\"\n    Custom exception raised for formatting-related errors.\n    \"\"\"\n\n    def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n        super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.FormattingError.__init__","title":"<code>__init__(message='An error occurred due to invalid formatting.')</code>","text":"Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def __init__(self, message=\"An error occurred due to invalid formatting.\"):\n    super().__init__(message)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.PagebreakXMLParser","title":"<code>PagebreakXMLParser</code>","text":"<p>Parses XML documents split by  tags, with optional grouping and tag retention. Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>class PagebreakXMLParser:\n    \"\"\"\n    Parses XML documents split by &lt;pagebreak&gt; tags, with optional grouping and tag retention.\n    \"\"\"\n\n    def __init__(self, text: str):\n        if not text or not text.strip():\n            raise ValueError(\"Input XML text is empty or whitespace.\")\n        self.original_text = text\n        self.cleaned_text = \"\"\n        self.pages: List[str] = []\n        self.pagebreak_tags: List[str] = []\n        self._xml_decl_pattern = re.compile(r\"^\\s*&lt;\\?xml[^&gt;]*\\?&gt;\\s*\", re.IGNORECASE)\n        self._document_open_pattern = re.compile(r\"^\\s*&lt;document&gt;\\s*\", re.IGNORECASE)\n        self._document_close_pattern = re.compile(r\"\\s*&lt;/document&gt;\\s*$\", re.IGNORECASE)\n        self._pagebreak_pattern = re.compile(r\"^\\s*&lt;pagebreak\\b[^&gt;]*/&gt;\\s*$\", re.IGNORECASE | re.MULTILINE)\n\n    def _remove_preamble_and_document_tags(self):\n        text = self._xml_decl_pattern.sub(\"\", self.original_text, count=1)\n        text = self._document_open_pattern.sub(\"\", text, count=1)\n        text = self._document_close_pattern.sub(\"\", text, count=1)\n        if not text.strip():\n            raise ValueError(\"No content found between &lt;document&gt; tags.\")\n        self.cleaned_text = text\n\n    def _split_on_pagebreaks(self):\n        self.pages = []\n        self.pagebreak_tags = re.findall(self._pagebreak_pattern, self.cleaned_text)\n        split_lines = re.split(self._pagebreak_pattern, self.cleaned_text)\n        for i, page_content in enumerate(split_lines):\n            page_content = page_content.strip()\n            # skip trailing empty after last pagebreak\n            if not page_content and (i &gt;= len(self.pagebreak_tags)):\n                continue\n            self.pages.append(page_content)\n\n    def _attach_pagebreaks(self, keep_pagebreaks: bool):\n        if not keep_pagebreaks:\n            return\n        for i in range(min(len(self.pages), len(self.pagebreak_tags))):\n            if self.pages[i]:\n                self.pages[i] = f\"{self.pages[i]}\\n{self.pagebreak_tags[i].strip()}\"\n            else:\n                self.pages[i] = self.pagebreak_tags[i].strip()\n\n    def _group_pages(self, page_groups: List[Tuple[int, int]]) -&gt; List[str]:\n        grouped_pages: List[str] = []\n        for start, end in page_groups:\n            if start &lt; 1 or end &lt; start:\n                continue  # skip invalid groups\n            if group := [\n                self.pages[i]\n                for i in range(start - 1, end)\n                if 0 &lt;= i &lt; len(self.pages)\n            ]:\n                grouped_pages.append(\"\\n\".join(group).strip())\n        return grouped_pages\n\n    def parse(\n        self,\n        page_groups: Optional[List[Tuple[int, int]]] = None,\n        keep_pagebreaks: bool = True,\n    ) -&gt; List[str]:\n        \"\"\"\n        Parses the XML and returns a list of page contents, optionally grouped and with pagebreaks retained.\n        \"\"\"\n        self._remove_preamble_and_document_tags()\n        self._split_on_pagebreaks()\n        self._attach_pagebreaks(keep_pagebreaks)\n        # Remove empty pages\n        self.pages = [p for p in self.pages if p]\n        if not self.pages:\n            raise ValueError(\"No pages found in the XML content after splitting on &lt;pagebreak&gt; tags.\")\n        return self._group_pages(page_groups) if page_groups else self.pages\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.PagebreakXMLParser.cleaned_text","title":"<code>cleaned_text = ''</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.xml_processing.PagebreakXMLParser.original_text","title":"<code>original_text = text</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.xml_processing.PagebreakXMLParser.pagebreak_tags","title":"<code>pagebreak_tags = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.xml_processing.PagebreakXMLParser.pages","title":"<code>pages = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/#tnh_scholar.xml_processing.xml_processing.PagebreakXMLParser.__init__","title":"<code>__init__(text)</code>","text":"Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def __init__(self, text: str):\n    if not text or not text.strip():\n        raise ValueError(\"Input XML text is empty or whitespace.\")\n    self.original_text = text\n    self.cleaned_text = \"\"\n    self.pages: List[str] = []\n    self.pagebreak_tags: List[str] = []\n    self._xml_decl_pattern = re.compile(r\"^\\s*&lt;\\?xml[^&gt;]*\\?&gt;\\s*\", re.IGNORECASE)\n    self._document_open_pattern = re.compile(r\"^\\s*&lt;document&gt;\\s*\", re.IGNORECASE)\n    self._document_close_pattern = re.compile(r\"\\s*&lt;/document&gt;\\s*$\", re.IGNORECASE)\n    self._pagebreak_pattern = re.compile(r\"^\\s*&lt;pagebreak\\b[^&gt;]*/&gt;\\s*$\", re.IGNORECASE | re.MULTILINE)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.PagebreakXMLParser.parse","title":"<code>parse(page_groups=None, keep_pagebreaks=True)</code>","text":"<p>Parses the XML and returns a list of page contents, optionally grouped and with pagebreaks retained.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def parse(\n    self,\n    page_groups: Optional[List[Tuple[int, int]]] = None,\n    keep_pagebreaks: bool = True,\n) -&gt; List[str]:\n    \"\"\"\n    Parses the XML and returns a list of page contents, optionally grouped and with pagebreaks retained.\n    \"\"\"\n    self._remove_preamble_and_document_tags()\n    self._split_on_pagebreaks()\n    self._attach_pagebreaks(keep_pagebreaks)\n    # Remove empty pages\n    self.pages = [p for p in self.pages if p]\n    if not self.pages:\n        raise ValueError(\"No pages found in the XML content after splitting on &lt;pagebreak&gt; tags.\")\n    return self._group_pages(page_groups) if page_groups else self.pages\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.join_xml_data_to_doc","title":"<code>join_xml_data_to_doc(file_path, data, overwrite=False)</code>","text":"<p>Joins a list of XML-tagged data with newlines, wraps it with  tags, and writes it to the specified file. Raises an exception if the file exists and overwrite is not set. <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the output file.</p> required <code>data</code> <code>List[str]</code> <p>List of XML-tagged data strings.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it exists.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file exists and overwrite is False.</p> <code>ValueError</code> <p>If the data list is empty.</p> Example <p>join_xml_data_to_doc(Path(\"output.xml\"), [\"Data\"], overwrite=True)</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def join_xml_data_to_doc(\n    file_path: Path, data: List[str], overwrite: bool = False\n) -&gt; None:\n    \"\"\"\n    Joins a list of XML-tagged data with newlines, wraps it with &lt;document&gt; tags,\n    and writes it to the specified file. Raises an exception if the file exists\n    and overwrite is not set.\n\n    Args:\n        file_path (Path): Path to the output file.\n        data (List[str]): List of XML-tagged data strings.\n        overwrite (bool): Whether to overwrite the file if it exists.\n\n    Raises:\n        FileExistsError: If the file exists and overwrite is False.\n        ValueError: If the data list is empty.\n\n    Example:\n        &gt;&gt;&gt; join_xml_data_to_doc(Path(\"output.xml\"), [\"&lt;tag&gt;Data&lt;/tag&gt;\"], overwrite=True)\n    \"\"\"\n    if file_path.exists() and not overwrite:\n        raise FileExistsError(\n            f\"The file {file_path} already exists and overwrite is not set.\"\n        )\n\n    if not data:\n        raise ValueError(\"The data list cannot be empty.\")\n\n    # Create the XML content\n    joined_data = \"\\n\".join(data)  # Joining data with newline\n    xml_content = f\"&lt;document&gt;\\n{joined_data}\\n&lt;/document&gt;\"\n\n    # Write to file\n    file_path.write_text(xml_content, encoding=\"utf-8\")\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.remove_page_tags","title":"<code>remove_page_tags(text)</code>","text":"<p>Removes  and  tags from a text string.</p> <p>Parameters: - text (str): The input text containing  tags. <p>Returns: - str: The text with  tags removed. Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def remove_page_tags(text):\n    \"\"\"\n    Removes &lt;page ...&gt; and &lt;/page&gt; tags from a text string.\n\n    Parameters:\n    - text (str): The input text containing &lt;page&gt; tags.\n\n    Returns:\n    - str: The text with &lt;page&gt; tags removed.\n    \"\"\"\n    # Remove opening &lt;page ...&gt; tags\n    text = re.sub(r\"&lt;page[^&gt;]*&gt;\", \"\", text)\n    # Remove closing &lt;/page&gt; tags\n    text = re.sub(r\"&lt;/page&gt;\", \"\", text)\n    return text\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.save_pages_to_xml","title":"<code>save_pages_to_xml(output_xml_path, text_pages, overwrite=False)</code>","text":"<p>Generates and saves an XML file containing text pages, with a  tag indicating the page ends. <p>Parameters:</p> Name Type Description Default <code>output_xml_path</code> <code>Path</code> <p>The Path object for the file where the XML file will be saved.</p> required <code>text_pages</code> <code>List[str]</code> <p>A list of strings, each representing the text content of a page.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrites the file if it exists. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input list of text_pages is empty or contains invalid types.</p> <code>FileExistsError</code> <p>If the file already exists and overwrite is False.</p> <code>PermissionError</code> <p>If the file cannot be created due to insufficient permissions.</p> <code>OSError</code> <p>For other file I/O-related errors.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def save_pages_to_xml(\n    output_xml_path: Path,\n    text_pages: List[str],\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Generates and saves an XML file containing text pages, with a &lt;pagebreak&gt; tag indicating the page ends.\n\n    Parameters:\n        output_xml_path (Path): The Path object for the file where the XML file will be saved.\n        text_pages (List[str]): A list of strings, each representing the text content of a page.\n        overwrite (bool): If True, overwrites the file if it exists. Default is False.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the input list of text_pages is empty or contains invalid types.\n        FileExistsError: If the file already exists and overwrite is False.\n        PermissionError: If the file cannot be created due to insufficient permissions.\n        OSError: For other file I/O-related errors.\n    \"\"\"\n    if not text_pages:\n        raise ValueError(\"The text_pages list is empty. Cannot generate XML.\")\n\n    # Check if the file exists and handle overwrite behavior\n    if output_xml_path.exists() and not overwrite:\n        raise FileExistsError(\n            f\"The file '{output_xml_path}' already exists. Set overwrite=True to overwrite.\"\n        )\n\n    try:\n        # Ensure the output directory exists\n        output_xml_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Write the XML file\n        with output_xml_path.open(\"w\", encoding=\"utf-8\") as xml_file:\n            # Write XML declaration and root element\n            xml_file.write(\"&lt;?xml version='1.0' encoding='UTF-8'?&gt;\\n\")\n            xml_file.write(\"&lt;document&gt;\\n\")\n\n            # Add each page with its content and &lt;pagebreak&gt; tag\n            for page_number, text in enumerate(text_pages, start=1):\n                if not isinstance(text, str):\n                    raise ValueError(\n                        f\"Invalid page content at index {page_number - 1}: expected a string.\"\n                    )\n\n                content = text.strip()\n                escaped_text = escape(content)\n                xml_file.write(f\"    {escaped_text}\\n\")\n                xml_file.write(f\"    &lt;pagebreak page='{page_number}' /&gt;\\n\")\n\n            # Close the root element\n            xml_file.write(\"&lt;/document&gt;\\n\")\n\n        print(f\"XML file successfully saved at {output_xml_path}\")\n\n    except PermissionError as e:\n        raise PermissionError(\n            f\"Permission denied while writing to {output_xml_path}: {e}\"\n        ) from e\n\n    except OSError as e:\n        raise OSError(\n            f\"An OS-related error occurred while saving XML file at {output_xml_path}: {e}\"\n        ) from e\n\n    except Exception as e:\n        raise RuntimeError(f\"An unexpected error occurred: {e}\") from e\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.split_xml_on_pagebreaks","title":"<code>split_xml_on_pagebreaks(text, page_groups=None, keep_pagebreaks=True)</code>","text":"<p>Splits an XML document into individual pages based on  tags. Optionally groups pages together based on page_groups and retains  tags if keep_pagebreaks is True. Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def split_xml_on_pagebreaks(\n    text: str,\n    page_groups: Optional[List[Tuple[int, int]]] = None,\n    keep_pagebreaks: bool = True,\n) -&gt; List[str]:\n    \"\"\"\n    Splits an XML document into individual pages based on &lt;pagebreak&gt; tags.\n    Optionally groups pages together based on page_groups\n    and retains &lt;pagebreak&gt; tags if keep_pagebreaks is True.\n    \"\"\"\n    parser = PagebreakXMLParser(text)\n    return parser.parse(page_groups=page_groups, keep_pagebreaks=keep_pagebreaks)\n</code></pre>"},{"location":"api/#tnh_scholar.xml_processing.xml_processing.split_xml_pages","title":"<code>split_xml_pages(text)</code>","text":"<p>Backwards-compatible helper that returns the page contents without pagebreak tags.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>XML document string.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of page strings.</p> Source code in <code>src/tnh_scholar/xml_processing/xml_processing.py</code> <pre><code>def split_xml_pages(text: str) -&gt; List[str]:\n    \"\"\"\n    Backwards-compatible helper that returns the page contents without pagebreak tags.\n\n    Args:\n        text: XML document string.\n\n    Returns:\n        List of page strings.\n    \"\"\"\n    return split_xml_on_pagebreaks(text, keep_pagebreaks=False)\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":"<p>Table of Contents:</p> <p>Architecture Overview - High-level conceptual view of TNH Scholar's layered, object-service oriented architecture with links to detailed design documents and ADRs.</p> <p>Ai Text Processing - Table of contents for architecture/ai-text-processing</p> <p>Configuration - Table of contents for architecture/configuration</p> <p>Docs System - Table of contents for architecture/docs-system</p> <p>Gen Ai Service - Table of contents for architecture/gen-ai-service</p> <p>Jvb Viewer - Table of contents for architecture/jvb-viewer</p> <p>Knowledge Base - Table of contents for architecture/knowledge-base</p> <p>Metadata - Table of contents for architecture/metadata</p> <p>Object Service - Table of contents for architecture/object-service</p> <p>Project Policies - Cross-cutting architectural policies and decisions affecting the entire TNH Scholar codebase</p> <p>Prompt System - Table of contents for architecture/prompt-system</p> <p>Setup Tnh - Table of contents for architecture/setup-tnh</p> <p>TNH-Gen CLI - Unified command-line interface for TNH Scholar GenAI operations</p> <p>Transcription - Table of contents for architecture/transcription</p> <p>Ui Ux - Table of contents for architecture/ui-ux</p> <p>Utilities - Table of contents for architecture/utilities</p> <p>Video Processing - Table of contents for architecture/video-processing</p> <p>Ytt Fetch - Table of contents for architecture/ytt-fetch</p> <p>This file auto-generated.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>This document provides a high-level view of the TNH Scholar architecture. It is intentionally brief and conceptual, with links to more detailed design documents and ADRs.</p> <p>TNH Scholar is built around a layered, object-service oriented architecture. At the highest level, it consists of:</p> <ul> <li>A set of CLI tools for end users,</li> <li>A GenAI Service for orchestrating AI model calls and prompt handling,</li> <li>A family of processing pipelines (audio, text, metadata),</li> <li>A knowledge and metadata layer for long-term corpus management.</li> </ul>"},{"location":"architecture/overview/#subsystem-directory-map","title":"Subsystem directory map","text":"<ul> <li>AI Text Processing \u2014 Text object system and AI text processing</li> <li>Configuration \u2014 Configuration management system</li> <li>Docs System \u2014 Documentation system architecture</li> <li>GenAI Service \u2014 GenAI service architecture and interfaces</li> <li>JVB Viewer \u2014 JVB viewer</li> <li>Knowledge Base \u2014 Knowledge base architecture</li> <li>Metadata \u2014 Metadata systems and JSON-LD</li> <li>Object Service \u2014 Core object service architecture</li> <li>Prompt System \u2014 Prompt system and pattern catalog</li> <li>Setup TNH \u2014 Setup tool architecture</li> <li>TNH-FAB \u2014 TNH Fabrication tool design</li> <li>Transcription \u2014 Audio transcription and diarization</li> <li>UI/UX \u2014 User interface and experience design</li> <li>Utilities \u2014 Utility functions and tools</li> <li>Video Processing \u2014 Video processing pipeline</li> <li>ytt-fetch \u2014 YouTube transcript fetching</li> </ul>"},{"location":"architecture/overview/#architectural-diagrams","title":"Architectural diagrams","text":"<pre><code>flowchart TD\n    U[User] --&gt; CLI[CLI Tools]\n    CLI --&gt;|Commands &amp; Config| SVC[GenAI Service]\n    CLI --&gt; PIPE[Processing Pipelines]\n    SVC --&gt; MODELS[AI Providers &amp; Models]\n    PIPE --&gt; DATA[Corpus &amp; Metadata Store]\n    SVC --&gt; DATA\n    DATA --&gt; APPS[Viewers &amp; Downstream Apps]</code></pre> <pre><code>  +--------+           +-----------------+\n  |  User  |  &lt;-----&gt;  |    CLI Tools    |\n  +--------+           +-----------------+\n                             |\n                             v\n                    +-----------------+\n                    |  GenAI Service  |\n                    +-----------------+\n                      /           \\\n                     v             v\n           +-----------------+   +-----------------+\n           |  AI Providers   |   | Processing      |\n           |  &amp; Models       |   | Pipelines       |\n           +-----------------+   +-----------------+\n                     \\           /\n                      v         v\n                    +-----------------+\n                    | Corpus &amp;        |\n                    | Metadata Store  |\n                    +-----------------+\n                             |\n                             v\n                    +-----------------+\n                    | Viewers &amp;       |\n                    | Downstream Apps |\n                    +-----------------+\n</code></pre>"},{"location":"architecture/overview/#key-components","title":"Key Components","text":""},{"location":"architecture/overview/#cli-tools","title":"CLI Tools","text":"<p>The CLI layer provides user-facing commands such as:</p> <ul> <li><code>audio-transcribe</code></li> <li><code>nfmt</code></li> <li><code>tnh-fab</code></li> <li><code>tnh-setup</code></li> <li><code>token-count</code></li> <li><code>ytt-fetch</code></li> </ul> <p>These tools are small, composable, and focused on a single responsibility. They generally:</p> <ul> <li>Accept file or directory inputs,</li> <li>Read configuration from a shared workspace or config file,</li> <li>Produce deterministic, reviewable outputs (text, JSON, or both).</li> </ul> <p>More details:</p> <ul> <li>CLI Overview</li> <li>CLI guides: Command Line Tools Overview</li> </ul>"},{"location":"architecture/overview/#genai-service","title":"GenAI Service","text":"<p>The GenAI Service is an internal orchestration layer that:</p> <ul> <li>Manages prompt patterns and prompt catalogs,</li> <li>Routes model requests to providers (for example, OpenAI),</li> <li>Enforces configuration policies (models, parameters, safety),</li> <li>Tracks fingerprints and provenance of AI-generated outputs.</li> </ul> <p>It is implemented as an object-service, following the architecture described in:</p> <ul> <li>ADR-A01: Object-Service Blueprint for GenAI Service</li> <li>ADR-A02: Pattern Catalog Integration &amp; Legacy Adoption (V1)</li> <li>ADR-A11: Model Parameters Fix</li> <li>ADR-A12: Prompt Fingerprints (V1)</li> </ul>"},{"location":"architecture/overview/#processing-pipelines","title":"Processing Pipelines","text":"<p>Processing pipelines connect the CLI layer, GenAI Service, and data layer. Major pipelines include:</p> <ul> <li>Audio and Speech Pipelines </li> <li>Chunking, diarization, and transcription for Dharma talks and related recordings.  </li> <li> <p>Key documents live under <code>architecture/transcription/</code>.</p> </li> <li> <p>Text Processing and Metadata Pipelines </p> </li> <li>Normalization, tagging, and metadata enrichment for books, journals, and other texts.  </li> <li>See AI text processing and prompt system designs under <code>architecture/ai-text-processing/</code> and <code>architecture/prompt-system/</code>.</li> </ul> <p>These pipelines are designed to be modular and testable, with clear seams for:</p> <ul> <li>Replacing providers or tools,</li> <li>Adding new steps (for example, a new tagging phase),</li> <li>Running partial flows in isolation during development.</li> </ul>"},{"location":"architecture/overview/#corpus-and-metadata-layer","title":"Corpus and Metadata Layer","text":"<p>The corpus and metadata layer is responsible for:</p> <ul> <li>Storing canonical text (and possibly aligned translations),</li> <li>Maintaining structured metadata (chapters, sections, paragraphs, exercises, footnotes),</li> <li>Capturing provenance and versioning information for each artifact.</li> </ul> <p>The exact backend may vary (for example, file-based JSON, SQL, or document stores), but the schema and contracts are intended to remain stable at the domain level.</p> <p>Related documents:</p> <ul> <li>Knowledge Base Architecture Strategy</li> <li>JSON-LD Metadata ADR</li> </ul>"},{"location":"architecture/overview/#architectural-principles","title":"Architectural Principles","text":"<p>Some guiding principles that shape the architecture:</p> <ul> <li> <p>Walking Skeleton First   Start with minimal end-to-end flows, then deepen each layer incrementally.</p> </li> <li> <p>Object-Service and Ports/Adapters   Keep domain logic separate from I/O concerns and provider specifics.</p> </li> <li> <p>Provenance and Traceability   Every AI-assisted transformation should be traceable back to:  </p> </li> <li>The source materials,  </li> <li>The prompts or patterns used,  </li> <li> <p>The models and parameters applied.</p> </li> <li> <p>Human-Centered Design   The system is meant to assist practitioners and researchers, not replace them. Human review is a core part of the design.</p> </li> </ul> <p>For a more detailed discussion of design principles and patterns, see:</p> <ul> <li>System Design</li> <li>Human\u2013AI Software Engineering Principles</li> </ul>"},{"location":"architecture/ai-text-processing/","title":"Ai Text Processing","text":"<p>Table of Contents:</p> <p>Adr - Table of contents for architecture/ai-text-processing/adr</p> <p>Design - Table of contents for architecture/ai-text-processing/design</p> <p>This file auto-generated.</p>"},{"location":"architecture/ai-text-processing/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-AT01: AI Text Processing Pipeline Redesign - Defines the modular TextObject pipeline, metadata handling, and configuration strategy for AI processing.</p> <p>ADR-AT02: TextObject Architecture Decision Records - Captures the historical TextObject design comparisons and links to the original/new design documents.</p> <p>ADR-AT03: AI Text Processing Object-Service Refactor - Three-tier refactor of ai_text_processing module for object-service compliance, GenAIService integration, and prompt system adoption</p> <p>This file auto-generated.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/","title":"ADR-AT01: AI Text Processing Pipeline Redesign","text":"<p>Establishes the TextProcessor abstraction, metadata strategy, and functional pipeline needed for AI text transformations.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-02-26</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#context","title":"Context","text":"<p>The TNH Scholar system requires a flexible, maintainable architecture for AI-based text processing that can:</p> <ul> <li>Handle multiple processing stages (translation, sectioning, etc.)</li> <li>Maintain process history and state</li> <li>Track metadata through processing pipeline</li> <li>Support configuration of processing steps</li> <li>Enable recovery from processing failures</li> <li>Scale to handle large text processing tasks</li> </ul> <p>The system must balance the needs of rapid prototyping with foundational architecture that can evolve toward production use.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#decision-drivers","title":"Decision Drivers","text":"<ol> <li>AI Processing Characteristics:</li> <li>High computational cost</li> <li>Long processing times</li> <li>Significant latency</li> <li> <p>Processing costs (API usage)</p> </li> <li> <p>System Requirements:</p> </li> <li>Pipeline composability</li> <li>State recovery capabilities</li> <li>Configuration flexibility</li> <li>Type safety</li> <li> <p>Metadata tracking</p> </li> <li> <p>Development Phase:</p> </li> <li>Rapid prototyping priority</li> <li>Minimal error handling initially</li> <li>Focus on core functionality</li> <li>Clear path to production evolution</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#decisions","title":"Decisions","text":""},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#1-core-architecture","title":"1. Core Architecture","text":""},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#11-base-processor-architecture","title":"1.1 Base Processor Architecture","text":"<p>Implement abstract base processor with metadata handling:</p> <pre><code>class TextProcessor(ABC):\n    \"\"\"Abstract base class for text processors.\"\"\"\n\n    @abstractmethod\n    def process(\n        self,\n        text_object: TextObject,\n        config: ProcessConfig,\n        input_metadata: Optional[Metadata] = None\n    ) -&gt; ProcessResult:\n        \"\"\"\n        Process text and return result with metadata.\n\n        Args:\n            text_object: Input text with existing metadata\n            config: Process configuration\n            input_metadata: Optional additional metadata to merge\n\n        Returns:\n            ProcessResult containing new text object and metadata\n        \"\"\"\n        pass\n\n    @staticmethod \n    def _prepare_metadata(\n        text_object: TextObject,\n        input_metadata: Optional[Metadata]\n    ) -&gt; Metadata:\n        \"\"\"\n        Merge input metadata with text object metadata.\n\n        Delegates actual merging to TextObject to maintain encapsulation\n        and consistent merge behavior.\n        \"\"\"\n        if input_metadata:\n            return text_object.merge_metadata(input_metadata)\n        return text_object.metadata\n</code></pre> <p>Adopt a functional pipeline architecture with explicit state tracking:</p> <pre><code>@dataclass\nclass ProcessState:\n    \"\"\"Complete state after a process.\"\"\"\n    text_object: TextObject\n    process_metadata: ProcessMetadata\n    generated_metadata: Metadata\n    config: ProcessConfig\n\n@dataclass\nclass ProcessResult:\n    \"\"\"Result of a text processing operation.\"\"\"\n    text_object: TextObject\n    process_metadata: ProcessMetadata\n    generated_metadata: Metadata\n</code></pre> <p>Key aspects:</p> <ul> <li>Pure functional processor interface</li> <li>Complete state capture</li> <li>Separation of process and content metadata</li> <li>Clear data flow</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#2-type-safety-approach","title":"2. Type Safety Approach","text":"<p>Extend existing Metadata class for type-safe containers:</p> <pre><code>class ProcessMetadata(Metadata):\n    \"\"\"Records information about a specific processing operation.\"\"\"\n    def __init__(\n        self,\n        process_name: str,\n        parameters: Dict[str, JsonValue],\n        timestamp: Optional[datetime] = None\n    )\n</code></pre> <p>Key aspects:</p> <ul> <li>Built on existing Metadata foundation</li> <li>Enforces required process fields</li> <li>Maintains JSON serializability</li> <li>Type-safe parameter handling</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#3-configuration-management","title":"3. Configuration Management","text":"<p>Implement hierarchical configuration system:</p> <pre><code>class BaseProcessConfig:\n    \"\"\"Base configuration with common fields.\"\"\"\n    template: Optional[str]\n    source_language: Optional[str]\n    metadata: Optional[Metadata]\n\nclass TranslationConfig(BaseProcessConfig):\n    \"\"\"Translation-specific configuration.\"\"\"\n    target_language: str\n    context_lines: int = 3\n    segment_size: Optional[int] = None\n</code></pre> <p>Key aspects:</p> <ul> <li>Common base configuration</li> <li>Process-specific extensions</li> <li>Type-safe parameter definitions</li> <li>Configuration inheritance</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#4-state-preservation","title":"4. State Preservation","text":"<p>Implement full state preservation strategy:</p> <pre><code>class Pipeline:\n    \"\"\"Maintains full process history.\"\"\"\n    def __init__(self, save_dir: Optional[Path] = None):\n        self.states: List[ProcessState] = []\n        self.save_dir = save_dir\n\n    def save_state(self, filepath: Optional[Path] = None) -&gt; None:\n        \"\"\"Save current pipeline state to disk.\"\"\"\n</code></pre> <p>Key aspects:</p> <ul> <li>Full in-memory state maintenance</li> <li>Simple disk persistence</li> <li>Manual and automatic save points</li> <li>State recovery capabilities</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#implementation-scope","title":"Implementation Scope","text":"<p>This high-level design intentionally omits several implementation concerns that will be addressed in later phases:</p> <ol> <li>Validation</li> <li>Data validation</li> <li>State validation</li> <li>Configuration validation</li> <li> <p>Type constraints</p> </li> <li> <p>Error Handling</p> </li> <li>Process failures</li> <li>State recovery</li> <li>Configuration errors</li> <li> <p>Type errors</p> </li> <li> <p>Operational Concerns</p> </li> <li>Logging</li> <li>Monitoring</li> <li>Performance optimization</li> <li>Resource management</li> </ol> <p>These concerns, while critical for production deployment, are deferred to maintain focus on core architectural decisions and relationships. This allows the design to evolve naturally as implementation requirements become clearer through prototyping and initial development.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#diagrams","title":"Diagrams","text":""},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#text-processing-pipeline-flow","title":"Text Processing Pipeline Flow","text":"<pre><code>classDiagram\n    %% Core Abstract &amp; Interfaces\n    class TextProcessor {\n        &lt;&lt;abstract&gt;&gt;\n        +process(text_object: TextObject, config: ProcessConfig, input_metadata: Metadata) ProcessResult\n        #_prepare_metadata(text_object: TextObject, input_metadata: Metadata) Metadata\n    }\n\n    class ProcessConfig {\n        &lt;&lt;interface&gt;&gt;\n    }\n\n    %% Domain Objects\n    class ProcessResult {\n        +text_object: TextObject\n        +process_metadata: ProcessMetadata\n        +generated_metadata: Metadata\n    }\n\n    class TextObject {\n        +content: NumberedText\n        +language: str\n        +sections: Optional~List[SectionObject]~\n        +metadata: Metadata\n        +merge_metadata(input_metadata: Metadata) Metadata\n    }\n\n    class SectionObject {\n        +title: Optional~str~\n        +section_range: SectionRange\n        +metadata: Optional~Metadata~\n    }\n\n    class SectionRange {\n        +start: int\n        +end: int\n    }\n\n    class Pipeline {\n        +states: List~ProcessState~\n        +save_dir: Path\n        +save_state(filepath: Path)\n        +load_state(filepath: Path)\n        +get_current_state() ProcessState\n        +rollback_to(state_index: int)\n    }\n\n    class ProcessState {\n        +text_object: TextObject\n        +process_metadata: ProcessMetadata\n        +generated_metadata: Metadata\n        +config: ProcessConfig\n    }\n\n    class BaseProcessConfig {\n        +template: Optional~Template~\n        +source_language: Optional~str~\n        +metadata: Optional~Metadata~\n    }\n\n    class Template {\n        +data: Dict~str, str~\n    }\n\n    class Metadata {\n        +data: Dict~str, JsonValue~\n        +merge(other: Metadata) Metadata\n    }\n\n    class ProcessMetadata {\n        +process_name: str\n        +parameters: Dict~str, JsonValue~\n        +timestamp: datetime\n    }\n\n    %% Concrete Implementations\n    class TranslationProcessor\n    class SectionProcessor\n    class SectionGenerator\n    class GeneralProcessor\n    class TranslationConfig {\n        +target_language: str\n        +context_lines: int\n        +segment_size: Optional~int~\n    }\n    class SectionConfig {\n        +section_rules: List~Rule~\n        +min_section_length: int\n    }\n\n    %% Relationships\n    TextProcessor &lt;|-- TranslationProcessor : extends\n    TextProcessor &lt;|-- SectionProcessor : extends\n    TextProcessor &lt;|-- SectionGenerator : extends\n    TextProcessor &lt;|-- GeneralProcessor : extends\n\n    TextProcessor ..&gt; ProcessResult : creates\n    ProcessResult --* ProcessState : becomes\n    Pipeline o-- ProcessState : contains\n    ProcessState o-- TextObject : contains\n    ProcessState o-- ProcessMetadata : contains\n    ProcessState o-- Metadata : contains\n    ProcessState --&gt; ProcessConfig : references\n\n    ProcessConfig &lt;|.. BaseProcessConfig : implements\n    BaseProcessConfig &lt;|-- TranslationConfig : extends\n    BaseProcessConfig &lt;|-- SectionConfig : extends\n\n    TextObject o-- \"0..*\" SectionObject : contains\n    TextObject o-- Metadata : has\n    SectionObject *-- SectionRange : has\n    SectionObject o-- Metadata : has optional\n\n    ProcessMetadata --|&gt; Metadata : extends\n    BaseProcessConfig o-- Template : has optional\n    BaseProcessConfig o-- Metadata : has optional</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#example-processing-sequence","title":"Example Processing Sequence","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant Pipeline\n    participant SectionProcessor\n    participant TranslationProcessor\n    participant Storage\n\n    Client-&gt;&gt;Pipeline: create_pipeline()\n\n    Note over Pipeline: Initial State\n\n    Client-&gt;&gt;Pipeline: process_text(section_config)\n    Pipeline-&gt;&gt;SectionProcessor: process(text_object, config)\n    SectionProcessor--&gt;&gt;Pipeline: ProcessResult\n    Pipeline-&gt;&gt;Storage: save_state()\n\n    Note over Pipeline: State After Sectioning\n\n    Client-&gt;&gt;Pipeline: process_text(translation_config)\n    Pipeline-&gt;&gt;TranslationProcessor: process(text_object, config)\n    TranslationProcessor--&gt;&gt;Pipeline: ProcessResult\n    Pipeline-&gt;&gt;Storage: save_state()\n\n    Note over Pipeline: Final State\n\n    Client-&gt;&gt;Pipeline: get_final_result()\n    Pipeline--&gt;&gt;Client: ProcessResult</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#data-flow-through-pipeline","title":"Data Flow Through Pipeline","text":"<pre><code>flowchart TD\n    subgraph Input\n        A[Raw Text] --&gt; |frontmatter extraction| C1[Content]\n        A --&gt; |frontmatter extraction| M1[File Metadata]\n        C1 --&gt; T[TextObject Creation]\n        M1 --&gt; T\n        M2[Input Metadata] --&gt; T\n    end\n\n    subgraph Pipeline\n        T --&gt; P1{SectionProcessor}\n        P1 -- ProcessResult --&gt; S1[State 1]\n        S1 --&gt; P2{TranslationProcessor}\n        P2 -- ProcessResult --&gt; S2[State 2]\n    end\n\n    subgraph Metadata Flow\n        M1 --&gt; PMD[Process Metadata]\n        PMD --&gt; GM1[Generated Metadata 1]\n        GM1 --&gt; GM2[Generated Metadata 2]\n    end\n\n    subgraph Output\n        S2 --&gt; FT[Final TextObject]\n        GM2 --&gt; FT\n    end\n\n    classDef metadata fill:#a9f,stroke:#333,stroke-width:2px;\n    class M1,M2,PMD,GM1,GM2 metadata;</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#consequences","title":"Consequences","text":""},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#positive","title":"Positive","text":"<ol> <li>Clean, Functional Design</li> <li>Clear data flow</li> <li>Easy to reason about</li> <li>Simple to test</li> <li> <p>Straightforward recovery</p> </li> <li> <p>Type Safety</p> </li> <li>Early error detection</li> <li>Clear interfaces</li> <li>IDE support</li> <li> <p>Self-documenting code</p> </li> <li> <p>Flexible Configuration</p> </li> <li>Easy to extend</li> <li>Type-safe parameters</li> <li>Clear hierarchy</li> <li> <p>Process-specific options</p> </li> <li> <p>Complete State Tracking</p> </li> <li>Full recovery capability</li> <li>Clear process history</li> <li>Simple debugging</li> <li>Easy state inspection</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#negative","title":"Negative","text":"<ol> <li>Memory Usage</li> <li>Full state history in memory</li> <li>Potential memory pressure</li> <li>May require paging</li> <li> <p>Duplicate state on disk</p> </li> <li> <p>Storage Overhead</p> </li> <li>Multiple state copies</li> <li>No automatic cleanup</li> <li>Growing disk usage</li> <li> <p>Basic save/load</p> </li> <li> <p>Implementation Complexity</p> </li> <li>Many classes to implement</li> <li>Configuration hierarchy</li> <li>State serialization</li> <li>Recovery logic</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#open-issues","title":"Open Issues","text":"<ol> <li>State Management</li> <li>Cleanup strategy for saved states</li> <li>Memory pressure handling</li> <li>Recovery validation</li> <li> <p>State versioning</p> </li> <li> <p>Configuration</p> </li> <li>Parameter validation rules</li> <li>Configuration versioning</li> <li>Default value handling</li> <li> <p>Configuration persistence</p> </li> <li> <p>Process Flow</p> </li> <li>Error recovery strategy</li> <li>Process validation</li> <li>Pipeline branching</li> <li> <p>Parallel processing</p> </li> <li> <p>Type System</p> </li> <li>Parameter type constraints</li> <li>Custom type definitions</li> <li>Validation rules</li> <li>Type conversion</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#implementation-plan","title":"Implementation Plan","text":"<ol> <li>Phase 1: Core Framework</li> <li>Basic Pipeline class</li> <li>ProcessState/Result</li> <li>Simple persistence</li> <li> <p>Type definitions</p> </li> <li> <p>Phase 2: Configuration</p> </li> <li>Base configuration</li> <li>Process configurations</li> <li>Configuration validation</li> <li> <p>Parameter typing</p> </li> <li> <p>Phase 3: State Management</p> </li> <li>State persistence</li> <li>Recovery mechanisms</li> <li>Cleanup utilities</li> <li>Validation tools</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#notes","title":"Notes","text":"<p>This design prioritizes:</p> <ul> <li>Clarity over optimization</li> <li>Type safety over flexibility</li> <li>State tracking over memory efficiency</li> <li>Simple persistence over sophisticated storage</li> </ul> <p>The approach provides a clear path from prototype to production while maintaining core architectural principles. The functional design with complete state tracking trades memory efficiency for reliability and recoverability, acknowledging that AI processing times dominate system resource considerations.</p> <p>Implementation should proceed incrementally, focusing first on core functionality while maintaining awareness of future requirements. The design allows for evolution toward more sophisticated state management, configuration handling, and error recovery as the system matures.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#adr-002-pipeline-state-management-strategy","title":"ADR 002: Pipeline State Management Strategy","text":""},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#context_1","title":"Context","text":"<p>The text processing pipeline transforms content through multiple stages (sectioning, translation, etc.). Initially considered keeping full state history in memory by creating new TextObject instances at each stage. Shifted to an in-place modification approach with disk checkpointing.</p> <p>Original approach: <pre><code># New object created at each stage\nsection_result = section_processor.process(text_obj)\ntranslate_result = translation_processor.process(section_result)\nformat_result = format_processor.process(translate_result)\n</code></pre></p> <p>New approach: <pre><code># Single object transformed and checkpointed\ntext_obj.transform(content=sectioned_text, process_tag=\"section\")\nsave_checkpoint(text_obj, \"after_section\")\ntext_obj.transform(content=translated_text, process_tag=\"translate\")\nsave_checkpoint(text_obj, \"after_translation\")\n</code></pre></p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#decision-drivers_1","title":"Decision Drivers","text":"<ol> <li>Resource Management</li> <li>Long processing pipelines create many objects</li> <li>Large text content duplicated across objects</li> <li>Memory pressure with multiple TextObjects</li> <li> <p>Need for state recovery on failure</p> </li> <li> <p>Pipeline Requirements</p> </li> <li>Must track processing history</li> <li>Need ability to recover from failures</li> <li>State checkpointing essential</li> <li>Process history must be maintained</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#decision","title":"Decision","text":"<p>Adopt in-place modification with disk checkpointing: - Single TextObject flows through pipeline - Object transformed in-place at each stage - Process history tracked in metadata - State checkpointed to disk after transforms</p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#consequences_1","title":"Consequences","text":""},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#positive_1","title":"Positive","text":"<ol> <li>Resource Efficiency</li> <li>Minimal memory usage</li> <li>No content duplication</li> <li>Clear cleanup points</li> <li> <p>Efficient state management</p> </li> <li> <p>Process Management</p> </li> <li>Simple state recovery</li> <li>Clear process tracking</li> <li>Explicit checkpoints</li> <li>Disk-based history</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#negative_1","title":"Negative","text":"<ol> <li>Code Clarity</li> <li>In-place modifications less obvious</li> <li>State changes need clear documentation</li> <li>Checkpoint management required</li> <li>More complex testing</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Use <code>transform()</code> method for clear intent</li> <li>Document state modifications</li> <li>Implement robust checkpoint system</li> <li>Maintain comprehensive process history</li> </ul> <p>The tradeoff of code clarity for resource efficiency is mitigated through clear method naming and documentation.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#adr-003-textobject-transform-method-design","title":"ADR 003: TextObject Transform Method Design","text":""},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#status_1","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#context_2","title":"Context","text":"<p>The TextObject class requires a method for modifying its state during pipeline processing. Initially considered conventional update patterns, but the processing pipeline performs transformational operations like translation that fundamentally change both content and metadata.</p> <p>Two main approaches were considered:</p> <ol> <li>Conventional <code>update()</code> method</li> <li>Domain-specific <code>transform()</code> method</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#decision-drivers_2","title":"Decision Drivers","text":"<ol> <li>Memory Efficiency</li> <li>Need to avoid creating multiple TextObject instances</li> <li>System performs disk-based checkpointing</li> <li> <p>In-place modification is acceptable</p> </li> <li> <p>Semantic Clarity</p> </li> <li>Operations fundamentally transform content</li> <li>Multiple aspects may change simultaneously (content, language, metadata)</li> <li>Changes are part of a processing pipeline</li> <li> <p>Need to track transformation history</p> </li> <li> <p>Processing Context</p> </li> <li>Translation changes both content and metadata language</li> <li>Sectioning modifies structure while preserving content</li> <li>Each step in pipeline represents a transformation</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#decision_1","title":"Decision","text":"<p>Implement <code>transform()</code> method for in-place modification:</p> <pre><code>def transform(\n    self,\n    content: str,\n    language: str, \n    process_tag: str,\n    sections: Optional[List[SectionObject]] = None,\n    timestamp: Optional[datetime] = None\n) -&gt; None:\n    \"\"\"Transform TextObject by modifying content and metadata in-place.\n\n    Represents a fundamental transformation of the object's state as part\n    of a processing pipeline. Records transformation in process history.\n    \"\"\"\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#consequences_2","title":"Consequences","text":""},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#positive_2","title":"Positive","text":"<ol> <li>Better semantic alignment with operations</li> <li>Clearer expression of pipeline transformations</li> <li>Self-documenting code</li> <li>More accurate process history tracking</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#negative_2","title":"Negative","text":"<ol> <li>Deviates from conventional update pattern</li> <li>In-place modification less obvious from method name</li> <li>Requires clear documentation about state modification</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#notes_1","title":"Notes","text":"<p>Choose <code>transform</code> over <code>update</code> because:</p> <ul> <li>Better represents domain operations</li> <li>Clearer about fundamental state changes</li> <li>Aligns with pipeline architecture</li> <li>Supports tracking transformation chain</li> </ul> <p>Memory efficiency through in-place modification is maintained while improving semantic clarity of operations.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#implementation-notes_1","title":"Implementation Notes","text":"<ul> <li>Clear documentation about in-place modification</li> <li>Consistent usage across processors</li> <li>Robust process history tracking</li> <li>Proper logging of transformations</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#adr-004-flexible-function-parameters-in-processing-pipeline","title":"ADR 004: Flexible Function Parameters in Processing Pipeline","text":""},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#status_2","title":"Status","text":"<p>Proposed (Prototype Phase)</p>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#context_3","title":"Context","text":"<p>During rapid prototyping of the processing pipeline, we need to handle various processing configurations while maintaining code flexibility. The system frequently needs optional parameters with sensible defaults, especially in text processing methods.</p> <p>Current pattern uses Optional types with None defaults:</p> <pre><code>def translate_text(\n    self,\n    text: TextObject,\n    segment_size: Optional[int] = None,\n    source_language: Optional[str] = None,\n    target_language: Optional[str] = None,\n    template_dict: Optional[Dict] = None,\n) -&gt; TextObject:\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#decision-drivers_3","title":"Decision Drivers","text":"<ol> <li>Prototype Phase Requirements</li> <li>Need for rapid iteration</li> <li>Flexibility in parameter combinations</li> <li>Easy testing with minimal parameters</li> <li> <p>Clear default behaviors</p> </li> <li> <p>Processing Context</p> </li> <li>Many parameters can be derived from context</li> <li>Sensible defaults exist for most parameters</li> <li>Some parameters depend on others</li> <li> <p>Parameters may be overridden by configuration</p> </li> <li> <p>Future Evolution</p> </li> <li>Will eventually move to configuration objects</li> <li>Need to maintain clear parameter relationships</li> <li>Must support transition to formal configuration</li> <li>Should document parameter dependencies</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#decision_2","title":"Decision","text":"<p>Use Optional types with None defaults for processing parameters:</p> <pre><code>def process_text(\n    self,\n    text: TextObject,\n    source_language: Optional[str] = None,  # Can be derived from text\n    segment_size: Optional[int] = None,     # Has calculable default\n    template_dict: Optional[Dict] = None,   # Optional customization\n) -&gt; TextObject:\n    \"\"\"Process text with flexible parameter configuration.\"\"\"\n    # Derive missing parameters\n    if not source_language:\n        source_language = text.language\n\n    # Calculate defaults when needed\n    if not segment_size:\n        segment_size = _calculate_segment_size(\n            text.content, DEFAULT_TARGET_TOKENS\n        )\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#consequences_3","title":"Consequences","text":""},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#positive_3","title":"Positive","text":"<ol> <li>Flexibility</li> <li>Easy to call with minimal parameters</li> <li>Clear default behaviors</li> <li>Simple parameter overriding</li> <li> <p>Supports rapid prototyping</p> </li> <li> <p>Clarity</p> </li> <li>Type hints maintain clarity</li> <li>Default values are explicit</li> <li>Parameter requirements are clear</li> <li> <p>Dependencies are visible</p> </li> <li> <p>Evolution Path</p> </li> <li>Easy transition to configuration objects</li> <li>Clear parameter relationships</li> <li>Documented default behaviors</li> <li>Maintains type safety</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#negative_3","title":"Negative","text":"<ol> <li>Validation Overhead</li> <li>Need to check for None values</li> <li>Parameter dependency checking required</li> <li>Default calculations may be repeated</li> <li> <p>Validation spread across code</p> </li> <li> <p>Documentation Requirements</p> </li> <li>Need to document default behaviors</li> <li>Parameter dependencies must be clear</li> <li>Default calculations should be explained</li> <li>Configuration patterns need description</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#implementation-notes_2","title":"Implementation Notes","text":"<ol> <li>Parameter Documentation</li> </ol> <pre><code>def process_text(\n    text: TextObject,              # Required base content\n    source_language: Optional[str] = None,  # Derived from text if None\n    segment_size: Optional[int] = None,     # Calculated if None\n):\n    \"\"\"Process text with flexible configuration.\n\n    Args:\n        text: Base content to process\n        source_language: Override text's language if provided\n        segment_size: Custom segment size or None for automatic\n    \"\"\"\n</code></pre> <ol> <li>Default Handling Pattern</li> </ol> <pre><code>def _resolve_parameters(\n    self,\n    text: TextObject,\n    source_language: Optional[str],\n    segment_size: Optional[int]\n) -&gt; tuple[str, int]:\n    \"\"\"Resolve processing parameters with defaults.\"\"\"\n    language = source_language or text.language\n    size = segment_size or _calculate_segment_size(text.content)\n    return language, size\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#future-migration","title":"Future Migration","text":"<p>This approach provides clear migration path to configuration objects:</p> <pre><code># Future configuration pattern\nclass ProcessConfig:\n    source_language: Optional[str] = None\n    segment_size: Optional[int] = None\n\n    def resolve(self, text: TextObject) -&gt; ResolvedConfig:\n        \"\"\"Resolve configuration with defaults.\"\"\"\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at01-ai-text-processing/#notes_2","title":"Notes","text":"<p>This flexible parameter pattern:</p> <ul> <li>Supports rapid prototyping needs</li> <li>Maintains type safety and clarity</li> <li>Documents parameter relationships</li> <li>Provides path to formal configuration</li> </ul> <p>The approach balances immediate flexibility needs with future evolution to more structured configuration handling.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/","title":"ADR-AT02: TextObject Architecture Decision Records","text":"<p>Summarizes the historical TextObject designs and the documents preserved for reference.</p> <ul> <li>Status: Accepted</li> <li>Date: 2025-02-01</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#context","title":"Context","text":"<p>During documentation efforts, we created a \"greenfield\" design document for TextObject without fully accounting for the existing implementation. This resulted in having both an as-built design and a potential alternative design that suggests simplifications.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#decision","title":"Decision","text":"<p>We will maintain three documents:</p> <ol> <li><code>design/textobject-original-design.md</code> - Documents the original implementation</li> <li><code>design/textobject-new-design.md</code> - Records the simpler new design</li> <li>This ADR file - Explains the context and tracks future decisions</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#consequences","title":"Consequences","text":"<ul> <li>Clear separation between actual and planned future designs</li> <li>Preserved thinking about potential simplifications</li> <li>Minimal documentation overhead by keeping related docs focused</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#adr-002-current-textobject-implementation-design","title":"ADR 002: Current TextObject Implementation Design","text":""},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#context_1","title":"Context","text":"<p>The TextObject system was implemented to provide structured text handling with explicit section boundaries and metadata support.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#decision_1","title":"Decision","text":"<p>Key design points of current implementation:</p> <ol> <li>Sections have explicit start_line and end_line</li> <li>LogicalSection uses Pydantic BaseModel</li> <li>Metadata follows Dublin Core standards</li> <li>Strong integration with NumberedText for line management</li> <li>ISO 639-1 language codes used but not strictly validated</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#consequences_1","title":"Consequences","text":"<ul> <li>More explicit section control but higher validation complexity</li> <li>Clear contract through Pydantic models</li> <li>Flexible metadata handling</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#adr-003-new-textobject-design-consideration","title":"ADR 003: New TextObject Design Consideration","text":""},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#status_1","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#context_2","title":"Context","text":"<p>During documentation, a simpler design was conceived that would reduce complexity through implicit section boundaries. Additionally section metadata and contextual data could be added to convey/preserve important information for users and for AI assistants.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#decision_2","title":"Decision","text":"<p>Key points of alternative design:</p> <ol> <li>Sections defined only by start_line</li> <li>End lines implicit (next section's start - 1)</li> <li>Guaranteed contiguous coverage</li> <li>Simplified validation</li> <li>Addition of metadata and context fields</li> <li>Separation of concerns for a TextObject (general representation) and TextObjectFormat (for API use).</li> </ol> <p>This design is preserved for consideration in future refactoring.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#consequences_2","title":"Consequences","text":"<ul> <li>Would simplify section management</li> <li>Would require migration effort</li> <li>Trade-off between explicitness and simplicity</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#adr-004-textobject-re-design-decisions","title":"ADR 004: TextObject Re-Design Decisions","text":""},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#status_2","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#context_3","title":"Context","text":"<p>The TextObject new design specification proposes some significant changes from existing implementations, particularly around section handling, while some aspects of metadata and language handling need clarification for the prototype phase.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#decisions","title":"Decisions","text":""},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#1-language-code-handling","title":"1. Language Code Handling","text":"<ul> <li>Language code validation (ISO 639-1 compliance) is deferred to production phase</li> <li>Prototype accepts any string value for language codes</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#2-metadata-requirements","title":"2. Metadata Requirements","text":"<ul> <li>No minimum required metadata fields for prototype phase</li> <li>Will establish standard placeholder value for unset fields:</li> <li>None for optional fields</li> <li>Empty string for required strings</li> <li>Empty lists for collections</li> <li>Metadata validation deferred to production phase</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#3-numberedtext-integration","title":"3. NumberedText Integration","text":"<ul> <li>TextObject maintains immutable reference to its NumberedText instance</li> <li>No modifications to line numbers allowed after TextObject creation</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#consequences_3","title":"Consequences","text":""},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#positive","title":"Positive","text":"<ul> <li>Simpler section management through implicit end lines</li> <li>Reduced validation complexity</li> <li>Guaranteed contiguous sections by design</li> <li>Clear separation from current implementation for migration planning</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#negative","title":"Negative","text":"<ul> <li>Will require migration from current dual start_line/end_line model</li> <li>Cannot explicitly represent gaps between sections (if that was ever needed)</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#neutral","title":"Neutral","text":"<ul> <li>Tools using TextObject will need to adapt to new section model</li> <li>May need transition period supporting both models</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#related","title":"Related","text":"<ul> <li>Original TextObject.md design specification</li> <li>Current response_format.py implementation</li> <li>tnh-fab implementation requirements</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#adr-005-textobject-metadata-management-and-processing-pipeline","title":"ADR 005: TextObject Metadata Management and Processing Pipeline","text":""},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#status_3","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#context_4","title":"Context","text":"<p>The TextObject system needs to handle metadata consistently across different processing workflows while maintaining simplicity for the prototype phase. Two main workflows exist:</p> <ol> <li>AI-based section generation and processing</li> <li>Pattern-based section generation for large texts</li> </ol> <p>Key challenges include:</p> <ul> <li>Consistent metadata handling across workflows</li> <li>Metadata extraction from various sources</li> <li>Maintaining processable sections with context</li> <li>Supporting very large texts</li> <li>Balancing simplicity with extensibility</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#decision_3","title":"Decision","text":""},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#1-metadata-format-and-storage","title":"1. Metadata Format and Storage","text":"<ul> <li>Use YAML frontmatter as the standard metadata format</li> <li>Base metadata structure on Dublin Core standards</li> <li>Generate metadata only once at start of pipeline</li> <li>Store metadata separately from content body</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#2-metadata-processing-strategy","title":"2. Metadata Processing Strategy","text":"<p>Simple, predictable process with clear precedence:</p> <pre><code>if YAML frontmatter exists:\n    use existing metadata as authoritative\n    run AI metadata extraction for validation\n    if differences found:\n        log warning showing differences\n        retain existing metadata\nelse:\n    if other metadata format detected:\n        log warning about potential conflict\n    extract head/tail sections\n    generate metadata via AI processing\n    add YAML frontmatter\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#3-minimal-metadata-requirements","title":"3. Minimal Metadata Requirements","text":"<p>Guarantee basic metadata fields:</p> <ul> <li>title</li> <li>description</li> <li>language</li> <li>date</li> <li>identifier</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#4-processing-pipeline-structure","title":"4. Processing Pipeline Structure","text":"<p>Standard pipeline for all workflows:</p> <pre><code>Metadata Management \u2192 Text Analysis \u2192 Section Generation \u2192 Section Processing \u2192 Assembly\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#5-text-analysis-strategies-for-investigation","title":"5. Text Analysis Strategies for Investigation","text":"<p>Multiple approaches should be tested to determine optimal text analysis strategy. Key considerations include processing efficiency, metadata quality, and handling of different text structures.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#proposed-analysis-methods","title":"Proposed Analysis Methods","text":"<ol> <li>Token-Based Approach</li> <li>Define boundaries by token count</li> <li>Consider GPT token encoding specifics</li> <li>Test various token thresholds</li> <li> <p>Evaluate impact on metadata quality</p> </li> <li> <p>Line-Based Approach</p> </li> <li>Use natural line breaks</li> <li>Test fixed line counts vs. percentage</li> <li>Consider paragraph boundaries</li> <li> <p>Evaluate handling of different line lengths</p> </li> <li> <p>Size-Based Approach</p> </li> <li>Use file size for initial categorization</li> <li>Test different size thresholds</li> <li>Consider memory efficiency</li> <li> <p>Evaluate processing speed</p> </li> <li> <p>Structural Approach</p> </li> <li>Detect document structure (headings, sections)</li> <li>Use natural document divisions</li> <li>Consider format-specific markers</li> <li>Test robustness across document types</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#testing-requirements","title":"Testing Requirements","text":"<ol> <li>Metrics to Evaluate</li> <li>Metadata extraction quality</li> <li>Processing time</li> <li>Token usage efficiency</li> <li>Memory requirements</li> <li> <p>Accuracy across document types</p> </li> <li> <p>Test Scenarios</p> </li> <li>Very small documents (&lt; 1000 tokens)</li> <li>Standard documents (1000-5000 tokens)</li> <li>Large documents (5000-128,000 tokens)</li> <li>Very large documents (&gt;128,000 tokens)</li> <li>Documents with varying structures</li> <li> <p>Multi-language documents</p> </li> <li> <p>Success Criteria</p> </li> <li>Consistent metadata quality</li> <li>Predictable performance</li> <li>Resource efficiency</li> <li>Reliable handling of edge cases</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#implementation-note","title":"Implementation Note","text":"<p>Initial implementation should support multiple analysis strategies to facilitate testing and comparison. Results will inform the final strategy selection.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#consequences_4","title":"Consequences","text":""},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#positive_1","title":"Positive","text":"<ul> <li>Consistent metadata handling across system</li> <li>Simple, predictable metadata generation</li> <li>Clear separation of metadata and content</li> <li>Support for future extensions</li> <li>Leverages existing tools and standards</li> <li>Minimal complexity in prototype phase</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#negative_1","title":"Negative","text":"<ul> <li>May need to refactor for more complex metadata needs</li> <li>Limited handling of non-YAML metadata</li> <li>Potential for metadata conflicts in some cases</li> <li>Head/tail analysis may miss important metadata</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#neutral_1","title":"Neutral","text":"<ul> <li>Single-pass metadata generation may require future revision</li> <li>Fixed head/tail sizes may need tuning</li> <li>Warning-only approach to metadata conflicts requires user attention</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#implementation-notes","title":"Implementation Notes","text":""},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#phase-1-current","title":"Phase 1 (Current)","text":"<ul> <li>Implement basic YAML frontmatter handling</li> <li>Basic Dublin Core metadata structure</li> <li>Simple head/tail AI analysis</li> <li>Warning system for metadata conflicts</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#future-considerations","title":"Future Considerations","text":"<ul> <li>Enhanced metadata extraction from various sources</li> <li>Metadata confidence tracking</li> <li>Metadata inheritance in sectioned documents</li> <li>Multiple metadata format support</li> <li>Streaming processing for very large texts</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#related-documents","title":"Related Documents","text":"<ul> <li>TextObject System Design Document</li> <li>Dublin Core Metadata Standard</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#adr-006-textobject-metadata-generation-and-storage","title":"ADR 006: TextObject Metadata Generation and Storage","text":""},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#status_4","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#context_5","title":"Context","text":"<p>TextObject needs to handle cases where input files lack YAML frontmatter metadata. The system needs a clean separation between metadata handling and TextObject's core functionality while maintaining simplicity during the prototype phase.</p> <p>Key challenges:</p> <ul> <li>Safe handling of source files</li> <li>Clear default behavior</li> <li>Separation of metadata generation from validation</li> <li>Future extensibility for different metadata formats</li> <li>File permission and access issues</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#decision_4","title":"Decision","text":"<p>Implement a separated metadata handling approach:</p> <ol> <li> <p>TextObject Responsibilities:</p> </li> <li> <p>Expect and validate YAML frontmatter only</p> </li> <li>Issue warning if metadata missing</li> <li>Delegate to MetadataGenerator for generation</li> <li> <p>Use standardized external metadata location</p> </li> <li> <p>MetadataGenerator Component:</p> </li> <li> <p>Handle metadata extraction logic</p> </li> <li>Manage file operations (save/modify)</li> <li> <p>Support two storage modes:   a. Save to separate file in tnh_metadata directory   b. Modify source file in-place (with permission check)</p> </li> <li> <p>Default Storage Structure:</p> </li> </ol> <pre><code>working_dir/\n\u251c\u2500\u2500 source_file.txt\n\u2514\u2500\u2500 tnh_metadata/\n    \u2514\u2500\u2500 source_file_meta.txt\n</code></pre> <ol> <li> <p>Prototype Phase Handling:</p> </li> <li> <p>Warning-only for non-YAML metadata formats</p> </li> <li>Basic permission checking</li> <li>Simple metadata extraction</li> <li>No backup system required yet</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#consequences_5","title":"Consequences","text":"<p>Positive:</p> <ul> <li>Clear separation of concerns</li> <li>Original files preserved by default</li> <li>Simple, predictable behavior</li> <li>Easy to extend for future metadata formats</li> <li>Minimal complexity for prototype</li> </ul> <p>Negative:</p> <ul> <li>Additional directory management needed</li> <li>Potential for metadata file/source file desync</li> <li>Extra file I/O operations</li> </ul> <p>Neutral:</p> <ul> <li>External metadata files may need cleanup management</li> <li>Future sync mechanism may be needed</li> <li>May need to revisit backup strategy in production</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#related_1","title":"Related","text":"<ul> <li>ADR 004: TextObject Re-Design Decisions</li> <li>ADR 005: TextObject Metadata Management and Processing Pipeline</li> <li>TextObject New Design specification</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at02-sectioning-textobject/#future-considerations_1","title":"Future Considerations","text":"<p>Interactive metadata handling could be implemented, allowing users to choose whether to modify files in place, save to external location, or use metadata only in memory. This would be particularly valuable when implementing GUI interfaces, providing explicit user control over metadata storage behavior while maintaining current automated pipeline capabilities through a mode parameter.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/","title":"ADR-AT03: AI Text Processing Object-Service Refactor","text":"<p>This ADR defines a comprehensive three-tier refactor of the <code>ai_text_processing</code> module to achieve object-service architecture compliance (ADR-OS01), integrate with GenAIService (ADR-A13), and adopt the new prompt system (ADR-PT04).</p> <ul> <li>Status: Draft</li> <li>Date: 2025-12-07</li> <li>Owner: Aaron Solomon</li> <li>Author: Aaron Solomon, Claude Sonnet 4.5</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#context","title":"Context","text":"<p>The current <code>ai_text_processing</code> module (<code>src/tnh_scholar/ai_text_processing/</code>) suffers from architectural debt accumulated during rapid prototyping:</p>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#current-pain-points","title":"Current Pain Points","text":"<ol> <li>Mixed Concerns: Business logic (text processing), transport (OpenAI API calls), and prompts intermingled in single files</li> <li>Tight Coupling: Direct OpenAI SDK dependencies scattered throughout processors (<code>openai_process_interface.py</code>, <code>line_translator.py</code>, etc.)</li> <li>Legacy Prompts: Hard-coded prompt strings in <code>prompts.py</code> instead of modular, versioned prompt templates</li> <li>No Protocol Contracts: Missing adapter/port boundaries between domain and infrastructure layers</li> <li>Testability: Difficult to test processors without mocking OpenAI SDK internals</li> <li>Configuration Sprawl: Ad-hoc configuration handling, no clear precedence rules</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#architectural-vision","title":"Architectural Vision","text":"<p>The refactor establishes three clear tiers:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Tier 1: Object-Service                    \u2502\n\u2502  (Domain models, protocols, adapters, mappers)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502                        \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Tier 2: GenAI \u2502          \u2502 Tier 3:       \u2502\n         \u2502 Service       \u2502          \u2502 Prompt System \u2502\n         \u2502 (ADR-A13)     \u2502          \u2502 (ADR-PT04)    \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502                          \u2502\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502  Transport Layer  \u2502\n                   \u2502  (OpenAI SDK,     \u2502\n                   \u2502   caching, etc.)  \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#design-drivers","title":"Design Drivers","text":"<ol> <li>Separation of Concerns: Domain logic isolated from transport and prompts</li> <li>Testability: Protocol-based contracts enable comprehensive unit testing</li> <li>Flexibility: Swap prompt systems, GenAI providers, or caching strategies without changing domain</li> <li>Consistency: Align with object-service architecture used across TNH Scholar (ADR-OS01)</li> <li>Migration Path: Incremental refactor without breaking existing functionality</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#decision","title":"Decision","text":""},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#tier-1-object-service-compliance","title":"Tier 1: Object-Service Compliance","text":""},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#11-domain-models","title":"1.1 Domain Models","text":"<p>Refactor existing models to follow domain-driven design:</p> <pre><code># domain/models.py\nfrom pydantic import BaseModel, Field\nfrom enum import Enum\n\nclass ProcessingTask(str, Enum):\n    \"\"\"Text processing task types.\"\"\"\n    TRANSLATION = \"translation\"\n    SECTIONING = \"sectioning\"\n    SUMMARIZATION = \"summarization\"\n    GENERAL = \"general\"\n\nclass TextProcessingRequest(BaseModel):\n    \"\"\"Domain request for text processing.\"\"\"\n    text_object: TextObject\n    task: ProcessingTask\n    target_language: str | None = None\n    section_count: int | None = None\n    custom_variables: dict[str, str] = Field(default_factory=dict)\n\nclass TextProcessingResult(BaseModel):\n    \"\"\"Domain result from text processing.\"\"\"\n    processed_text_object: TextObject\n    metadata: ProcessingMetadata\n    fingerprint: Fingerprint  # From ADR-PT04\n\nclass ProcessingMetadata(BaseModel):\n    \"\"\"Metadata about the processing operation.\"\"\"\n    task: ProcessingTask\n    model_used: str\n    prompt_key: str\n    prompt_version: str\n    token_usage: TokenUsage\n    processing_time_ms: int\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#12-protocol-contracts","title":"1.2 Protocol Contracts","text":"<p>Define ports for external dependencies:</p> <pre><code># domain/protocols.py\nfrom typing import Protocol\nfrom .models import TextProcessingRequest, TextProcessingResult\n\nclass TextProcessorPort(Protocol):\n    \"\"\"Port for text processing operations.\"\"\"\n\n    def process(self, request: TextProcessingRequest) -&gt; TextProcessingResult:\n        \"\"\"Process text according to request.\"\"\"\n        ...\n\nclass GenAIPort(Protocol):\n    \"\"\"Port for GenAI service integration.\"\"\"\n\n    def render_and_execute(\n        self,\n        prompt_key: str,\n        variables: dict[str, str],\n        model: str | None = None\n    ) -&gt; tuple[str, Fingerprint]:\n        \"\"\"Render prompt and execute via GenAI service.\"\"\"\n        ...\n\nclass PromptCatalogPort(Protocol):\n    \"\"\"Port for prompt discovery (ADR-PT04 integration).\"\"\"\n\n    def get_prompt_for_task(self, task: ProcessingTask) -&gt; str:\n        \"\"\"Get prompt key for processing task.\"\"\"\n        ...\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#13-adapters","title":"1.3 Adapters","text":"<p>Implement adapters to external systems:</p> <pre><code># adapters/genai_adapter.py\nfrom ..domain.protocols import GenAIPort\nfrom tnh_scholar.gen_ai_service.services.genai_service import GenAIService\nfrom tnh_scholar.gen_ai_service.pattern_catalog.adapters.prompts_adapter import PromptsAdapter\n\nclass GenAIServiceAdapter(GenAIPort):\n    \"\"\"Adapter for GenAIService integration.\"\"\"\n\n    def __init__(\n        self,\n        genai_service: GenAIService,\n        prompts_adapter: PromptsAdapter\n    ):\n        self._genai = genai_service\n        self._prompts = prompts_adapter\n\n    def render_and_execute(\n        self,\n        prompt_key: str,\n        variables: dict[str, str],\n        model: str | None = None\n    ) -&gt; tuple[str, Fingerprint]:\n        \"\"\"Render prompt via PromptsAdapter, execute via GenAIService.\"\"\"\n\n        # 1. Render prompt\n        rendered, fingerprint = self._prompts.render(\n            RenderRequest(\n                instruction_key=prompt_key,\n                variables=variables,\n                user_input=variables.get(\"input_text\", \"\")\n            )\n        )\n\n        # 2. Execute via GenAI\n        response = self._genai.execute(\n            messages=rendered.messages,\n            model=model or rendered.model,\n            response_format=rendered.response_format\n        )\n\n        return response.content, fingerprint\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#14-mappers","title":"1.4 Mappers","text":"<p>Pure mapping functions between layers:</p> <pre><code># mappers/text_processing_mapper.py\nfrom ..domain.models import TextProcessingRequest, ProcessingTask\nfrom tnh_scholar.ai_text_processing.text_object import TextObject\n\nclass TextProcessingMapper:\n    \"\"\"Maps between domain models and external representations.\"\"\"\n\n    @staticmethod\n    def to_processing_request(\n        text_object: TextObject,\n        task: ProcessingTask,\n        **kwargs\n    ) -&gt; TextProcessingRequest:\n        \"\"\"Map TextObject to domain request.\"\"\"\n        return TextProcessingRequest(\n            text_object=text_object,\n            task=task,\n            **kwargs\n        )\n\n    @staticmethod\n    def to_prompt_variables(\n        request: TextProcessingRequest\n    ) -&gt; dict[str, str]:\n        \"\"\"Map domain request to prompt variables.\"\"\"\n        variables = {\n            \"input_text\": request.text_object.get_text(),\n            **request.custom_variables\n        }\n\n        if request.target_language:\n            variables[\"target_language\"] = request.target_language\n        if request.section_count:\n            variables[\"section_count\"] = str(request.section_count)\n\n        return variables\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#tier-2-genaiservice-integration","title":"Tier 2: GenAIService Integration","text":"<p>Replace direct OpenAI SDK calls with GenAIService:</p>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#21-remove-direct-openai-dependencies","title":"2.1 Remove Direct OpenAI Dependencies","text":"<p>Before (current):</p> <pre><code># openai_process_interface.py (LEGACY)\nimport openai\n\ndef process_with_openai(prompt: str, text: str) -&gt; str:\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt + text}]\n    )\n    return response.choices[0].message.content\n</code></pre> <p>After (refactored):</p> <pre><code># services/text_processor_service.py\nfrom ..domain.protocols import GenAIPort, TextProcessorPort\nfrom ..domain.models import TextProcessingRequest, TextProcessingResult\n\nclass TextProcessorService(TextProcessorPort):\n    \"\"\"Service for text processing operations.\"\"\"\n\n    def __init__(self, genai: GenAIPort, prompt_catalog: PromptCatalogPort):\n        self._genai = genai\n        self._prompts = prompt_catalog\n\n    def process(self, request: TextProcessingRequest) -&gt; TextProcessingResult:\n        \"\"\"Process text via GenAI service.\"\"\"\n\n        # 1. Get prompt key for task\n        prompt_key = self._prompts.get_prompt_for_task(request.task)\n\n        # 2. Map request to variables\n        variables = TextProcessingMapper.to_prompt_variables(request)\n\n        # 3. Execute via GenAI adapter\n        result_text, fingerprint = self._genai.render_and_execute(\n            prompt_key=prompt_key,\n            variables=variables\n        )\n\n        # 4. Build result\n        return self._build_result(request, result_text, fingerprint)\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#22-dependency-injection","title":"2.2 Dependency Injection","text":"<p>Configure dependencies via factory:</p> <pre><code># config/factory.py\nfrom tnh_scholar.gen_ai_service.services.genai_service import GenAIService\nfrom tnh_scholar.prompt_system.adapters.git_catalog_adapter import GitPromptCatalog\nfrom ..adapters.genai_adapter import GenAIServiceAdapter\nfrom ..services.text_processor_service import TextProcessorService\n\ndef create_text_processor() -&gt; TextProcessorService:\n    \"\"\"Factory for creating configured text processor.\"\"\"\n\n    # 1. Initialize GenAI service\n    genai_service = GenAIService.from_config(GenAIConfig.from_env())\n\n    # 2. Initialize prompt system\n    prompt_catalog = GitPromptCatalog.from_config(\n        PromptCatalogConfig.from_env()\n    )\n    prompts_adapter = PromptsAdapter(\n        catalog=prompt_catalog,\n        renderer=PromptRenderer(...),\n        validator=PromptValidator()\n    )\n\n    # 3. Build adapters\n    genai_adapter = GenAIServiceAdapter(genai_service, prompts_adapter)\n    prompt_catalog_adapter = PromptCatalogAdapter(prompts_adapter)\n\n    # 4. Return service\n    return TextProcessorService(genai_adapter, prompt_catalog_adapter)\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#tier-3-prompt-system-integration","title":"Tier 3: Prompt System Integration","text":"<p>Migrate from <code>prompts.py</code> hard-coded strings to modular prompt system:</p>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#31-migrate-legacy-prompts","title":"3.1 Migrate Legacy Prompts","text":"<p>Before (current):</p> <pre><code># prompts.py (LEGACY)\nTRANSLATION_PROMPT = \"\"\"\nTranslate the following text to {target_language}:\n\n{input_text}\n\"\"\"\n\nSECTIONING_PROMPT = \"\"\"\nDivide the following text into {section_count} logical sections:\n\n{input_text}\n\"\"\"\n</code></pre> <p>After (migrated to prompt system):</p> <pre><code># prompts/translation.md\n---\nname: translation\nversion: 1.0\ndescription: Translate text to target language\ntask_type: translation\nrequired_variables: [input_text, target_language]\noptional_variables: []\ndefault_model: gpt-4\noutput_mode: text\ntags: [translation, text-processing]\n---\n\nTranslate the following text to {{target_language}}:\n\n{{input_text}}\n</code></pre> <pre><code># prompts/sectioning.md\n---\nname: sectioning\nversion: 1.0\ndescription: Divide text into logical sections\ntask_type: sectioning\nrequired_variables: [input_text, section_count]\noptional_variables: []\ndefault_model: gpt-4\noutput_mode: json\nresponse_format: sectioning_response\ntags: [sectioning, text-processing]\n---\n\nDivide the following text into {{section_count}} logical sections:\n\n{{input_text}}\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#32-prompt-catalog-adapter","title":"3.2 Prompt Catalog Adapter","text":"<p>Map processing tasks to prompt keys:</p> <pre><code># adapters/prompt_catalog_adapter.py\nfrom ..domain.protocols import PromptCatalogPort\nfrom ..domain.models import ProcessingTask\n\nclass PromptCatalogAdapter(PromptCatalogPort):\n    \"\"\"Adapter for prompt system integration.\"\"\"\n\n    # Task \u2192 Prompt Key mapping\n    TASK_PROMPT_MAP = {\n        ProcessingTask.TRANSLATION: \"translation\",\n        ProcessingTask.SECTIONING: \"sectioning\",\n        ProcessingTask.SUMMARIZATION: \"summarization\",\n        ProcessingTask.GENERAL: \"general_processing\"\n    }\n\n    def __init__(self, prompts_adapter: PromptsAdapter):\n        self._prompts = prompts_adapter\n\n    def get_prompt_for_task(self, task: ProcessingTask) -&gt; str:\n        \"\"\"Get prompt key for processing task.\"\"\"\n        prompt_key = self.TASK_PROMPT_MAP.get(task)\n        if not prompt_key:\n            raise ValueError(f\"No prompt configured for task: {task}\")\n\n        # Validate prompt exists\n        try:\n            self._prompts.introspect(prompt_key)\n        except PromptNotFoundError:\n            raise ValueError(f\"Prompt not found: {prompt_key}\")\n\n        return prompt_key\n</code></pre>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#migration-strategy","title":"Migration Strategy","text":""},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#phase-1-object-service-foundation-week-1-2","title":"Phase 1: Object-Service Foundation (Week 1-2)","text":"<ol> <li>Create <code>domain/</code> directory with models and protocols</li> <li>Implement mappers (pure functions, easy to test)</li> <li>Add unit tests for domain layer (no I/O dependencies)</li> <li>Deliverable: Domain models pass all tests</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#phase-2-genai-integration-week-2-3","title":"Phase 2: GenAI Integration (Week 2-3)","text":"<ol> <li>Implement <code>GenAIServiceAdapter</code> using existing GenAIService</li> <li>Replace <code>openai_process_interface.py</code> calls with adapter</li> <li>Add integration tests with mocked GenAI responses</li> <li>Deliverable: All processors use GenAIService, no direct OpenAI calls</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#phase-3-prompt-system-integration-week-3-4","title":"Phase 3: Prompt System Integration (Week 3-4)","text":"<ol> <li>Migrate prompts from <code>prompts.py</code> to <code>prompts/</code> directory</li> <li>Implement <code>PromptCatalogAdapter</code> with task mapping</li> <li>Update processors to use prompt keys instead of hard-coded strings</li> <li>Add prompt validation tests</li> <li>Deliverable: All prompts loaded from prompt system</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#phase-4-cleanup-deprecation-week-4-5","title":"Phase 4: Cleanup &amp; Deprecation (Week 4-5)","text":"<ol> <li>Mark legacy files (<code>prompts.py</code>, <code>openai_process_interface.py</code>) as deprecated</li> <li>Update documentation with migration guide</li> <li>Remove unused code after migration verification</li> <li>Deliverable: Clean architecture, no legacy code paths</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#consequences","title":"Consequences","text":""},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#positive","title":"Positive","text":"<ul> <li>Testability: Protocol-based contracts enable comprehensive unit testing without external dependencies</li> <li>Flexibility: Swap GenAI providers, prompt systems, or caching without changing domain logic</li> <li>Consistency: Aligns with object-service architecture used across TNH Scholar</li> <li>Maintainability: Clear separation of concerns makes code easier to understand and modify</li> <li>Reusability: Domain models and protocols can be shared across other modules</li> <li>Prompt Versioning: Modular prompts enable A/B testing, rollback, and collaborative editing</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#negative","title":"Negative","text":"<ul> <li>Migration Effort: Significant refactor required across entire <code>ai_text_processing</code> module</li> <li>Learning Curve: Team must understand object-service patterns, adapters, and protocols</li> <li>Abstraction Overhead: More layers may slow initial feature development</li> <li>Breaking Changes: Existing consumers of <code>ai_text_processing</code> API must be updated</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#risks","title":"Risks","text":"<ul> <li>Scope Creep: Three-tier refactor is ambitious; risk of incomplete migration</li> <li>Performance Regression: Additional abstraction layers may introduce overhead (mitigated by profiling)</li> <li>Coordination Complexity: Requires coordinated work across GenAIService and prompt system teams</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#mitigation-strategies","title":"Mitigation Strategies","text":"<ol> <li>Incremental Migration: Phase-based rollout allows early validation</li> <li>Backwards Compatibility: Keep legacy code paths functional during migration</li> <li>Test Coverage: Require 80%+ test coverage before deprecating legacy code</li> <li>Documentation: Provide migration guide and examples for consumers</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#alternative-1-partial-refactor-genai-only","title":"Alternative 1: Partial Refactor (GenAI Only)","text":"<p>Approach: Integrate GenAIService but keep hard-coded prompts.</p> <p>Rejected: Leaves technical debt in prompt management. Misses opportunity for full architectural alignment.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#alternative-2-big-bang-refactor","title":"Alternative 2: Big-Bang Refactor","text":"<p>Approach: Rewrite entire module in one PR.</p> <p>Rejected: Too risky. Incremental migration allows early feedback and reduces blast radius.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#alternative-3-greenfield-rewrite","title":"Alternative 3: Greenfield Rewrite","text":"<p>Approach: Build new <code>ai_text_processing_v2</code> module, deprecate old one.</p> <p>Rejected: Increases maintenance burden (two modules). Migration path unclear for consumers.</p>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#open-questions","title":"Open Questions","text":"<ol> <li>Performance Impact: What is the overhead of adapter/protocol layers? (Requires profiling)</li> <li>Prompt Versioning: How do we handle prompt schema changes that break existing processors?</li> <li>Error Handling: Should adapters map GenAI exceptions to domain-specific errors?</li> <li>Caching Strategy: Where does response caching live\u2014GenAI service or text processor layer?</li> <li>Legacy Code Removal: When can we safely delete <code>prompts.py</code> and <code>openai_process_interface.py</code>?</li> </ol>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#references","title":"References","text":""},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#related-adrs","title":"Related ADRs","text":"<ul> <li>ADR-OS01: Object-Service Architecture V3 - Object-service pattern foundation</li> <li>ADR-A13: GenAI Service - GenAI service architecture</li> <li>ADR-PT04: Prompt System Refactor - New prompt system architecture</li> <li>ADR-AT01: AI Text Processing Pipeline - Original text processing design</li> <li>ADR-AT02: TextObject Architecture - TextObject historical context</li> <li>ADR-TG01: CLI Architecture - CLI integration patterns</li> <li>ADR-TG02: Prompt Integration - CLI \u2194 prompt system integration</li> </ul>"},{"location":"architecture/ai-text-processing/adr/adr-at03-object-service-refactor/#external-resources","title":"External Resources","text":"<ul> <li>Hexagonal Architecture (Ports &amp; Adapters)</li> <li>Domain-Driven Design</li> <li>Dependency Injection Patterns</li> </ul> <p>This ADR defines the comprehensive refactor strategy that enables <code>tnh-gen</code> CLI implementation and modern prompt system adoption.</p>"},{"location":"architecture/ai-text-processing/design/","title":"Design","text":"<p>Table of Contents:</p> <p>TextObject Original Design - Legacy TextObject design notes capturing the original sectioning models, metadata strategy, and validation approach.</p> <p>TextObject System Design Document - Detailed blueprint for the modern TextObject pipeline, outlining segmentation models, metadata, and API surfaces.</p> <p>This file auto-generated.</p>"},{"location":"architecture/ai-text-processing/design/textobject-original-design/","title":"TextObject Original Design","text":"<p>Legacy TextObject design notes capturing the original sectioning models, metadata strategy, and validation approach.</p>"},{"location":"architecture/ai-text-processing/design/textobject-original-design/#overview","title":"Overview","text":"<p>TextObject provides structured text handling with explicit section boundaries and metadata support, built on Pydantic models for validation.</p>"},{"location":"architecture/ai-text-processing/design/textobject-original-design/#core-components","title":"Core Components","text":""},{"location":"architecture/ai-text-processing/design/textobject-original-design/#logicalsection","title":"LogicalSection","text":"<pre><code>class LogicalSection(BaseModel):\n    title: str\n    start_line: int\n    end_line: int\n</code></pre>"},{"location":"architecture/ai-text-processing/design/textobject-original-design/#textobject","title":"TextObject","text":"<p>Primary container managing:</p> <ul> <li>Section boundaries</li> <li>Language information</li> <li>Dublin Core metadata</li> <li>NumberedText content</li> </ul>"},{"location":"architecture/ai-text-processing/design/textobject-original-design/#key-design-points","title":"Key Design Points","text":""},{"location":"architecture/ai-text-processing/design/textobject-original-design/#section-management","title":"Section Management","text":"<ul> <li>Explicit start and end lines for sections</li> <li>Validation ensures no overlaps or gaps</li> <li>Ordered storage by line number</li> </ul>"},{"location":"architecture/ai-text-processing/design/textobject-original-design/#content-integration","title":"Content Integration","text":"<ul> <li>Immutable NumberedText reference</li> <li>Line number integrity maintained</li> <li>Section boundaries must match content</li> </ul>"},{"location":"architecture/ai-text-processing/design/textobject-original-design/#metadata","title":"Metadata","text":"<ul> <li>Dublin Core based structure</li> <li>Optional/required field handling</li> <li>Flexible additional metadata support</li> </ul>"},{"location":"architecture/ai-text-processing/design/textobject-original-design/#implementation-notes","title":"Implementation Notes","text":""},{"location":"architecture/ai-text-processing/design/textobject-original-design/#validation-requirements","title":"Validation Requirements","text":"<ul> <li>Sections must not overlap</li> <li>No gaps between sections allowed</li> <li>Line numbers must exist in content</li> <li>Language codes accepted but not validated</li> </ul>"},{"location":"architecture/ai-text-processing/design/textobject-original-design/#api-considerations","title":"API Considerations","text":"<ul> <li>Pydantic models ensure clean serialization</li> <li>Clear validation errors</li> <li>Type safety throughout</li> </ul>"},{"location":"architecture/ai-text-processing/design/textobject-system-design/","title":"TextObject System Design Document","text":"<p>Detailed blueprint for the modern TextObject pipeline, outlining segmentation models, metadata, and API surfaces.</p>"},{"location":"architecture/ai-text-processing/design/textobject-system-design/#1-overview-and-purpose","title":"1. Overview and Purpose","text":"<p>The TextObject system manages the division of large texts into processable segments while maintaining contextual integrity. It serves two key purposes:</p> <p>Primary Goal:</p> <ul> <li>Enable AI processing of large texts by breaking them into manageable chunks</li> <li>Preserve essential context across segmentation boundaries</li> <li>Provide rich contextual information to compensate for segmentation</li> </ul> <p>Secondary Goal:</p> <ul> <li>Maintain structured metadata for human analysis and documentation</li> <li>Support standard metadata practices (Dublin Core)</li> <li>Enable systematic text processing workflows</li> </ul>"},{"location":"architecture/ai-text-processing/design/textobject-system-design/#2-core-components","title":"2. Core Components","text":""},{"location":"architecture/ai-text-processing/design/textobject-system-design/#21-response-format-api-layer","title":"2.1 Response Format (API Layer)","text":"<p>The response format is optimized for AI interaction, emphasizing human-readable context:</p> <pre><code>class LogicalSection(BaseModel):\n    \"\"\"Represents a contextually meaningful segment of a larger text.\n\n    Sections should preserve natural breaks in content (e.g., explicit section markers, topic shifts,\n    argument development, narrative progression) while staying within specified size limits \n    in order to create chunks suitable for AI processing.\"\"\"\n    start_line: int = Field(\n        ..., \n        description=\"Starting line number that begins this logical segment\"\n    )\n    title: str = Field(\n        ...,\n        description=\"Descriptive title of section's key content\"\n    )\n\nclass TextObjectResponse(BaseModel):\n    \"\"\"Format for dividing large texts into AI-processable segments while\n    maintaining broader document context.\"\"\"\n    document_summary: str = Field(\n        ...,\n        description=\"Concise, comprehensive overview of the text's content and purpose\"\n    )\n    document_metadata: str = Field(\n        ...,\n        description=\"Available Dublin Core standard metadata in human-readable format\"\n    )\n    key_concepts: str = Field(\n        ...,\n        description=\"Important terms, ideas, or references that appear throughout the text\"\n    )\n    narrative_context: str = Field(\n        ...,\n        description=\"Concise overview of how the text develops or progresses as a whole\"\n    )\n    language: str = Field(..., description=\"ISO 639-1 language code\")\n    sections: List[LogicalSection]\n</code></pre> <p>Key Design Points:</p> <ul> <li>Separates document-level context into distinct conceptual units</li> <li>Uses human-readable format for metadata and context</li> <li>Maintains simple section structure for reliable AI processing</li> </ul>"},{"location":"architecture/ai-text-processing/design/textobject-system-design/#22-internal-representation","title":"2.2 Internal Representation","text":"<p>The internal system uses a richer structure based on Dublin Core standards:</p> <pre><code>class TextMetadata(BaseModel):\n    \"\"\"Rich metadata container following Dublin Core standards.\"\"\"\n\n    # Core Dublin Core elements with validation\n    title: str\n    creator: List[str]\n    subject: List[str]\n    description: str\n    publisher: Optional[str] = None\n    contributor: List[str] = Field(default_factory=list)\n    date: Optional[str] = None\n    type: str\n    format: str\n    identifier: Optional[str] = None\n    source: Optional[str] = None\n    language: str\n\n    # Additional fields\n    context: str = \"\"\n    additional_info: Dict[str, Any] = Field(default_factory=dict)\n\n    # Custom fields can be added through additional_info\n\n    class Config:\n        \"\"\"Pydantic model configuration.\"\"\"\n        extra = 'allow'  # Allows additional fields beyond those specified\n\n    def to_dublin_core(self) -&gt; Dict[str, Any]:\n        \"\"\"Extract Dublin Core fields as dictionary.\"\"\"\n        return self.model_dump(\n            exclude={'context', 'additional_info'},\n            exclude_none=True\n        )\n</code></pre>"},{"location":"architecture/ai-text-processing/design/textobject-system-design/#3-key-design-decisions","title":"3. Key Design Decisions","text":""},{"location":"architecture/ai-text-processing/design/textobject-system-design/#31-dual-layer-design","title":"3.1 Dual-Layer Design","text":"<ol> <li>AI Interface Layer (TextObjectResponse)</li> <li>Optimized for AI processing</li> <li>Human-readable context</li> <li> <p>Simplified structure</p> </li> <li> <p>Internal Layer (TextObject)</p> </li> <li>Strict validation</li> <li>Structured metadata</li> <li>Rich processing capabilities</li> </ol>"},{"location":"architecture/ai-text-processing/design/textobject-system-design/#32-metadata-approach","title":"3.2 Metadata Approach","text":"<ol> <li>AI Format</li> <li>Narrative document summary</li> <li>Human-readable metadata</li> <li>Context and key concepts separated</li> <li> <p>Focus on information relevant for processing</p> </li> <li> <p>Internal Format</p> </li> <li>Structured Dublin Core metadata</li> <li>Additional context storage</li> <li>Extensible design</li> </ol>"},{"location":"architecture/ai-text-processing/design/textobject-system-design/#33-content-integration","title":"3.3 Content Integration","text":"<p>Content management is delegated to NumberedText class:</p> <ul> <li>Clean separation of concerns</li> <li>Efficient text storage and access</li> <li>Section-aware interface</li> </ul>"},{"location":"architecture/ai-text-processing/design/textobject-system-design/#4-existing-implementation-details","title":"4. Existing implementation details","text":""},{"location":"architecture/ai-text-processing/design/textobject-system-design/#41-section-access","title":"4.1 Section Access","text":"<pre><code>def get_section_content(self, index: int) -&gt; str:\n    \"\"\"Retrieve content for specific section.\"\"\"\n    start = self.sections[index].start_line\n    end = (self.sections[index + 1].start_line \n           if index &lt; len(self.sections) - 1 \n           else self.total_lines + 1)\n    return self.content.get_segment(start, end)\n</code></pre>"},{"location":"architecture/ai-text-processing/design/textobject-system-design/#42-validation-of-textobject","title":"4.2 Validation of TextObject","text":"<pre><code>def _validate(self) -&gt; None:\n    \"\"\"Validate section integrity.\"\"\"\n    if not self.sections:\n        raise ValueError(\"TextObject must have at least one section\")\n\n    # Validate section ordering\n    for i, section in enumerate(self.sections):\n        if section.start_line &lt; 1:\n            raise ValueError(f\"Section {i}: start line must be &gt;= 1\")\n        if section.start_line &gt; self.total_lines:\n            raise ValueError(f\"Section {i}: start line exceeds text length\")\n        if i &gt; 0 and section.start_line &lt;= self.sections[i-1].start_line:\n            raise ValueError(f\"Section {i}: non-sequential start line\")\n</code></pre>"},{"location":"architecture/ai-text-processing/design/textobject-system-design/#6-future-considerations","title":"6. Future Considerations","text":"<ol> <li>Performance Optimization</li> <li>Index sections for faster access</li> <li> <p>Optimize metadata string parsing</p> </li> <li> <p>Extended Functionality</p> </li> <li>Section manipulation (merge/split)</li> <li>Advanced metadata querying</li> <li> <p>Enhanced validation rules</p> </li> <li> <p>Integration Enhancements</p> </li> <li>Expanded AI context generation</li> <li>Bulk processing capabilities</li> </ol> <p>This validation strategy ensures data integrity while providing clear feedback for both programmatic and human review of processing results.</p>"},{"location":"architecture/configuration/","title":"Configuration","text":"<p>Table of Contents:</p> <p>TNH Configuration Management - Architecture decisions and a phased plan for consolidating TNH Scholar configuration across modules, CLIs, and environments.</p> <p>This file auto-generated.</p>"},{"location":"architecture/configuration/tnh-configuration-management/","title":"TNH Configuration Management","text":"<p>Architecture decisions and a phased plan for consolidating TNH Scholar configuration across modules, CLIs, and environments.</p>"},{"location":"architecture/configuration/tnh-configuration-management/#adr-002-overall-strategy","title":"ADR 002: Overall Strategy","text":""},{"location":"architecture/configuration/tnh-configuration-management/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/configuration/tnh-configuration-management/#context","title":"Context","text":"<p>The tnh-scholar project currently uses module-level constants for configuration, defined at module headers. As the project grows, we need a structured approach to configuration management that maintains simplicity while providing necessary flexibility.</p> <p>Current approach:</p> <ul> <li>Module-level constants (e.g., DEFAULT_YDL_OPTIONS, DEFAULT_PATTERN_DIR)</li> <li>Some environment variables for critical values (e.g., OPENAI_API_KEY)</li> <li>CLI flags for runtime behavior (e.g., --verbose)</li> </ul>"},{"location":"architecture/configuration/tnh-configuration-management/#decision","title":"Decision","text":"<p>We will implement configuration management in phases, prioritizing simplicity and common use cases over excessive flexibility.</p>"},{"location":"architecture/configuration/tnh-configuration-management/#phase-1-current-prototype","title":"Phase 1: Current (Prototype)","text":"<ul> <li>Continue using module-level constants for defaults</li> <li>Maintain minimal environment variables (only for essential credentials)</li> <li>Pass CLI flags through call chain where needed</li> </ul>"},{"location":"architecture/configuration/tnh-configuration-management/#phase-2-user-configuration-next-step","title":"Phase 2: User Configuration (Next Step)","text":"<p>Implement user configuration file:</p> <pre><code># ~/.config/tnh_scholar/settings/config.yaml\nvideo_processing:\n  yt_dlp_verbose: false\n  default_language: en\npatterns:\n  path: ~/.config/tnh_scholar/patterns\n</code></pre> <p>Standard locations:</p> <ul> <li>Linux/Mac: ~/.config/tnh_scholar/settings/config.yaml</li> <li>Windows: %APPDATA%/tnh_scholar/settings/config.yaml</li> </ul> <p>Configuration precedence:</p> <ol> <li>CLI arguments (highest)</li> <li>User config file (~/.config/tnh_scholar/settings/config.yaml)</li> <li>Module defaults (lowest)</li> </ol>"},{"location":"architecture/configuration/tnh-configuration-management/#phase-3-component-configuration-future","title":"Phase 3: Component Configuration (Future)","text":"<p>Introduce configuration classes for major components:</p> <pre><code>@dataclass\nclass VideoConfig:\n    verbose: bool = False\n    default_language: str = \"en\"\n    yt_dlp_options: Dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"architecture/configuration/tnh-configuration-management/#explicitly-out-of-scope","title":"Explicitly Out of Scope","text":"<ul> <li>Project-level configuration files</li> <li>Complex environment variable mapping</li> <li>Multiple configuration file locations</li> <li>Configuration profiles/environments</li> </ul>"},{"location":"architecture/configuration/tnh-configuration-management/#rationale","title":"Rationale","text":"<ul> <li>Keep configuration simple and predictable</li> <li>Minimize cognitive overhead for users</li> <li>Focus on common use cases</li> <li>Avoid configuration sprawl</li> </ul>"},{"location":"architecture/configuration/tnh-configuration-management/#consequences","title":"Consequences","text":"<p>Positive:</p> <ul> <li>Clear, simple configuration path</li> <li>Single user configuration file</li> <li>Predictable behavior</li> <li>Easy to maintain and document</li> </ul> <p>Negative:</p> <ul> <li>Less flexibility for advanced users</li> <li>Some use cases may require code changes</li> <li>Limited environment-specific configuration</li> </ul>"},{"location":"architecture/configuration/tnh-configuration-management/#implementation-notes","title":"Implementation Notes","text":""},{"location":"architecture/configuration/tnh-configuration-management/#1-configuration-file-location","title":"1. Configuration File Location","text":"<ul> <li>Single location: ~/.config/tnh_scholar/config.yaml</li> <li>Created by tnh-setup command</li> <li>Simple YAML format</li> <li>Clear documentation of available options</li> </ul>"},{"location":"architecture/configuration/tnh-configuration-management/#2-configuration-loading","title":"2. Configuration Loading","text":"<pre><code>def load_config() -&gt; Dict[str, Any]:\n    \"\"\"Load configuration with simple precedence.\"\"\"\n    config_path = Path.home() / \".config/tnh_scholar/config.yaml\"\n    defaults = get_default_config()\n\n    if config_path.exists():\n        user_config = yaml.safe_load(config_path.read_text())\n        return {**defaults, **user_config}\n    return defaults\n</code></pre>"},{"location":"architecture/configuration/tnh-configuration-management/#3-cli-integration","title":"3. CLI Integration","text":"<pre><code>@click.option(\"--verbose\", is_flag=True)\ndef command(verbose: bool):\n    \"\"\"CLI flags override config file.\"\"\"\n    config = load_config()\n    if verbose:\n        config[\"verbose\"] = True\n</code></pre>"},{"location":"architecture/configuration/tnh-configuration-management/#migration-strategy","title":"Migration Strategy","text":"<ol> <li>Document all current module-level constants</li> <li>Create template config file with current defaults</li> <li>Implement config file loading</li> <li>Update modules to use configuration values</li> <li>Add configuration classes as needed</li> </ol>"},{"location":"architecture/configuration/tnh-configuration-management/#related-documents","title":"Related Documents","text":"<ul> <li>ADR 001: Video Processing Return Types and Configuration</li> <li>installation.md (for tnh-setup documentation)</li> </ul> <p>This ADR emphasizes your preference for simplicity while providing a clear path for necessary configuration management. It explicitly excludes potentially complex configuration options to maintain focus on making the tool approachable and maintainable.</p> <p>Does this align with your vision for the project's configuration management? Would you like me to adjust any aspects of it?</p>"},{"location":"architecture/docs-system/","title":"Docs System","text":"<p>Table of Contents:</p> <p>Adr - Table of contents for architecture/docs-system/adr</p> <p>Design - Table of contents for architecture/docs-system/design</p> <p>This file auto-generated.</p>"},{"location":"architecture/docs-system/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-DD01: Documentation System Reorganization Strategy - Rebuilds the documentation architecture with new directories, automation, and Prompt terminology.</p> <p>ADR-DD02: Documentation Main Content and Navigation Strategy - Defines content architecture, sync mechanisms, and navigation patterns for README.md, docs/index.md, and filesystem-driven documentation.</p> <p>ADR-DD03: Pattern to Prompt Terminology Standardization - Standardizes documentation terminology from 'Pattern' to 'Prompt' to align with industry conventions and gen-ai-service refactoring.</p> <p>adr-dd03-phase1-punchlist</p> <p>This file auto-generated.</p>"},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/","title":"ADR-DD01: Documentation System Reorganization Strategy","text":"<p>Defines the new documentation hierarchy, automation tooling, and naming standards for TNH Scholar.</p> <ul> <li>Status: Accepted</li> <li>Date: 2024-11-09</li> <li>Owner: Documentation Working Group (initially Codex + maintainers)</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/#context","title":"Context","text":"<p>The documentation footprint for TNH Scholar has grown organically:</p> <ul> <li><code>docs/index.md</code> and <code>README.md</code> describe the project at a high level but omit the current architecture, roadmap, or research context and still lean on the legacy \"Pattern\" terminology.</li> <li>End-user docs (<code>docs/getting-started</code>, <code>docs/user-guide</code>, <code>docs/cli</code>, <code>docs/api</code>) only cover a subset of the available tools and do not reflect the latest CLI surface, Prompt design, or evaluation workflows.</li> <li>Developer and architecture materials are split between <code>docs/development</code>, a large unindexed <code>docs/design</code> tree containing numerous ADRs, and <code>docs/docs-design</code> (the original documentation plan). Most of this content never appears in <code>mkdocs.yaml</code>, so the published site hides the majority of design history.</li> <li>Research work (<code>docs/research/**</code>) mixes current experiments with exploratory transcripts without indicating recency or ownership.</li> <li>There is no standard place for \"doc ops\" guidance (style guide, review checklists, roadmap), and the README is not a comprehensive introduction to TNH Scholar\u2019s goals, structure, design principles, or contribution opportunities.</li> </ul> <p>The current effort (TODO #9) should reorganize the documentation without losing information, surface up-to-date guidance, archive historical material appropriately, and ensure the MkDocs site mirrors the on-disk structure. It must also accommodate the ongoing rename from Pattern to Prompt (e.g., <code>docs/user-guide/patterns.md</code>, <code>docs/design/pattern/*</code>, <code>docs/cli/tnh-fab.md</code>, etc.).</p>"},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/#decision","title":"Decision","text":"<p>Adopt a documentation architecture that is source-of-truth in the repository, entirely reflected in MkDocs navigation, and explicit about currency vs. history.</p> <ol> <li>Unify the directory layout.</li> <li> <p>Replace the current mix of <code>design/</code>, <code>development/</code>, <code>docs-design/</code>, and <code>research/</code> subtrees with a single hierarchy:</p> <pre><code>docs/\n  index.md                    # mirrors README, includes vision + orientation\n  overview/                   # mission, roadmap, release notes, glossary\n  getting-started/            # install, quick start, configuration\n  user-guide/                 # workflows, Prompt usage, best practices\n  cli-reference/              # per-command auto-generated docs\n  prompt-templates/           # Prompt catalog, conventions, metadata schema\n  api-reference/              # mkdocstrings output + integration guides\n  architecture/               # system design, component deep-dives, ADRs\n    adr/                      # numbered ADRs (inc. migrated legacy files)\n  development/                # contributing, dev setup, testing, coding standards\n  research/                   # active experiments + summaries\n  docs-ops/                   # documentation roadmap, maintenance plan, style guide\n  archive/                    # frozen historical docs (design prototypes, transcripts, etc.)\n</code></pre> <p>Each folder gains an <code>index.md</code> that frames its content and links to authoritative children. Legacy directories (e.g., <code>docs/design/tnh-fab/...</code>) move into either <code>architecture/adr/</code> or <code>archive/design/tnh-fab/</code> depending on relevance.</p> </li> <li> <p>Enforce README \u2194 site parity.</p> </li> <li>Expand <code>README.md</code> into a full project introduction (vision, goals, architecture snapshot, CLI summary, development status, research focus, and where to contribute) and keep it synchronized with <code>docs/index.md</code>.</li> <li> <p>Include the documentation map (nav overview + major directories) so newcomers know where to look.</p> </li> <li> <p>Adopt Prompt terminology.</p> </li> <li>Rename <code>docs/user-guide/patterns.md</code> to <code>user-guide/prompt-templates.md</code> (mirrored in MkDocs nav and CLI docs).</li> <li> <p>Update all references in docs (and eventually CLI/API text) to use Prompt nomenclature while acknowledging the historical Pattern term in the archive.</p> </li> <li> <p>Surface everything through MkDocs.</p> </li> <li>Restructure <code>mkdocs.yaml</code> to follow the physical layout above, ensuring no Markdown lives outside the published nav.</li> <li> <p>Use nested sections (Material tabs) to distinguish current vs. archival content, with clear \"Historical\" banners and metadata.</p> </li> <li> <p>Introduce doc automation + QA.</p> </li> <li>Add a <code>scripts/docs/</code> toolkit with:<ul> <li><code>generate_cli_docs.py</code> \u2013 runs each Click/Typer command (<code>tnh-fab --help</code>, etc.) and produces Markdown in <code>docs/cli-reference</code>.</li> <li><code>generate_prompt_template_catalog.py</code> \u2013 inspects <code>patterns/</code> (soon <code>prompt_templates/</code>) and builds a catalog page (name, intent, inputs, outputs, maturity).</li> <li><code>sync_readme.py</code> \u2013 verifies that README sections map to <code>docs/index.md</code> and fails CI if they diverge.</li> </ul> </li> <li>Extend MkDocs with <code>mkdocstrings</code> (already configured) for API coverage and consider <code>mkdocs-gen-files</code> + <code>mkdocs-literate-nav</code> (or Material <code>navigation.instant</code>) to keep nav synchronized automatically.</li> <li> <p>Add CI jobs (<code>make docs-verify</code>) to run <code>mkdocs build</code>, automated link checking, and template generation.</p> </li> <li> <p>Document gaps + backlog.</p> </li> <li>Produce a living checklist in <code>docs/docs-ops/roadmap.md</code> describing missing or stale topics (see below).</li> <li> <p>Attach ownership metadata (module owner, last reviewed) at the top of each page.</p> </li> <li> <p>Codify Markdown + indexing standards.</p> </li> <li>Generate <code>documentation_index.md</code> at the repo root (and mirrored at <code>docs/docs-ops/documentation_index.md</code>) from front matter metadata so contributors can quickly locate any doc without digging into directories. The document index is treated as a build artifact and regenerated whenever docs change (<code>scripts/generate_doc_index.py</code> handles this and marks the files with <code>auto_generated: true</code>).</li> <li>Publish <code>docs/docs-ops/markdown-standards.md</code> to define file naming (lowercase kebab-case, hyphen-separated, no spaces), required YAML front matter (now including an <code>author</code> provenance field), mandated <code># Title</code> + single-paragraph description, link styles, and lint expectations. ADR filenames follow <code>adr-&lt;modulecode&gt;&lt;number&gt;-&lt;descriptor&gt;.md</code> (e.g., <code>adr-dd01-docs-reorg.md</code>).</li> <li>Adopt <code>markdownlint</code> (GitHub: DavidAnson/markdownlint) in CI/Make targets to enforce the standard automatically. <code>.markdownlint.json</code> disables MD025/MD013 to account for YAML-title duplication and long architecture tables; CI now runs <code>npx markdownlint '**/*.md'</code>.</li> <li>Provide an ADR template (<code>docs/docs-ops/adr-template.md</code>) that applies the standard (front matter + <code># Title</code> heading + one-sentence summary) while accommodating ADR metadata (Status, Date, Owner, Author). Also require module-specific storage: <code>docs/architecture/&lt;module&gt;/adr/ADR-&lt;module-code&gt;&lt;number&gt;.md</code>, where the module code is one to four lowercase letters (e.g., <code>adr-a01</code>, <code>adr-kb02</code>, <code>ADR-dd01</code>) to keep the catalog navigable.</li> <li>Update existing ADRs/docs over time to conform to the standards; new docs must comply immediately.</li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/#implementation-plan","title":"Implementation Plan","text":"<ol> <li>Inventory and tagging (Week 1).</li> <li>Tag each existing Markdown file as <code>current</code>, <code>needs-update</code>, or <code>historical</code>.</li> <li>Capture metadata (owner, last review date) in YAML front matter.</li> <li> <p>Identify Pattern\u2192Prompt rename scope via <code>rg</code> (currently 300+ hits).</p> </li> <li> <p>Filesystem reorganization (Week 1\u20132).</p> </li> <li>Create the target directory structure.</li> <li>Move files into their new homes, inserting shim <code>index.md</code> pages where necessary.</li> <li> <p>Preserve original filenames in <code>archive/</code> for traceability and add cross-links from the new canonical documents.</p> </li> <li> <p>Terminology + README sweep (Week 2).</p> </li> <li>Update README and <code>docs/index.md</code> with the comprehensive intro, project structure, research focus, and contribution pointers.</li> <li> <p>Rename <code>Pattern</code> to <code>Prompt</code> (docs + nav). Retain a glossary entry describing the rename.</p> </li> <li> <p>MkDocs + automation (Week 2\u20133).</p> </li> <li>Rewrite <code>mkdocs.yaml</code> nav to mirror the new hierarchy.</li> <li>Add doc-generation scripts and hook them into <code>make docs</code>.</li> <li> <p>Configure CI to run <code>mkdocs build</code> plus the generators (ensuring docs stay up to date).</p> </li> <li> <p>Historical archiving + discoverability (Week 3).</p> </li> <li>Move legacy ADRs/prototypes into <code>docs/archive/**</code> with short summaries in their parent section pointing users to the archive.</li> <li> <p>Build an archive index (chronological table with tags, e.g., <code>design</code>, <code>research</code>, <code>ops</code>).</p> </li> <li> <p>Gap-filling sprint planning (Week 3+).</p> </li> <li>Use the backlog below to create GitHub issues and assign owners.</li> <li>Track documentation coverage in <code>docs/docs-ops/roadmap.md</code>.</li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/#documentation-backlog-initial","title":"Documentation Backlog (initial)","text":"<ol> <li>Prompt Catalog + Standards.</li> <li>Naming conventions, metadata schema, versioning, testing strategy, migration guide from Patterns.</li> <li>Workflow Playbooks.</li> <li>End-to-end tutorials for research tasks (e.g., transcription \u2192 translation \u2192 Prompt evaluation).</li> <li>Evaluation + Benchmarking.</li> <li>How to run <code>evaluation/</code> scripts, metrics definitions, sample datasets.</li> <li>Knowledge Infrastructure.</li> <li>Vector store design, metadata extraction pipeline, ingestion policies.</li> <li>Deployment / Operations.</li> <li>Release checklist integration, environment promotion strategy, secrets management.</li> <li>Research Summaries.</li> <li>Digestible summaries of <code>docs/research/*</code> experiments with conclusions and follow-up work.</li> <li>Documentation Ops.</li> <li>Style guide, doc PR checklist, automation instructions, label taxonomy.</li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/#consequences","title":"Consequences","text":"<ul> <li>Positive</li> <li>Every document has a clear home and appears in the published site.</li> <li>README + docs/index become authoritative onboarding material.</li> <li>Historical context remains accessible but separated from current guidance.</li> <li>Automation keeps CLI/API docs and template catalogs in sync with the codebase.</li> <li> <p>Terminology alignment reduces confusion during the Pattern\u2192Prompt migration.</p> </li> <li> <p>Negative / Risks</p> </li> <li>Short-term churn as files move; open PRs may require rebase assistance.</li> <li>Automation scripts add maintenance burden and require CI resources.</li> <li>Contributors must learn the new structure; requires communication + contributor guide updates.</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/#open-questions-decisions","title":"Open Questions &amp; Decisions","text":"<ol> <li>Nav automation \u2013 adopt <code>mkdocs-literate-nav</code> once directories encode archival status (e.g., <code>archive/</code>, <code>architecture/adr/</code>). We'll author explicit <code>index.md</code> files but let literate-nav derive most of the hierarchy to keep nav and filesystem in sync automatically.</li> <li>Research storage \u2013 keep curated summaries in <code>docs/research/</code> but move bulky/raw transcripts to external storage (S3 or knowledge base). Each summary page links to the raw artifacts so we avoid bloating Git while retaining traceability.</li> <li>Single README \u2013 maintain one <code>README.md</code> as the universal entry point with early persona routing (user/developer/researcher). Each docs section gets a robust <code>index.md</code> that continues the onboarding for that persona.</li> <li>Review cadence \u2013 tie required documentation reviews to the release/CI pipeline instead of calendar schedules. Major version bumps (e.g., <code>0.x \u2192 1.0</code>, <code>1.15 \u2192 2.0</code>) must run a docs verification job that fails if key sections lack updates/acknowledgement. Smaller releases can reuse the automation but only warn on drift.</li> </ol> <p>Approval of this ADR green-lights the restructuring work for TODO #9 and provides a concrete blueprint for subsequent documentation updates.</p>"},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/#as-built-notes-addendums","title":"As-Built Notes &amp; Addendums","text":""},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/#addendum-2025-12-03-cli-documentation-consolidation","title":"Addendum 2025-12-03: CLI Documentation Consolidation","text":"<p>Context: During implementation, the planned separation of <code>docs/cli/</code> (guide material) and <code>docs/cli-reference/</code> (auto-generated reference docs) proved confusing and redundant. The existing content was already reference documentation (per-command pages like <code>tnh-fab.md</code>, <code>audio-transcribe.md</code>), not guide material.</p> <p>Decision:</p> <ol> <li>Consolidated all CLI documentation into single <code>docs/cli-reference/</code> directory containing:</li> <li>Overview and guide material (overview.md)</li> <li>Per-command reference documentation (individual command pages)</li> <li>Removed auto-generated CLI reference stubs and generation infrastructure</li> <li>Deferred comprehensive CLI reference generation (TODO #17) until after CLI refactor (blocked by TODO #8)</li> <li>Renamed <code>docs/cli/</code> to <code>docs/cli-reference/</code> to accurately reflect content type</li> </ol> <p>Rationale:</p> <ul> <li>The CLI structure is scheduled for overhaul, making current auto-generated stubs low-value</li> <li>Placeholder documentation with minimal content (\"run --help for help\") doesn't serve users</li> <li>Directory name <code>cli-reference</code> accurately describes the reference-style content</li> <li>Single location reduces navigation complexity and maintenance burden</li> <li>Aligns with actual as-built directory structure</li> </ul> <p>Implementation Changes:</p> <ul> <li>Removed auto-generated <code>docs/cli-reference/</code> stub files (2025-12-03)</li> <li>Removed <code>scripts/generate_cli_docs.py</code> from MkDocs build pipeline</li> <li>Renamed <code>docs/cli/</code> to <code>docs/cli-reference/</code></li> <li>Updated navigation scripts with consolidated <code>cli-reference</code> directory:</li> <li><code>scripts/generate_mkdocs_nav.py</code></li> <li><code>scripts/generate_subdir_indexes.py</code></li> <li>Created TODO #17 to track comprehensive CLI reference generation post-refactor</li> </ul> <p>Updated Directory Structure:</p> <pre><code>docs/\n  cli-reference/              # CLI overview + per-command reference (consolidated)\n  api/                        # API reference\n  project/                    # Project meta-docs\n  community/                  # Community resources\n</code></pre> <p>References:</p> <ul> <li>TODO #17: Comprehensive CLI Reference Documentation</li> <li>TODO #8: Clean Up CLI Tool Versions (blocks CLI reference work)</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/#addendum-2025-12-04-absolute-link-strategy","title":"Addendum 2025-12-04: Absolute Link Strategy","text":"<p>Context: During documentation reorganization, the team initially used relative links (e.g., <code>../../../cli-reference/overview.md</code>). With the deep hierarchical structure (<code>docs/architecture/*/adr/</code>, <code>docs/architecture/*/design/</code>), relative links became unwieldy, error-prone, and generated MkDocs warnings about absolute paths.</p> <p>Decision:</p> <ol> <li>Enable MkDocs 1.6+ absolute link validation in <code>mkdocs.yaml</code>:</li> </ol> <pre><code>validation:\n  links:\n    absolute_links: relative_to_docs\n</code></pre> <ol> <li>Standardize on absolute links for all internal documentation cross-references:</li> <li>Use <code>/cli-reference/overview.md</code> instead of <code>../../../cli-reference/overview.md</code></li> <li>Links starting with <code>/</code> are interpreted relative to <code>docs/</code> directory</li> <li> <p>MkDocs validates absolute links and converts them to proper relative links in HTML output</p> </li> <li> <p>Convert all existing relative links to absolute format (TODO #18)</p> </li> </ol> <p>Rationale:</p> <ul> <li>Clearer intent: <code>/architecture/docs-system/adr/...</code> immediately shows destination vs calculating <code>../../../</code> depth</li> <li>Easier refactoring: Search/replace <code>/old/path/file.md</code> \u2192 <code>/new/path/file.md</code> works across all docs; relative links require different updates per source location</li> <li>Automation friendly: Doc generation scripts construct absolute paths easily without calculating relative paths from each source file</li> <li>Less error-prone: No manual counting of <code>../</code> levels in deep hierarchies</li> <li>Better for complex structures: Multi-level architecture organization makes relative links unreadable</li> </ul> <p>Implementation:</p> <ul> <li>Added <code>validation.links.absolute_links: relative_to_docs</code> to mkdocs.yaml (2025-12-04)</li> <li>Converted links in this ADR to absolute format</li> <li>Created TODO #18 for systematic conversion across all documentation</li> </ul> <p>Impact:</p> <ul> <li>Eliminates MkDocs warnings about absolute links</li> <li>Improves documentation maintainability for future reorganizations</li> <li>Makes link targets immediately clear to readers and automation</li> <li>No functional change: MkDocs converts absolute links to relative in HTML output</li> </ul> <p>References:</p> <ul> <li>TODO #18: Convert Documentation Links to Absolute Paths</li> <li>MkDocs 1.6 Release Notes</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd01-docs-reorg-strategy/#addendum-3-auto-generated-documentation-index-system-2025-12-05","title":"Addendum 3: Auto-Generated Documentation Index System (2025-12-05)","text":"<p>Context:</p> <p>The original ADR specified a manually-maintained Documentation Map section in <code>/docs/index.md</code>. As the documentation grew, maintaining this static list became error-prone and duplicated information available from filesystem scanning.</p> <p>Decision:</p> <p>Implemented a dual-format auto-generated documentation index system:</p> <ol> <li><code>documentation_index.md</code> (comprehensive reference):</li> <li>Searchable table format with Title, Description, Created date, and Path</li> <li>Organized by category sections (Getting Started, Architecture, CLI Reference, etc.)</li> <li> <p>Generated from frontmatter metadata via <code>scripts/generate_doc_index.py</code></p> </li> <li> <p><code>documentation_map.md</code> (browsable navigation):</p> </li> <li>Clean hierarchical list format matching the original static Documentation Map</li> <li>Same category organization as documentation_index.md</li> <li>Auto-appended to <code>/docs/index.md</code> during build via <code>scripts/append_doc_map_to_index.py</code></li> </ol> <p>Implementation:</p> <ul> <li>Created <code>scripts/generate_doc_index.py</code> - scans docs/ directory, extracts frontmatter, generates both files</li> <li>Created <code>scripts/append_doc_map_to_index.py</code> - injects documentation_map.md content into index.md at build time</li> <li>Added both scripts to mkdocs.yaml gen-files plugin</li> <li>Excluded <code>documentation_map.md</code> from navigation (mkdocs.yaml exclude_docs) since it's embedded in index.md</li> <li>Respects existing EXCLUDE_PATTERNS to avoid indexing draft/archived files</li> <li>Filters out subdirectory <code>index.md</code> files from documentation_map.md (navigation landing pages that clutter the browsable list)</li> </ul> <p>Benefits:</p> <ul> <li>Single source of truth: Documentation structure derived from filesystem and frontmatter</li> <li>Always in sync: Regenerated on every <code>mkdocs build</code> or <code>mkdocs serve</code></li> <li>Two complementary formats:</li> <li>Browsable map for quick navigation on landing page</li> <li>Comprehensive searchable index with metadata for reference</li> <li>Zero maintenance: No manual updates needed when adding/removing docs</li> <li>Landing page friendly: Documentation Map visible on index.md without clicking</li> </ul> <p>Migration from original ADR:</p> <ul> <li>Original: Manually-maintained static Documentation Map in index.md</li> <li>Now: Auto-generated from filesystem, appended at build time</li> <li>Static map in source index.md remains as fallback but is replaced in built site</li> </ul> <p>References:</p> <ul> <li><code>/scripts/generate_doc_index.py</code> - Main index generation script</li> <li><code>/scripts/append_doc_map_to_index.py</code> - Build-time index.md augmentation</li> <li><code>/docs/documentation_index.md</code> - Auto-generated comprehensive reference</li> <li><code>/docs/documentation_map.md</code> - Auto-generated hierarchical navigation (excluded from site nav)</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/","title":"ADR-DD02: Documentation Main Content and Navigation Strategy","text":"<p>Establishes how README.md and docs/index.md relate, defines content inclusion patterns, and specifies navigation automation with mkdocs-literate-nav.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-11-23</li> <li>Owner: Documentation Working Group</li> <li>Supersedes: None</li> <li>Related: ADR-DD01: Documentation System Reorganization Strategy</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#context","title":"Context","text":"<p>Following ADR-DD01's filesystem reorganization and literate-nav adoption (TODO #9, Part 4e complete), we now have:</p> <ul> <li>README.md: Rich user-facing content with Vision &amp; Goals, Features, Quick Start, Installation, and Documentation Structure overview</li> <li>docs/index.md: Auto-generated sparse documentation map\u2014just a flat list of all documents by section</li> <li>Navigation: Filesystem-driven via <code>generate_mkdocs_nav.py</code> producing <code>docs-nav.md</code> for mkdocs-literate-nav</li> <li>Build automation: Scripts for CLI docs generation, doc-index generation, and README sync verification</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#current-problems","title":"Current Problems","text":"<ol> <li> <p>Content Divergence: README.md has substantial onboarding content (vision, features, quick start) that doesn't appear in docs/index.md. Newcomers arriving at the published documentation site see only a file listing, not the compelling project introduction.</p> </li> <li> <p>Document Purpose Confusion: Both documents serve entry points but target different contexts:</p> </li> <li>README.md: GitHub repository landing page, must be immediately actionable</li> <li> <p>docs/index.md: MkDocs site home, should orient users to the full documentation landscape</p> </li> <li> <p>Maintenance Burden: No clear strategy for what content belongs where, when duplication is acceptable, or how to keep critical sections synchronized.</p> </li> <li> <p>Navigation Clarity: Auto-generated <code>docs-nav.md</code> works well for filesystem traversal but lacks curation\u2014no persona-based entry points, no workflow-oriented groupings, no \"start here\" guidance.</p> </li> <li> <p>Content Reusability: Complex sections (installation steps, development setup, pattern system overview) appear in multiple places with manual duplication and drift risk.</p> </li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#design-constraints","title":"Design Constraints","text":"<ul> <li>Filesystem-driven navigation is non-negotiable (ADR-DD01 decision): Documentation structure must mirror <code>docs/</code> tree with literate-nav auto-generation</li> <li>README.md must remain editable: Cannot become a pure build artifact\u2014GitHub display requires direct file readability</li> <li>CI verification: Must detect when critical content drifts between README and documentation</li> <li>Incremental adoption: Solution must work with current tooling (mkdocs-literate-nav, mkdocs-gen-files) without major architectural changes</li> <li>Contributor accessibility: Documentation workflow should be obvious to new contributors without deep tooling knowledge</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#decision","title":"Decision","text":"<p>Adopt a Progressive Enhancement content strategy with phase-based implementation:</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#1-content-architecture","title":"1. Content Architecture","text":""},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#readmemd-concise-project-gateway","title":"README.md: Concise Project Gateway","text":"<p>README.md serves as the GitHub repository landing page and remains hand-maintained. It provides:</p> <ul> <li>Project description: 2-3 sentence elevator pitch</li> <li>Vision &amp; Goals: Why the project exists, what problems it solves (4-6 bullet points)</li> <li>Features: High-level capabilities overview (Core Tools summary, Pattern System summary)</li> <li>Quick Start: Minimal install + first command (PyPI install, tnh-setup, example usage)</li> <li>Documentation Overview: Brief orientation + link to full docs site</li> <li>Development: Pointer to DEV_SETUP.md and CONTRIBUTING.md</li> <li>Project Status: Current version, alpha/beta/stable designation</li> <li>Support &amp; Community: Links to issues, discussions, documentation</li> </ul> <p>README.md stays concise (target: readable in 2-3 scrolls, ~200-250 lines). Detailed content lives in the documentation site.</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#docsindexmd-comprehensive-documentation-hub","title":"docs/index.md: Comprehensive Documentation Hub","text":"<p>docs/index.md serves as the MkDocs site landing page and becomes the primary onboarding experience. It provides:</p> <ul> <li>Welcome Section: Same project description as README (synchronized manually or via drift reporting)</li> <li>Getting Started (Persona-Based): Curated entry points for different audiences:</li> <li>Practitioners: Using the CLI tools for dharma talk processing and translation</li> <li>Developers: Contributing code, running tests, understanding architecture</li> <li>Researchers: Exploring the knowledge base, evaluation workflows, and experiments</li> <li>Key Features: Expanded from README with links to deep-dive documentation</li> <li>Installation: Embedded detailed steps (can reference <code>_includes/installation.md</code> in Phase 2)</li> <li>Quick Reference: Common commands, pattern examples, troubleshooting links</li> <li>Architecture Overview: High-level system design with links to ADRs</li> <li>Contributing: How to participate, testing, documentation contributions</li> <li>Documentation Map: Auto-generated section listing (repositioned to bottom, renamed \"Complete Documentation Index\")</li> </ul> <p>docs/index.md is comprehensive (target: complete orientation for all personas, ~400-500 lines). It's the definitive \"start here\" page.</p> <p>Persona-based navigation is a key differentiator from README.md\u2014it helps users self-identify their path and find relevant documentation quickly.</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#shared-content-strategy","title":"Shared Content Strategy","text":"<p>Phase 1 (Current - Simple Independence with Drift Reporting): - Accept controlled duplication of introductory content (project description, vision, high-level features) - README and docs/index.md are independently maintained - Lightweight drift reporting script (<code>check_readme_docs_drift.py</code>) generates informational reports - Non-blocking: reports written to local log file (<code>docs_sync_report.txt</code>) for review - Manual sync decisions made during project check-ins based on drift reports</p> <p>Phase 2 (Planned - Selective Inclusion): - Extract complex, frequently reused content to <code>docs/_includes/</code>:   - <code>installation.md</code>: Detailed install steps (PyPI, prerequisites, dev setup)   - <code>development.md</code>: Development environment configuration   - <code>prompt-overview.md</code>: Prompt system introduction - Use mkdocs snippets plugin (<code>--8&lt;--</code> syntax) for transclusion - Keep README independent; include shared content in docs/index.md and other docs</p> <p>Phase 3 (Future - Templated Assembly, if needed): - If maintenance burden grows significantly, consider templating (Jinja2 via mkdocs-macros) - Would allow generating both README and docs/index.md from structured content - Defer this decision until Phase 2 proves insufficient</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#2-navigation-strategy","title":"2. Navigation Strategy","text":""},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#filesystem-driven-navigation-literate-nav","title":"Filesystem-Driven Navigation (Literate-Nav)","text":"<p>Continue current approach with refinements:</p> <ul> <li>Auto-generation: <code>generate_mkdocs_nav.py</code> produces <code>docs-nav.md</code> from <code>docs/</code> tree structure</li> <li>Front matter titles: Prefer YAML <code>title:</code> field over filename humanization (must match exactly per markdown standards)</li> <li>Sort order: Maintain curated top-level order in <code>TOP_LEVEL_ORDER</code> list</li> <li>Index page handling: Directory <code>index.md</code> becomes section landing page with overview + persona-appropriate navigation aids</li> </ul> <p>Navigation file <code>docs-nav.md</code> is a build artifact (regenerated on every docs build).</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#section-index-pages","title":"Section Index Pages","text":"<p>Every top-level directory must have an <code>index.md</code> that provides:</p> <ul> <li>Section purpose: What this documentation covers</li> <li>Target audience: Who should read this section</li> <li>Navigation aids: Curated list of key documents (complement to auto-generated nav)</li> <li>Prerequisites: What to read first (if applicable)</li> </ul> <p>Example structure for <code>docs/architecture/index.md</code>:</p> <pre><code>---\ntitle: \"Architecture\"\ndescription: \"System design, ADRs, and component deep-dives for TNH Scholar.\"\n---\n# Architecture\n\nThis section documents the design decisions, system architecture, and\ncomponent implementations for TNH Scholar.\n\n## Getting Started\n- **New to the codebase?** Start with [System Overview](/architecture/overview.md)\n- **Looking for decisions?** Browse [architecture docs](/architecture/overview.md)\n- **Need component details?** See subsystem design documents below\n\n## Key Resources\n- [GenAI Service](/architecture/gen-ai-service/design/genai-service-design-strategy.md) - Core AI integration layer\n- [Prompt System](/architecture/prompt-system/prompt-system-architecture.md) - Prompt management architecture\n- [Transcription Pipeline](/architecture/transcription/design/diarization-system-design.md) - Audio processing design\n\n## Subsystems\n- [AI Text Processing](/architecture/ai-text-processing/design/textobject-system-design.md) - Text transformation pipeline\n- [Knowledge Base](/architecture/knowledge-base/adr/adr-k01-kb-architecture-strategy.md) - Vector search and metadata\n- [Transcription](/architecture/transcription/design/diarization-system-design.md) - Audio-to-text with diarization\n- [Video Processing](/architecture/video-processing/adr/adr-vp01-video-processing.md) - YouTube integration\n</code></pre>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#documentation-map-integration","title":"Documentation Map Integration","text":"<p>The auto-generated \"Documentation Map\" in docs/index.md (current behavior) is retained but repositioned:</p> <ul> <li>Move to bottom of docs/index.md (after Welcome, Getting Started (Persona-Based), Features, Installation, Quick Reference, Architecture Overview, Contributing)</li> <li>Rename to \"Complete Documentation Index\"</li> <li>Add introductory text: \"Browse all documentation organized by topic. For persona-based entry points, see Getting Started above.\"</li> </ul> <p>This ensures the landing page prioritizes human-oriented navigation (persona paths) over exhaustive file listings (auto-generated map).</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#3-content-drift-monitoring-phase-1-mvp","title":"3. Content Drift Monitoring (Phase 1 MVP)","text":""},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#lightweight-drift-reporting","title":"Lightweight Drift Reporting","text":"<p>Instead of enforcing synchronization, Phase 1 uses informational drift reporting:</p> <p>Script: <code>scripts/check_readme_docs_drift.py</code></p> <p>The script compares sections between README.md and docs/index.md by dynamic heading detection:</p> <p>Approach:</p> <ul> <li>Extract all <code>## Section Name</code> headings from both files</li> <li>Compare sections with matching titles (case-sensitive)</li> <li>Warn on title mismatches (case, punctuation, spacing differences)</li> <li>Report sections that exist in one file but not the other</li> </ul> <p>Behavior:</p> <ol> <li>Extract all <code>## Level 2</code> sections from both files</li> <li>Match sections by heading text (exact match)</li> <li>For matched sections: generate unified diff if content differs</li> <li>For unmatched sections: report \"only in README\" or \"only in docs/index.md\"</li> <li>Warn on near-matches (e.g., \"Quick Start\" vs \"Quick start\" - case mismatch)</li> <li>Write report to <code>docs_sync_report.txt</code> (gitignored)</li> <li>Print report to console for CI visibility</li> <li>Always exit 0 (non-blocking, informational only)</li> </ol> <p>Report format:</p> <pre><code>================================================================================\nREADME.md \u2194 docs/index.md Drift Report\nGenerated: 2025-11-23 10:15:32\n================================================================================\n\nMatched Sections (compared):\n\u2713 Vision &amp; Goals: IDENTICAL\n\u2717 Features: DIFFERS\n\n--- README.md::Features\n+++ docs/index.md::Features\n@@ -1,5 +1,7 @@\n TNH Scholar provides several CLI tools:\n - audio-transcribe: Process audio files\n+- tnh-fab: Text processing with patterns\n - ytt-fetch: Download YouTube transcripts\n\nSections only in README.md:\n- Example Usage\n- Development\n\nSections only in docs/index.md:\n- Architecture Overview\n- Contributing\n- Complete Documentation Index\n\nTitle Mismatches (possible typos):\n\u26a0 \"Quick Start\" (README) vs \"Quick start\" (docs/index.md) - case mismatch\n\n\u26a0 DRIFT DETECTED - Review differences above\nThis is informational only - no action required unless intentional divergence.\n================================================================================\n</code></pre>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#integration-points","title":"Integration Points","text":"<p>Makefile target:</p> <pre><code>.PHONY: docs-drift\ndocs-drift:\n    @poetry run python scripts/check_readme_docs_drift.py\n\n.PHONY: docs-verify\ndocs-verify: docs-drift\n    @poetry run mkdocs build --strict\n</code></pre> <p>CI workflow (non-blocking):</p> <pre><code>- name: Check documentation drift\n  run: |\n    python scripts/check_readme_docs_drift.py\n    cat docs_sync_report.txt\n  continue-on-error: true\n</code></pre> <p>Gitignore:</p> <pre><code># Documentation drift reports (local review only)\ndocs_sync_report.txt\n</code></pre>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#usage-workflow","title":"Usage Workflow","text":"<ol> <li>During development: Run <code>make docs-drift</code> to see current drift status</li> <li>Before commits: Review <code>docs_sync_report.txt</code> locally</li> <li>In CI: Drift report printed in logs (visible but doesn't fail build)</li> <li>At project check-ins: Review accumulated drift, decide if manual sync warranted</li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#acceptable-divergence","title":"Acceptable Divergence","text":"<p>The following divergences are expected and acceptable:</p> <ul> <li>Depth: README gives high-level overview; docs/index.md provides detailed explanation</li> <li>Audience: README targets newcomers; docs/index.md serves all personas</li> <li>Navigation: README links to docs site; docs/index.md embeds navigation aids</li> <li>Examples: README shows minimal quick-start; docs/index.md includes comprehensive examples</li> </ul> <p>No sync enforcement - teams decide when alignment matters based on drift reports.</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#4-content-inclusion-patterns-future","title":"4. Content Inclusion Patterns (Future)","text":"<p>Status: Detailed design deferred to ADR-DD03: Content Reuse and Inclusion Strategies</p> <p>Phase 1 accepts controlled duplication with drift monitoring. When duplication becomes burdensome, ADR-DD03 will define:</p> <ul> <li>Inclusion hierarchy: Markdown snippets (<code>docs/_includes/</code>), macros (<code>docs/_templates/</code>), and generation scripts</li> <li>Tooling choices: mkdocs snippets plugin, mkdocs-macros, mkdocs-gen-files integration</li> <li>Naming conventions: Underscore-prefixed directories for non-user-facing content</li> <li>Migration strategy: Moving duplicated content to shared locations</li> </ul> <p>Current approach (Phase 1): Use generation scripts (Level 3) only for already-automated content (CLI docs, API reference, documentation index). No manual content inclusion yet.</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#5-maintenance-workflows","title":"5. Maintenance Workflows","text":""},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#updating-readmemd","title":"Updating README.md","text":"<ol> <li>Edit README.md directly in repository root</li> <li>Run <code>make docs-drift</code> to see if changes affect monitored sections</li> <li>Review <code>docs_sync_report.txt</code> to assess drift</li> <li>Decide if docs/index.md should be updated (no enforcement)</li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#updating-docsindexmd","title":"Updating docs/index.md","text":"<ol> <li>Edit docs/index.md directly in <code>docs/</code></li> <li>Run <code>make docs-drift</code> to check for drift in monitored sections</li> <li>Review report and update README.md if appropriate</li> <li>CI runs drift check but doesn't fail on drift (informational only)</li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#adding-new-documentation","title":"Adding New Documentation","text":"<ol> <li>Create markdown file in appropriate <code>docs/</code> subdirectory</li> <li>Add YAML front matter (title, description, owner, author, status)</li> <li>Navigation updates automatically via literate-nav</li> <li>If creating new top-level section, add to <code>TOP_LEVEL_ORDER</code> in <code>generate_mkdocs_nav.py</code></li> <li>Create section <code>index.md</code> with overview and navigation aids</li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#reorganizing-content","title":"Reorganizing Content","text":"<ol> <li>Move files using <code>git mv</code> to preserve history</li> <li>Update internal links (CI link checker will catch broken references)</li> <li>Navigation regenerates automatically</li> <li>Update section index pages if section purpose changes</li> <li>Run <code>make docs</code> to rebuild and verify</li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#6-implementation-phases","title":"6. Implementation Phases","text":""},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#phase-1-simple-independence-with-drift-reporting-current-priority","title":"Phase 1: Simple Independence with Drift Reporting (Current Priority)","text":"<p>Scope: Complete TODO #9, Part 3b</p> <ul> <li> Enhance docs/index.md with persona-based Getting Started</li> <li> Implement <code>check_readme_docs_drift.py</code> script</li> <li> Add <code>docs-drift</code> target to Makefile</li> <li> Add <code>docs_sync_report.txt</code> to <code>.gitignore</code></li> <li> Integrate drift reporting into CI (non-blocking)</li> <li> Create/update section index pages for all top-level directories</li> <li> Reposition auto-generated Documentation Map in docs/index.md</li> </ul> <p>Success Criteria: - docs/index.md provides comprehensive onboarding (400-500 lines) - README.md stays concise (200-250 lines) - Drift reporting runs in CI and generates local reports - No CI failures from drift (informational only) - All top-level sections have index.md with navigation aids</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#phase-2-content-inclusion-patterns-future","title":"Phase 2: Content Inclusion Patterns (Future)","text":"<p>Status: Deferred to ADR-DD03: Content Reuse and Inclusion Strategies</p> <p>Trigger: When 10+ instances of duplicated complex content (installation steps, development setup, etc.) cause maintenance burden.</p> <p>Approach Sketch:</p> <ul> <li>Use mkdocs snippets plugin (<code>--8&lt;--</code> syntax) for shared content in <code>docs/_includes/</code></li> <li>Keep README.md independent; include shared sections in docs/index.md and other docs</li> <li>Document inclusion patterns in markdown standards</li> </ul> <p>Decision Point: Revisit 6 months post-beta or when drift reporting shows repeated manual syncs of identical content.</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#phase-3-advanced-automation-future-if-needed","title":"Phase 3: Advanced Automation (Future, If Needed)","text":"<p>Status: Deferred to ADR-DD04: Documentation Generation and Templating (if Phase 2 proves insufficient)</p> <p>Trigger: High-churn content causing frequent drift despite inclusion patterns.</p> <p>Approach Sketch:</p> <ul> <li>Templated assembly using Jinja2 via mkdocs-macros</li> <li>Structured content storage (YAML/JSON) for frequently changing sections</li> <li>Generate both README.md and docs/index.md from templates</li> </ul> <p>Decision Point: Only proceed if clear ROI demonstrated (e.g., weekly README/docs updates causing sync overhead).</p>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li> <p>Templated Assembly (Full Jinja2): Rejected for Phase 1 due to build complexity and README becoming non-editable artifact. Deferred to Phase 3 if needed.</p> </li> <li> <p>README as Build Artifact: Rejected because GitHub requires readable README in repository view. Generated files hurt discoverability.</p> </li> <li> <p>Single Unified Document: Rejected because README and docs/index serve different contexts (GitHub vs. MkDocs site) and need different levels of detail.</p> </li> <li> <p>Manual Duplication Without Any Monitoring: Rejected due to drift risk observed in current state (README and docs/index.md had diverged significantly). Lightweight drift reporting provides awareness without enforcement overhead.</p> </li> <li> <p>Shared Content Directory (No Templating): Considered for Phase 1 but deferred to Phase 2. Initial implementation keeps documents independent with drift reporting only.</p> </li> <li> <p>Enforced Synchronization with Markers: Rejected for Phase 1 due to added complexity (marker management, CI failures on drift). Phase 1 prioritizes simplicity\u2014drift reporting provides awareness without enforcement burden. Can revisit in Phase 2 if drift proves problematic.</p> </li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#consequences","title":"Consequences","text":""},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#positive","title":"Positive","text":"<ul> <li>Clear ownership: README and docs/index.md have distinct purposes and audiences</li> <li>Flexibility: Each document optimized for its context (GitHub vs. MkDocs site)</li> <li>Safety net: Sync verification prevents critical content drift while allowing intentional divergence</li> <li>Incremental adoption: Start simple (independent docs), add inclusion patterns only when needed</li> <li>Contributor clarity: Obvious where content lives, when to update both files, how to verify</li> <li>Navigation automation: Literate-nav keeps structure in sync with filesystem</li> <li>Onboarding improvement: docs/index.md becomes comprehensive entry point (not just file list)</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#negative-risks","title":"Negative / Risks","text":"<ul> <li>Controlled duplication: Accept some redundancy between README intro and docs/index intro (mitigated by drift reporting for awareness)</li> <li>Manual sync decisions: Teams must review drift reports and decide when to sync (no enforcement)</li> <li>Drift accumulation: Without enforcement, documents could diverge significantly over time (mitigated by regular project check-in reviews)</li> <li>Tooling dependency: Relies on mkdocs-literate-nav, mkdocs-gen-files, and custom scripts (acceptable given ADR-DD01 commitment)</li> <li>Phase 2 transition: Moving to inclusion patterns requires coordination (defer until post-beta to minimize churn)</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#mitigation-strategies","title":"Mitigation Strategies","text":"<ol> <li>Document the workflow: Clear instructions in <code>docs/docs-ops/markdown-standards.md</code> and CONTRIBUTING.md</li> <li>Regular reviews: Include drift report in project check-in process</li> <li>Minimal monitoring surface: Only track high-level sections (Vision, Features, Quick Start, Installation)</li> <li>Defer complexity: Don't adopt inclusion patterns or sync enforcement until duplication causes real pain</li> <li>Template enforcement: ADR template and standards prevent structural inconsistency</li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#open-questions-future-decisions","title":"Open Questions &amp; Future Decisions","text":"<ol> <li> <p>Sync enforcement: Should we enforce synchronization in CI or keep reporting-only? Decision: Start with reporting-only in Phase 1; add enforcement in Phase 2 only if drift becomes problematic.</p> </li> <li> <p>Documentation Map position: Bottom of docs/index.md or separate page? Decision: Keep at bottom initially; move to dedicated <code>/documentation-index</code> page if docs/index.md exceeds 600 lines.</p> </li> <li> <p>Persona-based navigation: Should literate-nav group docs by persona (User/Developer/Researcher) in addition to topic? Decision: Defer to Phase 2; current topic-based structure sufficient for ~100 docs.</p> </li> <li> <p>Section index automation: Can section index pages be partially auto-generated? Decision: Keep hand-maintained for curation; auto-generation risks losing narrative flow.</p> </li> <li> <p>Link checking: Should CI verify all internal markdown links? Decision: Yes, add to TODO #9, Part 4d (link normalization task).</p> </li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd02-main-content-nav/#approval-tracking","title":"Approval &amp; Tracking","text":"<ul> <li>TODO Reference: TODO #9 (Documentation Reorganization, ADR-DD01), Part 3b</li> <li>Implementation Tracking: GitHub issues tagged <code>docs</code> + <code>adr-dd02</code></li> <li>Related ADRs:</li> <li>ADR-DD01: Documentation System Reorganization Strategy (accepted)</li> <li>ADR-DD03: Content Reuse and Inclusion Strategies (future - Phase 2)</li> <li>ADR-DD04: Documentation Generation and Templating (future - Phase 3, if needed)</li> <li>Review Cycle: Reassess Phase 2 transition 6 months post-beta or when 10+ instances of complex content duplication observed</li> </ul> <p>Approval of this ADR completes the content architecture design for TODO #9, Part 3b, and provides a roadmap for incremental documentation improvements through beta and beyond.</p>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/","title":"ADR-DD03: Pattern to Prompt Terminology Standardization","text":"<p>Standardizes TNH Scholar documentation to use \"Prompt\" terminology instead of \"Pattern\" to align with industry conventions, match the refactored gen-ai-service architecture (Prompt/PromptCatalog), and improve clarity for external stakeholders.</p> <ul> <li>Filename: <code>adr-dd03-pattern-prompt-terminology.md</code></li> <li>Heading: <code># ADR-DD03: Pattern to Prompt Terminology Standardization</code></li> <li>Status: Proposed</li> <li>Date: 2025-11-28</li> <li>Author: Claude Sonnet 4.5</li> <li>Owner: Documentation Working Group</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#context","title":"Context","text":"<p>TNH Scholar historically used \"Pattern\" to refer to engineered prompts for AI text processing. This terminology emerged from early prototyping and emphasized the engineering pattern aspect of reusable prompt structures. However, several factors now warrant standardizing on \"Prompt\":</p> <ol> <li> <p>Industry Alignment: The broader AI/LLM community uses \"Prompt\", \"Prompt Engineering\", and \"Prompt Catalog\" as standard terminology. External stakeholders (Parallax Press, potential users) expect this language.</p> </li> <li> <p>gen-ai-service Refactoring: The core <code>gen-ai-service</code> module has been refactored to use <code>Prompt</code> and <code>PromptCatalog</code> classes, aligning with common practice. Code now uses modern terminology.</p> </li> <li> <p>Mixed Terminology Confusion: Current docs contain mixed usage:</p> </li> <li>Some files use \"Pattern\" (legacy)</li> <li>Some use \"PromptTemplate\" (transitional)</li> <li>Some use \"Prompt/PromptTemplate\" or \"Pattern/PromptTemplate\" (hedging)</li> <li> <p>This creates cognitive overhead and suggests inconsistency</p> </li> <li> <p>Deprecated Code Base: Many modules still using \"Pattern\" terminology are already marked for deprecation/deletion (e.g., old tnh-fab implementations). Updating code is out of scope; focusing on docs provides immediate clarity.</p> </li> <li> <p>Documentation Reorganization Timing: The docs-reorg branch (ADR-DD01, ADR-DD02) provides a natural checkpoint to standardize terminology before merging and starting new work (VS Code integration, gen-ai-service ADRs).</p> </li> </ol> <p>Scope Note: This ADR focuses on documentation only. Code refactoring is tracked separately and many legacy modules are scheduled for deletion.</p>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#decision","title":"Decision","text":"<p>Standardize all documentation to use \"Prompt\" as the primary term, with the following guidelines:</p>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#primary-terminology","title":"Primary Terminology","text":"<ul> <li>Use \"Prompt\" when referring to engineered text inputs for AI models</li> <li>Use \"Prompt Catalog\" when referring to collections/repositories of prompts</li> <li>Use \"Prompt Engineering\" when discussing the practice of designing prompts</li> <li>Use \"PromptTemplate\" only in technical contexts where referring to the actual class/data structure name</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#handling-historical-references","title":"Handling Historical References","text":"<ol> <li>High-Level User-Facing Docs (index.md, getting-started/, user-guide/, README):</li> <li>Replace \"Pattern\" \u2192 \"Prompt\" throughout</li> <li> <p>Add a one-time explanatory note in a Glossary or FAQ section:      &gt; Historical Note: Earlier versions of TNH Scholar referred to prompts as \"Patterns\" to emphasize their engineering pattern nature. This terminology has been updated to align with industry standards. References to \"Pattern\" in older documentation or archived materials should be read as \"Prompt\".</p> </li> <li> <p>Architecture/Design Docs (architecture/, development/):</p> </li> <li>Update to \"Prompt\" in current/active ADRs and design documents</li> <li>In historical/archived docs: Add a callout at the top noting the terminology shift, but preserve original text for historical accuracy</li> <li> <p>Example callout:      <pre><code>&gt; **Note**: This document uses historical \"Pattern\" terminology. In current TNH Scholar documentation, \"Pattern\" has been replaced with \"Prompt\".\n</code></pre></p> </li> <li> <p>CLI Documentation (cli/):</p> </li> <li>Update to \"Prompt\" in all descriptions and examples</li> <li> <p>Note where CLI commands still use <code>pattern</code> flags/arguments (due to legacy code) with explanation:      &gt; Note: The <code>--pattern</code> flag is retained for backwards compatibility. It refers to prompts.</p> </li> <li> <p>Code Comments &amp; Docstrings: Out of scope for this ADR (tracked separately)</p> </li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#execution-plan","title":"Execution Plan","text":"<p>Phase 1: High-Priority Docs (Complete before docs-reorg merge) - [ ] Update <code>docs/index.md</code> and add concise historical terminology note near top - [ ] Update <code>README.md</code> - [ ] Update <code>docs/getting-started/*.md</code> - [ ] Update <code>docs/user-guide/patterns.md</code> \u2192 rename to <code>prompts.md</code> - [ ] Update current ADRs in <code>docs/architecture/*/adr/</code> that are still relevant - [ ] Rename <code>docs/architecture/prompt-system/</code> \u2192 <code>prompt-system/</code> and update nav</p> <p>Phase 2: Architecture &amp; Design Docs (Begin before merge, complete post-merge) - [ ] Add historical callouts to archived/legacy design documents - [ ] Update <code>docs/development/*.md</code> as they're actively edited - [ ] Update architecture design documents with Pattern\u2192Prompt terminology - [ ] Update <code>docs/cli/*.md</code> CLI documentation (many commands deprecated/scheduled for removal)</p> <p>Phase 3: Search &amp; Replace Validation (Post-merge) - [ ] Use <code>rg -i \"pattern\" docs/</code> to find remaining instances - [ ] Manual review each occurrence for context-appropriateness - [ ] Update navigation labels in <code>docs/nav.md</code> if needed</p>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#files-requiring-immediate-update-phase-1","title":"Files Requiring Immediate Update (Phase 1)","text":"<p>Based on grep analysis, prioritize these files:</p> <pre><code>docs/index.md\ndocs/getting-started/quick-start.md\ndocs/getting-started/installation.md\ndocs/getting-started/configuration.md\ndocs/user-guide/patterns.md\ndocs/user-guide/overview.md\ndocs/user-guide/best-practices.md\ndocs/cli/overview.md\ndocs/cli/tnh-fab.md\ndocs/architecture/docs-system/adr/adr-dd01-docs-reorg-strategy.md (update references)\ndocs/architecture/docs-system/adr/adr-dd02-main-content-nav.md (update references)\n</code></pre>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#consequences","title":"Consequences","text":""},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#positive","title":"Positive","text":"<ul> <li>Clarity for External Stakeholders: Parallax Press, new contributors, and users encounter familiar terminology</li> <li>Alignment with Code: Documentation matches the refactored gen-ai-service architecture</li> <li>Reduced Cognitive Load: Single consistent term across all current documentation</li> <li>Professional Presentation: Industry-standard language improves credibility</li> <li>Future-Proof: New ADRs (VS Code integration, gen-ai-service work) start with consistent terminology</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#negative","title":"Negative","text":"<ul> <li>Historical Document Context Loss: Archived docs may feel inconsistent unless callouts are added</li> <li>One-Time Effort: Requires systematic search-replace and manual review across ~65 files</li> <li>CLI Flag Confusion: Some CLI commands still use <code>--pattern</code> flags (legacy code); requires explanation in docs</li> <li>Link/Navigation Updates: May need to rename files/directories (e.g., <code>patterns.md</code> \u2192 <code>prompts.md</code>, <code>architecture/prompt-system/</code> \u2192 <code>architecture/prompt-system/</code>)</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#mitigation","title":"Mitigation","text":"<ul> <li>Add clear historical note in glossary/FAQ</li> <li>Use callouts in archived docs to preserve context</li> <li>Document CLI flag mismatches explicitly</li> <li>Phase the work: high-priority user-facing docs first, lower-priority archives later</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#alternative-1-keep-pattern-throughout","title":"Alternative 1: Keep \"Pattern\" Throughout","text":"<p>Rejected: Misaligns with industry standards, confuses external stakeholders, and diverges from refactored code (gen-ai-service uses Prompt/PromptCatalog).</p>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#alternative-2-use-prompttemplate-as-primary-term","title":"Alternative 2: Use \"PromptTemplate\" as Primary Term","text":"<p>Rejected: \"PromptTemplate\" is more verbose and primarily a technical class name. \"Prompt\" is the user-facing concept. Use \"PromptTemplate\" only in technical contexts where referring to the class/structure.</p>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#alternative-3-dual-terminology-patternprompt","title":"Alternative 3: Dual Terminology (Pattern/Prompt)","text":"<p>Rejected: Already tried this (Pattern/PromptTemplate hedging in docs). Creates confusion and suggests uncertainty. Pick one standard term.</p>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#alternative-4-defer-until-code-refactoring-complete","title":"Alternative 4: Defer Until Code Refactoring Complete","text":"<p>Rejected: Deprecated code modules are scheduled for deletion, not refactoring. Documentation benefits are immediate and don't require code changes. Waiting blocks clarity improvements.</p>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#open-questions","title":"Open Questions","text":""},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#resolved-during-adr-review","title":"Resolved During ADR Review","text":"<ol> <li>Directory Renaming: Should <code>docs/architecture/prompt-system/</code> be renamed to <code>docs/architecture/prompt-system/</code>?</li> <li> <p>Decision: \u2705 Yes, rename in Phase 1 (pre-merge) to ensure consistency</p> </li> <li> <p>User Guide File Renaming: Should <code>docs/user-guide/patterns.md</code> be renamed to <code>prompts.md</code>?</p> </li> <li> <p>Decision: \u2705 Yes, rename in Phase 1 to match nav labels; update nav.md and internal links accordingly</p> </li> <li> <p>CLI Command Documentation: How to handle CLI tools that still use <code>--pattern</code> flags?</p> </li> <li> <p>Decision: \u2705 Move to Phase \u2154 (post-merge) as many CLI tools are deprecated, scheduled for refactoring, or removal</p> </li> <li> <p>Glossary Location: Where should the historical terminology note live?</p> </li> <li>Decision: \u2705 Add brief, concise note to <code>docs/index.md</code> relatively high up (near top of document) to prevent confusion with legacy docs; defer full glossary to future work</li> </ol>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#remaining-open-questions","title":"Remaining Open Questions","text":"<p>None - all decisions resolved during review.</p>"},{"location":"architecture/docs-system/adr/adr-dd03-pattern-to-prompt/#references","title":"References","text":"<ul> <li>ADR-DD01: Documentation System Reorganization Strategy - References Pattern\u2192PromptTemplate rename</li> <li>ADR-DD02: Documentation Main Content and Navigation Strategy</li> <li>gen-ai-service Refactoring ADRs - Modern Prompt/PromptCatalog architecture</li> <li>TODO #9: Documentation Reorganization - Part 3b includes Pattern\u2192PromptTemplate rename task</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/","title":"ADR-DD03 Phase 1 Execution Punch List","text":"<p>Status: IN PROGRESS Started: 2025-11-28 Goal: Complete Pattern\u2192Prompt terminology updates before docs-reorg merge</p>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#phase-1-tasks-pre-merge","title":"Phase 1 Tasks (Pre-Merge)","text":""},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#1-add-historical-terminology-note","title":"1. Add Historical Terminology Note","text":"<ul> <li> Add concise note to <code>docs/index.md</code> near top explaining Pattern\u2192Prompt shift</li> <li>Location: After vision/intro, before main navigation sections</li> <li>Keep brief (2-3 sentences max)</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#2-core-documentation-updates","title":"2. Core Documentation Updates","text":""},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#readmemd","title":"README.md","text":"<ul> <li> Search for \"pattern\" references (case-insensitive)</li> <li> Update to \"prompt\" where referring to AI prompts</li> <li> Review and verify changes</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#docsindexmd","title":"docs/index.md","text":"<ul> <li> Update Pattern\u2192Prompt references</li> <li> Update \"Pattern/PromptTemplate\" \u2192 \"Prompt\"</li> <li> Verify navigation links</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#docsgetting-started","title":"docs/getting-started/","text":"<ul> <li> <code>quick-start.md</code> - Pattern\u2192Prompt</li> <li> <code>installation.md</code> - Pattern\u2192Prompt</li> <li> <code>configuration.md</code> - Pattern\u2192Prompt</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#docsuser-guide","title":"docs/user-guide/","text":"<ul> <li> Rename <code>patterns.md</code> \u2192 <code>prompts.md</code></li> <li> Update content: Pattern\u2192Prompt</li> <li> <code>overview.md</code> - Pattern\u2192Prompt</li> <li> <code>best-practices.md</code> - Pattern\u2192Prompt</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#3-architecture-documentation","title":"3. Architecture Documentation","text":""},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#directory-renaming","title":"Directory Renaming","text":"<ul> <li> Rename <code>docs/architecture/prompt-system/</code> \u2192 <code>prompt-system/</code></li> <li> Update all internal links pointing to pattern-system/</li> <li> Update nav.md references</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#adr-updates","title":"ADR Updates","text":"<ul> <li> <code>adr-dd01-docs-reorg-strat.md</code> - Update Pattern\u2192Prompt references</li> <li> <code>adr-dd02-docs-content-nav.md</code> - Update Pattern\u2192Prompt references</li> <li> Review <code>docs/architecture/prompt-system/adr/</code> ADRs for consistency</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#4-navigation-links","title":"4. Navigation &amp; Links","text":"<ul> <li> Update <code>docs/nav.md</code> - Pattern\u2192Prompt labels</li> <li> Search for broken links after directory rename: <code>rg \"pattern-system\" docs/</code></li> <li> Update <code>docs/documentation_index.md</code> if auto-generated content needs refresh</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#5-verification","title":"5. Verification","text":"<ul> <li> Run <code>rg -i \"pattern(?!s\\\\.md)\" docs/index.md docs/README.md docs/getting-started/ docs/user-guide/</code> to find remaining instances</li> <li> Manual review of changes</li> <li> Test mkdocs build: <code>make docs</code></li> <li> Verify navigation works correctly</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#6-commit-documentation","title":"6. Commit &amp; Documentation","text":"<ul> <li> Commit terminology updates with clear message</li> <li> Update CHANGELOG.md with ADR-DD03 Phase 1 completion</li> <li> Update AGENTLOG.md with session details</li> <li> Update TODO #9 with terminology work completion</li> </ul>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#files-to-update-quick-reference","title":"Files to Update (Quick Reference)","text":"<pre><code>docs/index.md\nREADME.md\ndocs/getting-started/quick-start.md\ndocs/getting-started/installation.md\ndocs/getting-started/configuration.md\ndocs/user-guide/patterns.md \u2192 prompts.md\ndocs/user-guide/overview.md\ndocs/user-guide/best-practices.md\ndocs/architecture/prompt-system/ \u2192 prompt-system/\ndocs/architecture/docs-system/adr/adr-dd01-docs-reorg-strat.md\ndocs/architecture/docs-system/adr/adr-dd02-docs-content-nav.md\ndocs/nav.md\n</code></pre>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#search-commands-for-validation","title":"Search Commands for Validation","text":"<pre><code># Find all \"pattern\" references in Phase 1 files\nrg -i \"pattern\" docs/index.md docs/getting-started/ docs/user-guide/\n\n# Find references to old directory name\nrg \"pattern-system\" docs/\n\n# Find Pattern/PromptTemplate hedging\nrg \"Pattern/Prompt\" docs/\n</code></pre>"},{"location":"architecture/docs-system/adr/adr-dd03-phase1-punchlist/#notes","title":"Notes","text":"<ul> <li>Keep \"pattern\" when referring to software design patterns (non-prompt contexts)</li> <li>Preserve historical callouts in archived documents</li> <li>CLI documentation (docs/cli/) deferred to Phase 2</li> </ul>"},{"location":"architecture/docs-system/design/","title":"Design","text":"<p>Table of Contents:</p> <p>Documentation Design - Reference for the documentation stack, covering tooling choices, information architecture, and publishing workflow.</p> <p>This file auto-generated.</p>"},{"location":"architecture/docs-system/design/documentation-design/","title":"Documentation Design","text":"<p>Reference for the documentation stack, covering tooling choices, information architecture, and publishing workflow.</p>"},{"location":"architecture/docs-system/design/documentation-design/#tools-technology-choices","title":"Tools &amp; Technology Choices","text":""},{"location":"architecture/docs-system/design/documentation-design/#static-site-generator","title":"Static Site Generator","text":"<ul> <li>Choice: MkDocs with Material theme</li> <li>Rationale:</li> <li>Markdown-based for ease of writing</li> <li>Good integration with Python tooling</li> <li>Modern, responsive design</li> <li>Active community support</li> </ul>"},{"location":"architecture/docs-system/design/documentation-design/#api-documentation","title":"API Documentation","text":"<ul> <li>Choice: MkDocstrings</li> <li>Rationale:</li> <li>Native MkDocs integration</li> <li>Support for Google-style docstrings</li> <li>Clean, hierarchical output</li> <li>Good code navigation features</li> </ul>"},{"location":"architecture/docs-system/design/documentation-design/#version-control","title":"Version Control","text":"<ul> <li>Choice: Git + GitHub Pages</li> <li>Rationale:</li> <li>Free hosting</li> <li>Automatic deployment</li> <li>Version tracking</li> <li>PR-based reviews</li> </ul>"},{"location":"architecture/docs-system/design/documentation-design/#structure-decisions","title":"Structure Decisions","text":""},{"location":"architecture/docs-system/design/documentation-design/#documentation-types","title":"Documentation Types","text":"<ol> <li>User Documentation (Markdown)</li> <li>Installation guides</li> <li>User manuals</li> <li> <p>Tutorials</p> </li> <li> <p>API Documentation (Docstrings)</p> </li> <li>Class/function documentation</li> <li>Code examples</li> <li> <p>Type hints</p> </li> <li> <p>Development Documentation</p> </li> <li>Architecture decisions</li> <li>Contribution guides</li> <li>Development setup</li> </ol>"},{"location":"architecture/docs-system/design/documentation-design/#file-organization","title":"File Organization","text":"<ul> <li>User-facing content in /user_guide</li> <li>API reference in /api</li> <li>Development docs in /development</li> </ul>"},{"location":"architecture/gen-ai-service/","title":"Gen Ai Service","text":"<p>Table of Contents:</p> <p>Adr - Table of contents for architecture/gen-ai-service/adr</p> <p>Design - Table of contents for architecture/gen-ai-service/design</p> <p>This file auto-generated.</p>"},{"location":"architecture/gen-ai-service/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-A01: Adopt Object-Service for GenAI Interactions - Standardizes GenAI interactions with an Object-Service pattern that separates domain shapes from provider orchestration.</p> <p>ADR-A02: PatternCatalog Integration (V1) - Describes the V1 contract for plugging the legacy PatternCatalog into GenAI Service via rendered system prompts.</p> <p>ADR-A08: Configuration / Parameters / Policy Taxonomy - Establishes the Config/Params/Policy taxonomy for GenAI Service to prevent parameter soup and clarify ownership.</p> <p>ADR-A09: V1 Simplified Implementation Pathway - Defines the minimum viable GenAI Service implementation that preserves architectural seams while shipping quickly.</p> <p>ADR-A11: Model Parameters and Strong Typing Fix - Enforces typed parameter objects and removes literals from GenAI Service so provider flows stay consistent.</p> <p>ADR-A12: Prompt System &amp; Fingerprinting Architecture (V1) - Replaces the Pattern Catalog adapter with a Prompt-first design that yields domain objects plus fingerprints.</p> <p>ADR-A13: Migrate All OpenAI Interactions to GenAIService - Retires the legacy OpenAI client and standardizes every caller on the typed GenAI Service pipeline.</p> <p>This file auto-generated.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a01-object-service-genai/","title":"ADR-A01: Adopt Object-Service for GenAI Interactions","text":"<p>Adopts the Object-Service pattern so GenAI requests use consistent domain objects while provider adapters stay isolated.</p> <p>Status: Accepted Date: 2025-10-02 Author: Aaron Solomon with GPT-5.0, Claude AI</p> <p>See also: GenAI Service Strategy</p>"},{"location":"architecture/gen-ai-service/adr/adr-a01-object-service-genai/#context","title":"Context","text":"<p>As we integrate Generative AI (GenAI) capabilities into our platform, we need a clear architectural approach to manage interactions with various GenAI providers. The complexity and variability of provider 1APIs, request/response formats, and operational policies necessitate a structured pattern that promotes maintainability, extensibility, and observability.</p> <p>The Object-Service pattern offers a clean separation between domain data structures (Objects) and the business logic that operates on them (Services). This approach allows us to encapsulate GenAI interaction logic within dedicated service components while keeping domain shapes explicit and reusable.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a01-object-service-genai/#decision","title":"Decision","text":"<p>We will adopt the Object-Service pattern for GenAI interactions by defining a central <code>GenAIService</code> responsible for orchestrating requests and responses to multiple GenAI providers. The domain shapes will be clearly defined to represent the core data exchanged:</p> <ul> <li> <p>CompletionRequest / CompletionResponse: Standardized domain objects representing the input prompt and the generated completion from a GenAI provider.</p> </li> <li> <p>ProviderRequest / ProviderResponse: Provider-specific request and response objects that adapt the generic Completion shapes to the particular API requirements of each GenAI service.</p> </li> <li> <p>PatternCatalog: A shared repository of prompt engineering patterns and templates that can be injected into requests to improve prompt quality and consistency.</p> </li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a01-object-service-genai/#configuration-taxonomy","title":"Configuration Taxonomy","text":"<p>To manage configuration complexity, we will organize configuration into three conceptual layers:</p> <ul> <li> <p>Config: Static configuration parameters such as API keys, endpoint URLs, and default timeouts.</p> </li> <li> <p>Params: Dynamic parameters that can be varied per request, such as prompt text, temperature, max tokens, etc.</p> </li> <li> <p>Policy: Operational policies including rate limits, safety filters, and content moderation rules that govern how requests are processed and responses validated.</p> </li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a01-object-service-genai/#policy-and-safety","title":"Policy and Safety","text":"<p>Given the risks associated with generative AI outputs, we will embed safety policies within the GenAIService implementation. For v1, content filtering and moderation will be limited to stubs and placeholders for future enhancement. Rate and cost limiting are recognized as significant operational concerns but will be kept minimal at this stage to allow flexibility and iteration.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a01-object-service-genai/#observability-and-provenance","title":"Observability and Provenance","text":"<p>To maintain transparency and traceability of GenAI interactions, we will implement observability features as primarily hooks and stubs at this stage. However, the necessary data structures and logging frameworks will be in place to enable detailed tracing, correlation IDs, and metadata tagging as the system matures.</p> <p>Timing is captured by an injected Observer via context-managed spans; the service reads span.duration_ms rather than computing durations itself.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a01-object-service-genai/#domain-data-structures-outline","title":"Domain Data Structures (Outline)","text":"<ul> <li> <p>CompletionRequest: Encapsulates the input prompt, parameters such as temperature, max tokens, and any contextual metadata.</p> </li> <li> <p>CompletionResponse: Contains the generated text, usage statistics, and any relevant metadata or provenance information.</p> </li> <li> <p>ProviderRequest: Adapts CompletionRequest into the specific request format required by a GenAI provider, including authentication and endpoint details.</p> </li> <li> <p>ProviderResponse: Parses and normalizes the provider's raw response into a CompletionResponse format.</p> </li> <li> <p>PatternRef, RenderedPattern, PatternCatalog: Structures to manage prompt engineering patterns; PatternRef identifies a pattern, RenderedPattern is the instantiated prompt text, and PatternCatalog holds reusable templates.</p> </li> <li> <p>CompletionParams: Defines dynamic parameters influencing generation such as temperature, top-p, max tokens, stop sequences.</p> </li> <li> <p>BudgetPolicy: Defines constraints related to cost, rate limits, and quotas.</p> </li> <li> <p>MediaPolicy: Governs usage of media types, content restrictions, and safety filters.</p> </li> <li> <p>SecurityConfig: Configuration for authentication, encryption, and access control.</p> </li> <li> <p>Provenance: Tracks metadata about request origin, timestamps, applied policies, and audit trails. In v1, provenance may later include <code>provider_request_id</code>, <code>routing_reason</code>, <code>retry_count</code>, and <code>policy_version</code> for multi-provider debugging.</p> </li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a01-object-service-genai/#architecture-diagram-v1","title":"Architecture Diagram (V1)","text":"<pre><code>flowchart LR\n    subgraph PatternLibrary[\"Pattern Library\"]\n        PL[\"patterns.py&lt;br&gt;Pattern &amp; PatternManager\"]\n    end\n    subgraph GenAIService[\"GenAI Service\"]\n        GA[\"GenAIService&lt;br&gt;(domain and service layer)\"]\n        PA[\"PatternsAdapter&lt;br&gt;(pattern_catalog/adapters/patterns_adapter.py)\"]\n        PB[\"ProvenanceBuilder&lt;br&gt;(infra/tracking/provenance.py)\"]\n        FH[\"fingerprint.py&lt;br&gt;(infra/tracking/fingerprint.py)\"]\n    end\n    subgraph Provider[\"LLM Provider\"]\n        PC[\"ProviderClient&lt;br&gt;(OpenAI / Anthropic / ...)\"]\n    end\n    U[\"Caller / Client\"] --&gt;|\"RenderRequest&lt;br&gt;(pattern_key, user_prompt, variables, model)\"| GA\n    GA --&gt;|\"lookup pattern&lt;br&gt;via adapter\"| PA\n    PA --&gt;|\"load pattern&lt;br&gt;PatternManager.load_pattern(name)\"| PL\n    PA --&gt;|\"render system&lt;br&gt;Pattern.apply_template(field_values=variables)\"| PL\n    PA --&gt;|\"RenderedPrompt&lt;br&gt;(system and user message)\"| GA\n    GA --&gt;|\"ProviderRequest&lt;br&gt;(RenderedPrompt and params)\"| PC\n    PC --&gt;|\"ProviderResponse&lt;br&gt;(completion)\"| GA\n    GA --&gt;|\"hash inputs&lt;br&gt;(pattern bytes, vars, user)\"| FH\n    GA --&gt;|\"build provenance&lt;br&gt;pattern and hashes and model\"| PB\n    PB --&gt;|\"Provenance\"| GA\n    GA --&gt;|\"CompletionEnvelope&lt;br&gt;(prompt, response, provenance)\"| U</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a01-object-service-genai/#reference-code-skeletons-v1","title":"Reference Code Skeletons (v1)","text":"<p>Below are minimal, typed skeletons to anchor implementation; no error handling, aligned to v1 prototyping.</p> ai_service/domain.py<pre><code>from __future__ import annotations\nfrom typing import Any, Optional, Literal, Protocol, Sequence\nfrom pathlib import Path\nfrom pydantic import BaseModel, Field\n\nRole = Literal[\"system\", \"user\", \"assistant\"]\nOutputMode = Literal[\"text\", \"json\"]\n\nclass PatternRef(BaseModel):\n    id: str\n    version: Optional[str] = None  # git tag/commit or semantic version\n\nclass RenderedPattern(BaseModel):\n    text: str\n    metadata_json: str\n    fingerprint: str  # sha256(text + metadata_json)\n\nclass MediaAttachment(BaseModel):\n    path: Optional[Path] = None\n    bytes_: Optional[bytes] = None\n    mime: str\n    width: Optional[int] = None\n    height: Optional[int] = None\n\nclass CompletionParams(BaseModel):\n    temperature: float = 0.2\n    top_p: float = 1.0\n    max_output_tokens: int = 1024\n    output_mode: OutputMode = \"text\"\n\nclass BudgetPolicy(BaseModel):\n    max_dollars: float = 0.20\n    fail_on_budget_exceeded: bool = True\n\nclass MediaPolicy(BaseModel):\n    allow_images: bool = True\n    max_image_count: int = 4\n    max_total_image_mb: float = 10.0\n\nclass SecurityConfig(BaseModel):\n    redact_pii: bool = True\n    max_input_chars: int = 120_000\n\nclass CompletionRequest(BaseModel):\n    pattern: PatternRef\n    variables: dict[str, Any] = Field(default_factory=dict)\n    images: list[MediaAttachment] = Field(default_factory=list)\n    params: CompletionParams = Field(default_factory=CompletionParams)\n    correlation_id: Optional[str] = None\n\nclass UsageInfo(BaseModel):\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n    dollars: float\n\nclass CompletionResult(BaseModel):\n    text: str\n    json_obj: Optional[dict[str, Any]] = None\n    model: str\n    usage: UsageInfo\n    latency_ms: int\n    warnings: list[str] = Field(default_factory=list)\n\nclass Provenance(BaseModel):\n    backend: str\n    model: str\n    correlation_id: str\n    pattern_id: str\n    pattern_fingerprint: str\n    started_at: str\n    completed_at: str\n    schema_version: str = \"1.0\"\n\nclass Envelope(BaseModel):\n    status: Literal[\"pending\", \"running\", \"succeeded\", \"failed\", \"timeout\"]\n    error: Optional[str] = None\n    diagnostics: dict[str, Any] = Field(default_factory=dict)\n    provenance: Provenance\n\nclass CompletionResponse(Envelope):\n    result: Optional[CompletionResult] = None  # present on success\n</code></pre> <pre><code>from __future__ import annotations\nfrom typing import Protocol, Any\n\nclass ObsSpan(Protocol):\n    def __enter__(self) -&gt; \"ObsSpan\": ...\n    def __exit__(self, exc_type, exc, tb) -&gt; None: ...\n    @property\n    def duration_ms(self) -&gt; int: ...\n\nclass Observer(Protocol):\n    def phase(self, name: str, **fields: Any) -&gt; ObsSpan: ...\n\nclass NoOpObserver:\n    def phase(self, name: str, **fields: Any) -&gt; ObsSpan:\n        class _Span:\n            def __enter__(self): return self\n            def __exit__(self, exc_type, exc, tb): pass\n            @property\n            def duration_ms(self) -&gt; int: return 0\n        return _Span()\n</code></pre> ai_service/providers/base.py<pre><code>from __future__ import annotations\nfrom typing import Optional, Literal, Protocol\nfrom pydantic import BaseModel\nfrom ..domain import MediaAttachment, UsageInfo\n\nOutputMode = Literal[\"text\", \"json\"]\n\nclass ProviderRequest(BaseModel):\n    input_text: str                  # Rendered prompt\n    images: list[MediaAttachment] = []\n    model: str\n    temperature: float\n    max_output_tokens: int\n    top_p: float\n    output_mode: OutputMode = \"text\"\n    correlation_id: str\n    system_text: Optional[str] = None  # for providers like Anthropic\n\nclass ProviderResponse(BaseModel):\n    text: str\n    usage: UsageInfo\n    model: str\n    finish_reason: Optional[str] = None\n    raw_response: Optional[dict] = None\n\nclass ProviderClient(Protocol):\n    def generate(self, request: ProviderRequest) -&gt; ProviderResponse: ...\n</code></pre> ai_service/router.py (updated)<pre><code>from dataclasses import dataclass\nfrom .patterns import PatternMeta\nfrom .domain import CompletionParams\n\n@dataclass\nclass RoutingDecision:\n    provider: str\n    model: str\n    reason: str\n\nclass ModelRouter:\n    def select(self, pattern_meta: PatternMeta, params: CompletionParams, policy: \"CompletionPolicy\") -&gt; RoutingDecision:\n        # Precedence: params.model_hint \u2192 pattern_meta.default_model_hint \u2192 config default\n        chosen = params.model_hint or pattern_meta.default_model_hint or \"gpt-4o-mini\"\n        return RoutingDecision(provider=\"openai\", model=chosen, reason=\"precedence/model_hint\u2192pattern_default\u2192global_default\")\n</code></pre> ai_service/patterns.py<pre><code>from __future__ import annotations\nfrom typing import Any\nfrom pydantic import BaseModel\nfrom .domain import PatternRef, RenderedPattern\n\nclass PatternMeta(BaseModel):\n    task_kind: str              # e.g., \"translate\", \"summarize\"\n    output_mode: str            # \"text\" | \"json\"\n    default_model_hint: str     # e.g., \"gpt-4o-mini\"\n\nclass PatternCatalog(Protocol):\n    def render(self, ref: PatternRef, variables: dict[str, Any]) -&gt; RenderedPattern: ...\n    def introspect(self, ref: PatternRef) -&gt; PatternMeta: ...\n</code></pre> ai_service/service.py (updated generate flow)<pre><code>from __future__ import annotations\nfrom typing import Any\nfrom pydantic import BaseModel\nfrom .domain import (\n    CompletionRequest, CompletionResponse, CompletionResult,\n    Envelope, Provenance, UsageInfo, RenderedPattern,\n)\nfrom .patterns import PatternCatalog\nfrom .providers.base import ProviderClient, ProviderRequest\nfrom .router import ModelRouter, RoutingDecision\nfrom .obs import Observer, NoOpObserver\n\ndef generate_ulid() -&gt; str:\n    import uuid\n    return str(uuid.uuid4())\n\nclass GenAIServiceConfig(BaseModel):\n    api_key: str\n    base_url: str | None = None\n    default_timeout_s: float = 60.0\n\nclass GenAIService:\n    def __init__(self, config: GenAIServiceConfig, catalog: PatternCatalog, client: ProviderClient, policy: \"CompletionPolicy\", *, observer: Observer | None = None):\n        self.config = config\n        self.catalog = catalog\n        self.client = client\n        self.policy = policy\n        self.router = ModelRouter()\n        self.obs: Observer = observer or NoOpObserver()\n\n    def _build_provider_request(self, req: CompletionRequest, rendered: RenderedPattern, routing: RoutingDecision) -&gt; ProviderRequest:\n        # v1 enforces a fixed temperature to improve determinism\n        return ProviderRequest(\n            input_text=rendered.text,\n            images=req.images,\n            model=routing.model,\n            temperature=0.2,\n            max_output_tokens=req.params.max_output_tokens,\n            top_p=req.params.top_p,\n            output_mode=req.params.output_mode,\n            correlation_id=req.correlation_id or generate_ulid(),\n        )\n\n    def generate(self, req: CompletionRequest) -&gt; CompletionResponse:\n        correlation_id = req.correlation_id or generate_ulid()\n\n        with self.obs.phase(\"introspect\", correlation_id=correlation_id, pattern_id=req.pattern.id):\n            pattern_meta = self.catalog.introspect(req.pattern)\n\n        with self.obs.phase(\"render\", correlation_id=correlation_id, pattern_id=req.pattern.id):\n            rendered = self.catalog.render(req.pattern, req.variables)\n\n        with self.obs.phase(\"route\", correlation_id=correlation_id, pattern_id=req.pattern.id):\n            routing = self.router.select(pattern_meta, req.params, self.policy)\n\n        provider_request = self._build_provider_request(req, rendered, routing)\n\n        with self.obs.phase(\"provider_call\", correlation_id=correlation_id, provider=\"openai\", model=routing.model) as s_call:\n            pr = self.client.generate(provider_request)\n\n        latency_ms = getattr(s_call, \"duration_ms\", 0)  # computed by observer span\n\n        result = CompletionResult(text=pr.text, json_obj=None, model=pr.model, usage=pr.usage, latency_ms=latency_ms)\n        prov = Provenance(\n            backend=\"openai\",\n            model=pr.model,\n            correlation_id=provider_request.correlation_id,\n            pattern_id=req.pattern.id,\n            pattern_fingerprint=rendered.fingerprint,\n            started_at=\"\",    # optional: observer can record wall/monotonic separately\n            completed_at=\"\",\n            schema_version=\"1.0\",\n        )\n        return CompletionResponse(status=\"succeeded\", result=result, provenance=prov)\n</code></pre> ai_service/errors.py<pre><code>class GenAIServiceError(Exception):\n    pass\n\nclass PolicyError(GenAIServiceError):\n    pass\n\nclass TransportError(GenAIServiceError):\n    pass\n\nclass ProviderError(GenAIServiceError):\n    pass\n\nclass FormatError(GenAIServiceError):\n    pass\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a01-object-service-genai/#consequences","title":"Consequences","text":"<ul> <li> <p>Pros: Clear separation of concerns, easier maintenance, and extensibility for adding new GenAI providers. Improved safety and compliance through embedded policies. Enhanced observability facilitates debugging and operational insights.</p> </li> <li> <p>Cons: Additional complexity in defining and maintaining domain shapes and service layers. Potential overhead in adapting provider-specific requests/responses.</p> </li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/","title":"ADR-A02: PatternCatalog Integration (V1)","text":"<p>Locks the GenAI Service to the existing PatternCatalog by defining how rendered templates populate system prompts and messages.</p> <p>Status:  Superseded by ADR-A12 Prompt System and FingerPrint Architecture (V1) Date: 2025-11-08 Author: Aaron Solomon, GPT 5.0 Related: ADR-A01 (Object-Service blueprint), ADR-A11 (Model params), ADR-P02 (Pattern catalog, if any)</p>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#context","title":"Context","text":"<ul> <li>A file-backed pattern library (patterns.py) provides single-template Jinja2 \"system prompts\" with optional front-matter (e.g., model_hint) and a PatternManager rooted at TNH_PATTERN_DIR. This library is first\u2011class, stable, and external to GenAI Service.</li> <li>The new <code>GenAIService</code> expects a <code>RenderedPrompt</code> with an optional system string and a list of messages, where the caller's freeform prompt is sent as a \"user\" message.</li> <li>We want a walking skeleton that fully reuses assets without content migration, while establishing durable provenance &amp; fingerprinting hooks.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#decision","title":"Decision","text":""},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#1-v1-prompt-composition-semantics","title":"1. V1 Prompt Composition (Semantics)","text":"<ul> <li>system = rendered template (Jinja2) using caller-provided variables</li> <li>messages = exactly one: <code>Message(role=\"user\", content=&lt;verbatim user prompt&gt;)</code></li> <li>No user-message templating in V1</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#rendering-engine-canonical-seam","title":"Rendering Engine (Canonical Seam)","text":"<p>V1 rendering is delegated entirely to the Pattern Library. All system prompt rendering must occur via:</p> <pre><code>Pattern.apply_template(field_values=variables)\n</code></pre> <p>This ensures:</p> <ul> <li>unified template semantics for all tooling consuming patterns,</li> <li>correct precedence ordering (default fields &lt; front\u2011matter &lt; caller variables),</li> <li>consistent behavior across the TNH\u2011Scholar ecosystem,</li> <li>and elimination of multiple Jinja engines with divergent policies.</li> </ul> <p>GenAI Service MUST NOT render templates with its own Jinja instance. The Pattern Library is the single source of truth for rendering semantics.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#2-adapter-of-record","title":"2. Adapter of Record","text":"<ul> <li><code>PatternsAdapter</code> wraps the PatternManager and exposes patterns to GenAI Service.</li> <li>The adapter accepts an optional base_path (Path). If provided, it uses this root; otherwise it uses the PatternManager default (TNH_PATTERN_DIR). This allows multiple catalog roots (fixtures, alternate repos).</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#3-front-matter-params-scoped","title":"3. Front-Matter \u2192 Params (Scoped)","text":"<ul> <li>V1 only surfaces <code>model_hint</code> to suggest a provider/model</li> <li>No direct mapping of other front-matter keys (e.g., <code>temperature</code>, <code>top_p</code>) into <code>CompletionParams</code> in V1</li> <li>Structure for future mapping must be modular (e.g., a small \"param mapping\" strategy object) rather than inline</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#4-missing-variable-policy-v1","title":"4. Missing-Variable Policy (V1)","text":"<ul> <li>Dev/test: missing variables raise (strict)</li> <li>Prod (later): a future toggle in Settings may allow safe rendering (empty on missing). Not implemented in V1.</li> </ul> <p>Rendering strictness note: <code>Pattern.apply_template</code> may use a permissive undefined policy in its current implementation. ADR\u2011A02 V1 accepts the Pattern Library\u2019s strictness behavior as-is. GenAI Service does not enforce its own strict/lenient Jinja policy; strictness will be governed by the Pattern Library or by a future ADR introducing global settings.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#5-discovery-namespacing","title":"5. Discovery &amp; Namespacing","text":"<ul> <li>Primary: PatternManager using <code>TNH_PATTERN_DIR</code></li> <li>Optional override: adapter-level <code>base_path</code> for alternative roots / fixtures</li> <li>Name resolution remains compatible; uniqueness is enforced per effective root</li> </ul> <p>Pattern Library Boundary:</p> <p>The pattern system is a first\u2011class shared utility within TNH\u2011Scholar. GenAI Service does not interpret pattern files directly; it consumes them through the PatternsAdapter. This ensures the Pattern Library can evolve independently and may later be extracted as a standalone package.</p> <p>Rendering boundary guarantee: GenAI Service does not execute Jinja directly. All rendering passes through <code>Pattern.apply_template</code>, which forms the canonical API seam. This guarantees that future changes to template semantics, front\u2011matter behavior, or variable precedence occur in one place only: the Pattern Library.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#6-fingerprinting-provenance-v1","title":"6. Fingerprinting &amp; Provenance (V1)","text":"<ul> <li>Establish a dedicated <code>infra/tracking/fingerprint.py</code> module with pure functions for deterministic hashing:</li> <li><code>pattern_content_hash</code>: based on template file bytes (post front-matter combine as defined by pattern library)</li> <li><code>resolved_vars_hash</code>: SHA-256 of JSON-canonicalized variables</li> <li><code>user_prompt_hash</code>: SHA-256 of raw user prompt</li> <li>Introduce a <code>ProvenanceBuilder</code> (module: <code>infra/tracking/provenance.py</code>) responsible for constructing a <code>Provenance</code> payload from inputs (pattern id/path, hashes, provider, model)</li> <li>Service code should call the builder, not construct provenance ad hoc. For V1, keep the payload minimal but place it in the correct module.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#7-data-shapes-v1","title":"7. Data Shapes (V1)","text":"<ul> <li><code>RenderRequest.variables</code>: prefer a named type alias for readability:</li> </ul> <pre><code>from typing import Any, Dict\nRenderVars = Dict[str, Any]\n</code></pre> <p>(A Pydantic model can be added later if schemas stabilize.)</p> <ul> <li><code>RenderedPrompt</code>: unchanged (<code>system: str|None</code>, <code>messages: list[Message]</code>)</li> <li><code>CompletionEnvelope.provenance</code>: produced by <code>ProvenanceBuilder</code> (see \"Provenance Payload\" below)</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#architecture-diagram-v1","title":"Architecture Diagram (V1)","text":"<pre><code>flowchart LR\n    subgraph PatternLibrary[\"Pattern Library\"]\n        PL[\"patterns.py&lt;br&gt;Pattern &amp; PatternManager\"]\n    end\n    subgraph GenAIService[\"GenAI Service\"]\n        GA[\"GenAIService&lt;br&gt;(domain and service layer)\"]\n        PA[\"PatternsAdapter&lt;br&gt;(pattern_catalog/adapters/patterns_adapter.py)\"]\n        PB[\"ProvenanceBuilder&lt;br&gt;(infra/tracking/provenance.py)\"]\n        FH[\"fingerprint.py&lt;br&gt;(infra/tracking/fingerprint.py)\"]\n    end\n    subgraph Provider[\"LLM Provider\"]\n        PC[\"ProviderClient&lt;br&gt;(OpenAI / Anthropic / ...)\"]\n    end\n    U[\"Caller / Client\"] --&gt;|\"RenderRequest&lt;br&gt;(pattern_key, user_prompt, variables, model)\"| GA\n    GA --&gt;|\"lookup pattern&lt;br&gt;via adapter\"| PA\n    PA --&gt;|\"load pattern&lt;br&gt;PatternManager.load_pattern(name)\"| PL\n    PA --&gt;|\"render system&lt;br&gt;Pattern.apply_template(field_values=variables)\"| PL\n    PA --&gt;|\"RenderedPrompt&lt;br&gt;(system and user message)\"| GA\n    GA --&gt;|\"ProviderRequest&lt;br&gt;(RenderedPrompt and params)\"| PC\n    PC --&gt;|\"ProviderResponse&lt;br&gt;(completion)\"| GA\n    GA --&gt;|\"hash inputs&lt;br&gt;(pattern bytes, vars, user)\"| FH\n    GA --&gt;|\"build provenance&lt;br&gt;pattern and hashes and model\"| PB\n    PB --&gt;|\"Provenance\"| GA\n    GA --&gt;|\"CompletionEnvelope&lt;br&gt;(prompt, response, provenance)\"| U</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#consequences","title":"Consequences","text":"<p>Pros:</p> <ul> <li>Clear boundary between Pattern Library and GenAI Service</li> <li>Clear seam for future param mapping</li> <li>Early placement of provenance/fingerprinting in the infra layer</li> <li>Testable, swappable pattern roots</li> <li>Canonical rendering semantics centralized in the Pattern Library</li> </ul> <p>Cons:</p> <ul> <li>No user-message templating and no rich multi-role patterns in V1</li> <li>Limited front-matter propagation</li> <li>Rendering strictness controlled by Pattern Library until stricter policy toggles are added</li> </ul> <p>Risk Mitigation:</p> <ul> <li>Clean seams (adapter + provenance builder) localize future changes</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#provenance-payload-v1-skeleton","title":"Provenance Payload (V1 Skeleton)","text":"<p>Produced by <code>infra/tracking/provenance.py</code>:</p> <pre><code>{\n  \"schema_version\": \"prov-1\",\n  \"pattern\": {\n    \"name\": \"summarize_chapter\",\n    \"base_path\": \"/abs/path/to/patterns\",\n    \"content_hash\": \"sha256:\u2026\"\n  },\n  \"variables\": {\n    \"hash\": \"sha256:\u2026\"\n  },\n  \"user_prompt\": {\n    \"hash\": \"sha256:\u2026\"\n  },\n  \"provider\": \"openai\",\n  \"model\": \"gpt-4o-mini\"\n}\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#model-selection-precedence","title":"Model Selection Precedence","text":"<p>V1 (walking skeleton): When both an explicit model is provided and the pattern includes a model_hint, the adapter MUST select the explicit model. In other words, RenderRequest.model overrides the pattern\u2019s model_hint.</p> <p>Future (policy-driven): This precedence MUST be governed by a configurable policy, not hard-coded in the adapter or domain models. Implement a ModelSelectionPolicy owned by the configuration layer (suggested path: src/tnh_scholar/config/render_policy.py) with an interface such as:</p> <pre><code>class ModelSelectionPolicy:\n    def choose_model(self, explicit: str | None, hint: str | None) -&gt; str | None:\n        ...\n</code></pre> <p>The GenAIService delegates to this policy to determine the effective model, allowing precedence (and other constraints, e.g., per-intent or per-tenant rules) to be changed without code edits. ADR-A02 V1 keeps the simple override rule to minimize surface area while policy infrastructure is bootstrapped.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#v1-clarifications-guarantees","title":"V1 Clarifications &amp; Guarantees","text":""},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#provenance-type-pydantic-v2","title":"Provenance type (Pydantic v2)","text":"<p><code>ProvenanceBuilder</code> returns a Pydantic v2 model named <code>Provenance</code> with <code>extra=\"forbid\"</code> and <code>frozen=True</code> and the fixed <code>schema_version=\"prov-1\"</code>.</p> <pre><code># src/tnh_scholar/gen_ai_service/infra/tracking/provenance.py\nfrom pathlib import Path\nfrom pydantic import BaseModel, Field, ConfigDict\n\nclass Provenance(BaseModel):\n    model_config = ConfigDict(frozen=True, extra=\"forbid\")\n\n    schema_version: str = Field(default=\"prov-1\")\n    pattern_name: str\n    pattern_base_path: Path\n    pattern_content_hash: str  # \"sha256:&lt;hex&gt;\"\n    variables_hash: str        # \"sha256:&lt;hex&gt;\"\n    user_prompt_hash: str      # \"sha256:&lt;hex&gt;\"\n    provider: str              # e.g. \"openai\"\n    model: str                 # e.g. \"gpt-4o-mini\"\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#hash-algorithm-collision-stance","title":"Hash algorithm &amp; collision stance","text":"<p>All hashes are SHA-256 of canonicalized bytes and are used for identity/fingerprints and cache keys. We prefix hex digests with the algorithm (e.g., <code>\"sha256:&lt;hex&gt;\"</code>). We accept the birthday-paradox collision risk for SHA-256 in V1 and do not apply salt/HMAC. If future requirements demand stronger guarantees, we will extend the schema to include <code>(algo, hex, length)</code> and/or add an HMAC.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#variable-canonicalization-definition","title":"Variable canonicalization (definition)","text":"<p>Variables are canonicalized prior to hashing as follows:</p> <ol> <li>Coerce values to JSON-encodable forms:</li> <li><code>Path</code> \u2192 <code>str(path)</code></li> <li><code>Enum</code> \u2192 <code>enum.value</code></li> <li><code>datetime/date/time</code> \u2192 ISO-8601 string (UTC if applicable)</li> <li><code>set</code> \u2192 sorted list</li> <li>custom objects \u2192 <code>str(obj)</code> in V1</li> <li>Serialize with <code>json.dumps(obj, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False)</code></li> <li>UTF-8 encode and hash with SHA-256; store as <code>\"sha256:&lt;hex&gt;\"</code>.</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#missing-variable-policy","title":"Missing-variable policy","text":"<p>In V1 the Jinja2 environment uses <code>undefined=StrictUndefined</code>. Missing variables raise <code>PatternRenderError</code> (a <code>ValueError</code> subclass) naming the missing keys. A future settings flag may switch to <code>SilentUndefined</code> and substitute empty strings.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#fingerprint-source-of-truth","title":"Fingerprint source of truth","text":"<p><code>pattern_content_hash</code> is computed over the exact bytes of the template file on disk including front-matter, not the rendered output.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#rendervars-evolution-promotion-criteria","title":"RenderVars evolution (promotion criteria)","text":"<p><code>RenderVars = dict[str, Any]</code> in V1. A given pattern is upgraded to a typed <code>VariablesModel</code> (Pydantic v2) and validated before rendering when both hold:</p> <ol> <li>The pattern is used in \u2265 3 call sites or appears in a public API surface; and</li> <li>The pattern\u2019s required variable fields are unchanged across two minor releases.</li> </ol> <p>When a <code>VariablesModel</code> exists, canonicalization MUST hash <code>model_dump()</code> of that model.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#provenancebuilder-signature-mechanical-contract","title":"ProvenanceBuilder signature (mechanical contract)","text":"<pre><code>def build_provenance(\n    *,\n    pattern_name: str,\n    pattern_base_path: Path,\n    pattern_content_hash: str,\n    variables_hash: str,\n    user_prompt_hash: str,\n    provider: str,\n    model: str,\n) -&gt; Provenance:\n    ...\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#deterministic-smoke-tests-v1","title":"Deterministic smoke tests (V1)","text":"<ul> <li>Changing only the user prompt changes only <code>user_prompt_hash</code>.</li> <li>Changing only one variable changes only <code>variables_hash</code>.</li> <li>Changing only the template file bytes changes only <code>pattern_content_hash</code>.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#minimal-implementation-plan","title":"Minimal Implementation Plan","text":""},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#a-adapter","title":"A. Adapter","text":"<ul> <li>Update <code>PatternsAdapter</code> to accept <code>base_path: Path | None</code></li> <li>If <code>base_path</code> is provided, initialize a <code>PatternManager</code> instance rooted there; else use the PatternManager default (TNH_PATTERN_DIR). Adapter uses a provided PatternManager instance; GenAI Service constructs one at startup and reuses it for all requests.</li> <li><code>get(name)</code> returns a lightweight wrapper with <code>model_hint</code> and defaults from front-matter</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#b-render","title":"B. Render","text":"<ul> <li>Merge variables per pattern library rules; render to system</li> <li>Build <code>messages = [Message(role=\"user\", content=user_prompt)]</code></li> <li>Return <code>RenderedPrompt</code></li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#c-fingerprint-provenance","title":"C. Fingerprint &amp; Provenance","text":"<p>New: <code>infra/tracking/fingerprint.py</code></p> <ul> <li><code>sha256_bytes(b: bytes) -&gt; str</code>  # returns \"sha256: -hex-\"</li> <li><code>hash_user_prompt(s: str) -&gt; str</code></li> <li><code>hash_vars(d: RenderVars) -&gt; str</code>  # uses the canonicalization rules above</li> <li><code>hash_pattern_bytes(b: bytes) -&gt; str</code></li> </ul> <p>New: <code>infra/tracking/provenance.py</code></p> <ul> <li>Pydantic model <code>Provenance</code> (frozen, extra=\"forbid\", <code>schema_version=\"prov-1\"</code>)</li> <li><code>build_provenance(...) -&gt; Provenance</code> (see signature in Clarifications)</li> </ul> <p>Jinja render policy:</p> <ul> <li>Use <code>undefined=StrictUndefined</code> so missing variables raise <code>PatternRenderError</code> in dev/test.</li> </ul> <p>Hash inputs:</p> <ul> <li><code>pattern_content_hash</code> over exact on-disk template bytes (including front-matter)</li> <li><code>variables_hash</code> over canonicalized variables JSON</li> <li><code>user_prompt_hash</code> over raw user prompt (UTF-8)</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#d-testing-smoke","title":"D. Testing (Smoke)","text":"<ul> <li>Dev strict mode: missing variable raises</li> <li>Content-stable fingerprint:</li> <li>Changing only user prompt changes only <code>user_prompt.hash</code></li> <li>Changing only a var changes only <code>variables.hash</code></li> <li>Changing only pattern file changes only <code>content_hash</code></li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#open-questions-deferred","title":"Open Questions (Deferred)","text":"<ul> <li>Global vs per-pattern toggle for safe rendering in prod</li> <li>Param mapping from front-matter beyond <code>model_hint</code> (e.g., <code>temperature</code>) via a pluggable strategy</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#migration-path-post-v1","title":"Migration Path (Post-V1)","text":"<ul> <li>Introduce multi-part patterns (system/user/assistant blocks) in a new catalog format while retaining this adapter for backward compatibility</li> <li>Add optional user-message templating with opt-in flags</li> <li>Expand provenance with sequence numbers, latency, token counts, and cached-hit markers when the usage/accounting layer is in place</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#filemodule-placement","title":"File/Module Placement","text":"<pre><code>src/tnh_scholar/\n  gen_ai_service/\n    pattern_catalog/\n      adapters/\n        patterns_adapter.py      # wraps PatternManager\n    models/\n      domain.py                         # RenderRequest/RenderedPrompt/Message (existing)\n    infra/\n      tracking/\n        fingerprint.py                    # pure hashing helpers\n        provenance.py                     # ProvenanceBuilder\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a02-patterncatalog-integration-v1/#type-alias-definition","title":"Type Alias Definition","text":"<pre><code># somewhere shared, e.g., gen_ai_service/models/types.py\nfrom typing import Any, Dict\n\nRenderVars = Dict[str, Any]\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/","title":"ADR-A08: Configuration / Parameters / Policy Taxonomy","text":"<p>Defines distinct Config, Params, and Policy layers so GenAI Service stays composable and avoids parameter soup.</p> <p>Status: Accepted Date: 2025-10-05 Author: Aaron Solomon, ChatGPT (GPT-5) Linked Docs: ADR-A01 (Object-Service Domain Design), ADR-A09 (Simplified Implementation Pathway)</p>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#context","title":"Context","text":"<p>This ADR defines the taxonomy and separation of Config, Params, and Policy within the <code>GenAIService</code> ecosystem. The intent is to maintain clear ownership boundaries, composability, and extendability across providers and environments while preserving simplicity for the early prototype.</p> <p>The taxonomy follows the principles defined in the Object-Service Design Blueprint and GenAI Service Strategy, where domain purity and flexibility are prioritized over short-term expediency.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#motivation","title":"Motivation","text":"<p>The original OpenAI interface used many keyword arguments, mixing runtime options, environment setup, and behavioral constraints. This violated the parameter soup antipattern. The refactor introduces explicit objects representing distinct layers of control:</p> <ul> <li>Config \u2192 Construction-time, environment and provider-level constants  </li> <li>Params \u2192 Per-call, user- or system-provided input for specific invocations  </li> <li>Policy \u2192 Behavioral and safety rules governing all operations  </li> </ul> <p>This separation improves clarity, traceability, and testability while making future provider additions (Anthropic, Gemini, etc.) straightforward.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#taxonomy-overview","title":"Taxonomy Overview","text":"Layer Description Lifecycle Example Fields Owner Config Stable construction-time constants and credentials. Static <code>api_key</code>, <code>base_url</code>, <code>timeout_s</code>, <code>default_model</code>, <code>price_table</code> System Params Per-call adjustable execution inputs. Ephemeral <code>pattern_ref</code>, <code>variables</code>, <code>images</code>, <code>temperature</code>, <code>max_output_tokens</code>, <code>output_mode</code>, <code>model_hint</code> Consumer / Caller Policy Behavioral rules governing service operation. Persistent / Runtime <code>BudgetPolicy</code>, <code>MediaPolicy</code>, <code>SecurityConfig</code>, <code>RetryPolicy</code> System / Admin"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#relationships","title":"Relationships","text":"<pre><code>CompletionRequest\n \u251c\u2500\u2500 Params (runtime, from caller)\n \u251c\u2500\u2500 (no Policy field on request in v1)\n \u2514\u2500\u2500 Config/Policy applied inside GenAIService\n</code></pre> <ul> <li>Params define what is being asked.</li> <li>Policy defines how it may be executed.</li> <li>Config defines where and with what limits.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#v1-implementation-scope","title":"V1 Implementation Scope","text":"<p>The following subset is implemented in v1 per ADR-A09 (Simplified Pathway):</p>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#config-genaiserviceconfig","title":"Config (GenAIServiceConfig)","text":"<pre><code>class GenAIServiceConfig(BaseModel):\n    api_key: str\n    base_url: str = \"https://api.openai.com/v1\"\n    default_model: str = \"gpt-5o-mini\"\n    default_timeout_s: float = 60.0\n    max_dollars: float = 0.10\n</code></pre> <ul> <li>Minimal; supports only OpenAI.</li> <li>Future: provider registry, dynamic pricing, and per-model metadata.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#params-completionparams","title":"Params (CompletionParams)","text":"<pre><code>class CompletionParams(BaseModel):\n    temperature: float = 0.2\n    top_p: float = 1.0\n    max_output_tokens: int = 1024\n    output_mode: Literal[\"text\", \"json\"] = \"text\"\n    model_hint: Optional[str] = None\n</code></pre> <ul> <li>Note: Request-scoped inputs (<code>pattern</code>, <code>variables</code>, <code>images</code>) belong on <code>CompletionRequest</code>, not in <code>CompletionParams</code>. This enforces domain purity and avoids parameter soup per the Human\u2011Agent Coding Principles.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#policy-completionpolicy","title":"Policy (CompletionPolicy)","text":"<pre><code>class BudgetPolicy(BaseModel):\n    max_dollars: float = 0.10\n    fail_on_budget_exceeded: bool = True\n\nclass MediaPolicy(BaseModel):\n    allow_images: bool = True\n    max_image_size_mb: int = 10\n\nclass RetryPolicy(BaseModel):\n    max_attempts: int = 2\n    backoff_ms: int = 250\n\nclass SecurityConfig(BaseModel):\n    redact_pii: bool = False\n    max_input_chars: int = 150_000\n\nclass CompletionPolicy(BaseModel):\n    budget: BudgetPolicy = BudgetPolicy()\n    media: MediaPolicy = MediaPolicy()\n    retry: RetryPolicy = RetryPolicy()\n    security: SecurityConfig = SecurityConfig()\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#ownership-and-precedence-rules","title":"Ownership and Precedence Rules","text":"Layer Can be Overridden By Notes Config System only Reconstructed when environment or provider changes Params Caller or PatternCatalog Pattern defaults are overridden by direct input Policy Admin or System Context Not mutable at call site (injected by service)"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#v1-decisions-deferrals","title":"V1 Decisions &amp; Deferrals","text":"<ul> <li>System message handling deferred: PatternCatalog may later render <code>system_text</code>, but v1 renders <code>user_text</code> only.  </li> <li>Fingerprinting scope: hooks and comments only in v1 (observability), no cache lookup yet.  </li> <li>Pricing: use a static, good\u2011enough price table in config; move to dynamic registry later.  </li> <li>Policy ownership: service\u2011level only in v1; per\u2011request overrides deferred to v2.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#future-directions-beyond-v1","title":"Future Directions (Beyond v1)","text":"<ol> <li>Dynamic Provider Discovery \u2014 <code>GenAIServiceConfig</code> loads from environment or registry (multi-provider).  </li> <li>Hierarchical Policies \u2014 Service-level defaults \u2192 Project-level overrides \u2192 Call-level exceptions.  </li> <li>Runtime Introspection \u2014 Query <code>GenAIService</code> for effective config/policy summary.  </li> <li>Extensible Params \u2014 Support embeddings, fine-tuning, RAG context injection.  </li> <li>Externalized Policy Registry \u2014 Optional YAML- or DB-backed runtime policies.</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#consequences","title":"Consequences","text":""},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#benefits","title":"Benefits","text":"<ul> <li>Eliminates \u201cparameter soup.\u201d  </li> <li>Makes routing, logging, and safety composable.  </li> <li>Simplifies dependency injection and testing.  </li> <li>Supports lightweight migration to other providers.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#drawbacks","title":"Drawbacks","text":"<ul> <li>Adds minor verbosity for simple one-off calls.  </li> <li>Requires schema evolution management for backward compatibility.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a08-config-params-policy-taxonomy/#status-next-steps","title":"Status &amp; Next Steps","text":"<ul> <li>Status: Proposed for v1 implementation.  </li> <li>Next Steps: </li> <li>Integrate with <code>GenAIService</code> skeleton.  </li> <li>Validate via functional test notebook.  </li> <li>Refine during <code>ADR-A09</code> walking skeleton build.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a09-v1-simplified/","title":"ADR-A09: V1 Simplified Implementation Pathway","text":"<p>Outlines the pared-down GenAI Service build that keeps public seams intact but stubs complex internals for faster delivery.</p> <p>Status: Accepted Date: 2025-10-05 Author: Aaron Solomon, ChatGPT (GPT\u20115) Linked Docs: GenAI Service \u2014 Design Strategy (v0.2), ADR\u2011A01 (Domain &amp; Object Service)</p>"},{"location":"architecture/gen-ai-service/adr/adr-a09-v1-simplified/#purpose","title":"Purpose","text":"<p>This ADR captures the simplified implementation pathway for the first working release of the <code>GenAIService</code>. It preserves the architectural intent and interfaces of the strategy document but scales back complexity to achieve a usable prototype quickly. The goal is to reach a point where the system can \u201cwork on itself\u201d via VS\u00a0Code integration for AI\u2011assisted development.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a09-v1-simplified/#guiding-principles","title":"Guiding Principles","text":"<ul> <li>Preserve all seams (router, safety, observability, policy) but stub or simplify internal logic.  </li> <li>Prioritize usability, determinism, and end\u2011to\u2011end working flow over completeness.  </li> <li>Keep the public shapes and interfaces stable for future ADRs.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a09-v1-simplified/#simplification-overview","title":"Simplification Overview","text":"Subsystem Simplified for v1 Deferred / Full in Future Routing Single provider (<code>OpenAIAdapter</code>); model selected by Pattern <code>model_hint</code> if available, otherwise falls back to default (<code>gpt-4o-mini</code>). Capability filtering, multi-provider, latency/budget heuristics Params / Policy Keep <code>temperature</code>, <code>max_output_tokens</code>, <code>output_mode</code>, and <code>max_dollars</code>. Use constants for other fields. Full taxonomy ADR\u2011A08 (ownership, precedence, defaults) Preflight Use fixed constants: <code>MAX_INPUT_CHARS</code>, naive token estimator (<code>len(text)/3</code>), <code>MAX_CONTEXT_TOKENS=128k</code>, <code>MAX_DOLLARS</code>. Model\u2011aware tokenizers and price tables Images Support basic JPEG/PNG via <code>Path</code>; generic handling (no image count limit, simple type validation). Byte uploads, EXIF stripping, type registry JSON mode Prompt for JSON; <code>json.loads</code> parse; warn on failure. Schema validation, structured field coercion Observability Single log line with <code>{cid, pattern_id, model, latency_ms, dollars}</code>; ULID correlation only. Metrics registry, retention, structured tracing Errors Implement <code>PolicyError</code>, <code>TransportError</code>, <code>FormatError</code>. Extended error taxonomy, redaction, retries w/ jitter Retries One retry on\u00a0429/5xx,\u00a0250\u202fms\u00a0backoff. Configurable policy, exponential/jitter, rate limiting PatternCatalog Only <code>render()</code>\u00a0and\u00a0<code>introspect()</code> returning <code>{task_kind,\u00a0default_model_hint,\u00a0output_mode}</code>. Linting, forbidden tokens, metadata schema ProviderRequest Minimal fields: text, images, model, temperature, max_output_tokens, output_mode, correlation_id. System messages, raw response, full transport Convenience API Add <code>openai_generate()</code> helper to simplify testing and notebook/extension use. None\u2014core UX improvement <p>Convenience API definition suggestion:</p> <pre><code># ai_service/convenience.py\ndef openai_generate(pattern_name: str, variables: dict[str, Any], *, temperature: float = 0.2, max_tokens: int = 1024, images: list[Path] | None = None) -&gt; str:\n    service = GenAIService.from_env()  # factory to be added later\n    req = CompletionRequest(\n        pattern=PatternRef(id=pattern_name),\n        variables=variables,\n        images=[MediaAttachment(path=p, mime=\"image/jpeg\") for p in (images or [])],\n        params=CompletionParams(temperature=temperature, max_output_tokens=max_tokens),\n    )\n    resp = service.generate(req)\n    if resp.status != \"succeeded\" or not resp.result:\n        raise RuntimeError(f\"Generation failed: {resp.error}\")\n    return resp.result.text\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a09-v1-simplified/#definition-of-done-prototype-ready","title":"Definition of Done (Prototype Ready)","text":"<ol> <li><code>GenAIService.generate()</code> works end\u2011to\u2011end for:</li> <li>Text\u202f\u2192\u202fText (translation, summarization)</li> <li>Text\u202f+\u202fImage\u202f\u2192\u202fText (vision\u2011in)</li> <li>Constants enforce budget, context, and size limits.</li> <li>OpenAIAdapter functional; AnthropicAdapter skeleton compiles.</li> <li>JSON\u00a0mode yields <code>warnings=[\"json\u2011parse\u2011failed\"]</code> when needed.</li> <li>Correlation\u00a0ID and basic structured log emitted.</li> <li><code>openai_generate()</code> convenience API available for direct IDE/extension use and rapid testing.</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a09-v1-simplified/#development-milestones","title":"Development Milestones","text":"<ol> <li>Skeleton compile pass: domain + provider base + OpenAI adapter.  </li> <li>Pattern render + fingerprint (example pattern).  </li> <li>Generate() text flow + log + usage.  </li> <li>Add image support (Path).  </li> <li>Add JSON mode.  </li> <li>Add budget/context guards.  </li> <li>Add simple_generate().  </li> <li>Add Anthropic skeleton.</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a09-v1-simplified/#rationale","title":"Rationale","text":"<p>This approach enables a walking, testable skeleton within minimal engineering effort. It allows for immediate integration with VS\u00a0Code extensions and the PatternCatalog system, creating an environment where AI agents can progressively enhance their own supporting infrastructure.</p> <ul> <li>Service-level policy only in v1; no per-request overrides.  </li> <li>Fingerprinting used for observability hooks only, no caching yet.  </li> <li>Static price table in config; dynamic pricing registry deferred.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a09-v1-simplified/#status-next-steps","title":"Status &amp; Next Steps","text":"<ul> <li>Review for approval by TNH Scholar core devs.  </li> <li>Upon acceptance, this ADR governs the immediate implementation sequence for <code>GenAIService\u202fv1</code>.  </li> <li>Follow\u2011ups:  </li> <li>ADR\u2011A04 (Routing Rules &amp; Guardrails)  </li> <li>ADR\u2011A08 (Config/Params/Policy taxonomy) complete</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a11-model-parameters-fix/","title":"ADR-A11: Model Parameters and Strong Typing Fix","text":"<p>Mandates typed models for all GenAI parameters and forbids literal dict plumbing to align with Config/Params/Policy rules.</p> <p>Status: Proposed Date: 2025-10-19 Author: Aaron Solomon and GPT-5 (TNH-Scholar project)</p>"},{"location":"architecture/gen-ai-service/adr/adr-a11-model-parameters-fix/#context","title":"Context","text":"<p>During early implementation of the <code>GenAIService</code>, hardcoded string literals and ad\u2011hoc <code>dict</code> structures appeared in several layers (notably within provider adapters and pattern rendering). These elements conflicted with the project\u2019s established design principles of strong typing, configuration\u2011driven behavior, and zero literals in application logic.</p> <p>This ADR refines the model\u2011parameter handling strategy to align with ADR\u2011A08 (Config Params Policy) and to enforce consistency between the orchestrator, providers, and policy layers.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a11-model-parameters-fix/#decision","title":"Decision","text":"<ol> <li>All parameters, settings, and payloads exchanged between internal components must use typed models rather than <code>dict</code> objects.</li> <li>No hardcoded default values or strings will appear within application code paths. Defaults originate from:</li> <li><code>Settings</code> (<code>pydantic.BaseSettings</code>) configuration</li> <li>Policy files under <code>runtime_assets/policies</code></li> <li>Pattern metadata (e.g., <code>model_hint</code>, <code>default_params</code>)</li> <li>All provider communication uses typed transport models: <code>ProviderRequest</code>, <code>ProviderResponse</code>, and <code>Usage</code>.</li> <li>All message data uses the strongly\u2011typed <code>Message</code> and <code>Role</code> classes.</li> <li>Provider adapters perform the only dict conversions necessary at SDK boundaries.</li> <li>Parameters such as <code>temperature</code>, <code>max_output_tokens</code>, and <code>seed</code> are encapsulated in a typed <code>ResolvedParams</code> model derived from ADR\u2011A08.</li> <li>The orchestrator constructs a <code>ProviderRequest</code> from these models; no intermediate dicts are permitted.</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a11-model-parameters-fix/#details-of-implementation","title":"Details of Implementation","text":"<ul> <li>New models: <code>ProviderRequest</code>, <code>ProviderResponse</code>, and <code>Usage</code> were added to <code>models/transport.py</code>.</li> <li>CompletionParams and ResolvedParams models unify configuration and runtime parameters.</li> <li>Provider adapters (e.g., <code>OpenAIAdapter</code>) were refactored to remove literal defaults and use only values passed through typed parameters.</li> <li>SafetyGate, PatternCatalog, and Service now reference typed <code>Message</code> objects rather than dicts.</li> <li>All orchestrator flows depend on <code>Settings</code> for defaults and ADR\u2011A08 policy merges.</li> <li>The <code>Config.ParamsPolicy</code> merges precedence as: Call \u2192 Pattern \u2192 Policy \u2192 Settings.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a11-model-parameters-fix/#consequences","title":"Consequences","text":"<ul> <li>Ensures compile\u2011time and IDE\u2011level validation of all parameters.</li> <li>Reduces configuration drift between environments.</li> <li>Improves readability and debugging by eliminating untyped or ad\u2011hoc dict structures.</li> <li>Establishes a consistent contract between service layers (orchestrator \u2194 adapter \u2194 provider).</li> <li>Facilitates future automatic code generation and static analysis.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a11-model-parameters-fix/#related","title":"Related","text":"<ul> <li>ADR\u2011A08: Config Params Policy  </li> <li>ADR\u2011A09: Simplified V1 Implementation  </li> <li>Design Guide: Strong Typing and Abstraction Preferences (Section 1.3)</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/","title":"ADR-A12: Prompt System &amp; Fingerprinting Architecture (V1)","text":"<p>Renames Patterns to Prompts and adds fingerprinted domain objects so GenAIService can track provenance cleanly.</p> <ul> <li>Status: Accepted</li> <li>Date: 2025-02-04</li> <li>Supersedes: ADR-A02 \u2013 Pattern Catalog V1</li> <li>Related: ADR-P03 (Prompt rename), ADR-A01 (Object-Service Blueprint), ADR-A11 (Model Parameters)</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#1-context","title":"1. Context","text":"<p>The GenAIService originally adopted a Pattern Catalog abstraction (ADR-A02), built around an <code>patterns.py</code> system, part of the TNH Scholar suite, from the AI Text Processing module. During implementation, two major issues emerged:</p> <ol> <li>Terminology friction \u2014 the industry-standard term is Prompt, and the name \u201cPattern\u201d conflicted with Python\u2019s regular expression <code>Pattern</code> class and caused ambiguity.</li> <li>Structural mismatch \u2014 ADR-A02 required the adapter to produce <code>RenderedPrompt + Provenance</code>.    However, provenance is a domain concern and should not be constructed inside the adapter, which sits at the boundary between external prompt files and domain objects.</li> <li>Lack of a clean identity model \u2014 The system needed a domain-level representation of \u201cwhat was rendered?\u201d for traceability, hashing, and future reproducibility.</li> </ol> <p>During refactoring, the project adopted <code>prompts.py</code> as the new API-facing module and rebuilt the adapter using modern conventions. This raised the need for a new ADR that defines the correct architectural shape for Prompt Catalog integration and Fingerprinting.</p> <p>This ADR replaces ADR-A02 completely.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#2-decision","title":"2. Decision","text":""},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#21-rename-pattern-to-prompt-everywhere","title":"2.1 Rename \u201cPattern\u201d to \u201cPrompt\u201d everywhere","text":"<ul> <li><code>patterns.py</code> \u2192 <code>prompts.py</code> </li> <li><code>Pattern</code> \u2192 <code>Prompt</code> </li> <li><code>PatternCatalog</code> \u2192 <code>PromptCatalog</code> </li> <li><code>patterns_adapter.py</code> \u2192 <code>prompts_adapter.py</code> </li> <li>All public APIs, internal adapters, and domain mapping use the term Prompt.</li> </ul> <p>This aligns with industry standards and removes ambiguity with Python\u2019s built\u2011in regex <code>Pattern</code> type.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#22-adapter-returns-pure-domain-objects-only","title":"2.2 Adapter returns pure domain objects only","text":"<p>The adapter is the boundary between:</p> <ul> <li>the external prompt disk/API system (<code>prompts.py</code>), and</li> <li>internal domain structures (<code>RenderedPrompt</code>, <code>Fingerprint</code>).</li> </ul> <p>Therefore:</p> <pre><code>PromptsAdapter.render(key, request)  \n    \u2192 (RenderedPrompt, Fingerprint)\n</code></pre> <p>The adapter MUST NOT construct Provenance. It only maps the prompt definition to domain structures and calculates the fingerprint.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#23-introduce-a-domain-value-object-fingerprint","title":"2.3 Introduce a domain value object: Fingerprint","text":"<p>Fingerprint is now a first-class domain model located in <code>domain.py</code>. It describes exactly what was rendered and is the identity anchor for provenance.</p> <pre><code>class Fingerprint(BaseModel):\n    schema_version: Literal[\"fp-1\"] = \"fp-1\"\n    prompt_key: str\n    prompt_name: str\n    prompt_base_path: str\n    prompt_content_hash: str\n    variables_hash: str\n    user_string_hash: str\n</code></pre> <p>Properties:</p> <ul> <li><code>schema_version</code> supports future evolution.</li> <li>Tracks prompt location, name, and content hashes.</li> <li>Includes template variable hash and user string hash.</li> <li>Immutable and serializable.</li> </ul> <p>The adapter constructs this object. The domain and service layers treat it as opaque identity metadata.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#24-move-provenance-construction-into-genaiservice","title":"2.4 Move Provenance construction into <code>GenAIService</code>","text":"<p>Provenance belongs to the business layer. It is constructed after the provider call completes, because it must include execution details (provider, model, retry count, timestamps, etc.).</p> <p><code>Provenance</code> lives in <code>domain.py</code> and now includes a Fingerprint:</p> <pre><code>class Provenance(BaseModel):\n    provider: str\n    model: str\n    sdk_version: str | None = None\n    started_at: datetime\n    finished_at: datetime\n    attempt_count: int = 1\n    fingerprint: Fingerprint\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#25-provenancepy-becomes-a-helperbuilder-module","title":"2.5 <code>provenance.py</code> becomes a helper/builder module","text":"<p>It now defines:</p> <pre><code>def build_provenance(\n    *,\n    fingerprint: Fingerprint,\n    provider: str,\n    model: str,\n    sdk_version: str | None,\n    started_at: datetime,\n    finished_at: datetime,\n    attempt_count: int,\n) -&gt; Provenance:\n    ...\n</code></pre> <p>This is the only approved way to construct a Provenance object.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#26-location-of-responsibilities","title":"2.6 Location of responsibilities","text":"Component Responsibility <code>prompts.py</code> Load/save Prompt objects from disk; provide template application. <code>prompts_adapter.py</code> Map external Prompt \u2192 RenderedPrompt + Fingerprint. No provenance. No provider logic. <code>domain.py</code> Define all domain models: RenderedPrompt, Fingerprint, Provenance, Message, Role, etc. <code>provenance.py</code> Builder for Provenance; no external API awareness. <code>service.py</code> (GenAIService) Orchestrates workflow, chooses model/provider, calls adapter, provider, and constructs final Provenance."},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#27-hashing-implementation-details-v1","title":"2.7 Hashing implementation details (V1)","text":"<p>The following helper functions exist in <code>infra/tracking/fingerprint.py</code>:</p> <pre><code>def hash_prompt_bytes(b: bytes) -&gt; str: ...\ndef hash_vars(vars_dict: dict[str, Any]) -&gt; str: ...\ndef hash_user_string(s: str) -&gt; str: ...\n</code></pre> <p>The adapter must use these functions when building the Fingerprint.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#3-consequences","title":"3. Consequences","text":""},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#31-cleaner-boundary","title":"3.1 Cleaner boundary","text":"<p>The external prompt system is now isolated behind a single mapping layer. Domain code never touches Prompt objects, disk paths, or parsing.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#32-provenance-is-now-domain-pure","title":"3.2 Provenance is now domain-pure","text":"<p>Provenance now depends only on:</p> <ul> <li>domain objects,</li> <li>runtime execution data.</li> </ul> <p>No external API leakage.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#33-fingerprinting-is-fully-replaceable","title":"3.3 Fingerprinting is fully replaceable","text":"<p>Future fingerprinting strategies (commit hash, dataset ID, model parameters, etc.) can evolve without altering adapter signatures or provenance structures.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#34-adr-a02-superseded","title":"3.4 ADR-A02 superseded","text":"<p>The PatternCatalog architecture is no longer used and should not be referenced except for historical context.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#4-status-of-adr-a02","title":"4. Status of ADR-A02","text":"<p>ADR-A02 remains in the repository for historical reference but is now marked:</p> <pre><code>Status: Superseded by ADR-A12\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#5-future-work","title":"5. Future Work","text":"<ul> <li>ADR-P03 will document the <code>patterns.py \u2192 prompts.py</code> rename in the general codebase.</li> <li>A future ADR may refine Fingerprint schema or introduce reproducible \u201crender bundles.\u201d</li> <li>A later version may define a PromptCatalog indexing format or in-memory caching strategy.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#6-decision-summary-implementation-ready","title":"6. Decision Summary (Implementation-Ready)","text":"<p>Implement immediately:</p> <ol> <li>Add <code>Fingerprint</code> and revised <code>Provenance</code> to <code>domain.py</code>.</li> <li>Update <code>PromptsAdapter.render()</code> to return <code>(RenderedPrompt, Fingerprint)</code>.</li> <li>Move all provenance construction to <code>GenAIService</code>.</li> <li>Update <code>provenance.py</code> to provide a pure builder.</li> <li>Use the hashing utilities in <code>infra/tracking/fingerprint.py</code>.</li> <li>Update all imports and method signatures accordingly.</li> <li>Mark ADR-A02 as \u201cSuperseded.\u201d</li> </ol> <p>This defines the authoritative V1 architecture for Prompt + Fingerprinting inside the Gen AI Service.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a12-prompt-system-fingerprinting-v1/#7-addendum-v1-prompt-policy-scope","title":"7. Addendum: V1 Prompt Policy Scope","text":"<p>For the initial V1 walking skeleton:</p> <ul> <li><code>apply_policy</code> is effectively a pass-through:</li> <li>It uses only the <code>RenderRequest</code> intent and explicit model override,     plus global/default settings.</li> <li>It does not use prompt metadata such as <code>model_hint</code> or <code>default_params</code>.</li> <li>The PromptsAdapter returns only <code>(RenderedPrompt, Fingerprint)</code>; no additional   prompt metadata is surfaced to the policy layer in V1.</li> <li>Prompt-aware policy (e.g., model selection or temperature tuned by prompt metadata)   is explicitly deferred to a future ADR.</li> </ul> <p>A later ADR (e.g Prompt Policy V1) will introduce either:</p> <ul> <li>a <code>RenderResult</code>/<code>PromptMeta</code> domain object, or</li> <li>a revised domain model, to connect prompt metadata to policy decisions in a principled way.</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/","title":"ADR-A13: Migrate All OpenAI Interactions to GenAIService","text":"<p>Consolidates all OpenAI usage onto GenAI Service to eliminate the brittle legacy singleton client and keep parity.</p> <p>Status: Proposed Date: 2025-11-16 Author: Aaron Solomon with Claude AI (Sonnet 4.5) Supersedes: Legacy openai_interface module usage</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#context","title":"Context","text":"<p>TNH Scholar currently has two parallel implementations for OpenAI interactions:</p> <ol> <li>Legacy System (<code>openai_interface/</code>):</li> <li>Singleton pattern with global state</li> <li>Direct OpenAI SDK calls</li> <li>Import-time <code>load_dotenv()</code> side effects</li> <li>Used by: ai_text_processing, audio_processing, journal_processing, CLI tools</li> <li>~27KB monolithic openai_interface.py file</li> <li>Batch processing support (run_oa_batch_jobs.py)</li> <li> <p>Token counting utilities</p> </li> <li> <p>Modern System (<code>gen_ai_service/</code>):</p> </li> <li>Object-Service pattern (ADR-A01)</li> <li>Typed domain models with Pydantic</li> <li>Provider abstraction layer</li> <li>Provenance tracking and fingerprinting</li> <li>Retry logic with tenacity</li> <li>Proper exception handling</li> <li>Currently used only in tests</li> </ol> <p>This duplication creates several problems:</p> <p>Code Divergence: Bug fixes and improvements must be applied to both systems, leading to inconsistency and maintenance burden.</p> <p>Configuration Complexity: Two different configuration approaches (singleton with env vars vs. Pydantic Settings) create confusion and potential bugs.</p> <p>Limited Extensibility: Legacy code is tightly coupled to OpenAI, making multi-provider support (Anthropic, etc.) difficult.</p> <p>Testing Challenges: Singleton pattern makes unit testing harder; need to mock global state.</p> <p>Architectural Inconsistency: New features use GenAIService while existing features use legacy client, creating a fragmented codebase.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#decision","title":"Decision","text":"<p>We will migrate all OpenAI interactions to use GenAIService as the exclusive interface for LLM provider calls.</p> <p>This migration will:</p> <ol> <li>Establish GenAIService as the Single Point of Entry for all LLM interactions</li> <li>Delete the legacy openai_interface module entirely after migration</li> <li>Update all consumers (ai_text_processing, audio_processing, CLI tools) to use GenAIService</li> <li>Preserve essential functionality (batch processing, token counting) by implementing adapters or utilities within gen_ai_service</li> <li>Maintain no backward compatibility - this is a breaking change requiring all callsites to update</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#architectural-principle","title":"Architectural Principle","text":"<p>All consumers must go through GenAIService, not use openai_client directly.</p> <p>This creates maximum flexibility for: - Multi-provider routing - Policy application - Safety filtering - Provenance tracking - Retry/fallback logic - Future enhancements</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#implementation-plan","title":"Implementation Plan","text":""},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-1-preparation-groundwork","title":"Phase 1: Preparation &amp; Groundwork","text":"<p>Objective: Ensure GenAIService has feature parity with legacy system</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#11-add-missing-utilities-to-gen_ai_service","title":"1.1 Add Missing Utilities to gen_ai_service","text":"<p>Location: <code>src/tnh_scholar/gen_ai_service/utils/</code></p> <ul> <li> Token counting utilities</li> <li>Extract from <code>openai_interface.token_count()</code></li> <li>Use tiktoken with model-specific encodings</li> <li>Support batch token counting for lists</li> <li> <p>File: <code>gen_ai_service/utils/token_utils.py</code></p> </li> <li> <p> Response extraction helpers</p> </li> <li>Equivalent to <code>get_completion_content()</code>, <code>get_completion_object()</code></li> <li>Extract text from CompletionEnvelope</li> <li>Extract structured objects from responses</li> <li>File: <code>gen_ai_service/utils/response_utils.py</code></li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#12-implement-batch-processing-support","title":"1.2 Implement Batch Processing Support","text":"<p>Location: <code>src/tnh_scholar/gen_ai_service/batch/</code></p> <p>The legacy system supports OpenAI Batch API via <code>run_single_batch()</code> and <code>run_oa_batch_jobs.py</code>. We need equivalent functionality.</p> <p>Options:</p> <p>Option A: Implement within GenAIService - Add <code>batch_generate()</code> method to GenAIService - Create <code>BatchRequest</code> and <code>BatchResponse</code> domain models - Manage batch job lifecycle (submit, poll, retrieve) - PRO: Unified interface, better abstraction - CON: More initial implementation work</p> <p>Option B: Create separate BatchService - Standalone service for batch operations - Still uses GenAIService internally for individual items - PRO: Separation of concerns, clearer responsibilities - CON: Another service to manage</p> <p>Option C: Adapter pattern - Create <code>LegacyBatchAdapter</code> that wraps GenAIService - Maintains similar interface to <code>run_single_batch()</code> - PRO: Easier migration, minimal refactoring - CON: Less clean architecture</p> <p>Decision: Use Option A (implement within GenAIService) - Better long-term architecture - Batch is a core LLM interaction pattern - Allows for future multi-provider batch support</p> <p>Implementation: <pre><code># gen_ai_service/service.py\ndef batch_generate(\n    self,\n    requests: List[RenderRequest],\n    batch_params: Optional[BatchParams] = None\n) -&gt; BatchResult:\n    \"\"\"Submit batch of requests to provider's batch API.\"\"\"\n    # Create batch job\n    # Poll for completion\n    # Return results with provenance\n</code></pre></p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#13-create-migration-adapters","title":"1.3 Create Migration Adapters","text":"<p>Location: <code>src/tnh_scholar/gen_ai_service/adapters/</code></p> <p>To ease migration, create adapters that provide legacy-like interfaces backed by GenAIService:</p> <p>File: <code>gen_ai_service/adapters/simple_completion.py</code> <pre><code>def simple_completion(\n    system_message: str,\n    user_message: str,\n    model: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    response_format: Optional[Type[BaseModel]] = None,\n) -&gt; Union[str, BaseModel]:\n    \"\"\"\n    Simple completion interface for easier migration.\n\n    Wraps GenAIService.generate() with a simpler API.\n    \"\"\"\n    # Create GenAIService\n    # Build RenderRequest\n    # Call generate()\n    # Extract and return result\n</code></pre></p> <p>This allows gradual migration with minimal disruption.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-2-migrate-core-modules","title":"Phase 2: Migrate Core Modules","text":"<p>Objective: Update core text processing and audio processing modules</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#21-migrate-ai_text_processingopenai_process_interfacepy","title":"2.1 Migrate ai_text_processing/openai_process_interface.py","text":"<p>Current dependencies: <pre><code>from tnh_scholar.openai_interface import (\n    get_completion_content,\n    get_completion_object,\n    run_immediate_completion_simple,\n    run_single_batch,\n    token_count,\n)\n</code></pre></p> <p>Migration strategy: 1. Replace with GenAIService import 2. Update <code>openai_process_text()</code> to use GenAIService 3. Use simple_completion adapter initially, then refactor to proper RenderRequest</p> <p>Files to update: - <code>ai_text_processing/openai_process_interface.py</code> - <code>ai_text_processing/ai_text_processing.py</code> (imports token_count)</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#22-migrate-audio_processing","title":"2.2 Migrate audio_processing","text":"<p>Files to update: Removed - <code>audio_processing/transcription_legacy.py</code></p> <p>Note: This appears to be legacy code (filename suggests it). Evaluate if it should be deleted instead of migrated.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#23-migrate-journal_processing","title":"2.3 Migrate journal_processing","text":"<p>Files to update: - <code>journal_processing/journal_process.py</code> (28KB - largest consumer)</p> <p>Strategy: - This is a large file with complex processing - Create specialized prompts in pattern catalog - Break into smaller functions using GenAIService - Consider refactoring as part of Priority 3 task (monolithic module refactoring)</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-3-migrate-cli-tools","title":"Phase 3: Migrate CLI Tools","text":"<p>Objective: Update all CLI tools to use GenAIService</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#31-migrate-token-count-cli","title":"3.1 Migrate token-count CLI","text":"<p>File: <code>cli_tools/token_count/token_count.py</code></p> <p>Current: Uses <code>openai_interface.token_count()</code> Update: Use <code>gen_ai_service/utils/token_utils.py</code></p> <p>Simple utility migration.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#32-update-other-cli-tools","title":"3.2 Update other CLI tools","text":"<p>Verify usage: Check if any other CLI tools import openai_interface - Most CLI tools currently use legacy ai_text_processing - They'll automatically use GenAIService once ai_text_processing is migrated - No direct updates needed</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-4-update-tests","title":"Phase 4: Update Tests","text":"<p>Objective: Migrate or delete legacy tests</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#41-migrate-useful-tests","title":"4.1 Migrate useful tests","text":"<p>File: <code>tests/openai_interface/test_openai_interface.py</code></p> <p>Strategy: - Review 19 existing tests - Migrate valuable test cases to test GenAIService equivalents - Focus on behavior, not implementation - Add to <code>tests/gen_ai_service/</code></p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#42-delete-obsolete-tests","title":"4.2 Delete obsolete tests","text":"<p>Tests tightly coupled to singleton pattern or internal implementation details can be deleted.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-5-handle-notebooks","title":"Phase 5: Handle Notebooks","text":"<p>Objective: Update or document notebook status</p> <p>Files: - <code>notebooks/ai_text_processing/section_processing_tests.ipynb</code> - <code>notebooks/video_processing/postprocessing_english.ipynb</code> - <code>notebooks/video_processing/postprocessing_viet.ipynb</code></p> <p>Strategy: - Add migration notice at top of each notebook - Either update to use GenAIService or mark as legacy/archived - Consider moving to <code>notebooks/legacy/</code> directory</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-6-deletion-cleanup","title":"Phase 6: Deletion &amp; Cleanup","text":"<p>Objective: Remove legacy code and update documentation</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#61-delete-openai_interface-module","title":"6.1 Delete openai_interface module","text":"<p>Once all migrations are complete: <pre><code>rm -rf src/tnh_scholar/openai_interface/\n</code></pre></p> <p>Files to delete: - <code>openai_interface/openai_interface.py</code> (27KB) - <code>openai_interface/run_oa_batch_jobs.py</code> - <code>openai_interface/__init__.py</code> - <code>openai_interface/gpt_batch_files/</code> (if present) - <code>tests/openai_interface/</code></p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#62-update-imports","title":"6.2 Update imports","text":"<p>Search for any remaining imports: <pre><code>grep -r \"from tnh_scholar.openai_interface\" src/\ngrep -r \"import.*openai_interface\" src/\n</code></pre></p> <p>Ensure all are removed.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#63-update-documentation","title":"6.3 Update documentation","text":"<ul> <li>Update README.md to mention only GenAIService</li> <li>Update architecture docs</li> <li>Create migration guide for any external users</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#implementation-checklist","title":"Implementation Checklist","text":""},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-1-preparation","title":"Phase 1: Preparation \u2705 / \ud83d\udea7 / \u2b1c","text":"<ul> <li> Create <code>gen_ai_service/utils/token_utils.py</code></li> <li> Implement <code>token_count(text, model)</code></li> <li> Implement <code>token_count_messages(messages, model)</code></li> <li> Implement <code>token_count_file(path, model)</code></li> <li> <p> Add tests</p> </li> <li> <p> Create <code>gen_ai_service/utils/response_utils.py</code></p> </li> <li> Implement <code>extract_text(envelope: CompletionEnvelope) -&gt; str</code></li> <li> Implement <code>extract_object(envelope: CompletionEnvelope) -&gt; BaseModel</code></li> <li> <p> Add tests</p> </li> <li> <p> Create <code>gen_ai_service/batch/</code></p> </li> <li> Define <code>BatchRequest</code>, <code>BatchResponse</code>, <code>BatchParams</code> models</li> <li> Implement <code>GenAIService.batch_generate()</code></li> <li> Add batch job polling logic</li> <li> <p> Add tests for batch processing</p> </li> <li> <p> Create <code>gen_ai_service/adapters/simple_completion.py</code></p> </li> <li> Implement <code>simple_completion()</code> function</li> <li> Implement <code>batch_completion()</code> function</li> <li> <p> Add tests</p> </li> <li> <p> Update documentation</p> </li> <li> Document new utilities</li> <li> Create migration guide draft</li> <li> Add examples</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-2-core-modules","title":"Phase 2: Core Modules \u2b1c","text":"<ul> <li> Migrate <code>ai_text_processing/openai_process_interface.py</code></li> <li> Update imports</li> <li> Refactor <code>openai_process_text()</code> to use GenAIService</li> <li> Update tests</li> <li> <p> Verify all callsites work</p> </li> <li> <p> Migrate <code>ai_text_processing/ai_text_processing.py</code></p> </li> <li> Update token_count imports</li> <li> Verify ProcessedSection and related classes work</li> <li> <p> Run integration tests</p> <ul> <li> Evaluate <code>audio_processing/transcription_legacy.py</code> (deleted)</li> <li> Decide: migrate or delete?</li> <li> If migrate: update to use GenAIService</li> <li> If delete: remove file and references</li> </ul> </li> <li> <p> Migrate <code>journal_processing/journal_process.py</code></p> </li> <li> Audit current usage patterns</li> <li> Create necessary prompts in pattern catalog</li> <li> Update to use GenAIService</li> <li> Consider refactoring (large file)</li> <li> Add tests</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-3-cli-tools","title":"Phase 3: CLI Tools \u2b1c","text":"<ul> <li> Migrate <code>cli_tools/token_count/token_count.py</code></li> <li> Update imports to use token_utils</li> <li> Test CLI command</li> <li> <p> Update CLI documentation</p> </li> <li> <p> Audit other CLI tools</p> </li> <li> Verify no direct openai_interface usage</li> <li> Test end-to-end workflows</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-4-tests","title":"Phase 4: Tests \u2b1c","text":"<ul> <li> Review <code>tests/openai_interface/test_openai_interface.py</code></li> <li> Identify valuable test cases</li> <li> Migrate to <code>tests/gen_ai_service/</code></li> <li> <p> Rewrite using GenAIService</p> </li> <li> <p> Add new tests</p> </li> <li> Test migration adapters</li> <li> Test batch processing</li> <li> Test token utilities</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-5-notebooks","title":"Phase 5: Notebooks \u2b1c","text":"<ul> <li> Update <code>notebooks/ai_text_processing/section_processing_tests.ipynb</code></li> <li> Update <code>notebooks/video_processing/postprocessing_english.ipynb</code></li> <li> Update <code>notebooks/video_processing/postprocessing_viet.ipynb</code></li> <li> Consider creating <code>notebooks/legacy/</code> directory</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#phase-6-deletion-cleanup_1","title":"Phase 6: Deletion &amp; Cleanup \u2b1c","text":"<ul> <li> Final verification</li> <li> Search for any remaining openai_interface imports</li> <li> Run full test suite</li> <li> <p> Test all CLI tools end-to-end</p> </li> <li> <p> Delete legacy code</p> </li> <li> Delete <code>src/tnh_scholar/openai_interface/</code></li> <li> Delete <code>tests/openai_interface/</code></li> <li> <p> Update .gitignore if needed</p> </li> <li> <p> Update documentation</p> </li> <li> Update README.md</li> <li> Update architecture docs</li> <li> Create MIGRATION.md guide</li> <li> <p> Update CHANGELOG.md</p> </li> <li> <p> Update TODO.md</p> </li> <li> Mark task #5 as complete</li> <li> Update progress summary</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#benefits","title":"Benefits","text":"<ol> <li>Single Source of Truth: All LLM interactions go through one well-designed service</li> <li>Better Testability: No singletons, proper dependency injection</li> <li>Multi-Provider Ready: GenAIService already has provider abstraction</li> <li>Provenance &amp; Observability: All completions tracked with metadata</li> <li>Type Safety: Pydantic models throughout</li> <li>Better Error Handling: Structured exceptions vs. string errors</li> <li>No Global State: All configuration passed explicitly</li> <li>Retry Logic: Built-in retry with exponential backoff</li> <li>Easier Maintenance: Single codebase to maintain</li> <li>Future-Proof: Architecture supports upcoming features (safety, routing, caching)</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#risks-mitigation","title":"Risks &amp; Mitigation","text":""},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#risk-1-breaking-changes","title":"Risk 1: Breaking Changes","text":"<p>Impact: Existing code will break Mitigation: - Provide migration adapters for smooth transition - Document all changes thoroughly - Migrate incrementally with tests at each step</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#risk-2-batch-processing-complexity","title":"Risk 2: Batch Processing Complexity","text":"<p>Impact: Batch API integration is non-trivial Mitigation: - Start with simple batch implementation - Test thoroughly with small batches - Document limitations clearly - Consider async patterns for better UX</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#risk-3-performance-differences","title":"Risk 3: Performance Differences","text":"<p>Impact: New code might have different performance characteristics Mitigation: - Benchmark before/after - Monitor token usage and costs - Profile slow operations - Optimize hot paths</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#risk-4-missing-features","title":"Risk 4: Missing Features","text":"<p>Impact: Legacy system might have features we haven't identified Mitigation: - Thorough code audit before deletion - Keep legacy code in git history - Document all legacy functionality - Create feature parity checklist</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#timeline-estimate","title":"Timeline Estimate","text":"<p>Phase 1 (Preparation): 2-3 days - Utilities and adapters are straightforward - Batch processing needs careful design</p> <p>Phase 2 (Core Modules): 3-4 days - journal_process.py is complex (28KB) - Need to create prompts in catalog - Thorough testing required</p> <p>Phase 3 (CLI Tools): 1 day - Mostly indirect through ai_text_processing - Simple utility migration</p> <p>Phase 4 (Tests): 1-2 days - Migrate valuable tests - Add new coverage</p> <p>Phase 5 (Notebooks): 1 day - Update or document</p> <p>Phase 6 (Cleanup): 1 day - Verification and documentation</p> <p>Total: ~9-13 days of focused work</p> <p>Can be done incrementally while maintaining working system.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#success-criteria","title":"Success Criteria","text":"<ul> <li> All uses of <code>openai_interface</code> removed from src/</li> <li> All uses of <code>openai_interface</code> removed from tests/</li> <li> <code>openai_interface/</code> directory deleted</li> <li> All CLI tools working with GenAIService</li> <li> All tests passing</li> <li> Test coverage maintained or improved</li> <li> Documentation updated</li> <li> No regression in functionality</li> <li> Performance acceptable (within 10% of legacy)</li> <li> Migration guide written</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#related-adrs","title":"Related ADRs","text":"<ul> <li>ADR-A01: Object-Service Pattern - Architectural foundation</li> <li>ADR-A02: Pattern Catalog V1 - Prompt management</li> <li>ADR-A08: Config Params Policy - Configuration approach</li> <li>ADR-A09: V1 Simplified - Current implementation</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#references","title":"References","text":"<ul> <li>Legacy implementation: <code>src/tnh_scholar/openai_interface/openai_interface.py</code></li> <li>Modern implementation: <code>src/tnh_scholar/gen_ai_service/service.py</code></li> <li>Provider abstraction: <code>src/tnh_scholar/gen_ai_service/providers/</code></li> <li>Existing tests: <code>tests/gen_ai_service/test_service.py</code></li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a13-migrate-openai-to-genaiservice/#notes","title":"Notes","text":"<p>This is a significant refactoring but sets the foundation for: - Multi-provider support (Anthropic, etc.) - Advanced features (caching, routing, safety) - Better observability and debugging - Cleaner, more maintainable codebase</p> <p>The investment in proper migration now will pay dividends as we add new LLM providers and capabilities.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/","title":"ADR-A14: File-Based Registry System for Provider Metadata","text":"<p>Establishes a human-editable, file-based registry for model capabilities, pricing tables, and provider metadata using JSON with Comments (JSONC) format to align with VS Code's native configuration system.</p> <ul> <li>Filename: <code>adr-a14-file-based-registry-system.md</code></li> <li>Heading: <code># ADR-A14: File-Based Registry System for Provider Metadata</code></li> <li>Status: Proposed</li> <li>Date: 2025-12-10</li> <li>Authors: Aaron Solomon, Anthropic Claude Sonnet 4.5</li> <li>Owner: aaronksolomon</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#adr-editing-policy","title":"ADR Editing Policy","text":"<p>IMPORTANT: This ADR is in proposed status. We may rewrite or edit the document as needed to refine the design. Once accepted and implementation begins, only addenda may be added.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#context","title":"Context","text":""},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#current-state","title":"Current State","text":"<p>Multiple modules currently contain hardcoded metadata that should be centralized and externalized:</p> <ol> <li>Pricing Constants (safety_gate.py:30):</li> </ol> <pre><code>_PRICE_PER_1K_TOKENS = 0.005  # placeholder until price tables are wired\n</code></pre> <ol> <li>Model Capabilities (model_router.py:25-34):</li> </ol> <pre><code>_MODEL_CAPABILITIES: Mapping[str, _Capability] = {\n    \"gpt-5o-mini\": _Capability(vision=True, structured=True),\n    \"gpt-5o\": _Capability(vision=True, structured=True),\n    # ... hardcoded capability map\n}\n</code></pre> <ol> <li>Context Limits (token_utils.py):</li> </ol> <pre><code>MODEL_CONTEXT_LIMITS = [\n    (\"gpt-5\", 128_000),\n    (\"gpt-4\", 128_000),\n    # ... hardcoded context limits\n]\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#problems-with-current-approach","title":"Problems with Current Approach","text":"<ol> <li>Scattered Metadata: Provider information duplicated across multiple modules</li> <li>Manual Updates: Pricing and capability changes require code modifications</li> <li>No Single Source of Truth: Inconsistencies between modules</li> <li>Poor Discoverability: Users can't easily see available models or pricing</li> <li>Testing Difficulty: Can't easily swap registries for testing</li> <li>No Audit Trail: Changes to pricing/capabilities not tracked</li> <li>VS Code Integration Gap: No alignment with upcoming VS Code extension configuration system</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#external-data-availability-research","title":"External Data Availability Research","text":"<p>OpenAI Pricing API: After investigation (Issue #2074), OpenAI does not provide a programmatic pricing endpoint. Community requests exist (OpenAI Forum) but the feature is marked \"not planned.\"</p> <p>Available Alternatives:</p> <ul> <li>Official pricing page: https://openai.com/api/pricing/</li> <li>Usage/Costs API: Provides spend tracking but not pricing rates</li> <li>Models list endpoint: Lists models but excludes pricing/capabilities</li> </ul> <p>Implication: We must maintain our own pricing data with manual or semi-automated updates from the official pricing page.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#vs-code-configuration-system-research","title":"VS Code Configuration System Research","text":"<p>VS Code Native Format: VS Code uses JSON with Comments (JSONC) for all configuration files:</p> <ul> <li><code>settings.json</code>, <code>tasks.json</code>, <code>launch.json</code> use JSONC format</li> <li>JSONC supports <code>//</code> and <code>/* */</code> comments plus trailing commas</li> <li>JSON Schema enables IntelliSense and validation in VS Code</li> <li>Extension contribution system uses JSON/JSONC exclusively</li> </ul> <p>TNH Scholar VS Code Integration (ADR-VSC01):</p> <ul> <li>Workspace config: <code>.vscode/tnh-scholar.json</code></li> <li><code>tnh-gen</code> CLI will consume JSON configs</li> <li>Aligning registry format creates ecosystem consistency</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#referenced-in-prior-adrs","title":"Referenced in Prior ADRs","text":"<ul> <li>ADR-A08: \"Future: provider registry, dynamic pricing, and per-model metadata\" (line 82)</li> <li>ADR-A09: \"Static price table in config; dynamic pricing registry deferred\" (line 103)</li> <li>ADR-OS01: Multiple TODOs about externalizing capability constants</li> <li>ADR-VSC01: VS Code integration strategy using JSON configuration</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#decision","title":"Decision","text":""},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#registry-architecture","title":"Registry Architecture","text":"<p>Implement a file-based registry system using JSON with Comments (JSONC) format to align with VS Code's native configuration system, enabling seamless integration with the upcoming VS Code extension and providing excellent IDE tooling support.</p>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#core-design-principles","title":"Core Design Principles","text":"<ol> <li>VS Code Native: JSONC format matches VS Code's <code>settings.json</code>, <code>package.json</code> conventions</li> <li>Human-Editable First: JSONC supports comments and trailing commas for readability</li> <li>IDE Integration: JSON Schema enables autocomplete and validation in VS Code</li> <li>Version-Controlled: Registry files live in <code>runtime_assets/registries/</code> and are committed to git</li> <li>Layered Precedence: User overrides \u2192 project defaults \u2192 system defaults</li> <li>Schema-Validated: All registry files validated via Pydantic models + JSON Schema</li> <li>Auto-Update Capable: Optional scripts to fetch and update from stable URLs</li> <li>Strongly Typed: Registry loader returns typed domain models, never dicts</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#file-structure","title":"File Structure","text":"<pre><code>runtime_assets/\n  registries/\n    providers/\n      openai.jsonc             # OpenAI models, pricing, capabilities\n      anthropic.jsonc          # Anthropic models (future)\n      schema.py                # Pydantic validation schemas\n      schema.json              # JSON Schema for VS Code autocomplete\n    overrides/                 # User-editable overrides (gitignored)\n      pricing-overrides.jsonc  # Local price adjustments\n      capability-overrides.jsonc\n    .registry-metadata.json    # Last-update timestamps, sources\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#registry-schema-providersopenaijsonc","title":"Registry Schema (providers/openai.jsonc)","text":"<pre><code>// runtime_assets/registries/providers/openai.jsonc\n{\n  \"$schema\": \"./schema.json\",\n  \"schema_version\": \"1.0\",\n  \"provider\": \"openai\",\n  \"last_updated\": \"2025-12-10\",\n  \"source_url\": \"https://openai.com/api/pricing/\",\n  \"update_method\": \"manual\",  // or \"auto-scrape\" when implemented\n\n  // Default provider settings\n  \"defaults\": {\n    \"base_url\": \"https://api.openai.com/v1\",\n    \"timeout_s\": 60.0,\n    \"max_retries\": 3\n  },\n\n  // Model registry\n  \"models\": {\n    \"gpt-5-mini\": {\n      \"display_name\": \"GPT-5 Mini\",\n      \"family\": \"gpt-5\",\n      \"capabilities\": {\n        \"vision\": true,\n        \"structured_output\": true,\n        \"function_calling\": true,\n        \"streaming\": true\n      },\n      \"context_window\": 128000,\n      \"max_output_tokens\": 16384,\n      \"pricing\": {\n        \"input_per_1k\": 0.00015,   // $0.15 per 1M input tokens\n        \"output_per_1k\": 0.0006,   // $0.60 per 1M output tokens\n        \"cached_input_per_1k\": 0.000075  // 50% discount for cached\n      },\n      \"training_cutoff\": \"2024-10\",\n      \"released\": \"2024-11-20\",\n      \"deprecated\": false,\n      \"aliases\": [\"gpt-5-mini-latest\"]\n    },\n\n    \"gpt-5o\": {\n      \"display_name\": \"GPT-5 Optimized\",\n      \"family\": \"gpt-5\",\n      \"capabilities\": {\n        \"vision\": true,\n        \"structured_output\": true,\n        \"function_calling\": true,\n        \"streaming\": true\n      },\n      \"context_window\": 128000,\n      \"max_output_tokens\": 16384,\n      \"pricing\": {\n        \"input_per_1k\": 0.0025,\n        \"output_per_1k\": 0.01,\n        \"cached_input_per_1k\": 0.00125\n      },\n      \"training_cutoff\": \"2024-10\",\n      \"released\": \"2024-11-20\",\n      \"deprecated\": false,\n      \"aliases\": [\"gpt-5o-latest\"]\n    },\n\n    \"gpt-4o\": {\n      \"display_name\": \"GPT-4 Optimized\",\n      \"family\": \"gpt-4\",\n      \"capabilities\": {\n        \"vision\": true,\n        \"structured_output\": true,\n        \"function_calling\": true,\n        \"streaming\": true\n      },\n      \"context_window\": 128000,\n      \"max_output_tokens\": 16384,\n      \"pricing\": {\n        \"input_per_1k\": 0.0025,\n        \"output_per_1k\": 0.01,\n        \"cached_input_per_1k\": 0.00125\n      },\n      \"training_cutoff\": \"2023-10\",\n      \"released\": \"2024-05-13\",\n      \"deprecated\": false\n    },\n\n    \"gpt-4o-mini\": {\n      \"display_name\": \"GPT-4o Mini\",\n      \"family\": \"gpt-4\",\n      \"capabilities\": {\n        \"vision\": true,\n        \"structured_output\": true,\n        \"function_calling\": true,\n        \"streaming\": true\n      },\n      \"context_window\": 128000,\n      \"max_output_tokens\": 16384,\n      \"pricing\": {\n        \"input_per_1k\": 0.00015,\n        \"output_per_1k\": 0.0006,\n        \"cached_input_per_1k\": 0.000075\n      },\n      \"training_cutoff\": \"2023-10\",\n      \"released\": \"2024-07-18\",\n      \"deprecated\": false\n    },\n\n    \"gpt-3.5-turbo\": {\n      \"display_name\": \"GPT-3.5 Turbo\",\n      \"family\": \"gpt-3.5\",\n      \"capabilities\": {\n        \"vision\": false,\n        \"structured_output\": false,\n        \"function_calling\": true,\n        \"streaming\": true\n      },\n      \"context_window\": 16385,\n      \"max_output_tokens\": 4096,\n      \"pricing\": {\n        \"input_per_1k\": 0.0005,\n        \"output_per_1k\": 0.0015\n      },\n      \"training_cutoff\": \"2021-09\",\n      \"released\": \"2022-11-28\",\n      \"deprecated\": false\n    }\n  },\n\n  // Rate limits (tier-based, can be overridden locally)\n  \"rate_limits\": {\n    \"tier_1\": {\n      \"requests_per_minute\": 500,\n      \"tokens_per_minute\": 30000\n    },\n    \"tier_2\": {\n      \"requests_per_minute\": 5000,\n      \"tokens_per_minute\": 450000\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#json-schema-for-vs-code-providersschemajson","title":"JSON Schema for VS Code (providers/schema.json)","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"$id\": \"https://tnh-scholar.org/schemas/provider-registry-v1.json\",\n  \"title\": \"TNH Scholar Provider Registry\",\n  \"description\": \"Registry of AI provider models, capabilities, and pricing\",\n  \"type\": \"object\",\n  \"required\": [\"schema_version\", \"provider\", \"last_updated\", \"defaults\", \"models\"],\n  \"properties\": {\n    \"$schema\": {\n      \"type\": \"string\",\n      \"description\": \"JSON Schema reference\"\n    },\n    \"schema_version\": {\n      \"type\": \"string\",\n      \"enum\": [\"1.0\"],\n      \"description\": \"Registry schema version\"\n    },\n    \"provider\": {\n      \"type\": \"string\",\n      \"enum\": [\"openai\", \"anthropic\"],\n      \"description\": \"Provider identifier\"\n    },\n    \"last_updated\": {\n      \"type\": \"string\",\n      \"format\": \"date\",\n      \"description\": \"Date of last registry update (YYYY-MM-DD)\"\n    },\n    \"source_url\": {\n      \"type\": \"string\",\n      \"format\": \"uri\",\n      \"description\": \"Source URL for pricing/capability information\"\n    },\n    \"update_method\": {\n      \"type\": \"string\",\n      \"enum\": [\"manual\", \"auto-scrape\", \"api\"],\n      \"description\": \"How this registry is updated\"\n    },\n    \"defaults\": {\n      \"type\": \"object\",\n      \"required\": [\"base_url\"],\n      \"properties\": {\n        \"base_url\": {\n          \"type\": \"string\",\n          \"format\": \"uri\",\n          \"description\": \"Default API base URL\"\n        },\n        \"timeout_s\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"description\": \"Default timeout in seconds\"\n        },\n        \"max_retries\": {\n          \"type\": \"integer\",\n          \"minimum\": 0,\n          \"description\": \"Default maximum retry attempts\"\n        }\n      }\n    },\n    \"models\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"$ref\": \"#/definitions/ModelInfo\"\n      },\n      \"description\": \"Model registry keyed by model identifier\"\n    },\n    \"rate_limits\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"$ref\": \"#/definitions/RateLimitTier\"\n      },\n      \"description\": \"Rate limit tiers\"\n    }\n  },\n  \"definitions\": {\n    \"ModelInfo\": {\n      \"type\": \"object\",\n      \"required\": [\"display_name\", \"family\", \"capabilities\", \"context_window\", \"max_output_tokens\", \"pricing\"],\n      \"properties\": {\n        \"display_name\": {\n          \"type\": \"string\",\n          \"description\": \"Human-readable model name\"\n        },\n        \"family\": {\n          \"type\": \"string\",\n          \"description\": \"Model family (e.g., gpt-4, gpt-5)\"\n        },\n        \"capabilities\": {\n          \"$ref\": \"#/definitions/ModelCapabilities\"\n        },\n        \"context_window\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"description\": \"Maximum context window size in tokens\"\n        },\n        \"max_output_tokens\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"description\": \"Maximum output tokens\"\n        },\n        \"pricing\": {\n          \"$ref\": \"#/definitions/ModelPricing\"\n        },\n        \"training_cutoff\": {\n          \"type\": \"string\",\n          \"description\": \"Training data cutoff (YYYY-MM format)\"\n        },\n        \"released\": {\n          \"type\": \"string\",\n          \"format\": \"date\",\n          \"description\": \"Model release date\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\",\n          \"description\": \"Whether model is deprecated\"\n        },\n        \"aliases\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"description\": \"Alternative names for this model\"\n        }\n      }\n    },\n    \"ModelCapabilities\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"vision\": {\n          \"type\": \"boolean\",\n          \"description\": \"Supports image inputs\"\n        },\n        \"structured_output\": {\n          \"type\": \"boolean\",\n          \"description\": \"Supports JSON mode / structured outputs\"\n        },\n        \"function_calling\": {\n          \"type\": \"boolean\",\n          \"description\": \"Supports function/tool calling\"\n        },\n        \"streaming\": {\n          \"type\": \"boolean\",\n          \"description\": \"Supports streaming responses\"\n        },\n        \"audio_input\": {\n          \"type\": \"boolean\",\n          \"description\": \"Supports audio inputs\"\n        },\n        \"audio_output\": {\n          \"type\": \"boolean\",\n          \"description\": \"Supports audio outputs\"\n        }\n      }\n    },\n    \"ModelPricing\": {\n      \"type\": \"object\",\n      \"required\": [\"input_per_1k\", \"output_per_1k\"],\n      \"properties\": {\n        \"input_per_1k\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"description\": \"Price per 1K input tokens in USD\"\n        },\n        \"output_per_1k\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"description\": \"Price per 1K output tokens in USD\"\n        },\n        \"cached_input_per_1k\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"description\": \"Price per 1K cached input tokens in USD\"\n        }\n      }\n    },\n    \"RateLimitTier\": {\n      \"type\": \"object\",\n      \"required\": [\"requests_per_minute\", \"tokens_per_minute\"],\n      \"properties\": {\n        \"requests_per_minute\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"description\": \"Maximum requests per minute\"\n        },\n        \"tokens_per_minute\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"description\": \"Maximum tokens per minute\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#pydantic-schema-providersschemapy","title":"Pydantic Schema (providers/schema.py)","text":"<pre><code>\"\"\"Pydantic schemas for registry validation.\n\nAll registry JSONC files must validate against these schemas.\n\"\"\"\nfrom datetime import date\nfrom typing import Dict, Literal\n\nfrom pydantic import BaseModel, Field, HttpUrl\n\n\nclass ModelCapabilities(BaseModel):\n    \"\"\"Model capability flags.\"\"\"\n    vision: bool = False\n    structured_output: bool = False\n    function_calling: bool = False\n    streaming: bool = False\n    audio_input: bool = False\n    audio_output: bool = False\n\n\nclass ModelPricing(BaseModel):\n    \"\"\"Per-model pricing in dollars per 1K tokens.\"\"\"\n    input_per_1k: float = Field(ge=0, description=\"Input token price per 1K\")\n    output_per_1k: float = Field(ge=0, description=\"Output token price per 1K\")\n    cached_input_per_1k: float | None = Field(\n        None, ge=0, description=\"Cached input price (if supported)\"\n    )\n\n\nclass ModelInfo(BaseModel):\n    \"\"\"Complete model metadata.\"\"\"\n    display_name: str\n    family: str\n    capabilities: ModelCapabilities\n    context_window: int = Field(gt=0)\n    max_output_tokens: int = Field(gt=0)\n    pricing: ModelPricing\n    training_cutoff: str | None = None\n    released: date | None = None\n    deprecated: bool = False\n    aliases: list[str] = Field(default_factory=list)\n\n\nclass ProviderDefaults(BaseModel):\n    \"\"\"Provider-level defaults.\"\"\"\n    base_url: HttpUrl\n    timeout_s: float = Field(gt=0, default=60.0)\n    max_retries: int = Field(ge=0, default=3)\n\n\nclass RateLimitTier(BaseModel):\n    \"\"\"Rate limit configuration for a tier.\"\"\"\n    requests_per_minute: int = Field(gt=0)\n    tokens_per_minute: int = Field(gt=0)\n\n\nclass ProviderRegistry(BaseModel):\n    \"\"\"Root registry schema for a provider.\"\"\"\n    schema_version: Literal[\"1.0\"] = \"1.0\"\n    provider: str = Field(min_length=1)\n    last_updated: date\n    source_url: HttpUrl | None = None\n    update_method: Literal[\"manual\", \"auto-scrape\", \"api\"] = \"manual\"\n\n    defaults: ProviderDefaults\n    models: Dict[str, ModelInfo]\n    rate_limits: Dict[str, RateLimitTier] = Field(default_factory=dict)\n\n    class Config:\n        extra = \"forbid\"  # Fail on unknown fields\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#registry-loader-api","title":"Registry Loader API","text":"<pre><code># gen_ai_service/config/registry.py\n\"\"\"Registry loader for provider metadata.\n\nProvides singleton access to validated provider registries with layered\nprecedence: user overrides \u2192 project defaults \u2192 system defaults.\n\"\"\"\nimport json\nimport re\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom pydantic import ValidationError\n\nfrom tnh_scholar.gen_ai_service.models.errors import ConfigurationError\nfrom tnh_scholar.runtime_assets.registries.providers.schema import (\n    ModelInfo,\n    ProviderRegistry,\n)\n\n\ndef _load_jsonc(path: Path) -&gt; dict:\n    \"\"\"Load JSON with Comments (JSONC) file.\n\n    Strips comments and trailing commas before parsing.\n    Compatible with VS Code's JSONC format.\n    \"\"\"\n    with path.open() as f:\n        content = f.read()\n\n    # Strip single-line comments (// ...)\n    content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)\n\n    # Strip multi-line comments (/* ... */)\n    content = re.sub(r'/\\*.*?\\*/', '', content, flags=re.DOTALL)\n\n    # Strip trailing commas before } or ]\n    content = re.sub(r',(\\s*[}\\]])', r'\\1', content)\n\n    return json.loads(content)\n\n\nclass RegistryLoader:\n    \"\"\"Loads and caches provider registries.\"\"\"\n\n    def __init__(self, registry_root: Path | None = None):\n        \"\"\"Initialize registry loader.\n\n        Args:\n            registry_root: Path to registries directory. Defaults to\n                runtime_assets/registries in the package.\n        \"\"\"\n        if registry_root is None:\n            from tnh_scholar import TNH_ROOT\n            registry_root = TNH_ROOT / \"runtime_assets\" / \"registries\"\n\n        self.registry_root = registry_root\n        self.providers_dir = registry_root / \"providers\"\n        self.overrides_dir = registry_root / \"overrides\"\n        self._cache: Dict[str, ProviderRegistry] = {}\n\n    def get_provider(self, provider: str) -&gt; ProviderRegistry:\n        \"\"\"Load and validate provider registry.\n\n        Args:\n            provider: Provider name (e.g., \"openai\", \"anthropic\")\n\n        Returns:\n            Validated ProviderRegistry instance\n\n        Raises:\n            ConfigurationError: If registry file missing or invalid\n        \"\"\"\n        if provider in self._cache:\n            return self._cache[provider]\n\n        registry_path = self.providers_dir / f\"{provider}.jsonc\"\n        if not registry_path.exists():\n            raise ConfigurationError(\n                f\"Provider registry not found: {registry_path}\"\n            )\n\n        try:\n            data = _load_jsonc(registry_path)\n            registry = ProviderRegistry.model_validate(data)\n\n            # Apply user overrides if present\n            self._apply_overrides(registry, provider)\n\n            self._cache[provider] = registry\n            return registry\n\n        except ValidationError as e:\n            raise ConfigurationError(\n                f\"Invalid provider registry {registry_path}: {e}\"\n            ) from e\n        except json.JSONDecodeError as e:\n            raise ConfigurationError(\n                f\"Invalid JSON in registry {registry_path}: {e}\"\n            ) from e\n\n    def get_model(self, provider: str, model: str) -&gt; ModelInfo:\n        \"\"\"Get model info with alias resolution.\n\n        Args:\n            provider: Provider name\n            model: Model name or alias\n\n        Returns:\n            ModelInfo instance\n\n        Raises:\n            ConfigurationError: If model not found\n        \"\"\"\n        registry = self.get_provider(provider)\n\n        # Direct lookup\n        if model in registry.models:\n            return registry.models[model]\n\n        # Alias lookup\n        for model_name, info in registry.models.items():\n            if model in info.aliases:\n                return info\n\n        raise ConfigurationError(\n            f\"Model {model} not found in {provider} registry. \"\n            f\"Available: {', '.join(registry.models.keys())}\"\n        )\n\n    def _apply_overrides(self, registry: ProviderRegistry, provider: str) -&gt; None:\n        \"\"\"Apply user overrides if present (in-place modification).\"\"\"\n        override_path = self.overrides_dir / f\"{provider}-overrides.jsonc\"\n        if not override_path.exists():\n            return\n\n        # Load and merge overrides (pricing, rate limits, etc.)\n        overrides = _load_jsonc(override_path)\n\n        # Example: override pricing for specific models\n        if \"pricing\" in overrides:\n            for model_name, pricing_data in overrides[\"pricing\"].items():\n                if model_name in registry.models:\n                    registry.models[model_name].pricing = ModelPricing.model_validate(\n                        pricing_data\n                    )\n\n\n@lru_cache(maxsize=1)\ndef get_registry_loader() -&gt; RegistryLoader:\n    \"\"\"Get singleton registry loader.\"\"\"\n    return RegistryLoader()\n\n\ndef get_model_info(provider: str, model: str) -&gt; ModelInfo:\n    \"\"\"Convenience function to get model info.\n\n    Args:\n        provider: Provider name (e.g., \"openai\")\n        model: Model name or alias\n\n    Returns:\n        ModelInfo with capabilities, pricing, limits\n\n    Example:\n        &gt;&gt;&gt; info = get_model_info(\"openai\", \"gpt-4o-mini\")\n        &gt;&gt;&gt; print(f\"Context: {info.context_window}, Price: ${info.pricing.input_per_1k}\")\n    \"\"\"\n    return get_registry_loader().get_model(provider, model)\n\n\ndef list_models(provider: str) -&gt; list[str]:\n    \"\"\"List available models for a provider.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        List of model identifiers\n    \"\"\"\n    registry = get_registry_loader().get_provider(provider)\n    return list(registry.models.keys())\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#integration-with-existing-modules","title":"Integration with Existing Modules","text":""},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#1-update-model_routerpy","title":"1. Update model_router.py","text":"<pre><code># routing/model_router.py\nfrom tnh_scholar.gen_ai_service.config.registry import get_model_info\n\ndef select_provider_and_model(...) -&gt; ResolvedParams:\n    \"\"\"Intent-aware routing with registry-based capability checks.\"\"\"\n\n    # Look up model capabilities from registry\n    model_info = get_model_info(params.provider, params.model)\n\n    structured_needed = params.output_mode == \"json\"\n\n    if structured_needed and not model_info.capabilities.structured_output:\n        # Pick a structured-capable fallback\n        fallback = _pick_structured_fallback(params.provider, settings.default_model)\n        routing_reason = f\"{routing_reason} \u2192 switched to {fallback}\"\n        model = fallback\n\n    # ... rest of logic\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#2-update-safety_gatepy","title":"2. Update safety_gate.py","text":"<pre><code># safety/safety_gate.py\nfrom tnh_scholar.gen_ai_service.config.registry import get_model_info\n\ndef _estimate_cost(\n    provider: str,\n    model: str,\n    tokens_in: int,\n    max_tokens_out: int,\n    *,\n    use_cache: bool = False\n) -&gt; float:\n    \"\"\"Estimate cost using registry pricing.\"\"\"\n    model_info = get_model_info(provider, model)\n\n    input_price = model_info.pricing.input_per_1k\n    if use_cache and model_info.pricing.cached_input_per_1k:\n        input_price = model_info.pricing.cached_input_per_1k\n\n    input_cost = (tokens_in / 1000.0) * input_price\n    output_cost = (max_tokens_out / 1000.0) * model_info.pricing.output_per_1k\n\n    return input_cost + output_cost\n\n\ndef _context_limit_for_model(provider: str, model: str) -&gt; int:\n    \"\"\"Get context limit from registry.\"\"\"\n    model_info = get_model_info(provider, model)\n    return model_info.context_window\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#3-update-settingspy","title":"3. Update settings.py","text":"<pre><code># config/settings.py\nfrom tnh_scholar.gen_ai_service.config.registry import get_model_info\n\n@model_validator(mode=\"after\")\ndef validate_max_output_tokens(cls, values):\n    \"\"\"Validate against registry-sourced context limits.\"\"\"\n    model = values.default_model\n    provider = values.default_provider\n    max_tokens = values.default_max_output_tokens\n\n    model_info = get_model_info(provider, model)\n    limit = model_info.context_window\n\n    if max_tokens &gt; limit:\n        raise ValueError(\n            f\"default_max_output_tokens={max_tokens} exceeds \"\n            f\"context limit for {model} ({limit})\"\n        )\n    return values\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#auto-update-mechanism","title":"Auto-Update Mechanism","text":""},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#update-script-scriptsupdate_registrypy","title":"Update Script (scripts/update_registry.py)","text":"<pre><code>#!/usr/bin/env python\n\"\"\"Update provider registries from external sources.\n\nUsage:\n    poetry run python scripts/update_registry.py openai --dry-run\n    poetry run python scripts/update_registry.py openai --apply\n    poetry run python scripts/update_registry.py --all --apply\n\"\"\"\nimport argparse\nfrom datetime import date\nfrom pathlib import Path\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom tnh_scholar import TNH_ROOT\n\n\nclass RegistryUpdater:\n    \"\"\"Updates provider registry files.\"\"\"\n\n    def __init__(self, registry_root: Path):\n        self.registry_root = registry_root\n\n    def update_openai(self, dry_run: bool = True) -&gt; dict:\n        \"\"\"Update OpenAI registry.\n\n        Strategy:\n        1. Fetch official pricing page HTML\n        2. Parse pricing table (BeautifulSoup)\n        3. Validate against current registry\n        4. Show diff and update if --apply\n\n        Returns:\n            Dict of changes detected\n        \"\"\"\n        # For V1, manual update only\n        print(\"OpenAI pricing must be updated manually from:\")\n        print(\"https://openai.com/api/pricing/\")\n\n        registry_path = self.registry_root / \"providers\" / \"openai.jsonc\"\n        print(f\"\\nRegistry location: {registry_path}\")\n\n        # Future: implement web scraping with BeautifulSoup\n        # resp = requests.get(\"https://openai.com/api/pricing/\")\n        # soup = BeautifulSoup(resp.text, 'html.parser')\n        # pricing_table = soup.find('table', class_='pricing')\n        # ...\n\n        return {\"status\": \"manual_update_required\"}\n\n    def check_staleness(self, provider: str) -&gt; int:\n        \"\"\"Check days since last registry update.\"\"\"\n        import json\n\n        registry_path = self.registry_root / \"providers\" / f\"{provider}.jsonc\"\n\n        # Load using simplified JSONC parser\n        with registry_path.open() as f:\n            content = f.read()\n            # Strip comments for parsing\n            import re\n            content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)\n            content = re.sub(r'/\\*.*?\\*/', '', content, flags=re.DOTALL)\n            content = re.sub(r',(\\s*[}\\]])', r'\\1', content)\n            data = json.loads(content)\n\n        last_updated = date.fromisoformat(data[\"last_updated\"])\n        days_old = (date.today() - last_updated).days\n\n        return days_old\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Update provider registries\")\n    parser.add_argument(\"provider\", nargs=\"?\", help=\"Provider to update (or --all)\")\n    parser.add_argument(\"--all\", action=\"store_true\", help=\"Update all providers\")\n    parser.add_argument(\"--apply\", action=\"store_true\", help=\"Apply changes\")\n    parser.add_argument(\"--check-staleness\", action=\"store_true\")\n\n    args = parser.parse_args()\n\n    registry_root = TNH_ROOT / \"runtime_assets\" / \"registries\"\n    updater = RegistryUpdater(registry_root)\n\n    if args.check_staleness:\n        for provider in [\"openai\"]:  # Add more as implemented\n            days = updater.check_staleness(provider)\n            status = \"\u26a0\ufe0f STALE\" if days &gt; 90 else \"\u2705 OK\"\n            print(f\"{provider}: {days} days old {status}\")\n        return\n\n    if args.all or args.provider == \"openai\":\n        updater.update_openai(dry_run=not args.apply)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#ci-integration","title":"CI Integration","text":"<p>Add to <code>.github/workflows/registry-check.yml</code>:</p> <pre><code>name: Registry Staleness Check\n\non:\n  schedule:\n    - cron: '0 0 * * 1'  # Weekly on Mondays\n  workflow_dispatch:\n\njobs:\n  check-staleness:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n      - run: poetry install\n      - name: Check registry staleness\n        id: check\n        run: |\n          poetry run python scripts/update_registry.py --check-staleness\n          days=$(poetry run python scripts/update_registry.py --check-staleness | grep openai | awk '{print $3}')\n          if [ \"$days\" -gt 90 ]; then\n            echo \"stale=true\" &gt;&gt; $GITHUB_OUTPUT\n            echo \"days=$days\" &gt;&gt; $GITHUB_OUTPUT\n          fi\n      - name: Create issue if stale\n        if: steps.check.outputs.stale == 'true'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const days = ${{ steps.check.outputs.days }};\n            github.rest.issues.create({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              title: `Registry Update Needed: Pricing data ${days} days old`,\n              body: `Provider registries may be out of date.\\n\\nPlease review and update:\\n- Check https://openai.com/api/pricing/\\n- Update runtime_assets/registries/providers/openai.jsonc\\n- Commit changes with verification`,\n              labels: ['maintenance', 'registry-update']\n            });\n</code></pre>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#vs-code-integration","title":"VS Code Integration","text":""},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#vscodesettingsjson","title":".vscode/settings.json","text":"<pre><code>{\n  // Associate schema with registry files\n  \"json.schemas\": [\n    {\n      \"fileMatch\": [\n        \"runtime_assets/registries/providers/*.jsonc\",\n        \".vscode/tnh-scholar.json\"\n      ],\n      \"url\": \"./runtime_assets/registries/providers/schema.json\"\n    }\n  ],\n\n  // Enable JSONC for registry files\n  \"files.associations\": {\n    \"**/registries/**/*.jsonc\": \"jsonc\",\n    \".vscode/tnh-scholar.json\": \"jsonc\"\n  }\n}\n</code></pre> <p>This enables:</p> <ul> <li>\u2705 Autocomplete for model names, pricing fields</li> <li>\u2705 Inline validation errors for invalid values</li> <li>\u2705 Hover documentation for fields</li> <li>\u2705 IntelliSense suggestions</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#consequences","title":"Consequences","text":""},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#positive","title":"Positive","text":"<ol> <li>Single Source of Truth: All provider metadata centralized in validated registries</li> <li>VS Code Native: JSONC format matches VS Code ecosystem (settings.json, package.json)</li> <li>IDE Integration: JSON Schema enables autocomplete and validation</li> <li>Human-Editable: JSONC comments document pricing sources and changes</li> <li>Version Controlled: Changes tracked in git with full history</li> <li>Type-Safe: Pydantic validation prevents invalid data</li> <li>Testable: Easy to swap registries for testing different configurations</li> <li>Discoverable: Users can browse <code>runtime_assets/registries/</code> to see models</li> <li>Extensible: Adding new providers requires only a new JSONC file</li> <li>Override-Friendly: Users can locally override pricing without code changes</li> <li>Audit Trail: Registry metadata tracks update sources and dates</li> <li>CI Integration: Automated staleness checks prevent outdated pricing</li> <li>Ecosystem Consistency: Same format as workspace config, extension settings</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#negative","title":"Negative","text":"<ol> <li>Manual Updates Required: No programmatic OpenAI pricing API available</li> <li>Additional Files: New directory structure and registry files to maintain</li> <li>Migration Work: Existing hardcoded values must be moved to registries</li> <li>Learning Curve: Developers must understand registry lookup patterns</li> <li>Validation Overhead: Runtime validation adds minor startup cost (mitigated by caching)</li> <li>JSONC Parsing: Requires custom parser (regex-based comment stripping)</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#risk-mitigation","title":"Risk Mitigation","text":"<ul> <li>Stale Data Risk: CI checks + manual update reminders every 90 days</li> <li>Invalid Registry Risk: Pydantic validation fails fast with clear errors</li> <li>Performance Risk: Singleton + LRU caching ensures one-time load cost</li> <li>Parse Errors: Clear error messages with line numbers for syntax issues</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#alternative-1-yaml-format","title":"Alternative 1: YAML Format","text":"<p>Pros: Human-readable, no comment workarounds, multiline strings</p> <p>Cons:</p> <ul> <li>Not VS Code native (settings.json uses JSONC)</li> <li>Ecosystem inconsistency with VS Code extension</li> <li>No JSON Schema support in VS Code</li> <li>Additional dependency (<code>pyyaml</code>)</li> <li>Less familiar to web developers</li> </ul> <p>Rejected: JSONC is clearly superior for VS Code integration</p>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#alternative-2-database-backed-registry","title":"Alternative 2: Database-Backed Registry","text":"<p>Pros: Dynamic updates, multi-user edits, query flexibility</p> <p>Cons:</p> <ul> <li>Requires database setup and migration system</li> <li>Overkill for read-heavy workload</li> <li>Harder to version-control and review changes</li> <li>More complex deployment</li> </ul> <p>Rejected: Too heavy for current needs; file-based is sufficient for V1</p>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#alternative-3-embedded-python-constants","title":"Alternative 3: Embedded Python Constants","text":"<p>Pros: No I/O, fastest access</p> <p>Cons:</p> <ul> <li>Already causing problems (hardcoded literals everywhere)</li> <li>No user overrides without code changes</li> <li>Poor discoverability</li> <li>No audit trail</li> </ul> <p>Rejected: This is the current problematic state we're fixing</p>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#alternative-4-external-api-service","title":"Alternative 4: External API Service","text":"<p>Pros: Always up-to-date, no manual work</p> <p>Cons:</p> <ul> <li>Network dependency for startup</li> <li>Single point of failure</li> <li>Requires hosting and maintenance</li> <li>OpenAI doesn't provide this</li> </ul> <p>Rejected: Not feasible given OpenAI's lack of pricing API</p>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#open-questions","title":"Open Questions","text":"<ol> <li>Multi-Provider Pricing: Should we create a unified pricing comparison tool?</li> <li>Cached Token Pricing: How to handle prompt caching discounts in cost estimates? (Now included in schema)</li> <li>Registry Versioning: Should we support multiple registry versions simultaneously?</li> <li>Update Frequency: What's the right cadence for checking staleness? (Proposed: 90 days)</li> <li>Web Scraping Legality: Is automated scraping of OpenAI's pricing page acceptable? (Prefer manual for now)</li> <li>Model Deprecation: How to handle deprecated models still in user code?</li> <li>Regional Pricing: Do we need to support region-specific pricing? (Deferred to V2)</li> </ol>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#implementation-plan","title":"Implementation Plan","text":""},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#phase-1-core-registry-week-1","title":"Phase 1: Core Registry (Week 1)","text":"<ul> <li> Create directory structure <code>runtime_assets/registries/</code></li> <li> Implement Pydantic schemas in <code>providers/schema.py</code></li> <li> Create JSON Schema in <code>providers/schema.json</code></li> <li> Create <code>openai.jsonc</code> with current models/pricing</li> <li> Implement <code>RegistryLoader</code> with JSONC support</li> <li> Add unit tests for registry loading and validation</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#phase-2-integration-week-1-2","title":"Phase 2: Integration (Week 1-2)","text":"<ul> <li> Refactor <code>model_router.py</code> to use registry</li> <li> Refactor <code>safety_gate.py</code> to use registry pricing</li> <li> Update <code>settings.py</code> validation to use registry</li> <li> Remove hardcoded constants from all modules</li> <li> Update tests to use registry fixtures</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#phase-3-tooling-week-2","title":"Phase 3: Tooling (Week 2)","text":"<ul> <li> Create <code>scripts/update_registry.py</code> skeleton</li> <li> Implement staleness checking</li> <li> Add CI workflow for staleness alerts</li> <li> Document registry update procedures</li> <li> Create user override example in docs</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#phase-4-documentation-week-2","title":"Phase 4: Documentation (Week 2)","text":"<ul> <li> Add registry usage to developer guide</li> <li> Document override mechanism</li> <li> Create registry maintenance playbook</li> <li> Add VS Code setup instructions</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#related-adrs","title":"Related ADRs","text":"<ul> <li>ADR-A08: Config/Params/Policy Taxonomy (referenced registries as future work)</li> <li>ADR-A09: V1 Simplified Pathway (deferred registry to post-V1)</li> <li>ADR-OS01: Object-Service Architecture (strong typing requirements)</li> <li>ADR-VSC01: VS Code Integration Strategy (JSON configuration system)</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#references","title":"References","text":"<ul> <li>OpenAI Pricing Page</li> <li>OpenAI API Pricing Issue #2074</li> <li>OpenAI Developer Community: Pricing API Request</li> <li>VS Code Settings Documentation</li> <li>VS Code JSON Editing</li> <li>VS Code Extension Contribution Points</li> <li>JSONC Specification</li> <li>Pydantic V2 Settings Documentation</li> </ul>"},{"location":"architecture/gen-ai-service/adr/adr-a14-file-based-registry-system/#as-built-notes-addenda","title":"As-Built Notes &amp; Addenda","text":"<p>This section will be populated during implementation. Never edit the original Context/Decision/Consequences sections - always append addenda here.</p>"},{"location":"architecture/gen-ai-service/design/","title":"Design","text":"<p>Table of Contents:</p> <p>GenAI Service \u2014 Design Strategy - Strategy for unifying GenAI Service capabilities, personas, and phased releases.</p> <p>OpenAI Interface Migration Plan - Step-by-step plan for migrating from the legacy  module to the typed GenAI Service.</p> <p>This file auto-generated.</p>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/","title":"GenAI Service \u2014 Design Strategy","text":"<p>Strategy for unifying GenAI Service capabilities, personas, and phased releases.</p> <p>Status: Accepted Date 9-30-2025 Audience: TNH Scholar core developers Doc type: Strategy (pre-ADR) Related: ADR-P02 Pattern &amp; PatternCatalog (artifact vs service), Object-Service Blueprint, Human-AI Coding Principles</p>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#1-purpose","title":"1) Purpose","text":"<p>Create a modular, observable, policy-aware GenAI Service that:</p> <ul> <li>Powers single-message text workflows now (translation, summarization, extraction),</li> <li>Adds image-assisted prompts (vision-in) as a first-class option,</li> <li>Establishes stable domain shapes and provider-agnostic seams to later support Anthropic and others,</li> <li>Enforces budget, safety, and determinism (via PatternCatalog + fingerprinting),</li> <li>Is small enough to ship immediately, yet extensible to streaming, batching, A/B routing, and tools.</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#2-scope-v1","title":"2) Scope (v1)","text":"<ul> <li>In scope</li> <li>Single request \u2192 single response (text or JSON text); optional image attachments</li> <li>Model controls (temp/top_p/max_tokens), caps (max_dollars, max_tokens)</li> <li>PatternCatalog integration (template resolution, fingerprint)</li> <li>Policy gates (pre/post), redacted structured logging, basic metrics &amp; usage accounting</li> <li> <p>Provider abstraction (OpenAIAdapter v1; AnthropicAdapter skeleton)</p> </li> <li> <p>Out of scope (for v1)</p> </li> <li>Conversation memory, RAG orchestration, tool/function calling flows</li> <li>Long-running batch runners; complex schedulers</li> <li>Multi-provider federation/ensembles (kept as seam)</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#3-design-principles","title":"3) Design Principles","text":"<ol> <li>Object Service: single public entry point (<code>generate</code>) with strict, typed domain models.  </li> <li>Transport vs Domain: domain is pure and immutable; adapters handle all wire formats.  </li> <li>Determinism: render with PatternCatalog + PatternFingerprint for reproducibility &amp; cache keys.  </li> <li>Policy-first: cost/time guards, safety scans, JSON validation before/after provider calls.  </li> <li>Provider-agnostic by construction: a <code>ProviderClient</code> protocol + adapters per vendor.  </li> <li>Observability by default: correlation IDs, structured logs, metrics; redact at the boundary.  </li> <li>Walking skeleton: ship minimal viable flow; evolve behind stable interfaces.  </li> <li>Testability: pure helpers, narrow adapters, golden prompt snapshots, contract tests.</li> </ol>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#31-configuration-taxonomy-config-vs-params-vs-policy","title":"3.1) Configuration Taxonomy (Config vs Params vs Policy)","text":"<p>To align with the Object-Service Blueprint and avoid parameter soup, split concerns explicitly (ADR-A08 will formalize details):</p> <ul> <li>Config (construction-time, stable): credentials, timeouts, price table, default routing.</li> <li>Params (per-call inputs): pattern reference, variables, images.</li> <li>Policy (behavioral toggles): budgets, output mode, model behavior caps, media limits.</li> </ul> <pre><code># ai_service/config.py\nclass GenAIServiceConfig(BaseModel):\n    api_key: str\n    organization_id: Optional[str] = None\n    base_url: Optional[str] = None\n    default_timeout_s: float = 60.0\n    price_table_version: str = \"v1\"\n\n# ai_service/domain.py\nclass CompletionParams(BaseModel):\n    temperature: float = 0.2\n    top_p: float = 1.0\n    max_output_tokens: int = 1024\n    output_mode: Literal[\"text\", \"json\"] = \"text\"\n    model_hint: Optional[str] = None\n\nclass CompletionRequest(BaseModel):\n    pattern: PatternRef\n    variables: dict[str, Any]\n    images: list[MediaAttachment] = []\n    params: CompletionParams\n</code></pre>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#4-high-level-architecture","title":"4) High-Level Architecture","text":"<pre><code>app (callers)\n\u2502\n\u25bc\nGenAIService (Application / Orchestrator)\n\u251c\u2500 PatternCatalog (resolve+render+fingerprint)   \u2190 ADR-002\n\u251c\u2500 SafetyGate (pre/post checks)\n\u251c\u2500 ModelRouter (intent \u2192 provider+model)\n\u251c\u2500 RateLimiter &amp; RetryPolicy\n\u251c\u2500 UsageAccounting\n\u251c\u2500 Tracer &amp; Metrics\n\u2514\u2500 ProviderClient (protocol)\n    \u251c\u2500 OpenAIAdapter (v1)\n    \u2514\u2500 AnthropicAdapter (skeleton)\n</code></pre> <p>Transport boundary: CompletionRequest (domain) \u2500\u2500render + policy\u2500\u2500\u25b6 ProviderRequest (transport) \u2500\u2500\u25b6 ProviderClient</p> <p>Domain Models (immutables):</p> <ul> <li><code>PatternRef</code> (id/version), <code>RenderedPattern</code> (text, metadata, fingerprint)  </li> <li><code>MediaAttachment</code> (Path/bytes, mime, dims)  </li> <li><code>ModelHint</code> (name, temperature, max_output_tokens, top_p)  </li> <li><code>SafetyPolicy</code> (max_input_chars, max_dollars, allow_image, redact_logs)  </li> <li><code>CompletionRequest</code> (patternRef/variables or direct text, images, model hint, output mode)  </li> <li><code>CompletionResult</code> (text/json_obj, model, usage, latency_ms, warnings, correlation_id)  </li> <li><code>ProviderRequest</code> / <code>ProviderResponse</code> (transport-layer normalized shapes)</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#5-request-lifecycle-v1","title":"5) Request Lifecycle (v1)","text":"<ol> <li>Assemble Request</li> <li>Caller provides <code>PatternRef</code> + <code>variables</code> (preferred) or raw text (escape hatch),</li> <li>Optional images as <code>MediaAttachment[]</code>,</li> <li> <p><code>CompletionParams</code> (includes model override fields) + <code>BudgetPolicy</code>/<code>MediaPolicy</code>.</p> </li> <li> <p>Introspect + Render Pattern</p> </li> <li><code>PatternCatalog.introspect(id, version?) \u2192 PatternMeta</code></li> <li><code>PatternCatalog.render(id, version?, variables) \u2192 RenderedPattern</code></li> <li> <p>Compute <code>fingerprint = sha256(rendered_text + metadata)</code></p> </li> <li> <p>Routing (select provider/model)</p> </li> <li><code>ModelRouter</code> chooses <code>(provider, model)</code> using precedence: override \u2192 pattern default \u2192 global default</li> <li>Guardrail planning: ensure required capabilities (e.g., vision-in) are supported by the candidate.</li> <li> <p>Produce a short <code>RoutingDecision.reason</code> for diagnostics.</p> </li> <li> <p>Pre-Flight Safety &amp; Policy (model-aware)</p> </li> <li>Estimate tokens/cost for the selected provider/model via pricing tables; fail fast if <code>&gt; max_dollars</code></li> <li>Enforce <code>SecurityConfig.max_input_chars</code>, required variables</li> <li> <p>Optional PII scrubbing for logs (never mutate payload)</p> </li> <li> <p>Build Provider Request &amp; Call</p> </li> <li>Build a ProviderRequest from rendered text + params/policy + routing decision via helper</li> <li> <p>Call <code>ProviderClient.generate(provider_request)</code> with retry policy (retry 429/5xx with jitter)</p> <pre><code>```python\n    def _build_provider_request(self, req: CompletionRequest, rendered: RenderedPattern, routing: RoutingDecision) -&gt; ProviderRequest:\n        # v1 enforces a fixed temperature to improve determinism\n        return ProviderRequest(\n            input_text=rendered.text,\n            images=req.images,\n            model=routing.model,\n            temperature=0.2,\n            max_output_tokens=req.params.max_output_tokens,\n            top_p=req.params.top_p,\n            output_mode=req.params.output_mode,\n            correlation_id=req.correlation_id or generate_ulid(),\n        )\n```\n</code></pre> </li> <li> <p>Post-Flight</p> </li> <li>Parse/validate output; for JSON mode, schema-validate</li> <li>If JSON parse fails \u2192 return text + <code>warnings=[\"json-parse-failed\"]</code></li> <li> <p>Update usage accounting; log redacted trace; emit metrics</p> <p>The service uses an injected Observer to demarcate phases (render, route, call, parse). Timing is measured in the Observer; the service may read the provider-call span\u2019s duration to populate result latency.</p> </li> <li> <p>Return</p> </li> <li><code>CompletionResponse</code> (status envelope) with <code>result</code>, <code>provenance</code>, <code>correlation_id</code>, usage, warnings</li> </ol> <p>Routing notes (v1):</p> <p>For v1 we keep routing deterministic and simple. Precedence is consumer override \u2192 PatternMeta.default_model_hint \u2192 global default. Two immediate guardrails are applied: (a) if images are present, chosen model must support vision-in; (b) if estimated tokens exceed model context, fail fast. Future work (ADR-A04) will add capability filtering, budget/latency heuristics, and optional fallbacks.</p>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#6-provider-abstraction","title":"6) Provider Abstraction","text":"<pre><code>class ProviderRequest(BaseModel):\n    input_text: str                  # Rendered prompt (post-pattern)\n    images: list[MediaAttachment] = []\n    model: str\n    temperature: float\n    max_output_tokens: int\n    top_p: float\n    output_mode: Literal[\"text\", \"json\"] = \"text\"\n    correlation_id: str\n    system_text: Optional[str] = None  # System message population deferred in v1; PatternCatalog may emit `system_text` later.\n\nclass ProviderResponse(BaseModel):\n    text: str\n    usage: UsageInfo\n    model: str\n    finish_reason: Optional[str] = None\n    raw_response: Optional[dict] = None\n\nclass ProviderClient(Protocol):\n    def generate(self, request: ProviderRequest) -&gt; ProviderResponse: ...\n</code></pre>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#6a-response-envelope","title":"6a) Response Envelope","text":"<p>Adopt a standard status envelope so callers can uniformly handle success/failure and partials.</p> <pre><code>class Envelope(BaseModel):\n    status: Literal[\"pending\", \"running\", \"succeeded\", \"failed\", \"timeout\"]\n    error: Optional[str] = None\n    diagnostics: dict[str, Any] = {}\n    provenance: \"Provenance\"\n\nclass CompletionResult(BaseModel):\n    text: str\n    json_obj: Optional[dict] = None\n    model: str\n    usage: UsageInfo\n    latency_ms: int\n    warnings: list[str] = []\n\nclass CompletionResponse(Envelope):\n    result: Optional[CompletionResult] = None  # Only present on success\n</code></pre>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#7-patterncatalog-integration","title":"7) PatternCatalog Integration","text":"<ul> <li>Terminology: Patterns are the artifacts; PatternCatalog is the managing service (see ADR-002).</li> <li>Responsibilities</li> <li><code>introspect(ref)</code> yields <code>PatternMeta</code> (task kind, default model hint, expected output mode)</li> <li>Pattern lookup (id \u2192 content, metadata), Jinja2 render with strict var checks</li> <li>Fingerprint (rendered_text + metadata) for cache keys and observability</li> <li>Optional template lint: length bounds, forbidden substrings, required headers</li> <li>Interfaces</li> <li>render(ref, variables) -&gt; RenderedPattern</li> <li>introspect(ref) -&gt; PatternMeta (task kind, default model caps, expected output mode)</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#v1-implementation-note-reuse-existing-tnhscholar-patterns","title":"V1 implementation note \u2014 reuse existing TNH\u2011Scholar patterns","text":"<p>For V1 we will wrap the existing module <code>ai_text_processing.patterns</code> to provide the <code>PatternCatalog</code> API, avoiding a disruptive migration.</p> <ul> <li>Wrapper: <code>ai_service/patterns_adapter.py</code> exposes <code>render()</code> and <code>introspect()</code> by delegating to the current TNH\u2011Scholar pattern system.</li> <li>Mapping: existing pattern IDs and variable names are preserved; the adapter raises strict errors on missing variables.</li> <li>Fingerprint: compute <code>sha256(rendered_text + metadata_json)</code> in the adapter (non\u2011breaking to upstream code).</li> <li>Ownership: no changes to <code>ai_text_processing.patterns</code> in V1; a dedicated ADR will cover converging the two systems post\u2011V1.</li> <li>Test plan: golden renders compare adapter output to the legacy <code>ai_text_processing.patterns</code> output for a fixed set of patterns.</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#service-flow-using-introspect","title":"Service flow using introspect","text":"<pre><code>pattern_meta = PatternCatalog.introspect(pattern_ref)\nrendered = PatternCatalog.render(pattern_ref, variables)\nfingerprint = sha256(rendered.text + rendered.metadata_json)\n\n# Routing influenced by introspection\nintent = pattern_meta.task_kind\nprovider, model = ModelRouter.select(intent, override=CompletionParams(...))\n\n# Validate expected output mode from pattern vs request\nassert pattern_meta.output_mode in (\"text\", \"json\")\n</code></pre>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#8-safety-policy-split-across-configpolicyparams","title":"8) Safety &amp; Policy (split across Config/Policy/Params)","text":"<ul> <li>Input size guard via <code>SecurityConfig.max_input_chars</code></li> <li>Budget guard via <code>BudgetPolicy.max_dollars</code></li> <li>Optional PII redaction for logs via <code>SecurityConfig.redact_pii</code></li> <li>Media limits via <code>MediaPolicy</code> (count/MB), optional EXIF strip</li> <li>Post-flight:</li> <li>Output JSON schema (when requested)</li> <li>Optional content filters (basic profanity/PII detection for display surfaces)</li> <li>Error taxonomy:</li> <li>PolicyError (budget/size/validation)</li> <li>TransportError (HTTP/SDK)</li> <li>ProviderError (non-retryable)</li> <li>FormatError (JSON parsing/validation)</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#9-observability","title":"9) Observability","text":"<ul> <li>Tracing: correlation_id (ULID), pattern_id, pattern_fingerprint, provider, model, attempts, latency</li> <li>Logging: single structured line per phase (render \u2192 policy \u2192 call \u2192 parse) with redaction</li> <li>Metrics:</li> <li>llm_calls_total{provider, model, intent, outcome}</li> <li>llm_latency_ms_bucket{provider, model, intent}</li> <li>llm_retries_total{provider, model}</li> <li>llm_dollars_total{provider, model}</li> <li>Usage accounting: prompt/completion tokens and $ per call + rolling window counters</li> </ul> <p>Provenance (aligned with tracing):</p> <pre><code>class Provenance(BaseModel):\n    backend: str            # e.g., \"openai\"\n    model: str\n    correlation_id: str\n    pattern_id: str\n    pattern_fingerprint: str\n    started_at: str\n    completed_at: str\n    schema_version: str = \"1.0\"\n    # Optional (v1.1+): provider_request_id, routing_reason, retry_count, policy_version\n</code></pre>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#9a-observation-seam-observer-clocktimer","title":"9a) Observation Seam (Observer + Clock/Timer)","text":"<p>We establish an observation seam now so the service never performs low-level timing or unit conversions. The service emits phase events; an injected Observer (with a Clock/Timer) measures and records durations. Default implementations can be no-ops.</p> <pre><code>from __future__ import annotations\nfrom typing import Protocol, Any\n\nclass Clock(Protocol):\n    def now_ms(self) -&gt; int: ...  # monotonic milliseconds\n\nclass ObsSpan(Protocol):\n    def __enter__(self) -&gt; \"ObsSpan\": ...\n    def __exit__(self, exc_type, exc, tb) -&gt; None: ...\n    @property\n    def duration_ms(self) -&gt; int: ...\n\nclass Observer(Protocol):\n    def phase(self, name: str, **fields: Any) -&gt; ObsSpan: ...\n\nclass NoOpClock:\n    def now_ms(self) -&gt; int:  # placeholder monotonic clock\n        return 0\n\nclass NoOpSpan:\n    def __enter__(self) -&gt; \"NoOpSpan\":\n        return self\n    def __exit__(self, exc_type, exc, tb) -&gt; None:\n        pass\n    @property\n    def duration_ms(self) -&gt; int:\n        return 0\n\nclass NoOpObserver:\n    def phase(self, name: str, **fields: Any) -&gt; ObsSpan:\n        return NoOpSpan()\n</code></pre> <p>The concrete implementation (e.g., BasicObserver) can wrap time.perf_counter_ns() or OpenTelemetry without changing GenAIService.</p>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#v1-decisions-deferrals","title":"V1 Decisions &amp; Deferrals","text":"<ul> <li>System message handling deferred: PatternCatalog may later render <code>system_text</code>, but v1 renders <code>user_text</code> only.  </li> <li>Fingerprinting scope: hooks and comments only in v1 (observability), no cache lookup yet.  </li> <li>Pricing: use a static, good\u2011enough price table in config; move to dynamic registry later.  </li> <li>Policy ownership: service\u2011level only in v1; per\u2011request overrides deferred to v2.</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#10-configuration","title":"10) Configuration","text":"<ul> <li>GenAIServiceConfig: credentials (OpenAI/Anthropic), base URLs, default timeout, price table version</li> <li>RoutingPolicy: intent \u2192 (provider, model) mapping; guardrails</li> <li>SecurityConfig: redaction and input-size bounds</li> <li>BudgetPolicy: per-call or default budget caps</li> <li>Retry/backoff policy</li> <li>Secrets: env or Secret Manager; keys never logged; redact in traces.</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#11-testing-strategy","title":"11) Testing Strategy","text":"<ul> <li>Unit (pure): Pattern rendering, fingerprint determinism, cost estimator, router decisions, safety gates</li> <li>Adapter (mocked): Wire payloads, error mapping, usage extraction</li> <li>Contract (opt-in live): Minimal smoke gated by RUN_LIVE=1</li> <li>Golden tests: Snapshot rendered prompts (hash), schema-valid JSON outputs for fixed seeds</li> <li>Property tests: Renderer never exceeds size limits; required vars enforced</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#12-migration-plan-from-openai_interfacepy","title":"12) Migration Plan (from openai_interface.py)","text":"<p>This project is in 0.x prototyping. We will refactor callers to use <code>GenAIService.generate()</code> directly; no backward compatibility shims are required. Provenance fields (pattern id, fingerprint, correlation id) will be included from the start to enable future reproducibility, even though we do not aim to recreate results during early prototyping.</p> <p>PatternCatalog V1 migration: The <code>PatternCatalog</code> used by <code>GenAIService</code> is a thin wrapper over the existing <code>ai_text_processing.patterns</code> module. Callers migrate to <code>GenAIService.generate()</code> without changing pattern IDs/variables; the adapter handles rendering, metadata, and fingerprinting.</p>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#13-roadmap-post-v1","title":"13) Roadmap (post-v1)","text":"<ul> <li>Streaming responses; token-level handlers</li> <li>Batch jobs with idempotency keys; resumable on failure</li> <li>A/B routing &amp; canaries (per intent)</li> <li>Cache (pattern_fingerprint + variables + model) with TTL and budget heuristics</li> <li>Tool/function calling seam in domain layer</li> <li>Richer safety (semantic jailbreak detection, content classifiers)</li> <li>Multi-provider policy (price/perf aware router; automatic failover)</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#14-deliverables-v1-walking-skeleton","title":"14) Deliverables (v1 Walking Skeleton)","text":"<ul> <li>ai_service/domain.py: dataclasses (PatternRef, ModelHint, MediaAttachment, CompletionRequest/Result)</li> <li>ai_service/policy.py: SafetyPolicy, pre/post gates, pricing estimator</li> <li>ai_service/patterns_adapter.py: Thin wrapper around <code>ai_text_processing.patterns</code> exposing <code>PatternCatalog</code> (<code>render</code>, <code>introspect</code>, <code>fingerprint</code>).</li> <li>ai_service/patterns.py: Facade/typing for <code>PatternCatalog</code>; may later switch the backing implementation without affecting callers.</li> <li>ai_service/router.py: ModelRouter (intent \u2192 provider/model)</li> <li>ai_service/providers/base.py: ProviderClient protocol, ProviderRequest/ProviderResponse</li> <li>ai_service/providers/openai_adapter.py: concrete OpenAI adapter</li> <li>ai_service/service.py: GenAIService.generate() orchestration</li> <li>ai_service/obs.py: tracer, metrics, usage accounting</li> <li>tests/\u2026: golden renders, adapter mocks, policy tests</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#15-rationale-trade-offs","title":"15) Rationale &amp; Trade-offs","text":"<ul> <li>Why single entrypoint? Centralized policy/observability, simple testing.  </li> <li>Why fingerprint? Reproducibility, cache keys, transparent science.  </li> <li>Why strict shapes? Easier refactors, provider swaps, and property testing.  </li> <li>Why not tools/RAG now? Keeps v1 small, ships value, preserves clean seams.  </li> <li>Why ProviderRequest object? Eliminates parameter soup, creates a stable transport boundary to adapters.  </li> <li>Service-level policy ownership in v1; per-request deferred.</li> <li>Fingerprinting used for observability only in v1.  </li> <li>Static price table used initially; dynamic pricing planned.</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#16-open-existing-items-for-adrs","title":"16) Open / Existing Items for ADRs","text":"<ul> <li>ADR-A01: Object-Service adoption &amp; domain shapes  </li> <li>ADR-A02: PatternCatalog integration &amp; fingerprinting semantics (ref ADR-002)  </li> <li>ADR-A03: Safety policy (limits, PII redaction, JSON schema validation)  </li> <li>ADR-A04: Model routing rules &amp; provider guardrails  </li> <li>ADR-A05: Observability contract (logs/metrics/retention/redaction)  </li> <li>ADR-A06: Error taxonomy &amp; retry/backoff policy  </li> <li>ADR-A07: Image attachment policy (limits, EXIF, mime support)  </li> <li>ADR-A08: Config vs Params vs Policy taxonomy (ownership, precedence, defaults)</li> <li>ADR-A09: V1 Simplified Implementation Pathway</li> <li>ADR-A10: Unify <code>ai_text_processing.patterns</code> with <code>PatternCatalog</code> (adapter \u2192 consolidation plan, deprecation policy)</li> <li>ADR-A11: Minor model and parameters fix</li> </ul>"},{"location":"architecture/gen-ai-service/design/genai-service-design-strategy/#17-acceptance-for-v1","title":"17) Acceptance for v1","text":"<ul> <li>Deterministic prompt rendering via PatternCatalog + fingerprint  </li> <li>Policy gates enforce caps; clear error taxonomy  </li> <li>OpenAIAdapter returns normalized CompletionResponse (status envelope + result)  </li> <li>Observability: correlation IDs, structured logs, basic metrics  </li> <li>AnthropicAdapter skeleton builds cleanly (provider-agnostic seam proven)  </li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/","title":"OpenAI Interface Migration Plan","text":"<p>Step-by-step plan for migrating from the legacy <code>openai_interface</code> module to the typed GenAI Service.</p> <p>Status: Phase 1 Complete - Utilities Ready ADR: ADR-A13: Legacy Client Migration Goal: Delete <code>openai_interface/</code> module, use <code>GenAIService</code> exclusively</p>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#why-migrate","title":"Why Migrate?","text":"Legacy System Modern System (GenAIService) Singleton with global state Dependency injection No type safety Full Pydantic validation OpenAI-only Multi-provider ready No provenance tracking Full metadata &amp; fingerprinting Basic retry Exponential backoff with tenacity Scattered error handling Structured exceptions Import-time side effects Clean initialization"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#quick-start-what-changes","title":"Quick Start: What Changes?","text":""},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#before-legacy","title":"Before (Legacy)","text":"<pre><code>from tnh_scholar.openai_interface import (\n    run_immediate_completion_simple,\n    get_completion_content,\n    token_count,\n)\n\n# Call with simple params\ncompletion = run_immediate_completion_simple(\n    system_message=\"You are a helpful assistant\",\n    user_message=\"Translate this text\",\n    max_tokens=1000,\n)\ntext = get_completion_content(completion)\ntokens = token_count(text)\n</code></pre>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#after-modern","title":"After (Modern)","text":"<pre><code>from tnh_scholar.gen_ai_service import GenAIService\nfrom tnh_scholar.gen_ai_service.models.domain import RenderRequest, Message\nfrom tnh_scholar.gen_ai_service.utils.token_utils import token_count\nfrom tnh_scholar.gen_ai_service.utils.response_utils import extract_text\n\n# Initialize service\nservice = GenAIService()\n\n# Create request\nrequest = RenderRequest(\n    instruction_key=\"translate\",  # Reference to prompt in catalog\n    user_input=\"Translate this text\",\n    intent=\"translation\",\n)\n\n# Generate completion\nenvelope = service.generate(request)\n\n# Extract results\ntext = extract_text(envelope)\ntokens = token_count(text)\n</code></pre>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#migration-adapter-temporary","title":"Migration Adapter (Temporary)","text":"<pre><code>from tnh_scholar.gen_ai_service.adapters.simple_completion import simple_completion\n\n# Easier transition - similar interface to legacy\ntext = simple_completion(\n    system_message=\"You are a helpful assistant\",\n    user_message=\"Translate this text\",\n    max_tokens=1000,\n)\n</code></pre>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#file-impact-analysis","title":"File Impact Analysis","text":""},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#legacy-module-removed","title":"Legacy Module (Removed) \u2705","text":"<pre><code>src/tnh_scholar/openai_interface/      \u274c removed\ntests/openai_interface/               \u274c removed\n</code></pre>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#need-migration","title":"Need Migration \ud83d\udd04","text":"<p>High Priority (Core functionality):</p> <ul> <li><code>ai_text_processing/openai_process_interface.py</code> - Main interface layer</li> <li><code>ai_text_processing/ai_text_processing.py</code> - Uses token_count</li> <li><code>journal_processing/journal_process.py</code> - Large consumer (28KB)</li> </ul> <p>Medium Priority (CLI Tools):</p> <ul> <li><code>cli_tools/token_count/token_count.py</code> - Simple utility</li> </ul> <p>Low Priority:</p> <ul> <li>(Completed) Removed <code>audio_processing/transcription_legacy.py</code></li> </ul> <p>Documentation:</p> <ul> <li><code>notebooks/ai_text_processing/section_processing_tests.ipynb</code></li> <li><code>notebooks/video_processing/postprocessing_english.ipynb</code></li> <li><code>notebooks/video_processing/postprocessing_viet.ipynb</code></li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#migration-phases","title":"Migration Phases","text":""},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#phase-0-quick-wins-already-done","title":"\u2705 Phase 0: Quick Wins (Already Done)","text":"<ul> <li> GenAIService implemented</li> <li> OpenAI provider adapter working</li> <li> Basic tests passing</li> <li> Provenance tracking functional</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#phase-1-preparation-2-3-days","title":"\ud83d\udea7 Phase 1: Preparation (2-3 days)","text":"<p>Create missing utilities in gen_ai_service:</p> <pre><code># gen_ai_service/utils/token_utils.py\ndef token_count(text: str, model: str = \"gpt-4o\") -&gt; int\ndef token_count_messages(messages: List[Message], model: str) -&gt; int\ndef token_count_file(path: Path, model: str) -&gt; int\n\n# gen_ai_service/utils/response_utils.py\ndef extract_text(envelope: CompletionEnvelope) -&gt; str\ndef extract_object(envelope: CompletionEnvelope) -&gt; BaseModel\n\n# gen_ai_service/adapters/simple_completion.py\ndef simple_completion(\n    system_message: str,\n    user_message: str,\n    model: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    response_format: Optional[Type[BaseModel]] = None,\n) -&gt; Union[str, BaseModel]\n</code></pre> <p>Implement batch processing:</p> <pre><code># gen_ai_service/batch/\n@dataclass\nclass BatchParams:\n    poll_interval: int = 60\n    max_wait_time: int = 3600\n\nclass GenAIService:\n    def batch_generate(\n        self,\n        requests: List[RenderRequest],\n        batch_params: Optional[BatchParams] = None\n    ) -&gt; BatchResult:\n        \"\"\"Submit batch requests to provider's batch API.\"\"\"\n</code></pre> <p>Deliverables:</p> <ul> <li> Token utilities with tests</li> <li> Response utilities with tests</li> <li> Simple completion adapter with tests</li> <li> Batch processing with tests</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#phase-2-core-modules-3-4-days","title":"\ud83d\udea7 Phase 2: Core Modules (3-4 days)","text":"<p>Migrate ai_text_processing:</p> <ol> <li>Update <code>openai_process_interface.py</code>:</li> </ol> <pre><code># OLD\nfrom tnh_scholar.openai_interface import run_immediate_completion_simple\n\n# NEW\nfrom tnh_scholar.gen_ai_service.adapters.simple_completion import simple_completion\n</code></pre> <ol> <li>Update <code>ai_text_processing.py</code>:</li> </ol> <pre><code># OLD\nfrom tnh_scholar.openai_interface import token_count\n\n# NEW\nfrom tnh_scholar.gen_ai_service.utils.token_utils import token_count\n</code></pre> <p>Migrate journal_processing:</p> <p>This is the largest consumer (28KB file). Strategy:</p> <ul> <li>Create prompts in pattern catalog for journal operations</li> <li>Replace openai_interface calls with GenAIService</li> <li>Consider refactoring into smaller modules</li> </ul> <p>Deliverables:</p> <ul> <li> ai_text_processing fully migrated</li> <li> journal_processing fully migrated</li> <li> All existing functionality working</li> <li> Tests updated and passing</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#phase-3-cli-tools-1-day","title":"\ud83d\udea7 Phase 3: CLI Tools (1 day)","text":"<p>Migrate token-count:</p> <pre><code># cli_tools/token_count/token_count.py\n\n# OLD\nfrom tnh_scholar.openai_interface import token_count_file\n\n# NEW\nfrom tnh_scholar.gen_ai_service.utils.token_utils import token_count_file\n</code></pre> <p>Deliverables:</p> <ul> <li> All CLI tools working</li> <li> End-to-end tests passing</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#phase-4-tests-1-2-days","title":"\ud83d\udea7 Phase 4: Tests (1-2 days)","text":"<p>Migrate valuable tests:</p> <ul> <li>Review 19 tests in <code>tests/openai_interface/test_openai_interface.py</code></li> <li>Port behavior tests to <code>tests/gen_ai_service/</code></li> <li>Delete implementation-specific tests</li> </ul> <p>Add new tests:</p> <ul> <li>Migration adapters</li> <li>Batch processing</li> <li>Token utilities</li> <li>Response utilities</li> </ul> <p>Deliverables:</p> <ul> <li> Test coverage maintained or improved</li> <li> All tests passing</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#phase-5-notebooks-1-day","title":"\ud83d\udea7 Phase 5: Notebooks (1 day)","text":"<p>Update or archive:</p> <ul> <li>Add migration notice to notebook headers</li> <li>Update to use GenAIService OR</li> <li>Move to <code>notebooks/legacy/</code> directory</li> </ul> <p>Deliverables:</p> <ul> <li> Notebooks updated or documented</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#phase-6-deletion-1-day","title":"\ud83d\udea7 Phase 6: Deletion (1 day)","text":"<p>Final verification:</p> <pre><code># Search for any remaining imports\ngrep -r \"from tnh_scholar.openai_interface\" src/\ngrep -r \"import.*openai_interface\" src/\n\n# Should return nothing\n</code></pre> <p>Delete legacy code:</p> <pre><code>rm -rf src/tnh_scholar/openai_interface/\nrm -rf tests/openai_interface/\n</code></pre> <p>Update documentation:</p> <ul> <li>README.md - remove legacy references</li> <li>Architecture docs</li> <li>Create MIGRATION.md guide for users</li> <li>Update CHANGELOG.md</li> </ul> <p>Deliverables:</p> <ul> <li> openai_interface deleted</li> <li> All imports removed</li> <li> Documentation updated</li> <li> Full test suite passing</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test each new utility in isolation</li> <li>Mock GenAIService for adapter tests</li> <li>Test error cases</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#integration-tests","title":"Integration Tests","text":"<ul> <li>Test ai_text_processing end-to-end</li> <li>Test journal_processing workflows</li> <li>Test CLI tools with real prompts</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#regression-tests","title":"Regression Tests","text":"<ul> <li>Compare outputs before/after migration</li> <li>Verify token counts match</li> <li>Check response formats</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#performance-tests","title":"Performance Tests","text":"<ul> <li>Benchmark key operations</li> <li>Monitor API call patterns</li> <li>Track token usage</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#rollback-plan","title":"Rollback Plan","text":"<p>If migration fails:</p> <ol> <li>Git Tags: Tag each phase completion</li> </ol> <pre><code>git tag migration-phase-1-complete\n</code></pre> <ol> <li>Feature Flags: Use environment variable during transition</li> </ol> <pre><code>USE_LEGACY_CLIENT = os.getenv(\"TNH_USE_LEGACY_OPENAI\", \"false\") == \"true\"\n</code></pre> <ol> <li>Branching Strategy:</li> <li>Main work in <code>migration/unify-openai-client</code> branch</li> <li>Merge phases incrementally</li> <li> <p>Can revert specific commits if needed</p> </li> <li> <p>Keep Legacy in Git History:</p> </li> <li>Don't delete until migration 100% complete</li> <li>Can cherry-pick from history if needed</li> </ol>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#success-metrics","title":"Success Metrics","text":"Metric Target Measurement Test Coverage \u2265 Current (5%) pytest --cov Performance Within 10% of legacy Benchmark script Token Usage No increase Monitor API costs Error Rate \u2264 Legacy rate Error tracking Migration Complete 100% No legacy imports"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#communication-plan","title":"Communication Plan","text":""},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#internal-team","title":"Internal Team","text":"<ul> <li>Announce migration start</li> <li>Daily progress updates</li> <li>Flag any blockers immediately</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#external-if-applicable","title":"External (if applicable)","text":"<ul> <li>Migration guide in docs</li> <li>Deprecation warnings in code</li> <li>Version bump to indicate breaking change</li> </ul>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#faq","title":"FAQ","text":"<p>Q: Can I use GenAIService and legacy client together? A: During migration, yes. But the goal is to eliminate legacy entirely.</p> <p>Q: What if I need a feature that only exists in legacy? A: Document it in the ADR and implement in GenAIService before migrating that code.</p> <p>Q: Will this affect API costs? A: No. Same OpenAI calls, just better organized. May actually reduce costs due to better caching/retry logic.</p> <p>Q: What about batch processing? A: Will be implemented in Phase 1. May have different interface but same functionality.</p> <p>Q: Do I need to update my prompts? A: Possibly. Prompts should move to the pattern catalog for better management.</p> <p>Q: What if something breaks? A: Use git tags to roll back to last working phase. Report issues immediately.</p>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#next-steps","title":"Next Steps","text":"<ol> <li>Review ADR-A13 - Understand full context and rationale</li> <li>Estimate effort - Confirm timeline for your specific modules</li> <li>Start Phase 1 - Create utilities and adapters</li> <li>Test incrementally - Don't wait until end to test</li> <li>Document issues - Track problems and solutions</li> <li>Celebrate - Delete legacy code when done! \ud83c\udf89</li> </ol>"},{"location":"architecture/gen-ai-service/design/openai-interface-migration-plan/#resources","title":"Resources","text":"<ul> <li>Full ADR: ADR-A13-legacy-client-migration.md</li> <li>GenAIService docs: ADR-A01-domain-service.md</li> <li>Pattern Catalog: ADR-A02-pattern-catalog-v1.md</li> <li>Example usage: <code>tests/gen_ai_service/test_service.py</code></li> </ul> <p>Questions or concerns? Add them to the ADR or TODO.md for tracking.</p>"},{"location":"architecture/jvb-viewer/","title":"Jvb Viewer","text":"<p>Table of Contents:</p> <p>Design - Table of contents for architecture/jvb-viewer/design</p> <p>This file auto-generated.</p>"},{"location":"architecture/jvb-viewer/design/","title":"Design","text":"<p>Table of Contents:</p> <p>Generate Markdown Translation JSON Pairs - Instructions for producing paired Vietnamese-English Markdown JSON lines from scanned journal pages with strict formatting.</p> <p>Generate Markdown Vietnamese - Guidelines for rewriting Vietnamese journal pages into structured Markdown with one sentence per line and preserved metadata.</p> <p>JVB Viewer \u2014 Version 2 Strategy &amp; High\u2011Level Design - Strategy for a projection-first VS Code-based viewer/editor that reconciles OCR outputs into a canonical JSON artifact.</p> <p>LU\u00c2N-H\u1ed2I - Transcribed Markdown sample of the 'Lu\u00e2n-H\u1ed3i M\u1ed9t Th\u1ef1c-Th\u1ec3' article used to validate viewer output.</p> <p>This file auto-generated.</p>"},{"location":"architecture/jvb-viewer/design/generate-md-translation-json-pairs/","title":"Generate Markdown Translation JSON Pairs","text":"<p>Instructions for producing paired Vietnamese-English Markdown JSON lines from scanned journal pages with strict formatting.</p>"},{"location":"architecture/jvb-viewer/design/generate-md-translation-json-pairs/#task","title":"Task","text":"<p>Read the provided journal page and return a JSON array named \"lines\". Each element is a pair of Markdown lines: the original Vietnamese (\"vi\") and the English translation (\"en\").</p> <p>Core rules (read carefully):</p> <ol> <li>One sentence or fragment per line. Do not wrap long lines.</li> <li>Paragraphs are represented by a blank pair between groups:</li> <li>Headings become Markdown headings (#, ##, ###, ####) on a single line.</li> <li>Bullets/lists: each bullet is one line.</li> <li>Use <code>-</code> only for unordered lists.</li> <li>Use numbering such as <code>1.</code>, <code>2.</code> for ordered lists.</li> <li>Nested lists are allowed. Use tabs to indicate nesting.</li> <li>Where appropriate and clear, tables may also be created using the <code>|</code> pipe symbol.</li> <li>Block quotes: prefix each quoted line with &gt;  in both languages (one fragment per line).</li> <li>Poetry / verse / special layout: use a fenced block:<ul> <li>Open with: plaintext (on its own line, as a pair)</li> <li>Put one verse line per JSON pair (no extra blank lines inside)</li> <li>Close with: plaintext</li> </ul> </li> <li>Inline emphasis: keep Markdown clear and minimal, using only <code>*</code><ul> <li>italic, bold, bold italic</li> </ul> </li> <li>Match punctuation to the source document for Vietnamese, and natural for English</li> <li>No extra commentary in the JSON. Do not add keys other than \"vi\" and \"en\", and the required outer \"lines\" array.</li> </ol>"},{"location":"architecture/jvb-viewer/design/generate-md-translation-json-pairs/#output-format-strict","title":"Output format (strict)","text":"<pre><code>{\n  \"lines\": [\n    {\"vi\": \"&lt;markdown line 1&gt;\", \"en\": \"&lt;markdown line 1 translated&gt;\"},\n    {\"vi\": \"&lt;markdown line 2&gt;\", \"en\": \"&lt;markdown line 2 translated&gt;\"},\n    {\"vi\": \"\", \"en\": \"\"},                       // paragraph break\n    // \u2026\n  ]\n}\n</code></pre>"},{"location":"architecture/jvb-viewer/design/generate-md-translation-json-pairs/#granularity-guidance","title":"Granularity guidance","text":"<ul> <li>\u201cSentence or fragment\u201d = a grammatically coherent unit. Headings and captions are fragments.</li> <li>For lists, the bullet item is the unit. If it spans multiple sentences, all those sentences should be included on the line.</li> <li>For tight paragraphs with multiple sentences: split into one JSON pair per sentence, then place a blank pair to end the paragraph.</li> </ul>"},{"location":"architecture/jvb-viewer/design/generate-md-translation-json-pairs/#complex-layout-guidance","title":"Complex layout guidance","text":"<ul> <li>For text that is laid out in a complex fashion on the page, interpret to a minimal reduction that is feasible in markdown but still communicates some of the logical intent: e.g. use paragraphs, lists or other structures to demarcate information.</li> </ul>"},{"location":"architecture/jvb-viewer/design/generate-md-translation-json-pairs/#language-policy","title":"Language policy","text":"<ul> <li>vi = Vietnamese as faithfully read from the page.</li> <li>en = idiomatic, accurate English translation in the style of Thich Nhat Hanh on the same line.</li> <li>Preserve names, dates, and terms; translate only where appropriate (e.g., don\u2019t localize proper nouns).</li> </ul>"},{"location":"architecture/jvb-viewer/design/generate-md-translation-json-pairs/#input","title":"Input","text":"<p>You will receive an image of a scanned page. Use all signals (layout, headings, bullets, punctuation) to infer Markdown.</p>"},{"location":"architecture/jvb-viewer/design/generate-md-translation-json-pairs/#output","title":"Output","text":"<p>Return only the JSON object.</p> <p>\u2e3b</p> <p>Example of Expected JSON:</p> <pre><code>{\n  \"lines\": [\n    {\"vi\": \"# C\u00e2u chuy\u1ec7n th\u1ed1ng nh\u1ea5t\", \"en\": \"# The Story of Unification\"},\n    {\"vi\": \"## L\u1eddi m\u1edf \u0111\u1ea7u\", \"en\": \"## Foreword\"},\n\n    {\"vi\": \"\", \"en\": \"\"},\n\n    {\"vi\": \"Th\u1ef1c v\u1eady, v\u1ea5n \u0111\u1ec1 th\u1ed1ng nh\u1ea5t \u0111\u00e3 l\u00e0m ch\u00fang ta tr\u0103n tr\u1edf nhi\u1ec1u n\u0103m.\", \"en\": \"Indeed, the issue of unification has troubled us for many years.\"},\n    {\"vi\": \"Nh\u01b0ng v\u1edbi tinh th\u1ea7n h\u00f2a h\u1ee3p, ch\u00fang ta c\u00f3 th\u1ec3 b\u01b0\u1edbc \u0111i c\u00f9ng nhau.\", \"en\": \"Yet, with a spirit of harmony, we can walk together.\"},\n\n    {\"vi\": \"\", \"en\": \"\"},\n\n    {\"vi\": \"- Ph\u00e1t huy l\u00f2ng t\u1eeb bi trong m\u1ecdi h\u00e0nh x\u1eed.\", \"en\": \"- Cultivate compassion in every action.\"},\n\n    {\"vi\": \"\", \"en\": \"\"},\n\n    {\"vi\": \"```plaintext\", \"en\": \"```plaintext\"},\n    {\"vi\": \"S\u00e1ng mai s\u01b0\u01a1ng m\u1ecfng ph\u1ee7 hi\u00ean ch\u00f9a\", \"en\": \"Tomorrow\u2019s dawn, a thin mist veils the temple eaves\"},\n    {\"vi\": \"Ti\u1ebfng chu\u00f4ng nh\u1eb9 kh\u1ebd g\u1ecdi t\u00ean ng\u01b0\u1eddi\", \"en\": \"A gentle bell softly calls our names\"},\n    {\"vi\": \"```\", \"en\": \"```\"}\n    {\"vi\": \"| Ti\u1ebfng chu\u00f4ng |\", \"en\": \"| Bell |\" }\n  ]\n}\n</code></pre>"},{"location":"architecture/jvb-viewer/design/generate-md-vietnamese/","title":"Generate Markdown Vietnamese","text":"<p>Guidelines for rewriting Vietnamese journal pages into structured Markdown with one sentence per line and preserved metadata.</p>"},{"location":"architecture/jvb-viewer/design/generate-md-vietnamese/#task","title":"Task","text":"<p>Read the provided journal page and return a Markdown version of the page. Each sentence or fragment (such as a heading) of the Journal page must be on its own line.</p>"},{"location":"architecture/jvb-viewer/design/generate-md-vietnamese/#context","title":"Context","text":"<p>This journal is from Phat Giao Viet Nam 1956, Volume 16, edited by Thich Nhat Hanh.</p> <p>The page is from a section of the journal with the following metadata:</p> <pre><code>   {\n        \"title_vi\": \"LU\u00c2N H\u1ed2I M\u1ed8T TH\u1ef0C TH\u1ec2\",\n        \"title_en\": \"REINCARNATION AS A REALITY\",\n        \"author\": \"TH\u1ea0C \u0110\u1ee8C\",\n        \"summary\": \"This article discusses the Buddhist concept of reincarnation, emphasizing its reality for those who have attained enlightenment. It explores the relationship between karma and rebirth, and how modern psychology and scientific discoveries are beginning to align with these ancient teachings.\",\n        \"keywords\": [\n            \"Reincarnation\",\n            \"Karma\",\n            \"Buddhism\",\n            \"Psychology\"\n        ],\n        \"start_page\": 30,\n        \"end_page\": 34\n    }\n</code></pre> <p>Core rules (read carefully):</p> <ol> <li>CRITICAL:<ul> <li>One sentence or fragment per line.</li> <li>Do not wrap long lines.</li> <li>Do not add trailing spaces to lines.</li> </ul> </li> <li>Paragraphs are represented by a blank line (normal Markdown).</li> <li>Headings become Markdown headings (#, ##, ###, ####) on a single line.</li> <li>Bullets/lists: each bullet is on one line.</li> <li>Use <code>-</code> only for unordered lists.</li> <li>Use numbering: <code>1.</code>, <code>2.</code> for ordered lists.</li> <li>Nested lists are allowed. Use double (2) spaces to indicate nesting.</li> <li>Where appropriate, tables may also be created the <code>|</code> pipe symbol.</li> <li>Block quotes: prefix each quoted line with &gt;</li> <li> <p>For Poetry / verse / special layout (such as a box): use 9 spaces or more of indent:</p> <pre><code> this is an line of verse\n    this is another more indented line\n</code></pre> </li> <li> <p>Inline emphasis: use only <code>*</code>: italic, bold, bold italic</p> </li> <li>Match punctuation with the source document exactly if the punctuation symbol exists in UTF, otherwise use a close approximation.</li> <li>Include the page number on the bottom of the page if present.</li> <li>Use --- as rule lines where present.</li> <li>Incomplete sentences at the beginning or end of the page should be rendered exactly as is.<ul> <li>They will be fragments on their own line without ellipsis or other formatting, except what is in the original text.</li> </ul> </li> <li>Add no commentary.</li> </ol>"},{"location":"architecture/jvb-viewer/design/generate-md-vietnamese/#granularity","title":"Granularity","text":"<ul> <li>\u201cSentence or fragment\u201d = a grammatically coherent unit. Headings, captions, phrases, sentences.</li> <li>For lists, the bullet item is the unit. If a list item spans multiple sentences, all those sentences should be included on the line.</li> <li>For tight paragraphs with multiple sentences: split into one sentence per line, then place a blank line to separate the paragraphs.</li> </ul>"},{"location":"architecture/jvb-viewer/design/generate-md-vietnamese/#complex-layout","title":"Complex Layout","text":"<ul> <li>For text that is laid out in a complex fashion on the page, interpret to a minimal reduction that is feasible in markdown but still communicates the logical intent: e.g. use heading levels, paragraphs, lists, tables, or other structures to demarcate information.</li> <li>Attempt to capture as much contextual/structural meaning as possible using appropriate and possibly creative Markdown styling.</li> </ul>"},{"location":"architecture/jvb-viewer/design/generate-md-vietnamese/#correction-policy","title":"Correction Policy","text":"<ul> <li>Make the best optical interpretation of the text possible, using context for difficult to scan characters.</li> <li>Use a box symbol \u25a2 (U+25A1) for completely illegible characters</li> <li>Do not correct grammar, spelling or other structural errors where the graphical marks are clear.</li> </ul>"},{"location":"architecture/jvb-viewer/design/generate-md-vietnamese/#input","title":"Input","text":"<p>You will receive an image of a scanned page. Use all signals (layout, headings, bullets, punctuation) to build the Markdown.</p>"},{"location":"architecture/jvb-viewer/design/generate-md-vietnamese/#output","title":"Output","text":"<p>Return the markdown formatted text in a markdown fence block as in:</p> <pre><code>    &lt;markdown formatted text&gt;\n</code></pre> <p>to allow easier use of the markdown data.</p>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/","title":"JVB Viewer \u2014 Version 2 Strategy &amp; High\u2011Level Design","text":"<p>Strategy for a projection-first VS Code-based viewer/editor that reconciles OCR outputs into a canonical JSON artifact.</p> <p>ID (proposed): JVB-STRAT-001 \u00b7 Status: Draft \u00b7 Owner: Aaron + TNH\u2011Scholar team \u00b7 Last updated: 2025-09-19</p>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#0-summary","title":"0) Summary","text":"<p>We will build a projection\u2011first JVB viewer/editor anchored in VS Code. The canonical artifact per page is a minimal JSON capturing: (a) page image reference, (b) word bboxes + text, \u00a9 sentence groupings, (d) dual text sources (Google OCR vs. AI Chat-GPT vision) with provenance/confidence, (e) chosen text, and (f) optional structural cues (columns, heading level, emphasis).</p> <p>Editing happens in a VS Code webview overlay on the scanned page with a bottom edit/annotate panel for quick human reconciliation and translation touch\u2011ups. Exports (HTML/EPUB/TEI/DOCX/Markdown) remain optional projections built later. This avoids the fragile, error\u2011prone XML of v1 while aligning tightly with the program\u2019s UI/UX platform strategy.</p>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#1-context-motivation","title":"1) Context &amp; Motivation","text":"<ul> <li>V1 recap: custom browser viewer + AI\u2011generated XML. Two blocking issues:</li> <li>UI lift: bespoke navigation/controls were heavy; goal is clean viewing &amp; editing \u2192 VS Code is a better host.</li> <li>Data fragility: XML contained structural &amp; page\u2011alignment errors (off\u2011by\u20111 section spans). Fixing was tedious.</li> <li>V2 direction: regenerate translation + structure from images with a small, typed layout DSL (JSON), edited via overlay in VS Code. Document\u2011level metadata (section titles and page spans) will be corrected upfront and used as guidance.</li> </ul> <p>Why now: Modern AI vision gives reliable sentence/fragment pairs. We can capture just enough structure on the first pass (columns, heading levels, bold/italic/underline) while the GPU is \u201chot,\u201d minimizing re\u2011runs.</p>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#2-goals-nongoals","title":"2) Goals &amp; Non\u2011Goals","text":""},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#goals","title":"Goals","text":"<ul> <li>Beautiful, faithful facsimile overlay on the scanned page.</li> <li>Fast human reconciliation between dual sources (Google OCR vs AI vision) per sentence.</li> <li>Sentence/fragment\u2011level translations with roles (title/heading/body/poem_line/caption\u2026).</li> <li>Compute-aware capture of structural cues on first pass: columns, heading levels (1\u20134), bold/italic/underline, quote/poem/caption detection.</li> <li>Fit naturally in VS Code (desktop + vscode.dev) with Git\u2011native collaboration (small JSON per page).</li> </ul>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#nongoals-for-v2","title":"Non\u2011Goals (for V2)**","text":"<ul> <li>Print\u2011perfect typesetting or full kerning fidelity (good enough overlay is sufficient).</li> <li>Immediate TEI/DOCX round\u2011trip perfection (exports are later projections).</li> <li>One single monolithic file; we prefer per\u2011page JSON to reduce PR conflicts.</li> </ul>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#3-strategic-approach","title":"3) Strategic Approach","text":"<p>Projection\u2011first architecture. Canonical source = minimal per\u2011page JSON. Views:</p> <ul> <li>Overlay View (primary): HTML/SVG/Canvas over the page image. Hover/select sentence \u2192 highlight bbox union.</li> <li>Edit Panel (bottom): shows VI (GOCR vs AI) diff/chooser and editable EN translation; one\u2011click \u201cadopt AI/GOCR,\u201d mark reviewed.</li> <li>Modes: overlay VI / overlay EN / overlay with subscript EN / overlay both (VI+EN) / side\u2011by\u2011side (original vs overlay).</li> </ul> <p>Document metadata (journal summary, section titles, page ranges) is tracked separately and used to label sentences and power navigation.</p>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#4-minimal-data-model-v0","title":"4) Minimal Data Model (v0)","text":"<p>Per page JSON (one file per page, Git\u2011friendly). Keep it small; add fields only when a feature needs them.</p> <pre><code>{\n  \"schema_version\": \"v0.1\",\n  \"page\": {\n    \"id\": \"p045\",\n    \"image_uri\": \"images/1956-05/p045.png\",\n    \"size\": { \"w\": 2480, \"h\": 3508 },\n    \"words\": [\n      { \"id\": \"w1\", \"bbox\": [210,220,340,280], \"text\": \"PH\u1eacT\" },\n      { \"id\": \"w2\", \"bbox\": [360,220,470,280], \"text\": \"-GI\u00c1O\" }\n    ],\n    \"sentences\": [\n      {\n        \"id\": \"s1\",\n        \"word_ids\": [\"w1\",\"w2\"],\n        \"role\": \"title\",\n        \"section_id\": \"sec-0004\",\n        \"sources\": [\n          { \"kind\": \"gocr\", \"lang\": \"vi\", \"text\": \"C\u00e2u chuy\u1ec7n th\u1ed1ng nh\u1ea5t\", \"confidence\": 0.87 },\n          { \"kind\": \"ai\",   \"lang\": \"vi\", \"text\": \"C\u00e2u chuy\u1ec7n th\u1ed1ng nh\u1ea5t\", \"confidence\": 0.96 },\n          { \"kind\": \"ai\",   \"lang\": \"en\", \"text\": \"The Story of Unification\", \"confidence\": 0.95 }\n        ],\n        \"chosen\": { \"lang\": \"vi\", \"source_kind\": \"ai\", \"text\": \"C\u00e2u chuy\u1ec7n th\u1ed1ng nh\u1ea5t\" },\n        \"style\": { \"heading_level\": 1, \"bold\": true, \"italic\": false, \"underline\": false }\n      }\n    ],\n    \"translations\": [\n      { \"sentence_id\": \"s1\", \"lang\": \"en\", \"text\": \"The Story of Unification\", \"status\": \"draft\" }\n    ]\n  }\n}\n</code></pre> <p>Document metadata JSON (one per issue):</p> <pre><code>{\n  \"doc_meta\": {\n    \"doc_id\": \"pgvn-1956-05\",\n    \"journal_summary\": \"\u2026\",\n    \"sections\": [\n      { \"section_id\": \"sec-0001\", \"title_vi\": \"Trang ti\u00eau \u0111\u1ec1\", \"title_en\": \"Title Page\", \"start_page\": 1, \"end_page\": 1 },\n      { \"section_id\": \"sec-0002\", \"title_vi\": \"M\u1ee5c l\u1ee5c\", \"title_en\": \"Table of Contents\", \"start_page\": 2, \"end_page\": 2 },\n      { \"section_id\": \"sec-0004\", \"title_vi\": \"C\u00e2u chuy\u1ec7n th\u1ed1ng nh\u1ea5t\", \"title_en\": \"The Story of Unification\", \"start_page\": 4, \"end_page\": 5 }\n    ]\n  }\n}\n</code></pre>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#5-processing-pipeline-walking-skeleton","title":"5) Processing Pipeline (walking skeleton)","text":"<ol> <li>Ingest </li> <li>Load <code>doc_meta.json</code>.  </li> <li>For each page image: load Google OCR (if available) and run AI vision to produce VI sentences + EN drafts and style cues.</li> <li>Assemble per\u2011page JSON </li> <li>Words (bbox,text), sentence groupings (list of word_ids), <code>sources[]</code> with <code>{kind, lang, text, confidence}</code>.  </li> <li>Choose initial VI source (simple heuristic: higher confidence if edit distance \u2264 \u03c4). Attach <code>section_id</code> using page ranges.  </li> <li>Capture first\u2011pass structure: <code>columns</code> (1,2), <code>heading_level</code> (0\u20134), <code>bold/italic/underline</code>, and coarse roles (<code>title/heading/body/poem_line/caption</code>).</li> <li>Render </li> <li>VS Code webview overlay (Canvas/SVG), hit\u2011testing on sentence bbox unions.  </li> <li>Bottom panel shows VI (GOCR vs AI) diff + EN editor; mark reviewed.</li> <li>Persist </li> <li><code>Cmd+S</code>: write updated per\u2011page JSON. Git tracks small diffs; PR review is the editorial checkpoint.</li> <li>(Later) Exporters </li> <li>HTML/EPUB reading edition; TEI/DOCX optional once semantics stabilize.</li> </ol>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#6-uiux-plan-vs-code-native-webcapable","title":"6) UI/UX Plan (VS Code native; web\u2011capable)","text":"<ul> <li>Overlay panel (primary):  </li> <li>Hover: highlight sentence bbox; tooltip shows role, source, confidence.  </li> <li>Click: select; bottom panel opens.  </li> <li>Drag or Shift\u2011click: multi\u2011select for batch actions.  </li> <li>Bottom edit/annotate panel:  </li> <li>VI (GOCR vs AI) side\u2011by\u2011side diff + radio\u2011choose + \u201cMerge\u201d button.  </li> <li>EN translation textarea with status (draft/reviewed/final).  </li> <li>Modes: overlay VI/EN/subscript EN/subscript both/side\u2011by\u2011side.  </li> <li>Hotkeys: <code>C</code> (cycle VI source), <code>T</code> (toggle modes), <code>\u2191/\u2193</code> (prev/next sentence), <code>R</code> (reviewed).  </li> <li>Section breadcrumb from <code>doc_meta</code>: e.g., <code>The Story of Unification \u00b7 p4</code>.</li> </ul> <p>Performance: batch draw to Canvas; spatial index for hit\u2011testing; lazy render for large pages.</p>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#7-alignment-structure-compute-notes","title":"7) Alignment, Structure &amp; Compute Notes","text":"<ul> <li>Sentence alignment: start without char spans; sentence bbox = union of its <code>word_ids</code>. If VI source changes, recompute union.  </li> <li>Columns: detect via x\u2011band clustering of <code>word</code>/<code>line</code> centroids; store as <code>page.columns = 1|2</code> hint.  </li> <li>Heading levels: simple features (font size proxy = bbox height, y\u2011position bands, centeredness, leading whitespace) \u2192 classify into 0\u20134.  </li> <li>Emphasis: bold/italic/underline from AI vision; optional later refinement via glyph thickness heuristics.  </li> <li>Compute economy: capture all first\u2011pass cues while the model is warm; persist; later passes use cached JSON and avoid re\u2011visioning pages.</li> </ul>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#8-integration-of-dual-sources-gocr-vs-ai","title":"8) Integration of Dual Sources (GOCR vs AI)","text":"<ul> <li>Store both under <code>sentence.sources[]</code>; keep <code>confidence</code> + <code>trace_id</code>.  </li> <li>Auto\u2011choose high\u2011confidence agreement; flag disagreements (<code>needs_review: true</code>).  </li> <li>UI exposes a one\u2011click chooser and a merge helper; commit choice into <code>sentence.chosen</code>.  </li> <li>Batch adoption tool per section: \u201cAdopt AI when <code>confidence &gt;= 0.95</code> and edit\u2011distance \u2264 2\u201d.</li> </ul>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#9-risks-mitigations","title":"9) Risks &amp; Mitigations","text":"<ul> <li>Schema creep \u2192 Keep v0 minimal; add fields behind explicit features.  </li> <li>Round\u2011trip loss \u2192 Defer TEI/DOCX until semantics stabilize; treat JSON as canonical.  </li> <li>Human ergonomics \u2192 No raw JSON editing; all edits via overlay/panel; provide small PR diffs per page.  </li> <li>OCR noise &amp; drift \u2192 Provide \u201cre\u2011snap sentence\u201d to neighbor words; keep an <code>audits[]</code> log for revertibility.  </li> <li>GPU cost \u2192 Batch pages; cache all first\u2011pass cues; only re\u2011run AI on truly changed pages.</li> </ul>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#10-alternatives-considered","title":"10) Alternatives Considered","text":"<ul> <li>Re\u2011commit to XML (ALTO/TEI) as canonical: higher barrier to edit; brittle early on.  </li> <li>Browser\u2011only custom app: higher UI lift; diverges from the VS Code platform strategy.  </li> <li>DOCX/ODF as canonical: painful diffing/merges; poor fit for sentence\u2011level overlays.</li> </ul>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#11-milestones-walking-skeleton-v2-beta","title":"11) Milestones (walking skeleton \u2192 v2 beta)","text":"<ul> <li>M0 (Prototype overlay): static HTML showing page image, word bboxes, selectable sentences; bottom panel with VI/EN.  </li> <li>M1 (VS Code extension): load/save per\u2011page JSON; overlay modes; section breadcrumb.  </li> <li>M2 (Dual\u2011source UI): GOCR vs AI diff chooser; batch adoption; \u201creviewed\u201d status.  </li> <li>M3 (Structure cues): columns, heading levels, emphasis flags captured and rendered.  </li> <li>M4 (v2 beta): section\u2011level navigation; export HTML; light theming/typography.</li> </ul>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#12-alignment-with-humanagent-coding-principles","title":"12) Alignment with Human\u2011Agent Coding Principles","text":"<ul> <li>Single responsibility &amp; small helpers: separation of ingest, assemble, render, persist.  </li> <li>State explicit &amp; local: page\u2011scoped JSON; no global hidden state.  </li> <li>Validation before mutation: preview diffs, PR reviews.  </li> <li>Walking skeleton first: ship overlay + editor with minimal model before exporters.  </li> <li>Type safety: Pydantic models mirror JSON; enums for roles/modes.  </li> <li>No premature optimization: only add spans/blocks/char\u2011level detail if a UI feature demands it.</li> </ul>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#13-open-questions","title":"13) Open Questions","text":"<ul> <li>What is the minimum role set that covers the journal? (title, heading, subtitle, body, poem_line, quote, caption, aside?)  </li> <li>Do we need paragraph grouping in v2, or can we stay sentence\u2011centric and infer paragraphs visually?  </li> <li>Should glossary/terminology support land in v2 (suggestions in EN panel), or v3?  </li> <li>What is the desired export target priority (HTML reading edition vs TEI vs DOCX)?  </li> <li>Will public editors use vscode.dev (PR\u2011based), and do we need contributor onboarding tooling?</li> </ul>"},{"location":"architecture/jvb-viewer/design/jvb-viewer-v2-strategy/#14-appendix-suggested-repo-layout-per-issue","title":"14) Appendix \u2014 Suggested Repo Layout (per issue)","text":"<pre><code>/JVB-1956-05/\n  doc_meta.json\n  /pages/\n    p001.json\n    p002.json\n    ...\n  /images/\n    p001.png\n    p002.png\n    ...\n  /exports/   (generated)\n    html/\n    epub/\n    tei/\n</code></pre> <p>Decision to record (provisional): Adopt JVB-STRAT-001 as the working strategy for Version 2. Treat this document as a Strategy Note (pre\u2011ADR). As details solidify (e.g., sentence alignment specifics, exporter contracts), split into focused ADRs (e.g., ADR\u2011006 Overlay Rendering Contract, ADR\u2011007 Dual\u2011Source Reconciliation).</p>"},{"location":"architecture/jvb-viewer/design/luan-hoi/","title":"LU\u00c2N-H\u1ed2I","text":"<p>Transcribed Markdown sample of the 'Lu\u00e2n-H\u1ed3i M\u1ed9t Th\u1ef1c-Th\u1ec3' article used to validate viewer output.</p>"},{"location":"architecture/jvb-viewer/design/luan-hoi/#mot-thuc-the","title":"M\u1ed8T TH\u1ef0C-TH\u1ec2","text":"<p>TH\u1ea0C-\u0110\u1ee8C</p> <p>\u0110\u1ed1i v\u1edbi nh\u1eefng b\u1eadc \u0111\u1ec7-t\u1eed \u0111\u00e3 gi\u00e1c-ng\u1ed9, Lu\u00e2n-H\u1ed3i l\u00e0 m\u1ed9t th\u1ef1c-th\u1ec3 hi\u1ec3n nhi\u00ean kh\u00f4ng c\u1ea7n b\u00e0n c\u00e3i. Nh\u01b0ng c\u00e1c b\u1eadc \u1ea5y ch\u1ec9 c\u00f3 xu\u1ea5t hi\u1ec7n nhi\u1ec1u trong th\u1eddi-\u0111\u1ea1i \u0111\u1ee9c Ph\u1eadt hay h\u01a1n n\u1eefa l\u00e0 trong nh\u1eefng th\u1ebf-k\u1ef7 k\u1ebf c\u1eadn m\u00e0 th\u00f4i. Ng\u00e0y nay, r\u1ea5t \u00edt ng\u01b0\u1eddi ch\u1ee9ng \u0111\u01b0\u1ee3c qu\u1ea3 A-la-H\u00e1n, v\u00e0 c\u0169ng \u00edt th\u1ebf \u00edt ng\u01b0\u1eddi c\u00f3 th\u1ec3 t\u1ef1 m\u00ecnh ch\u1ee9ng-nghi\u1ec7m \u0111\u01b0\u1ee3c nh\u1eefng gi\u00e1o-l\u00fd Ph\u1eadt d\u1ea1y. C\u0169ng do \u0111\u00f3 m\u00e0 gi\u00e1o-l\u00fd Nghi\u1ec7p-B\u00e1o Lu\u00e2n-H\u1ed3i ng\u00e0y nay \u0111\u00e3 tr\u1edf th\u00e0nh nh\u1eefng ch\u1ee7-\u0111\u1ec1 tin-ng\u01b0\u1ee1ng. V\u1ea5n-\u0111\u1ec1 Lu\u00e2n-H\u1ed3i \u0111\u00e3 tr\u1edf th\u00e0nh m\u1ed9t v\u1ea5n-\u0111\u1ec1 tin-ng\u01b0\u1ee1ng c\u00e1-nh\u00e2n.</p> <p>Theo l\u1eddi Ph\u1eadt d\u1ea1y, ch\u00fang ta tin-t\u01b0\u1edfng r\u1eb1ng do nh\u1eefng nghi\u1ec7p-nh\u00e2n qu\u00e1 kh\u1ee9 m\u00e0 ta c\u00f3 ki\u1ebfp s\u1ed1ng hi\u1ec7n-t\u1ea1i. B\u1ea3n-th\u00e2n c\u1ee7a ta sinh-ho\u1ea1t trong ho\u00e0n-c\u1ea3nh hi\u1ec7n t\u1ea1i l\u00e0 k\u1ebft qu\u1ea3 c\u1ee7a nh\u1eefng \u00fd t\u01b0\u1edfng, ng\u00f4n ng\u1eef v\u00e0 h\u00e0nh \u0111\u1ed9ng c\u1ee7a ta t\u1eeb nh\u1eefng ki\u1ebfp tr\u01b0\u1edbc. Do v\u00f4-minh th\u01b0\u1eddng xuy\u00ean, ch\u00fang ta t\u1ea1o-nghi\u1ec7p v\u00e0 sinh ho\u1ea1t trong cu\u1ed9c s\u1ed1ng v\u00f4-th\u01b0\u1eddng, kh\u1ed5 kh\u00f4ng v\u00e0 v\u00f4-ng\u00e3. L\u1ea1i c\u0169ng do v\u00f4-minh m\u00e0 ta l\u1ea7m t\u01b0\u1edfng c\u00f3 m\u1ed9t B\u1ea3n-Ng\u00e3 ri\u00eang bi\u1ec7t, th\u01b0\u1eddng c\u00f2n, b\u1ea5t bi\u1ebfn. S\u1ef1 v\u1ecdng-t\u01b0\u1edfng v\u1ec1 B\u1ea3n-Ng\u00e3 \u1ea5y \u0111\u00e3 l\u00e0m ph\u00e1t sinh v\u00e0 tr\u01b0\u1edfng th\u00e0nh nhi\u1ec1u d\u1ee5c-v\u1ecdng, v\u00e0 ch\u00ednh d\u1ee5c-v\u1ecdng th\u00fac \u0111\u1ea9y ta t\u1ea1o th\u00eam v\u1ecdng-nghi\u1ec7p \u0111\u1ec3 r\u1ed3i ph\u1ea3i b\u1ecb sinh-t\u1eed lu\u00e2n-h\u1ed3i m\u00e3i m\u00e3i trong cu\u1ed9c \u0111\u1eddi kh\u1ed5 \u0111au.</p> <p>Bao gi\u1edd ta di\u1ec7t t\u1eadn g\u1ed1c nh\u1eefng v\u1ecdng-nghi\u1ec7p, ta s\u1ebd \u0111\u01b0\u1ee3c gi\u1ea3i-tho\u00e1t ra ngo\u00e0i v\u00f2ng sinh-t\u1eed l\u01b0u-chuy\u1ec3n. L\u00fac b\u1ea5y gi\u1edd</p> <p>30  </p> <p>PH\u1eacT-GI\u00c1O VI\u1ec6T-NAM</p>"},{"location":"architecture/knowledge-base/","title":"Knowledge Base","text":"<p>Table of Contents:</p> <p>Adr - Table of contents for architecture/knowledge-base/adr</p> <p>This file auto-generated.</p>"},{"location":"architecture/knowledge-base/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-K01: Preliminary Architectural Strategy for TNH Scholar Knowledge Base - Proposes a phased managed-to-open-source knowledge base rollout to validate retrieval before scaling.</p> <p>This file auto-generated.</p>"},{"location":"architecture/knowledge-base/adr/adr-k01-kb-architecture-strategy/","title":"ADR-K01: Preliminary Architectural Strategy for TNH Scholar Knowledge Base","text":"<p>Outlines a two-phase managed-to-open knowledge base plan so we can validate retrieval quality before heavy investment.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-02-14</li> <li>Stakeholders: Research tooling team, GenAI service maintainers, metadata working group</li> </ul>"},{"location":"architecture/knowledge-base/adr/adr-k01-kb-architecture-strategy/#context","title":"Context","text":"<p>The research note in <code>docs/research/tnh-scholar-knowledge-vector-search.md</code> captures objectives, content scope, architectural options, and open questions for the TNH Scholar Knowledge Base. The document establishes the target user base, outlines the ingestion pipeline, compares embedding/search/storage technologies, and lists evaluation metrics. However, several foundational choices remain unresolved (chunking strategy, multilingual plan, metadata schema, and the commercial vs. open-source stack). An initial Architectural Decision Record is needed to align stakeholders on a phased approach while these investigations continue.</p>"},{"location":"architecture/knowledge-base/adr/adr-k01-kb-architecture-strategy/#decision","title":"Decision","text":"<p>Adopt a two-phase strategy:</p> <ol> <li>Phase 1 \u2013 Learning Prototype</li> <li>Use managed services for speed: OpenAI text-embedding-3-large, a lightweight vector store (Pinecone or Chroma), and a simple ingestion pipeline leveraging existing transcription tools.</li> <li>Focus on English content first, using paragraph-level chunks with conservative overlap to validate retrieval quality.</li> <li> <p>Capture user feedback from monastics/senior researchers to refine query patterns and metadata expectations.</p> </li> <li> <p>Phase 2 \u2013 Production Architecture</p> </li> <li>Based on Phase 1 metrics, evaluate migration to an open-source or hybrid stack (e.g., BGE/E5 embeddings + Weaviate/Qdrant/Elasticsearch with BM25).</li> <li>Finalize a bilingual metadata schema and chunking policy, incorporating cross-lingual retrieval requirements.</li> <li>Introduce advanced ranking (re-rankers or intent-aware routing) only after core precision/recall targets are met.</li> </ol>"},{"location":"architecture/knowledge-base/adr/adr-k01-kb-architecture-strategy/#rationale","title":"Rationale","text":"<ul> <li>The research document identifies rapid prototyping as the recommended path (<code>docs/research/tnh-scholar-knowledge-vector-search.md:200-209</code>). Managed embeddings and hosted vector DBs minimize infrastructure drag while user needs crystallize.</li> <li>Paragraph-level chunks provide good context/precision balance (<code>docs/research/tnh-scholar-knowledge-vector-search.md:76-85</code>). We can adjust chunking once evaluation data indicates better boundaries.</li> <li>Focusing on English first limits scope while the multilingual strategy (separate indices vs. cross-lingual embeddings) is still under investigation (<code>docs/research/tnh-scholar-knowledge-vector-search.md:210-248</code>).</li> <li>A phased migration plan keeps the door open for cost/control optimization once we have empirical data on query mix, cost-per-query, and operational complexity.</li> </ul>"},{"location":"architecture/knowledge-base/adr/adr-k01-kb-architecture-strategy/#consequences","title":"Consequences","text":"<ul> <li>Positive: Enables a demonstrable prototype in weeks, surfaces real user queries, and produces concrete metrics needed for later ADRs.</li> <li>Neutral/Deferred: Multilingual retrieval, sophisticated reranking, and hybrid search remain research tracks tied to Phase 1 learnings.</li> <li>Negative: Short-term vendor lock-in (OpenAI + Pinecone) and recurring API costs until the open-source evaluation completes.</li> </ul>"},{"location":"architecture/knowledge-base/adr/adr-k01-kb-architecture-strategy/#next-steps-open-questions","title":"Next Steps &amp; Open Questions","text":"<ol> <li>Chunking Experiments: Run A/B tests across paragraph, sliding-window, and hierarchical strategies to inform ADR-K02.</li> <li>Metadata Schema Draft: Collaborate with the metadata working group to prototype the bibliographic/content/structural fields listed in the research doc (<code>docs/research/tnh-scholar-knowledge-vector-search.md:48-75</code>).</li> <li>Multilingual Plan: Prototype cross-language retrieval on a bilingual subset to assess whether cross-lingual embeddings or dual indices perform better.</li> <li>Cost &amp; Privacy Analysis: Document the operational cost envelope for the managed stack and the data-handling implications before onboarding sensitive transcripts.</li> <li>Evaluation Harness: Build the test query set, gold judgments, and logging needed to compute precision, recall, MAP, and nDCG as outlined (<code>docs/research/tnh-scholar-knowledge-vector-search.md:214-273</code>).</li> </ol> <p>Approval of this ADR should be revisited once Phase 1 metrics and user feedback reports are available.</p>"},{"location":"architecture/metadata/","title":"Metadata","text":"<p>Table of Contents:</p> <p>Adr - Table of contents for architecture/metadata/adr</p> <p>This file auto-generated.</p>"},{"location":"architecture/metadata/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-MD01: Adoption of JSON-LD for Metadata Management - Chooses JSON-LD as the canonical metadata format to capture provenance, relationships, and future semantic queries.</p> <p>ADR-MD02: Metadata Infrastructure Object-Service Integration - Defines metadata system's role as foundational infrastructure in the object-service architecture, establishing patterns for cross-layer usage and ensuring compliance with design principles.</p> <p>This file auto-generated.</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/","title":"ADR-MD01: Adoption of JSON-LD for Metadata Management","text":"<p>Commits to JSON-LD metadata so provenance, multilingual transformations, and semantic relationships stay queryable.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-01-30</li> </ul>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#context","title":"Context","text":"<p>TNH Scholar needs a robust metadata management system to track content through various processing stages, particularly for multilingual content processing and human-AI collaborative workflows. The system must support both embedded metadata in text files and associated metadata for binary files, while enabling future expansion to database storage.</p> <p>Two primary approaches were considered:</p> <ol> <li>Simple YAML frontmatter with basic key-value pairs</li> <li>JSON-LD based metadata using semantic web standards</li> </ol> <p>The initial inclination was toward YAML frontmatter for its simplicity and readability. However, deeper analysis revealed that JSON-LD's semantic capabilities align well with TNH Scholar's document processing and provenance tracking needs.</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#decision-drivers","title":"Decision Drivers","text":"<ol> <li>Need to track content transformations through multiple processing stages</li> <li>Requirement to maintain clear provenance for AI-assisted translations</li> <li>Future requirements for web-based interfaces showing processing history</li> <li>Importance of standardized metadata for content management</li> <li>Value of semantic relationships in understanding content connections</li> <li>Long-term extensibility requirements</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#decision","title":"Decision","text":"<p>We will adopt JSON-LD as TNH Scholar's primary metadata format, implemented through a phased approach:</p> <p>Phase 1: File-based Storage</p> <ul> <li>Embedded JSON-LD in text files using frontmatter</li> <li>Sidecar JSON-LD files for binary content</li> <li>Basic metadata validation and processing through pyld</li> </ul> <p>Phase 2: Enhanced Processing</p> <ul> <li>Expanded semantic relationships</li> <li>Improved validation</li> <li>Training data extraction capabilities</li> </ul> <p>Phase 3: Database Integration</p> <ul> <li>Central metadata storage</li> <li>Unified querying</li> <li>Maintained backward compatibility with file-based storage</li> </ul>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#technical-implementation","title":"Technical Implementation","text":"<p>Initial implementation will center on a Frontmatter class handling JSON-LD:</p> <pre><code>class Frontmatter:\n    \"\"\"Handles JSON-LD frontmatter embedding and extraction.\"\"\"\n\n    SCHEMA_ORG = \"https://schema.org/\"\n    DC_CONTEXT = \"http://purl.org/dc/elements/1.1/\"\n\n    @staticmethod\n    def extract(content: str) -&gt; Tuple[Dict[str, Any], str]:\n        \"\"\"Extract JSON-LD frontmatter and content from text.\"\"\"\n\n    @staticmethod\n    def embed(metadata: Dict[str, Any], content: str) -&gt; str:\n        \"\"\"Embed metadata as JSON-LD frontmatter.\"\"\"\n\n    @staticmethod\n    def validate_jsonld(metadata: Dict[str, Any]) -&gt; bool:\n        \"\"\"Validate if the metadata is valid JSON-LD.\"\"\"\n</code></pre>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#example-usage","title":"Example Usage","text":"<p>Document processing workflow metadata:</p> <pre><code>{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Translation\",\n  \"@id\": \"translation_123_revised\",\n  \"translationOf\": {\"@id\": \"transcript_123_raw\"},\n  \"basedOn\": {\"@id\": \"translation_123_draft\"},\n  \"sourceLanguage\": \"vi\",\n  \"targetLanguage\": \"en\",\n  \"processingStage\": \"human_revised\",\n  \"revisor\": \"John Doe\",\n  \"revisionDate\": \"2024-01-30\"\n}\n</code></pre>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#consequences","title":"Consequences","text":""},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#positive","title":"Positive","text":"<ol> <li>Rich semantic relationships between content</li> <li>Standard vocabularies through schema.org and Dublin Core</li> <li>Strong support for content provenance tracking</li> <li>Better foundation for future web interfaces</li> <li>Industry-standard metadata format</li> <li>Enhanced machine readability for AI processing</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#negative","title":"Negative","text":"<ol> <li>Increased complexity compared to YAML</li> <li>Steeper learning curve for contributors</li> <li>Additional dependency on pyld library</li> <li>More complex validation requirements</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#neutral","title":"Neutral","text":"<ol> <li>Changes to existing metadata handling required</li> <li>Need for migration strategy from current formats</li> <li>Documentation requirements for JSON-LD usage</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#alternative-approaches-considered","title":"Alternative Approaches Considered","text":""},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#simple-yaml-frontmatter","title":"Simple YAML Frontmatter","text":"<pre><code>type: translation\nid: translation_123\nsource_id: transcript_123\nlanguage: vi\ntarget_language: en\n</code></pre> <p>While simpler, this approach lacks semantic richness and standardization.</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#custom-metadata-format","title":"Custom Metadata Format","text":"<p>Creating a custom format was rejected due to:</p> <ul> <li>Reinventing existing solutions</li> <li>Lack of standardization</li> <li>Limited tool support</li> </ul>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#implementation-strategy","title":"Implementation Strategy","text":"<ol> <li> <p>Initial Phase</p> </li> <li> <p>Implement basic JSON-LD frontmatter handling</p> </li> <li>Focus on core metadata fields</li> <li> <p>Simple validation</p> </li> <li> <p>Enhancement Phase</p> </li> <li> <p>Add semantic relationship support</p> </li> <li>Improve validation</li> <li> <p>Develop migration tools</p> </li> <li> <p>Integration Phase</p> </li> <li> <p>Database integration</p> </li> <li>Advanced querying</li> <li>Web interface support</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#notes","title":"Notes","text":"<p>This decision prioritizes long-term system capabilities over short-term simplicity. The initial complexity investment is justified by:</p> <ol> <li>Enhanced content relationship tracking</li> <li>Better support for human-AI workflow management</li> <li>Improved potential for web interface development</li> <li>Standard compliance for potential interoperability</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#related-documents","title":"Related Documents","text":"<ul> <li>TNH Scholar System Design (system-design.md)</li> <li>Pattern System Documentation (patterns.md)</li> <li>Text Processing Documentation</li> </ul> <p>The semantic capabilities of JSON-LD align particularly well with TNH Scholar's vision for cyclical learning and content processing improvements as outlined in the system architecture document.</p> <p>Here's the ADR documentation for the metadata implementation:</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#adr-002-metadata-implementation-strategy","title":"ADR 002: Metadata Implementation Strategy","text":"<p>02-01-2025</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#context_1","title":"Context","text":"<p>TNH Scholar needs a flexible metadata storage solution during its rapid prototyping phase. The system currently uses <code>Dict[str, Any]</code> throughout for metadata storage, but requires a more controlled yet still flexible approach that:</p> <ol> <li>Maintains JSON serializability for AI pipeline integration</li> <li>Preserves dict-like operations (especially the | operator for combining metadata)</li> <li>Allows schema flexibility during prototyping</li> <li>Provides clear extension points for future structure</li> </ol> <p>Two main approaches were considered:</p> <ol> <li>Type alias: <code>Metadata = Dict[str, Any]</code></li> <li>Custom class implementing MutableMapping</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#decision-drivers_1","title":"Decision Drivers","text":"<ol> <li>Need for JSON serializability in AI pipelines</li> <li>Heavy use of dict union operations (|) in existing code</li> <li>Requirement for maximum flexibility during prototyping</li> <li>Future extensibility requirements</li> <li>Minimal overhead during development</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#decision_1","title":"Decision","text":"<p>Implement a custom <code>Metadata</code> class using <code>MutableMapping</code> that provides dict-like behavior while ensuring JSON serializability:</p> <pre><code>from collections.abc import MutableMapping\nfrom typing import Any, Dict, Optional, Union, Iterator, Mapping\n\nJsonValue = Union[str, int, float, bool, list, dict, None]\n\nclass Metadata(MutableMapping):\n    \"\"\"\n    Flexible metadata container that behaves exactly like a dict while ensuring\n    JSON serializability. Designed for AI processing pipelines where schema\n    flexibility is prioritized over structure.\n    \"\"\"\n    def __init__(self, data: Optional[Union[Dict[str, JsonValue], 'Metadata']] = None) -&gt; None:\n        self._data: Dict[str, JsonValue] = {}\n        if data is not None:\n            self.update(data._data if isinstance(data, Metadata) else data)\n\n    # [Core implementation as shown above...]\n</code></pre> <p>This implementation was chosen over a simple type alias because it provides:</p> <ul> <li>JSON serializability guarantees</li> <li>Full dict-like behavior including all operators</li> <li>Clear extension points for future enhancements</li> <li>Type safety for JSON values</li> <li>Explicit serialization methods</li> </ul>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#consequences_1","title":"Consequences","text":""},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#positive_1","title":"Positive","text":"<ol> <li>Ensures metadata remains JSON-serializable</li> <li>Maintains all dict operations including | operator</li> <li>Makes metadata objects self-identifying</li> <li>Provides clear path for adding validation/structure later</li> <li>Explicit serialization methods improve code clarity</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#negative_1","title":"Negative","text":"<ol> <li>Slightly more complex than simple type alias</li> <li>Need to implement and maintain custom class</li> <li>Must ensure all dict operations are properly supported</li> <li>Minor performance overhead compared to raw dict</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#neutral_1","title":"Neutral","text":"<ol> <li>Changes required to existing Dict[str, Any] usage</li> <li>Need to document class behavior and limitations</li> <li>May need to add features as dict usage patterns emerge</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#alternative-approaches-considered_1","title":"Alternative Approaches Considered","text":""},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#type-alias","title":"Type Alias","text":"<pre><code>Metadata = Dict[str, Any]\n</code></pre> <p>Rejected because it provides no guarantees about JSON serializability and no extension points for future enhancements.</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#pydantic-model","title":"Pydantic Model","text":"<p>Rejected as too structured for current prototyping needs.</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#attrsdataclasses","title":"attrs/dataclasses","text":"<ul> <li>Python's built-in dataclass or attrs library</li> <li>Offers strong typing and validation</li> <li>Rejected as requiring too rigid structure for current needs</li> </ul>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#jsonschemajson-schema","title":"jsonschema/JSON Schema","text":"<ul> <li>Provides flexible schema validation</li> <li>Rejected as overkill for current metadata needs</li> <li>Could be considered for future validation requirements</li> </ul>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#existing-metadata-libraries","title":"Existing Metadata Libraries","text":"<ul> <li><code>python-metadata</code>: Dedicated metadata handling</li> <li><code>metadatastore</code>: Scientific metadata management</li> <li><code>dublin-core-metadata</code>: Dublin Core implementation</li> <li>All rejected as adding unnecessary complexity during prototyping</li> </ul>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#related-project-approaches","title":"Related Project Approaches","text":"<p>Examined metadata handling in:</p> <ul> <li>Documentation tools (Sphinx, mkdocs, pelican)</li> <li>Git metadata systems</li> <li>Python package metadata</li> </ul> <p>Found these approaches either too specialized or too complex for current needs</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#implementation-strategy_1","title":"Implementation Strategy","text":"<ol> <li>Initial Implementation</li> <li>Create Metadata class with full dict behavior</li> <li>Ensure JSON value type constraints</li> <li> <p>Add basic serialization methods</p> </li> <li> <p>Migration</p> </li> <li>Replace Dict[str, Any] usage with Metadata class</li> <li>Update existing metadata handling code</li> <li> <p>Document any behavioral differences</p> </li> <li> <p>Future Considerations</p> </li> <li>Potential addition of schema validation</li> <li>Integration with Dublin Core standards</li> <li>Enhanced metadata merging strategies</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#notes_1","title":"Notes","text":"<p>This decision prioritizes flexibility and simplicity during prototyping while ensuring basic guarantees about metadata structure and behavior. The implementation can evolve toward more structured approaches as requirements solidify.</p> <p>The design specifically supports AI pipeline integration by maintaining JSON compatibility while providing full dict-like operations for easy metadata manipulation.</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#related-documents_1","title":"Related Documents","text":"<ul> <li>ADR 001: Adoption of JSON-LD for Metadata Management</li> <li>TNH Scholar System Design (system-design.md)</li> <li>Pattern System Documentation (patterns.md)</li> </ul> <p>Yes, good points about the deeper validation needs and potential extensibility. Let's draft an ADR to document these considerations:</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#adr-003-metadata-validation-and-serialization-strategy","title":"ADR 003: Metadata Validation and Serialization Strategy","text":"<p>02-23-2024</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#status_1","title":"Status","text":"<p>Proposed - Prototyping Phase</p>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#context_2","title":"Context","text":"<p>The TNH Scholar system needs flexible metadata handling that balances immediate prototyping needs with potential future requirements. Current implementation using JsonValue typing provides basic type safety but has several limitations and considerations that need to be documented.</p> <p>Key Issues: 1. Recursive Validation    - Current JsonValue validation is shallow    - Nested dictionaries may contain non-serializable objects    - List contents are not validated</p> <ol> <li>Object Serialization</li> <li>Some objects may have valid serialization methods</li> <li>Current approach limited to primitive JSON types</li> <li> <p>No standard interface for serializable objects</p> </li> <li> <p>Type Processing</p> </li> <li>Current processing happens at initialization</li> <li>No validation on subsequent updates</li> <li>Limited to predefined type processors</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#current-implementation-prototype-phase","title":"Current Implementation (Prototype Phase)","text":"<pre><code>JsonValue = Union[str, int, float, bool, list, dict, None]\n\nclass Metadata(MutableMapping):\n    _type_processors = {\n        Path: lambda p: str(p.resolve()),\n        datetime: lambda d: d.isoformat(),\n    }\n\n    def __init__(self, data: Optional[Union[Dict[str, Any], 'Metadata']] = None):\n        self._data: Dict[str, JsonValue] = {}\n        if data is not None:\n            raw_data = data._data if isinstance(data, Metadata) else data\n            processed_data = {\n                k: self._process_value(v) for k, v in raw_data.items()\n            }\n            self.update(processed_data)\n</code></pre>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#future-considerations","title":"Future Considerations","text":"<ol> <li>Deep Validation</li> </ol> <pre><code>def validate_json_value(value: Any, path: str = \"\") -&gt; bool:\n    if isinstance(value, dict):\n        return all(\n            validate_json_value(v, f\"{path}.{k}\") \n            for k, v in value.items()\n        )\n    if isinstance(value, list):\n        return all(\n            validate_json_value(v, f\"{path}[{i}]\") \n            for i, v in enumerate(value)\n        )\n    return isinstance(value, (str, int, float, bool, type(None)))\n</code></pre> <ol> <li>Serializable Interface</li> </ol> <pre><code>class Serializable(Protocol):\n    def to_dict(self) -&gt; Dict[str, JsonValue]: ...\n\nclass Metadata(MutableMapping):\n    def _process_value(self, value: Any) -&gt; JsonValue:\n        if isinstance(value, Serializable):\n            return value.to_dict()\n        # existing processing...\n</code></pre> <ol> <li>Update Validation</li> </ol> <pre><code>def __setitem__(self, key: str, value: Any) -&gt; None:\n    self._data[key] = self._process_value(value)\n</code></pre>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#decision_2","title":"Decision","text":"<p>For the prototyping phase:</p> <ol> <li>Keep current shallow validation</li> <li>Document known limitations</li> <li>Use type processors for common cases</li> <li>Accept some type safety compromises for flexibility</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#consequences_2","title":"Consequences","text":"<p>Positive:</p> <ul> <li>Simple, workable implementation for prototyping</li> <li>Clear path for future enhancement</li> <li>Basic type safety for common cases</li> <li>Flexible enough for rapid development</li> </ul> <p>Negative:</p> <ul> <li>Incomplete validation</li> <li>Potential for invalid nested data</li> <li>No standardized object serialization</li> <li>Some type safety compromises</li> </ul>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#future-directions","title":"Future Directions","text":"<ol> <li>Validation Options:</li> <li>Full recursive validation</li> <li>Schema-based validation</li> <li> <p>Custom validation rules</p> </li> <li> <p>Serialization Enhancement:</p> </li> <li>Standard serialization protocol</li> <li>Custom serializers registry</li> <li> <p>Validation hooks</p> </li> <li> <p>Type Processing:</p> </li> <li>Extended type processor registry</li> <li>Custom processor registration</li> <li>Update validation</li> </ol>"},{"location":"architecture/metadata/adr/adr-md01-json-ld-metadata/#notes_2","title":"Notes","text":"<p>This design purposefully favors flexibility and simplicity during prototyping while documenting paths for future enhancement. The current implementation acknowledges and accepts certain limitations in favor of development velocity.</p>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/","title":"ADR-MD02: Metadata Infrastructure Object-Service Integration","text":"<p>Establishes metadata system (<code>tnh_scholar.metadata</code>) as foundational cross-cutting infrastructure that supports the object-service architecture (ADR-OS01) while maintaining design principle compliance.</p> <ul> <li>Status: Accepted</li> <li>Date: 2025-12-07</li> <li>Authors: Aaron Solomon, Claude Sonnet 4.5</li> <li>Related: ADR-MD01, ADR-OS01, ADR-PT04</li> </ul>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#context","title":"Context","text":""},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#discovery","title":"Discovery","text":"<p>During implementation of ADR-PT04 (Prompt System Refactor), we discovered that:</p> <ol> <li>Metadata already exists: <code>tnh_scholar.metadata</code> provides <code>Metadata</code>, <code>Frontmatter</code>, and <code>ProcessMetadata</code></li> <li>Duplication was occurring: Services were reimplementing YAML frontmatter parsing</li> <li>Broader pattern identified: ALL .md files in TNH Scholar (prompts, corpus, derivatives, docs) use metadata frontmatter</li> <li>Self-reflexive design: TNH Scholar operates on its own metadata-bearing artifacts</li> </ol>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#architectural-questions","title":"Architectural Questions","text":"<ol> <li>Is metadata a service? No\u2014it's foundational infrastructure with no external dependencies</li> <li>Does it need ports/adapters? No\u2014pure utility classes used across all layers</li> <li>How does it fit object-service architecture? Cross-cutting concern, available to all layers</li> <li>Should services reuse or reimplement? ALWAYS reuse; metadata is foundational</li> </ol>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#object-service-compliance-assessment","title":"Object-Service Compliance Assessment","text":"<p>Current implementation (src/tnh_scholar/metadata/metadata.py):</p> <p>\u2705 Compliant aspects: - Strong typing (<code>Metadata</code>, <code>ProcessMetadata</code> are typed classes) - Pure functions (no side effects in <code>Frontmatter.extract()</code>) - JSON serialization via <code>Metadata.to_dict()</code> (type-safe) - Type processors for <code>Path</code>, <code>datetime</code> conversion</p> <p>\u274c Non-compliant aspects: - No transport/domain separation - Mixes file I/O with domain logic in <code>Frontmatter.extract_from_file()</code> - Logging in utility code - No mapper pattern for domain schema validation</p>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#decision","title":"Decision","text":""},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#1-metadata-system-role","title":"1. Metadata System Role","text":"<p>Metadata is FOUNDATIONAL INFRASTRUCTURE, not a service:</p> <ul> <li>Available everywhere: All layers (domain, service, adapter, mapper, transport) can import</li> <li>No protocols/ports: Pure utility classes with no abstraction needed</li> <li>Cross-cutting concern: Supports object-service architecture without being one</li> <li>Self-reflexive enabler: System can reason about its own artifacts</li> </ul>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#2-integration-patterns-with-object-service-layers","title":"2. Integration Patterns with Object-Service Layers","text":""},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#pattern-1-mappers-use-frontmatter-for-md-files","title":"Pattern 1: Mappers Use Frontmatter for .md Files","text":"<p>Rule: Never reimplement YAML frontmatter parsing; always use <code>Frontmatter.extract()</code>.</p> <pre><code># \u2705 CORRECT: mappers/prompt_mapper.py\nfrom tnh_scholar.metadata import Frontmatter\n\nclass PromptMapper:\n    def to_domain_prompt(self, file_content: str) -&gt; Prompt:\n        # Use shared infrastructure\n        metadata_obj, body = Frontmatter.extract(file_content)\n\n        # Validate against domain schema\n        prompt_metadata = PromptMetadata.model_validate(metadata_obj.to_dict())\n\n        return Prompt(metadata=prompt_metadata, template=body)\n</code></pre> <pre><code># \u274c WRONG: Don't reimplement\ndef _parse_frontmatter(content: str) -&gt; dict:\n    # Reinventing the wheel\n    match = re.match(r'^---\\n(.*?)\\n---\\n', content, re.DOTALL)\n    return yaml.safe_load(match[1])  # Duplication!\n</code></pre> <p>Benefits: - Consistent frontmatter handling across all .md files - JSON-LD support (ADR-MD01) available when needed - BOM handling, whitespace normalization already implemented - Future enhancements benefit all consumers</p>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#pattern-2-domain-models-use-metadata-for-flexible-fields","title":"Pattern 2: Domain Models Use Metadata for Flexible Fields","text":"<p>Rule: Replace <code>Dict[str, Any]</code> with <code>Metadata</code> for type-safe, JSON-serializable metadata storage.</p> <pre><code># \u2705 CORRECT: domain/models.py\nfrom tnh_scholar.metadata import Metadata\n\nclass DocumentResult(BaseModel):\n    content: str\n    metadata: Metadata  # JSON-serializable, dict-like\n\nclass TranslationResult(BaseModel):\n    text: str\n    source_metadata: Metadata\n    output_metadata: Metadata\n</code></pre> <p>Benefits: - Type safety (<code>Metadata</code> ensures JSON-serializable values) - Auto-conversion (<code>Path</code> \u2192 <code>str</code>, <code>datetime</code> \u2192 ISO format) - Dict-like interface (familiar <code>|</code>, <code>[]</code> operators) - Explicit serialization (<code>to_dict()</code>, <code>to_yaml()</code>)</p>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#pattern-3-services-track-provenance-with-processmetadata","title":"Pattern 3: Services Track Provenance with ProcessMetadata","text":"<p>Rule: Use <code>ProcessMetadata</code> to record transformation steps in multi-stage pipelines.</p> <pre><code># \u2705 CORRECT: services/translation_service.py\nfrom tnh_scholar.metadata import ProcessMetadata, Metadata\n\nclass TranslationService:\n    def translate(self, doc: Document) -&gt; Document:\n        result = self._translate_content(doc)\n\n        # Track transformation\n        result.metadata.add_process_info(\n            ProcessMetadata(\n                step=\"translation\",\n                processor=\"genai_service\",\n                tool=\"gpt-4o\",\n                source_lang=doc.metadata.get(\"language\"),\n                target_lang=\"en\",\n                timestamp=datetime.now(),  # Auto-converted to ISO\n            )\n        )\n\n        return result\n</code></pre> <p>Benefits: - Automatic provenance chain (stored in <code>tnh_metadata_process</code> field) - Supports semantic queries (JSON-LD compatible) - Reproducibility (track exact tools/versions used) - Self-reflexive operations (system can analyze its own transformations)</p>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#pattern-4-mappers-separate-infrastructure-from-domain-validation","title":"Pattern 4: Mappers Separate Infrastructure from Domain Validation","text":"<p>Rule: Mappers use <code>Frontmatter</code> (infrastructure), then validate with domain schemas (Pydantic models).</p> <pre><code># \u2705 CORRECT: Two-step process\nclass CorpusMapper:\n    def to_domain_document(self, file_content: str) -&gt; CorpusDocument:\n        # Step 1: Infrastructure (Frontmatter parsing)\n        metadata_obj, body = Frontmatter.extract(file_content)\n\n        # Step 2: Domain validation (Pydantic schema)\n        corpus_metadata = CorpusMetadata.model_validate(metadata_obj.to_dict())\n\n        return CorpusDocument(metadata=corpus_metadata, content=body)\n</code></pre> <p>Why separate? - Infrastructure concern: YAML parsing, BOM handling, whitespace - Domain concern: Required fields, business rules, semantic validation - Separation of concerns: Metadata module doesn't know about domain schemas - Reusability: Same Frontmatter code works for prompts, corpus, derivatives</p>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#3-object-service-architecture-modifications","title":"3. Object-Service Architecture Modifications","text":""},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#updated-layer-model-with-metadata","title":"Updated Layer Model with Metadata","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Foundational Infrastructure (Cross-Cutting)             \u2502\n\u2502  \u2022 tnh_scholar.metadata (Metadata, Frontmatter, Process) \u2502\n\u2502  \u2022 Available to ALL layers below                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Application Layer                         \u2502\n    \u2502  \u2022 CLI, notebooks, web, Streamlit          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Service Layer                             \u2502\n    \u2502  \u2022 Orchestrators (use ProcessMetadata)     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Adapter + Mapper Layer                    \u2502\n    \u2502  \u2022 Mappers use Frontmatter.extract()       \u2502\n    \u2502  \u2022 Adapters use Metadata for flexible data \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Transport Layer                           \u2502\n    \u2502  \u2022 Uses Metadata.to_dict() for JSON        \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#metadata-as-horizontal-infrastructure","title":"Metadata as \"Horizontal\" Infrastructure","text":"<p>Unlike services (which flow vertically: Application \u2192 Service \u2192 Adapter \u2192 Transport), metadata is horizontal\u2014available at every layer:</p> Layer Metadata Usage Application Display metadata in UIs, format for users Service Track provenance with <code>ProcessMetadata</code> Adapter Store flexible provider-specific data in <code>Metadata</code> Mapper Parse .md files with <code>Frontmatter.extract()</code> Transport Serialize with <code>Metadata.to_dict()</code> for JSON"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#4-compliance-improvements-needed","title":"4. Compliance Improvements Needed","text":""},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#issue-1-file-io-mixed-with-domain-logic","title":"Issue 1: File I/O Mixed with Domain Logic","text":"<p>Current (metadata/metadata.py:262-264):</p> <pre><code>class Frontmatter:\n    @classmethod\n    def extract_from_file(cls, file: Path) -&gt; tuple[Metadata, str]:\n        text_str = read_str_from_file(file)  # \u274c File I/O in domain utility\n        return cls.extract(text_str)\n</code></pre> <p>Recommendation: Mark as adapter-level helper, or move to separate <code>FrontmatterFileAdapter</code>:</p> <pre><code># Option A: Keep but document as adapter-level\nclass Frontmatter:\n    \"\"\"Pure frontmatter parsing (no I/O).\"\"\"\n\n    @staticmethod\n    def extract(content: str) -&gt; tuple[Metadata, str]:\n        \"\"\"Extract frontmatter from string (pure function).\"\"\"\n        ...\n\n    @classmethod\n    def extract_from_file(cls, file: Path) -&gt; tuple[Metadata, str]:\n        \"\"\"ADAPTER-LEVEL: Convenience for file-based workflows.\n\n        Note: This method performs I/O. For pure parsing, use extract().\n        Services should inject file content via transport layer.\n        \"\"\"\n        text_str = read_str_from_file(file)\n        return cls.extract(text_str)\n</code></pre> <pre><code># Option B: Separate adapter (stricter compliance)\n# adapters/frontmatter_file_adapter.py\nclass FrontmatterFileAdapter:\n    \"\"\"Adapter for reading frontmatter from files.\"\"\"\n\n    def __init__(self, transport: FileTransport):\n        self._transport = transport\n\n    def extract_from_file(self, file: Path) -&gt; tuple[Metadata, str]:\n        content = self._transport.read_text(file)\n        return Frontmatter.extract(content)\n</code></pre> <p>Decision: Keep Option A for rapid prototype phase; document as adapter-level helper. Consider Option B post-1.0 if strict layer separation becomes critical.</p>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#issue-2-logging-in-utility-code","title":"Issue 2: Logging in Utility Code","text":"<p>Current (metadata/metadata.py:22-36):</p> <pre><code>def safe_yaml_load(yaml_str: str, *, context: str = \"unknown\") -&gt; dict:\n    try:\n        data = yaml.safe_load(yaml_str)\n        if not isinstance(data, dict):\n            logger.warning(...)  # \u274c Side effect in utility function\n            return {}\n    except ScannerError as e:\n        logger.error(...)  # \u274c Side effect\n    except yaml.YAMLError as e:\n        logger.error(...)  # \u274c Side effect\n    return {}\n</code></pre> <p>Recommendation: Raise typed exceptions; let callers decide logging strategy:</p> <pre><code># metadata/errors.py (new file)\nclass MetadataError(Exception):\n    \"\"\"Base error for metadata operations.\"\"\"\n    pass\n\nclass FrontmatterParseError(MetadataError):\n    \"\"\"YAML frontmatter parsing failed.\"\"\"\n    pass\n\nclass InvalidMetadataError(MetadataError):\n    \"\"\"Metadata not a valid dict.\"\"\"\n    pass\n\n# metadata/metadata.py\ndef safe_yaml_load(yaml_str: str, *, context: str = \"unknown\") -&gt; dict:\n    \"\"\"Parse YAML string to dict.\n\n    Raises:\n        FrontmatterParseError: If YAML parsing fails\n        InvalidMetadataError: If result is not a dict\n    \"\"\"\n    try:\n        data = yaml.safe_load(yaml_str)\n        if not isinstance(data, dict):\n            raise InvalidMetadataError(\n                f\"YAML in [{context}] is not a dict, got {type(data)}\"\n            )\n        return data\n    except yaml.ScannerError as e:\n        raise FrontmatterParseError(\n            f\"YAML scanner error in [{context}]: {e}\"\n        ) from e\n    except yaml.YAMLError as e:\n        raise FrontmatterParseError(\n            f\"YAML error in [{context}]: {e}\"\n        ) from e\n</code></pre> <p>Benefits: - Pure functions (no side effects) - Callers choose logging strategy (service layer logs, transport retries, etc.) - Typed errors enable better error handling - Testable without mocking loggers</p> <p>Decision: Implement for 0.2.0; track in TODO as \"Metadata error handling improvements\".</p>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#5-design-principles-summary","title":"5. Design Principles Summary","text":""},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#when-to-use-metadata-infrastructure","title":"When to Use Metadata Infrastructure","text":"Scenario Use Metadata? Pattern Parsing .md files with frontmatter \u2705 YES <code>Frontmatter.extract()</code> in mappers Flexible metadata storage \u2705 YES <code>Metadata</code> instead of <code>Dict[str, Any]</code> Tracking transformation provenance \u2705 YES <code>ProcessMetadata</code> in services Service-to-service data contracts \u274c NO Use Pydantic domain models Provider-specific vendor data \u2705 YES <code>Metadata</code> for flexible fields Strict business rules validation \u274c NO Use Pydantic schemas (validate after <code>Frontmatter.extract()</code>)"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#metadata-vs-pydantic-models","title":"Metadata vs. Pydantic Models","text":"<p>Use <code>Metadata</code> when: - Schema is flexible (user-supplied, vendor-specific) - Need dict-like operations (<code>|</code>, <code>[]</code>, iteration) - JSON serialization is primary concern - Provenance tracking with <code>ProcessMetadata</code></p> <p>Use Pydantic models when: - Schema is well-defined (domain objects) - Need strict validation (required fields, types) - Want IDE autocomplete and type checking - Encoding business rules</p> <p>Use both when (common pattern): <pre><code>class DocumentResult(BaseModel):\n    \"\"\"Domain model with strict validation.\"\"\"\n    id: str\n    content: str\n    language: str  # Strict field\n\n    # Flexible metadata for extensions\n    custom_metadata: Metadata = Metadata()\n</code></pre></p>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#consequences","title":"Consequences","text":""},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#positive","title":"Positive","text":"<ol> <li>No duplication: Services reuse frontmatter parsing instead of reimplementing</li> <li>Consistent behavior: All .md files parsed the same way (prompts, corpus, docs)</li> <li>Type safety: <code>Metadata</code> ensures JSON-serializable values (no serialization surprises)</li> <li>Future-ready: JSON-LD support (ADR-MD01) available when needed</li> <li>Provenance tracking: <code>ProcessMetadata</code> enables reproducible transformations</li> <li>Self-reflexive: System can operate on its own metadata-bearing artifacts</li> <li>Object-service aligned: Clear patterns for metadata usage in each layer</li> </ol>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#negative","title":"Negative","text":"<ol> <li>Mixed concerns (current): <code>Frontmatter.extract_from_file()</code> has I/O, needs documentation</li> <li>Logging side effects (current): <code>safe_yaml_load()</code> logs instead of raising exceptions</li> <li>Learning curve: Developers must understand when to use <code>Metadata</code> vs Pydantic</li> </ol>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#risks","title":"Risks","text":"<ol> <li>Temptation to expand: Metadata should stay simple; avoid adding service-specific logic</li> <li>Over-use: Don't use <code>Metadata</code> for everything; Pydantic models better for strict schemas</li> </ol>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#implementation-plan","title":"Implementation Plan","text":""},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#phase-1-documentation-immediate-014","title":"Phase 1: Documentation (Immediate - 0.1.4)","text":"<ul> <li> Document metadata role in ADR-OS01 (Section 3.3)</li> <li> Create ADR-MD02 (this document)</li> <li> Update ADR-PT04 addendum with as-built notes</li> <li> Add usage examples to metadata module docstrings</li> </ul>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#phase-2-api-cleanup-020","title":"Phase 2: API Cleanup (0.2.0)","text":"<ul> <li> Document <code>Frontmatter.extract_from_file()</code> as adapter-level helper</li> <li> Refactor <code>safe_yaml_load()</code> to raise typed exceptions (remove logging)</li> <li> Create <code>metadata/errors.py</code> with <code>MetadataError</code> hierarchy</li> <li> Update tests to catch typed exceptions</li> </ul>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#phase-3-broader-adoption-030","title":"Phase 3: Broader Adoption (0.3.0+)","text":"<ul> <li> Audit codebase for <code>Dict[str, Any]</code> usage \u2192 replace with <code>Metadata</code> where appropriate</li> <li> Add <code>ProcessMetadata</code> to translation/transcription pipelines</li> <li> Enable JSON-LD semantic queries in knowledge base (future)</li> </ul>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#open-questions","title":"Open Questions","text":"<ol> <li>Should <code>Metadata</code> support nested validation? Currently shallow; consider recursive validation for nested dicts/lists</li> <li>JSON-LD activation? When to fully enable schema.org vocabularies (deferred to knowledge base implementation)</li> <li>Metadata versioning? Should <code>Metadata</code> track schema versions for migrations?</li> </ol>"},{"location":"architecture/metadata/adr/adr-md02-metadata-object-service-integration/#references","title":"References","text":"<ul> <li>ADR-MD01: JSON-LD Metadata Strategy</li> <li>ADR-OS01: Object-Service Architecture V3</li> <li>ADR-PT04: Prompt System Refactor</li> <li>src/tnh_scholar/metadata/metadata.py</li> </ul> <p>Approval: Accepted 2025-12-07 (Aaron Solomon)</p>"},{"location":"architecture/object-service/","title":"Object Service","text":"<p>Table of Contents:</p> <p>Object-Service Design Gaps - Gaps, resolved items, and outstanding work needed to fully satisfy the Object-Service design blueprint.</p> <p>Object-Service Design Overview - High-level overview of TNH Scholar's layered architecture for complex objects and API-backed services.</p> <p>Object-Service Implementation Status - Implementation status, resolved gaps, and outstanding work for the Object-Service design architecture.</p> <p>Adr - Table of contents for architecture/object-service/adr</p> <p>This file auto-generated.</p>"},{"location":"architecture/object-service/object-service-design-gaps/","title":"Object-Service Design Gaps","text":"<p>Gaps, resolved items, and outstanding work needed to fully satisfy the Object-Service design blueprint.</p> <p>Purpose: Track progress on implementing ADR-OS01: Object-Service Design Architecture V3 Status: In Progress Last Updated: 2025-11-29</p> <p>Canonical Example: The GenAIService follows the object-service layers (domain/service/adapters/transport) and is the preferred reference implementation for new services until a dedicated sample is published.</p>"},{"location":"architecture/object-service/object-service-design-gaps/#resolved-items","title":"\u2705 Resolved Items","text":""},{"location":"architecture/object-service/object-service-design-gaps/#dependency-management-environment-resolved","title":"Dependency Management &amp; Environment (Resolved)","text":"<p>Status: \u2705 RESOLVED</p> <p>Resolution:</p> <ul> <li><code>pyproject.toml</code> exists with full dependency specifications</li> <li>Python 3.12.4 requirement established</li> <li>Pydantic V2 adopted throughout project</li> <li>Poetry used for dependency management</li> <li>Dev dependencies clearly separated</li> </ul> <p>Reference: See project <code>pyproject.toml</code> for complete configuration.</p>"},{"location":"architecture/object-service/object-service-design-gaps/#code-style-and-design-standards-resolved","title":"Code Style and Design Standards (Resolved)","text":"<p>Status: \u2705 RESOLVED</p> <p>Resolution:</p> <ul> <li>Style guide established: docs/development/style-guide.md</li> <li>Design principles documented: docs/development/design-principles.md</li> <li>Strong typing standards enforced (no dicts in app layer, Protocol/ABC usage)</li> <li>Google-style docstrings adopted</li> </ul> <p>Reference: Style Guide, Design Principles</p>"},{"location":"architecture/object-service/object-service-design-gaps/#priority-1-critical-gaps-must-address","title":"Priority 1: Critical Gaps (Must Address)","text":""},{"location":"architecture/object-service/object-service-design-gaps/#1-complete-runnable-example","title":"1. Complete Runnable Example","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>End-to-end working example showing all layers integrated</li> <li>Actual runnable file with imports, initialization, and execution</li> <li>Both sync (GenAI) and async (diarization) full implementations</li> <li>How to bootstrap the application from scratch</li> </ul> <p>Required Deliverable:</p> <ul> <li><code>examples/complete_genai_example.py</code> - Full working GenAI service</li> <li><code>examples/complete_diarization_example.py</code> - Full working async service</li> <li><code>examples/README.md</code> - How to run the examples</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#2-configuration-loading-flow","title":"2. Configuration Loading Flow","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How Settings cascade: env vars \u2192 .env \u2192 defaults</li> <li>Validation errors: what happens when API keys missing?</li> <li>Multiple environment support (dev/staging/prod)</li> <li>Secrets management (vault, AWS Secrets Manager?)</li> </ul> <p>Required Deliverable:</p> <ul> <li>Complete Settings loading logic with error handling</li> <li>Example <code>.env.example</code> file</li> <li>Documentation on configuration precedence</li> <li>Error messages for misconfiguration</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Required vs optional API keys (fail fast or lazy validation)?</li> <li>Support for multiple .env files (.env.local, .env.production)?</li> <li>Config validation at startup or on first use?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#3-error-handling-implementation-details","title":"3. Error Handling Implementation Details","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Retry logic: exponential backoff with jitter concrete implementation</li> <li>Circuit breaker pattern for failing providers?</li> <li>Fallback strategies when primary provider fails?</li> <li>Error propagation through layers (when to catch, when to re-raise?)</li> <li>Logging of errors vs user-facing error messages</li> </ul> <p>Required Deliverable:</p> <ul> <li>Complete retry decorator or class</li> <li>Circuit breaker implementation</li> <li>Error handling examples for each layer</li> <li>Error message guidelines (internal vs external)</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use tenacity library or custom retry logic?</li> <li>Circuit breaker per-provider or per-endpoint?</li> <li>How many retries before giving up?</li> <li>Should we expose provider errors to users?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#4-asyncawait-patterns","title":"4. Async/Await Patterns","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>When to use <code>async def</code> vs regular <code>def</code>?</li> <li>How to structure async adapters vs sync adapters?</li> <li>Event loop management</li> <li>Mixing sync and async code (running sync code in async context)</li> </ul> <p>Required Deliverable:</p> <ul> <li>Guidelines for async protocol design</li> <li>Example async adapter implementation</li> <li>Pattern for sync-to-async bridging</li> <li>Event loop best practices</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Support both sync and async variants of each protocol?</li> <li>Use asyncio.run() or manage loop explicitly?</li> <li>How to handle blocking SDK calls in async context?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#6-resource-management","title":"6. Resource Management","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Connection pooling for HTTP clients</li> <li>When to create/close clients (per-request vs singleton?)</li> <li>Resource limits (max concurrent requests)</li> <li>Memory management for large files/responses</li> </ul> <p>Required Deliverable:</p> <ul> <li>Client lifecycle management pattern</li> <li>Connection pool configuration</li> <li>Resource limit examples</li> <li>Memory-efficient file handling</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use context managers for all clients?</li> <li>Share connection pools across services?</li> <li>Max concurrent requests per provider?</li> <li>Streaming vs buffering for large responses?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#priority-2-important-clarifications-should-address","title":"Priority 2: Important Clarifications (Should Address)","text":""},{"location":"architecture/object-service/object-service-design-gaps/#7-logging-strategy","title":"7. Logging Strategy","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Structured logging format</li> <li>What to log at each layer (domain vs transport)</li> <li>PII handling in logs</li> <li>Log levels for different scenarios</li> <li>Correlation IDs for tracing requests</li> </ul> <p>Required Deliverable:</p> <ul> <li>Logging configuration example</li> <li>Logger setup per module</li> <li>PII redaction utilities</li> <li>Correlation ID propagation pattern</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use Python logging, structlog, or loguru?</li> <li>JSON logs in production?</li> <li>Log sampling for high-volume operations?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#8-testing-infrastructure","title":"8. Testing Infrastructure","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How to create test fixtures for each protocol</li> <li>Mock vs real API testing strategy</li> <li>VCR/cassette pattern for recording HTTP interactions</li> <li>How to test polling logic without waiting</li> <li>Coverage expectations</li> </ul> <p>Required Deliverable:</p> <ul> <li>Base test fixtures for each protocol type</li> <li>Mock provider implementations</li> <li>Example VCR tests (if using)</li> <li>Fast polling tests (time mocking)</li> <li>Coverage target (80%? 90%?)</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use pytest-vcr or responses library?</li> <li>Run integration tests in CI?</li> <li>Separate unit/integration test commands?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#9-type-checking-validation","title":"9. Type Checking &amp; Validation","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>When to use <code>mypy --strict</code>?</li> <li>Runtime validation patterns with Pydantic</li> <li>Optional vs required fields strategy</li> <li>Custom validators examples</li> </ul> <p>Required Deliverable:</p> <ul> <li>mypy configuration</li> <li>Type checking guidelines</li> <li>Pydantic validator examples</li> <li>Type stub handling for untyped libraries</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Require 100% type coverage?</li> <li>Use Pydantic v2 validators or v1 style?</li> <li>Type check tests as well?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#10-serialization-persistence","title":"10. Serialization &amp; Persistence","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How to serialize Envelopes for storage?</li> <li>JSON encoding of complex types (datetime, Path, etc.)</li> <li>Database schema if persisting results</li> <li>File format for storing provenance</li> </ul> <p>Required Deliverable:</p> <ul> <li>Envelope serialization utilities</li> <li>JSON encoder for custom types</li> <li>Example persistence layer</li> <li>Schema migration strategy</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Store as JSON, pickle, or other format?</li> <li>Database or filesystem storage?</li> <li>Compress large payloads?</li> <li>Retention policy for old results?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#11-rate-limiting-implementation","title":"11. Rate Limiting Implementation","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Config has <code>rate_limit_rps</code> but no implementation</li> <li>Token bucket vs leaky bucket algorithm?</li> <li>Per-provider limits vs global limits</li> <li>Queue management when rate limited</li> </ul> <p>Required Deliverable:</p> <ul> <li>Rate limiter implementation</li> <li>Configuration per provider</li> <li>Queue/backpressure handling</li> <li>Rate limit exceeded error handling</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use existing library (aiolimiter, ratelimit) or custom?</li> <li>Client-side rate limiting only or server-enforced?</li> <li>How to handle burst traffic?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#12-timeout-handling","title":"12. Timeout Handling","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Connect timeout vs read timeout vs total timeout</li> <li>How timeouts propagate through layers</li> <li>Graceful degradation vs hard failure</li> <li>Deadline propagation in nested calls</li> </ul> <p>Required Deliverable:</p> <ul> <li>Timeout configuration examples</li> <li>Deadline propagation pattern</li> <li>Timeout error handling</li> <li>Partial result handling on timeout</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Default timeout values per operation type?</li> <li>User-configurable timeouts?</li> <li>What happens to in-flight requests on timeout?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#priority-3-nice-to-have-consider-adding","title":"Priority 3: Nice to Have (Consider Adding)","text":""},{"location":"architecture/object-service/object-service-design-gaps/#13-performance-patterns","title":"13. Performance Patterns","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Caching strategies (where, when, invalidation)</li> <li>Batch request optimization</li> <li>Streaming response handling</li> <li>Memory-efficient processing of large files</li> </ul> <p>Required Deliverable:</p> <ul> <li>Caching layer examples</li> <li>Batch processing utilities</li> <li>Streaming adapter pattern</li> <li>Large file handling guide</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#14-security-considerations","title":"14. Security Considerations","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>API key rotation strategy</li> <li>Input sanitization</li> <li>Output validation (prevent injection)</li> <li>Rate limiting to prevent abuse</li> </ul> <p>Required Deliverable:</p> <ul> <li>Security checklist</li> <li>Input validation examples</li> <li>API key rotation procedure</li> <li>Abuse prevention guidelines</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#15-monitoring-metrics","title":"15. Monitoring &amp; Metrics","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Which metrics to track (latency, cost, errors)</li> <li>Prometheus exposition format example</li> <li>Health check endpoints</li> <li>Alerting thresholds</li> </ul> <p>Required Deliverable:</p> <ul> <li>Metrics collection layer</li> <li>Prometheus exporter example</li> <li>Health check implementation</li> <li>Alerting rule examples</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#16-documentation-standards","title":"16. Documentation Standards","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Docstring format (Google, NumPy, or reStructuredText?)</li> <li>Type annotation requirements</li> <li>README structure for each module</li> <li>API documentation generation (Sphinx?)</li> </ul> <p>Required Deliverable:</p> <ul> <li>Documentation style guide</li> <li>Example module documentation</li> <li>API doc generation setup</li> <li>Contribution guidelines</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#17-development-workflow","title":"17. Development Workflow","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How to run locally for development</li> <li>Hot reload setup</li> <li>Debug configuration</li> <li>Pre-commit hooks (formatting, linting)</li> </ul> <p>Required Deliverable:</p> <ul> <li>Development setup guide</li> <li>IDE configuration examples</li> <li>Pre-commit hook configuration</li> <li>Debugging tips and tools</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#18-deployment","title":"18. Deployment","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Docker container structure</li> <li>Environment variable injection</li> <li>Health checks and readiness probes</li> <li>Scaling considerations</li> </ul> <p>Required Deliverable:</p> <ul> <li>Dockerfile</li> <li>docker-compose.yml for local dev</li> <li>Kubernetes manifests (if applicable)</li> <li>Deployment checklist</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#19-migration-patterns","title":"19. Migration Patterns","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How to handle schema changes</li> <li>Backward compatibility strategy</li> <li>Data migration scripts</li> <li>API versioning (if exposing HTTP API)</li> </ul> <p>Required Deliverable:</p> <ul> <li>Migration guide</li> <li>Schema versioning strategy</li> <li>Backward compatibility testing</li> <li>Breaking change protocol</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#20-protocol-extension-examples","title":"20. Protocol Extension Examples","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How to add a new provider (step-by-step)</li> <li>How to add a new service following the pattern</li> <li>Common pitfalls and how to avoid them</li> </ul> <p>Required Deliverable:</p> <ul> <li>Provider implementation guide</li> <li>Service implementation guide</li> <li>Troubleshooting guide</li> <li>Checklist for new implementations</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#specific-open-questions","title":"Specific Open Questions","text":""},{"location":"architecture/object-service/object-service-design-gaps/#mapper-pattern","title":"Mapper Pattern","text":"<ul> <li>Should mappers be stateless functions or stateful classes?</li> <li>When to choose each approach?</li> <li>Should mappers handle validation or just transformation?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#protocol-vs-abc","title":"Protocol vs ABC","text":"<ul> <li>Concrete guidance on when structural typing isn't enough</li> <li>When to use ABC for enforcement vs Protocol for flexibility</li> <li>How to document Protocol requirements?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#error-granularity","title":"Error Granularity","text":"<ul> <li>Should <code>ProviderError</code> have subclasses per error type or per provider?</li> <li>How fine-grained should error classification be?</li> <li>Standard error codes across providers?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#streaming-support","title":"Streaming Support","text":"<ul> <li>How to handle streaming responses in the protocol?</li> <li>Sync vs async streaming</li> <li>Backpressure handling</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#testing-mocks","title":"Testing Mocks","text":"<ul> <li>Should mocks live in test files or reusable fixtures?</li> <li>Shared mock implementations across test suites?</li> <li>How to keep mocks in sync with real implementations?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#policy-precedence","title":"Policy Precedence","text":"<ul> <li>When policies conflict, exact resolution algorithm?</li> <li>How to detect and warn about conflicts?</li> <li>Should policy conflicts be errors or warnings?</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#work-plan-template","title":"Work Plan Template","text":"<p>For each item above, the completion checklist should include:</p> <ul> <li> Design decision documented</li> <li> Implementation completed</li> <li> Example code provided</li> <li> Tests written</li> <li> Documentation updated</li> <li> Code review completed</li> <li> Merged to main branch</li> </ul>"},{"location":"architecture/object-service/object-service-design-gaps/#notes","title":"Notes","text":"<ul> <li>Items marked \u274c need to be addressed</li> <li>Items marked \u2705 are complete</li> <li>Items marked \u26a0\ufe0f are partially complete</li> <li>Priority 1 items should be completed before Priority 2</li> <li>Priority 2 items should be completed before Priority 3</li> </ul>"},{"location":"architecture/object-service/object-service-design-overview/","title":"Object-Service Design Overview","text":"<p>High-level overview of TNH Scholar's layered architecture for complex objects and API-backed services.</p>"},{"location":"architecture/object-service/object-service-design-overview/#purpose","title":"Purpose","text":"<p>This document provides a high-level introduction to the Object-Service design pattern used throughout TNH Scholar. For complete architectural details, see ADR-OS01: Object-Service Design Architecture V3.</p>"},{"location":"architecture/object-service/object-service-design-overview/#core-principle","title":"Core Principle","text":"<p>Goal: Ship reliable, composable features fast by separating concerns and keeping boundaries explicit.</p>"},{"location":"architecture/object-service/object-service-design-overview/#architectural-layers","title":"Architectural Layers","text":"<pre><code>Application Layer (CLI, notebooks, web, Streamlit)\n  \u2514\u2500 Orchestrators (thin): &lt;Feature&gt;Processor, ResultWriter\n        \u25b2\n        \u2502 domain objects / protocols\n        \u2502\nDomain Service Layer\n  \u2514\u2500 &lt;Feature&gt;Service (Protocol-based orchestrator)\n     \u2514\u2500 ProviderClient (impl: OpenAIClient, PyannoteClient, etc.)\n        \u2514\u2500 RequestMapper (bi-directional: domain \u2194 transport)\n           \u2514\u2500 TransportClient (HTTP, polling, streaming)\n        \u25b2\n        \u2502 transport models (anti-corruption boundary)\n        \u2502\nTransport Layer\n  \u2514\u2500 VendorClient  (upload/start/status/poll, retries, rate-limit)\n     JobPoller     (backoff, jitter, deadline)\n</code></pre>"},{"location":"architecture/object-service/object-service-design-overview/#key-design-contracts","title":"Key Design Contracts","text":"<ol> <li>Config at init, Params per call, Response envelope always</li> <li>Service protocol is minimal: <code>start()</code>, <code>get_response()</code>, <code>generate()</code> for async jobs; <code>generate()</code> or <code>run()</code> for sync</li> <li>Adapter maps API shapes \u2192 canonical domain shapes (bi-directional)</li> <li>All payloads use strong typing (Pydantic or dataclass models)</li> <li>No literals or untyped dicts in application logic</li> </ol>"},{"location":"architecture/object-service/object-service-design-overview/#primary-benefits","title":"Primary Benefits","text":"<ul> <li>Separation of Concerns: Domain logic isolated from API transport details</li> <li>Testability: Each layer can be tested independently with clear interfaces</li> <li>Flexibility: Swap providers without changing application code</li> <li>Type Safety: Strong typing throughout prevents runtime errors</li> <li>Maintainability: Clear boundaries make refactoring safer</li> </ul>"},{"location":"architecture/object-service/object-service-design-overview/#core-services","title":"Core Services","text":"<p>Current TNH Scholar services following this architecture:</p> <ul> <li>GenAIService: AI text processing with OpenAI/Anthropic providers</li> <li>TranscriptionService: Audio transcription with Whisper/AssemblyAI providers</li> <li>DiarizationService: Speaker diarization with Pyannote provider</li> </ul> <p>Future services planned:</p> <ul> <li>PromptCatalogService: Enhanced prompt management</li> <li>EmbeddingService: Text embeddings for semantic search</li> <li>VectorStoreService: Vector database integration</li> </ul>"},{"location":"architecture/object-service/object-service-design-overview/#implementation-status","title":"Implementation Status","text":"<p>See object-service-implementation-status.md for current gaps, resolved items, and planned work.</p>"},{"location":"architecture/object-service/object-service-design-overview/#related-documentation","title":"Related Documentation","text":"<ul> <li>ADR-OS01: Object-Service Design Architecture V3 - Complete architectural specification</li> <li>Implementation Status - Gaps analysis and progress</li> <li>Design Principles - General design philosophy</li> <li>Conceptual Architecture - High-level system model</li> </ul>"},{"location":"architecture/object-service/object-service-design-overview/#quick-reference","title":"Quick Reference","text":""},{"location":"architecture/object-service/object-service-design-overview/#service-protocol-example","title":"Service Protocol Example","text":"<pre><code>from typing import Protocol\n\nclass TextProcessingService(Protocol):\n    \"\"\"Protocol for text processing services.\"\"\"\n\n    def generate(\n        self,\n        request: ProcessingRequest\n    ) -&gt; ProcessingResponse:\n        \"\"\"Process text and return result.\"\"\"\n        ...\n</code></pre>"},{"location":"architecture/object-service/object-service-design-overview/#adapter-pattern-example","title":"Adapter Pattern Example","text":"<pre><code>class OpenAIAdapter:\n    \"\"\"Maps domain models to/from OpenAI API.\"\"\"\n\n    def to_api_request(\n        self,\n        domain_request: ProcessingRequest\n    ) -&gt; OpenAICompletionRequest:\n        \"\"\"Map domain request to OpenAI format.\"\"\"\n        ...\n\n    def from_api_response(\n        self,\n        api_response: OpenAICompletion\n    ) -&gt; ProcessingResponse:\n        \"\"\"Map OpenAI response to domain format.\"\"\"\n        ...\n</code></pre> <p>For complete examples, templates, and detailed patterns, see ADR-OS01.</p>"},{"location":"architecture/object-service/object-service-implementation-status/","title":"Object-Service Implementation Status","text":"<p>Implementation status, resolved gaps, and outstanding work for the Object-Service design architecture.</p> <p>Purpose: Track progress on implementing ADR-OS01: Object-Service Design Architecture V3 Status: In Progress Last Updated: 2025-11-29</p>"},{"location":"architecture/object-service/object-service-implementation-status/#resolved-items","title":"\u2705 Resolved Items","text":""},{"location":"architecture/object-service/object-service-implementation-status/#dependency-management-environment-resolved","title":"Dependency Management &amp; Environment (Resolved)","text":"<p>Status: \u2705 RESOLVED</p> <p>Resolution:</p> <ul> <li><code>pyproject.toml</code> exists with full dependency specifications</li> <li>Python 3.12.4 requirement established</li> <li>Pydantic V2 adopted throughout project</li> <li>Poetry used for dependency management</li> <li>Dev dependencies clearly separated</li> </ul> <p>Reference: See project <code>pyproject.toml</code> for complete configuration.</p>"},{"location":"architecture/object-service/object-service-implementation-status/#code-style-and-design-standards-resolved","title":"Code Style and Design Standards (Resolved)","text":"<p>Status: \u2705 RESOLVED</p> <p>Resolution:</p> <ul> <li>Style guide established: docs/development/style-guide.md</li> <li>Design principles documented: docs/development/design-principles.md</li> <li>Strong typing standards enforced (no dicts in app layer, Protocol/ABC usage)</li> <li>Google-style docstrings adopted</li> </ul> <p>Reference: Style Guide, Design Principles</p>"},{"location":"architecture/object-service/object-service-implementation-status/#priority-1-critical-gaps-must-address","title":"Priority 1: Critical Gaps (Must Address)","text":""},{"location":"architecture/object-service/object-service-implementation-status/#1-complete-runnable-example","title":"1. Complete Runnable Example","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>End-to-end working example showing all layers integrated</li> <li>Actual runnable file with imports, initialization, and execution</li> <li>Both sync (GenAI) and async (diarization) full implementations</li> <li>How to bootstrap the application from scratch</li> </ul> <p>Required Deliverable:</p> <ul> <li><code>examples/complete_genai_example.py</code> - Full working GenAI service</li> <li><code>examples/complete_diarization_example.py</code> - Full working async service</li> <li><code>examples/README.md</code> - How to run the examples</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#2-dependency-management-environment","title":"2. Dependency Management &amp; Environment","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>No <code>pyproject.toml</code> or <code>requirements.txt</code> specification</li> <li>Which versions of libraries? (Pydantic v1 vs v2? OpenAI SDK version?)</li> <li>Python version requirements</li> <li>Development dependencies (pytest versions, type checkers)</li> </ul> <p>Required Deliverable:</p> <ul> <li><code>pyproject.toml</code> with all dependencies and versions</li> <li>Minimum Python version specified</li> <li>Dev dependencies section</li> <li>Optional dependencies per provider</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use Poetry, pip-tools, or plain requirements.txt?</li> <li>Pydantic v2 only or support v1 compatibility?</li> <li>Pin exact versions or use ranges?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#3-configuration-loading-flow","title":"3. Configuration Loading Flow","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How Settings cascade: env vars \u2192 .env \u2192 defaults</li> <li>Validation errors: what happens when API keys missing?</li> <li>Multiple environment support (dev/staging/prod)</li> <li>Secrets management (vault, AWS Secrets Manager?)</li> </ul> <p>Required Deliverable:</p> <ul> <li>Complete Settings loading logic with error handling</li> <li>Example <code>.env.example</code> file</li> <li>Documentation on configuration precedence</li> <li>Error messages for misconfiguration</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Required vs optional API keys (fail fast or lazy validation)?</li> <li>Support for multiple .env files (.env.local, .env.production)?</li> <li>Config validation at startup or on first use?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#4-error-handling-implementation-details","title":"4. Error Handling Implementation Details","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Retry logic: exponential backoff with jitter concrete implementation</li> <li>Circuit breaker pattern for failing providers?</li> <li>Fallback strategies when primary provider fails?</li> <li>Error propagation through layers (when to catch, when to re-raise?)</li> <li>Logging of errors vs user-facing error messages</li> </ul> <p>Required Deliverable:</p> <ul> <li>Complete retry decorator or class</li> <li>Circuit breaker implementation</li> <li>Error handling examples for each layer</li> <li>Error message guidelines (internal vs external)</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use tenacity library or custom retry logic?</li> <li>Circuit breaker per-provider or per-endpoint?</li> <li>How many retries before giving up?</li> <li>Should we expose provider errors to users?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#5-asyncawait-patterns","title":"5. Async/Await Patterns","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>When to use <code>async def</code> vs regular <code>def</code>?</li> <li>How to structure async adapters vs sync adapters?</li> <li>Event loop management</li> <li>Mixing sync and async code (running sync code in async context)</li> </ul> <p>Required Deliverable:</p> <ul> <li>Guidelines for async protocol design</li> <li>Example async adapter implementation</li> <li>Pattern for sync-to-async bridging</li> <li>Event loop best practices</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Support both sync and async variants of each protocol?</li> <li>Use asyncio.run() or manage loop explicitly?</li> <li>How to handle blocking SDK calls in async context?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#6-resource-management","title":"6. Resource Management","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Connection pooling for HTTP clients</li> <li>When to create/close clients (per-request vs singleton?)</li> <li>Resource limits (max concurrent requests)</li> <li>Memory management for large files/responses</li> </ul> <p>Required Deliverable:</p> <ul> <li>Client lifecycle management pattern</li> <li>Connection pool configuration</li> <li>Resource limit examples</li> <li>Memory-efficient file handling</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use context managers for all clients?</li> <li>Share connection pools across services?</li> <li>Max concurrent requests per provider?</li> <li>Streaming vs buffering for large responses?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#priority-2-important-clarifications-should-address","title":"Priority 2: Important Clarifications (Should Address)","text":""},{"location":"architecture/object-service/object-service-implementation-status/#7-logging-strategy","title":"7. Logging Strategy","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Structured logging format</li> <li>What to log at each layer (domain vs transport)</li> <li>PII handling in logs</li> <li>Log levels for different scenarios</li> <li>Correlation IDs for tracing requests</li> </ul> <p>Required Deliverable:</p> <ul> <li>Logging configuration example</li> <li>Logger setup per module</li> <li>PII redaction utilities</li> <li>Correlation ID propagation pattern</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use Python logging, structlog, or loguru?</li> <li>JSON logs in production?</li> <li>Log sampling for high-volume operations?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#8-testing-infrastructure","title":"8. Testing Infrastructure","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How to create test fixtures for each protocol</li> <li>Mock vs real API testing strategy</li> <li>VCR/cassette pattern for recording HTTP interactions</li> <li>How to test polling logic without waiting</li> <li>Coverage expectations</li> </ul> <p>Required Deliverable:</p> <ul> <li>Base test fixtures for each protocol type</li> <li>Mock provider implementations</li> <li>Example VCR tests (if using)</li> <li>Fast polling tests (time mocking)</li> <li>Coverage target (80%? 90%?)</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use pytest-vcr or responses library?</li> <li>Run integration tests in CI?</li> <li>Separate unit/integration test commands?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#9-type-checking-validation","title":"9. Type Checking &amp; Validation","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>When to use <code>mypy --strict</code>?</li> <li>Runtime validation patterns with Pydantic</li> <li>Optional vs required fields strategy</li> <li>Custom validators examples</li> </ul> <p>Required Deliverable:</p> <ul> <li>mypy configuration</li> <li>Type checking guidelines</li> <li>Pydantic validator examples</li> <li>Type stub handling for untyped libraries</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Require 100% type coverage?</li> <li>Use Pydantic v2 validators or v1 style?</li> <li>Type check tests as well?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#10-serialization-persistence","title":"10. Serialization &amp; Persistence","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How to serialize Envelopes for storage?</li> <li>JSON encoding of complex types (datetime, Path, etc.)</li> <li>Database schema if persisting results</li> <li>File format for storing provenance</li> </ul> <p>Required Deliverable:</p> <ul> <li>Envelope serialization utilities</li> <li>JSON encoder for custom types</li> <li>Example persistence layer</li> <li>Schema migration strategy</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Store as JSON, pickle, or other format?</li> <li>Database or filesystem storage?</li> <li>Compress large payloads?</li> <li>Retention policy for old results?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#11-rate-limiting-implementation","title":"11. Rate Limiting Implementation","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Config has <code>rate_limit_rps</code> but no implementation</li> <li>Token bucket vs leaky bucket algorithm?</li> <li>Per-provider limits vs global limits</li> <li>Queue management when rate limited</li> </ul> <p>Required Deliverable:</p> <ul> <li>Rate limiter implementation</li> <li>Configuration per provider</li> <li>Queue/backpressure handling</li> <li>Rate limit exceeded error handling</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Use existing library (aiolimiter, ratelimit) or custom?</li> <li>Client-side rate limiting only or server-enforced?</li> <li>How to handle burst traffic?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#12-timeout-handling","title":"12. Timeout Handling","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Connect timeout vs read timeout vs total timeout</li> <li>How timeouts propagate through layers</li> <li>Graceful degradation vs hard failure</li> <li>Deadline propagation in nested calls</li> </ul> <p>Required Deliverable:</p> <ul> <li>Timeout configuration examples</li> <li>Deadline propagation pattern</li> <li>Timeout error handling</li> <li>Partial result handling on timeout</li> </ul> <p>Questions to Resolve:</p> <ul> <li>Default timeout values per operation type?</li> <li>User-configurable timeouts?</li> <li>What happens to in-flight requests on timeout?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#priority-3-nice-to-have-consider-adding","title":"Priority 3: Nice to Have (Consider Adding)","text":""},{"location":"architecture/object-service/object-service-implementation-status/#13-performance-patterns","title":"13. Performance Patterns","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Caching strategies (where, when, invalidation)</li> <li>Batch request optimization</li> <li>Streaming response handling</li> <li>Memory-efficient processing of large files</li> </ul> <p>Required Deliverable:</p> <ul> <li>Caching layer examples</li> <li>Batch processing utilities</li> <li>Streaming adapter pattern</li> <li>Large file handling guide</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#14-security-considerations","title":"14. Security Considerations","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>API key rotation strategy</li> <li>Input sanitization</li> <li>Output validation (prevent injection)</li> <li>Rate limiting to prevent abuse</li> </ul> <p>Required Deliverable:</p> <ul> <li>Security checklist</li> <li>Input validation examples</li> <li>API key rotation procedure</li> <li>Abuse prevention guidelines</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#15-monitoring-metrics","title":"15. Monitoring &amp; Metrics","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Which metrics to track (latency, cost, errors)</li> <li>Prometheus exposition format example</li> <li>Health check endpoints</li> <li>Alerting thresholds</li> </ul> <p>Required Deliverable:</p> <ul> <li>Metrics collection layer</li> <li>Prometheus exporter example</li> <li>Health check implementation</li> <li>Alerting rule examples</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#16-documentation-standards","title":"16. Documentation Standards","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Docstring format (Google, NumPy, or reStructuredText?)</li> <li>Type annotation requirements</li> <li>README structure for each module</li> <li>API documentation generation (Sphinx?)</li> </ul> <p>Required Deliverable:</p> <ul> <li>Documentation style guide</li> <li>Example module documentation</li> <li>API doc generation setup</li> <li>Contribution guidelines</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#17-development-workflow","title":"17. Development Workflow","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How to run locally for development</li> <li>Hot reload setup</li> <li>Debug configuration</li> <li>Pre-commit hooks (formatting, linting)</li> </ul> <p>Required Deliverable:</p> <ul> <li>Development setup guide</li> <li>IDE configuration examples</li> <li>Pre-commit hook configuration</li> <li>Debugging tips and tools</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#18-deployment","title":"18. Deployment","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>Docker container structure</li> <li>Environment variable injection</li> <li>Health checks and readiness probes</li> <li>Scaling considerations</li> </ul> <p>Required Deliverable:</p> <ul> <li>Dockerfile</li> <li>docker-compose.yml for local dev</li> <li>Kubernetes manifests (if applicable)</li> <li>Deployment checklist</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#19-migration-patterns","title":"19. Migration Patterns","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How to handle schema changes</li> <li>Backward compatibility strategy</li> <li>Data migration scripts</li> <li>API versioning (if exposing HTTP API)</li> </ul> <p>Required Deliverable:</p> <ul> <li>Migration guide</li> <li>Schema versioning strategy</li> <li>Backward compatibility testing</li> <li>Breaking change protocol</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#20-protocol-extension-examples","title":"20. Protocol Extension Examples","text":"<p>Status: \u274c Not Addressed</p> <p>What's Missing:</p> <ul> <li>How to add a new provider (step-by-step)</li> <li>How to add a new service following the pattern</li> <li>Common pitfalls and how to avoid them</li> </ul> <p>Required Deliverable:</p> <ul> <li>Provider implementation guide</li> <li>Service implementation guide</li> <li>Troubleshooting guide</li> <li>Checklist for new implementations</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#specific-open-questions","title":"Specific Open Questions","text":""},{"location":"architecture/object-service/object-service-implementation-status/#mapper-pattern","title":"Mapper Pattern","text":"<ul> <li>Should mappers be stateless functions or stateful classes?</li> <li>When to choose each approach?</li> <li>Should mappers handle validation or just transformation?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#protocol-vs-abc","title":"Protocol vs ABC","text":"<ul> <li>Concrete guidance on when structural typing isn't enough</li> <li>When to use ABC for enforcement vs Protocol for flexibility</li> <li>How to document Protocol requirements?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#error-granularity","title":"Error Granularity","text":"<ul> <li>Should <code>ProviderError</code> have subclasses per error type or per provider?</li> <li>How fine-grained should error classification be?</li> <li>Standard error codes across providers?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#streaming-support","title":"Streaming Support","text":"<ul> <li>How to handle streaming responses in the protocol?</li> <li>Sync vs async streaming</li> <li>Backpressure handling</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#testing-mocks","title":"Testing Mocks","text":"<ul> <li>Should mocks live in test files or reusable fixtures?</li> <li>Shared mock implementations across test suites?</li> <li>How to keep mocks in sync with real implementations?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#policy-precedence","title":"Policy Precedence","text":"<ul> <li>When policies conflict, exact resolution algorithm?</li> <li>How to detect and warn about conflicts?</li> <li>Should policy conflicts be errors or warnings?</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#work-plan-template","title":"Work Plan Template","text":"<p>For each item above, the completion checklist should include:</p> <ul> <li> Design decision documented</li> <li> Implementation completed</li> <li> Example code provided</li> <li> Tests written</li> <li> Documentation updated</li> <li> Code review completed</li> <li> Merged to main branch</li> </ul>"},{"location":"architecture/object-service/object-service-implementation-status/#notes","title":"Notes","text":"<ul> <li>Items marked \u274c need to be addressed</li> <li>Items marked \u2705 are complete</li> <li>Items marked \u26a0\ufe0f are partially complete</li> <li>Priority 1 items should be completed before Priority 2</li> <li>Priority 2 items should be completed before Priority 3</li> </ul>"},{"location":"architecture/object-service/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-OS01: Object-Service Design Architecture V3 - Establishes layered architecture for complex objects and API-backed services across TNH Scholar, defining clear boundaries between domain, service, and transport layers.</p> <p>This file auto-generated.</p>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/","title":"ADR-OS01: Object-Service Design Architecture V3","text":"<p>Practical, opinionated architecture for design, implementation, and evolution of complex objects and API-backed services across the TNH Scholar suite.</p> <ul> <li>Filename: <code>adr-os01-design-v3.md</code></li> <li>Heading: <code># ADR-OS01: Object-Service Design Architecture V3</code></li> <li>Status: Accepted</li> <li>Date: 2025-10-24</li> <li>Authors: Aaron Solomon, OpenAI GPT-5, Anthropic Claude Sonnet 4.5</li> <li>Version: 3.0 (Integrated)</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#0-big-picture-at-a-glance","title":"0. Big Picture (At a Glance)","text":"<p>Goal: Ship reliable, composable features fast by separating concerns and keeping boundaries explicit.</p>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#architectural-overview","title":"Architectural Overview","text":"<pre><code>Application Layer (CLI, notebooks, web, Streamlit)\n  \u2514\u2500 Orchestrators (thin): &lt;Feature&gt;Processor, ResultWriter\n        \u25b2\n        \u2502 domain objects / protocols\n        \u2502\nDomain Service Layer\n  \u2514\u2500 &lt;Feature&gt;Service (Protocol-based orchestrator)\n     \u2514\u2500 ProviderClient (impl: OpenAIClient, PyannoteClient, etc.)\n        \u2514\u2500 RequestMapper (bi-directional: domain \u2194 transport)\n           \u2514\u2500 TransportClient (HTTP, polling, streaming)\n        \u25b2\n        \u2502 transport models (anti-corruption boundary)\n        \u2502\nTransport Layer\n  \u2514\u2500 VendorClient  (upload/start/status/poll, retries, rate-limit)\n     JobPoller     (backoff, jitter, deadline)\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#unified-layer-model","title":"Unified Layer Model","text":"<pre><code>graph TD\n  A[App / CLI] --&gt; B[Service Layer / Orchestrator]\n  B --&gt; C[Port / Protocol]\n  C --&gt; D[Adapter + Mapper]\n  D --&gt; E[Transport Client]\n  E --&gt; F[External Provider / SDK]\n\n  subgraph Domain Layer\n    B\n    C\n  end\n\n  subgraph Infrastructure Layer\n    D\n    E\n  end</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#core-contracts","title":"Core Contracts","text":"<ul> <li>Config at init, Params per call, Response envelope always</li> <li>Service protocol is minimal: <code>start()</code>, <code>get_response()</code>, <code>generate()</code> for async jobs; <code>generate()</code> or <code>run()</code> for sync</li> <li>Adapter maps API shapes \u2192 canonical domain shapes (bi-directional)</li> <li>All payloads use strong typing (Pydantic or dataclass models)</li> <li>No literals or untyped dicts in application logic</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#1-design-goals-principles","title":"1. Design Goals &amp; Principles","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#11-core-design-goals","title":"1.1 Core Design Goals","text":"<ol> <li>Strong Typing: All payloads, parameters, and responses use Pydantic or dataclass models</li> <li>No Literals: No string or numeric literals in app logic; defaults come from Settings or Policies</li> <li>Bi-directional Adapters: Each adapter maps domain \u2192 SDK (request) and SDK \u2192 domain (response)</li> <li>Protocol-based Ports: All boundaries declared as <code>Protocol</code> interfaces (structural typing)</li> <li>Replaceable Providers: Any external dependency isolated behind an adapter implementing common protocol</li> <li>Declarative Configuration: Every component derives runtime config from typed sources</li> <li>Transport / Domain Separation: Domain logic independent of serialization or transmission</li> <li>No Side-Effects in Domain: All side-effects (I/O, network) live exclusively in adapters or clients</li> <li>Explicit Boundaries: Clear separation between config, params, policy, and context</li> </ol>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#12-rule-of-thumb","title":"1.2 Rule of Thumb","text":"<ul> <li>Wire behavior \u2192 Client</li> <li>Canonical data shape \u2192 Adapter/Service  </li> <li>Usage intent \u2192 Params/Policy</li> <li>Long-lived configuration \u2192 Config/Settings</li> <li>Defaults \u2192 Settings &lt; Pattern &lt; Policy &lt; Request (precedence order)</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#2-core-concepts-naming-taxonomy","title":"2. Core Concepts &amp; Naming Taxonomy","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#21-configuration-runtime-concepts","title":"2.1 Configuration &amp; Runtime Concepts","text":"Concept Type Scope Purpose Example Settings <code>BaseSettings</code> Application Environment/deployment config API keys, base URLs, default models Config <code>&lt;Thing&gt;Config</code> Service/Client Construction-time settings; immutable TransportConfig, ProviderConfig Params <code>&lt;Thing&gt;Params</code> Per-call Request-specific inputs; vary run-to-run DiarizationParams, RenderRequest Policy <code>DomainPolicy</code>, <code>ServicePolicy</code> Service Opinionated behavior toggles ParamsPolicy, completion_mode, overlap handling Pattern <code>Pattern</code> Domain Prompt templates with metadata System/user message templates Context <code>ExecutionContext</code> Pipeline Ephemeral runtime state (optional) Current stage, accumulated metadata"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#22-response-result-concepts","title":"2.2 Response &amp; Result Concepts","text":"Concept Type Purpose Presence Envelope <code>Envelope</code> Universal response wrapper Always Result <code>&lt;Feature&gt;Result</code> Domain success payload Only when <code>status == \"succeeded\"</code> Response <code>ProviderResponse</code> Transport-level response At adapter boundary Provenance <code>Provenance</code> Metadata about execution Always (in Envelope)"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#23-architectural-concepts","title":"2.3 Architectural Concepts","text":"Concept Purpose Location Implements Client Pure transport/HTTP operations <code>transport/</code> Protocol (e.g., <code>ProviderClient</code>) Mapper Bi-directional type translation Inside Adapter Pure functions or classes Adapter Transport \u2194 domain mapping <code>adapters/</code> or <code>providers/</code> Protocol + uses Mapper Service Orchestration, composes ports <code>service/</code> Protocol Processor Application fa\u00e7ade <code>app/</code> Thin; wires persistence Port Abstract interface Domain layer <code>Protocol</code> definition"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#3-layer-structure-responsibilities","title":"3. Layer Structure &amp; Responsibilities","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#31-conceptual-layers","title":"3.1 Conceptual Layers","text":"Layer Responsibility Example Classes Allowed Dependencies Application Entry points, user interaction CLI, notebooks, Streamlit Service protocols only Service (Orchestrator) Coordinates flow, composes ports <code>GenAIService</code>, <code>DiarizationService</code> Ports, Settings, Policy Ports (Protocols) Abstract interfaces <code>ProviderClient</code>, <code>AudioDiarizer</code> Domain models only Adapters Implement protocols, SDK mapping <code>OpenAIClient</code>, <code>PyannoteClient</code> Mappers, Transport models Mappers Bi-directional type translation <code>OpenAIRequestMapper</code> Transport + SDK types Transport Clients HTTP, polling, streaming <code>VendorClient</code>, <code>JobPoller</code> Config only Domain Models Typed business objects <code>CompletionResult</code>, <code>SpeakerBlock</code> Nothing (pure) Transport Models I/O-level contracts <code>ProviderRequest</code>, <code>JobStatusResponse</code> Nothing (pure)"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#32-dependency-direction","title":"3.2 Dependency Direction","text":"<p>Dependencies flow inward: <code>Application \u2192 Service \u2192 Port \u2192 (Adapter \u2192 Mapper \u2192 Transport Client) \u2192 External</code>.</p> <p>Forbidden dependencies:</p> <ul> <li>Domain \u2192 Infrastructure</li> <li>Models \u2192 Services</li> <li>Ports \u2192 Implementations</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#4-configuration-taxonomy","title":"4. Configuration Taxonomy","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#41-settings-application-wide","title":"4.1 Settings (Application-wide)","text":"<pre><code># config/settings.py\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nclass Settings(BaseSettings):\n    \"\"\"Application-wide environment configuration.\"\"\"\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        extra=\"ignore\"\n    )\n\n    # Provider credentials\n    openai_api_key: str | None = None\n    openai_org: str | None = None\n    assemblyai_api_key: str | None = None\n\n    # Defaults\n    default_provider: str = \"openai\"\n    default_model: str = \"gpt-4o-mini\"\n    default_temperature: float = 0.2\n    default_max_output_tokens: int = 512\n\n    # System configuration\n    log_level: str = \"INFO\"\n    enable_tracing: bool = False\n\n    @classmethod\n    def from_env(cls) -&gt; \"Settings\":\n        return cls()\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#42-transport-configuration","title":"4.2 Transport Configuration","text":"<pre><code># config/transport.py\nfrom pydantic import BaseModel, Field\n\nclass TransportConfig(BaseModel):\n    \"\"\"Pure transport/HTTP configuration; only the client sees this.\"\"\"\n    base_url: str\n    api_key: str = Field(repr=False)\n    organization: str | None = None\n    connect_timeout_s: float = 10.0\n    read_timeout_s: float = 30.0\n    max_retries: int = 5\n    backoff_base_s: float = 0.5\n    backoff_max_s: float = 8.0\n    rate_limit_rps: float = 5.0\n\n    @classmethod\n    def from_env(cls, prefix: str) -&gt; \"TransportConfig\":\n        import os\n        return cls(\n            base_url=os.environ.get(f\"{prefix}_BASE_URL\", \"\"),\n            api_key=os.environ.get(f\"{prefix}_API_KEY\", \"\"),\n        )\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#43-policy-structures","title":"4.3 Policy Structures","text":"<pre><code># config/policy.py\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass DomainPolicy(BaseModel):\n    \"\"\"Domain concerns about how the app wants to use the service.\"\"\"\n    completion_mode: Literal[\"snapshot\", \"wait\"] = \"wait\"\n    deadline_s: float | None = None\n    allow_partials: bool = True\n    overlap_mode: Literal[\"ignore\", \"merge\", \"separate\"] = \"merge\"\n\nclass ServicePolicy(BaseModel):\n    \"\"\"Adapter/service-side normalization and mapping decisions.\"\"\"\n    default_label: str = \"DEFAULT\"\n    normalize_case: bool = True\n    min_unit_size: int = 0\n    preserve_formatting: bool = True\n\nclass ParamsPolicy(BaseModel):\n    \"\"\"GenAI-specific parameter precedence policy.\"\"\"\n    provider: str\n    model: str\n    temperature: float\n    max_output_tokens: int\n    seed: int | None = None\n    top_p: float | None = None\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#44-policy-precedence","title":"4.4 Policy Precedence","text":"<p>Order of precedence (highest to lowest):</p> <ol> <li>Explicit request parameters (RenderRequest, FeatureParams)</li> <li>Pattern defaults (from Pattern catalog)</li> <li>Config policy (YAML or database)</li> <li>Settings defaults (environment/deployment)</li> </ol>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#5-canonical-domain-envelope","title":"5. Canonical Domain Envelope","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#51-envelope-structure","title":"5.1 Envelope Structure","text":"<pre><code># domain/models.py\nfrom typing import Any, Literal\nfrom pydantic import BaseModel\n\nclass Provenance(BaseModel):\n    \"\"\"Metadata about how/when/with what the result was produced.\"\"\"\n    backend: str                        # e.g., \"openai\", \"pyannote\", \"assemblyai\"\n    model: str | None = None\n    job_id: str | None = None\n    started_at: str | None = None\n    completed_at: str | None = None\n    schema_version: str = \"1.0\"\n    policy_version: str = \"1.0\"\n    params: dict = {}                   # Effective parameters used\n    system_version: str | None = None   # Git SHA/tag for reproducibility\n\nclass Envelope(BaseModel):\n    \"\"\"Universal response wrapper for all services.\"\"\"\n    status: Literal[\"pending\", \"running\", \"succeeded\", \"failed\", \"timeout\"]\n    result: Any | None = None           # Feature-specific; present only on success\n    error: str | None = None\n    diagnostics: dict = {}              # Curated metrics, not raw dump\n    provenance: Provenance\n\n    def succeeded(self) -&gt; bool:\n        return self.status == \"succeeded\"\n\n    def failed(self) -&gt; bool:\n        return self.status == \"failed\"\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#52-envelope-invariants","title":"5.2 Envelope Invariants","text":"<ul> <li><code>result</code> only present when <code>status == \"succeeded\"</code></li> <li><code>error</code> only present for <code>status == \"failed\"</code></li> <li><code>provenance.job_id</code> set when available (embed at transport boundary)</li> <li><code>diagnostics</code> contains curated, actionable metrics (not raw dumps)</li> <li><code>provenance.params</code> records effective policy/parameters used</li> <li>Always include <code>schema_version</code> and <code>policy_version</code> in provenance</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#6-service-protocols","title":"6. Service Protocols","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#61-protocol-design-rules","title":"6.1 Protocol Design Rules","text":"<ul> <li>Use <code>Protocol</code> for interfaces (structural typing)</li> <li>Use <code>ABC</code> only when enforcing init-time invariants or providing mixins</li> <li>Every service has exactly one orchestrator class</li> <li>Every adapter implements exactly one protocol</li> <li>Keep protocol interfaces minimal (3-5 methods maximum)</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#62-async-job-service-protocol","title":"6.2 Async Job Service Protocol","text":"<pre><code># domain/service.py\nfrom pathlib import Path\nfrom typing import Protocol\nfrom .models import Envelope\n\nclass FeatureParams(BaseModel):\n    \"\"\"API-facing parameters (safe subset).\"\"\"\n    pass\n\nclass AsyncFeatureService(Protocol):\n    \"\"\"Protocol for services that support async job patterns.\"\"\"\n\n    def start(\n        self, \n        input_path: Path, \n        params: FeatureParams | None = None\n    ) -&gt; str:\n        \"\"\"Start async job, return job_id.\"\"\"\n        ...\n\n    def get_response(\n        self, \n        job_id: str, \n        *, \n        wait_until_complete: bool = False\n    ) -&gt; Envelope:\n        \"\"\"Get job status/result. If wait=True, poll until complete.\"\"\"\n        ...\n\n    def generate(\n        self, \n        input_path: Path, \n        params: FeatureParams | None = None,\n        *, \n        wait_until_complete: bool = True\n    ) -&gt; Envelope:\n        \"\"\"Convenience: start + wait for result.\"\"\"\n        ...\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#63-sync-requestresponse-protocol","title":"6.3 Sync Request/Response Protocol","text":"<pre><code># domain/service.py\nclass SyncFeatureService(Protocol):\n    \"\"\"Protocol for synchronous request/response services.\"\"\"\n\n    def generate(self, req: RequestParams) -&gt; ResultModel:\n        \"\"\"Synchronous generation; returns domain result directly.\"\"\"\n        ...\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#64-genai-service-protocol-example","title":"6.4 GenAI Service Protocol (Example)","text":"<pre><code>class GenAIServicePort(Protocol):\n    \"\"\"Protocol for GenAI text generation services.\"\"\"\n\n    def generate(self, req: RenderRequest) -&gt; CompletionResult:\n        \"\"\"Generate completion from rendered prompt.\"\"\"\n        ...\n\nclass GenAIService:\n    \"\"\"Orchestrator for GenAI generation.\"\"\"\n\n    def __init__(\n        self,\n        settings: Settings,\n        catalog: PatternCatalog | None = None,\n        provider: ProviderClient | None = None,\n    ):\n        self.settings = settings\n        self.catalog = catalog or LegacyPatternCatalog()\n        self.provider = provider or OpenAIClient(\n            settings.openai_api_key,\n            settings.openai_org\n        )\n\n    def generate(self, req: RenderRequest) -&gt; CompletionResult:\n        # 1. Get pattern from catalog\n        pattern = self.catalog.get(req.pattern_key)\n\n        # 2. Render prompt with context\n        rendered = self.catalog.render(pattern, req)\n\n        # 3. Apply policy precedence\n        params = apply_policy(\n            req.intent,\n            pattern.default_params,\n            req.model_hint\n        )\n\n        # 4. Build transport request\n        provider_req = ProviderRequest(\n            provider=params.provider,\n            model=params.model,\n            messages=rendered.messages,\n            system=rendered.system,\n            temperature=params.temperature,\n            max_output_tokens=params.max_output_tokens,\n            seed=params.seed,\n        )\n\n        # 5. Call provider through port\n        resp = self.provider.generate(provider_req)\n\n        # 6. Return domain result\n        return CompletionResult(\n            text=resp.text,\n            usage=resp.usage,\n            model=resp.model,\n            provider=resp.provider,\n            prompt_fingerprint=rendered.fingerprint,\n        )\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#7-transport-models-domain-models","title":"7. Transport Models &amp; Domain Models","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#71-transport-models-at-boundaries","title":"7.1 Transport Models (at boundaries)","text":"<pre><code># models/transport.py\nfrom pydantic import BaseModel\n\nclass Message(BaseModel):\n    \"\"\"Universal message format for chat-based APIs.\"\"\"\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n\nclass ProviderRequest(BaseModel):\n    \"\"\"Transport-level request for any GenAI provider.\"\"\"\n    provider: str\n    model: str\n    messages: list[Message]\n    system: str | None = None\n    temperature: float\n    max_output_tokens: int\n    seed: int | None = None\n    top_p: float | None = None\n\nclass ProviderResponse(BaseModel):\n    \"\"\"Transport-level response from GenAI provider.\"\"\"\n    text: str\n    usage: Usage\n    model: str\n    provider: str\n    finish_reason: str | None = None\n\nclass JobStatusResponse(BaseModel):\n    \"\"\"Transport-level job status for async services.\"\"\"\n    status: str                    # \"pending\" | \"running\" | \"succeeded\" | \"failed\"\n    job_id: str                    # Embed job_id for propagation\n    elapsed_s: float = 0.0\n    payload: dict = {}             # Raw vendor payload (optional)\n    error: str | None = None\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#72-domain-models","title":"7.2 Domain Models","text":"<pre><code># models/domain.py\nfrom pydantic import BaseModel\n\nclass CompletionResult(BaseModel):\n    \"\"\"Domain result for GenAI completions.\"\"\"\n    text: str\n    usage: Usage\n    model: str\n    provider: str\n    prompt_fingerprint: str\n\nclass DiarizationResult(BaseModel):\n    \"\"\"Domain result for speaker diarization.\"\"\"\n    segments: list[SpeakerBlock]\n    num_speakers: int\n    duration_s: float\n\nclass SpeakerBlock(BaseModel):\n    \"\"\"Individual speaker segment.\"\"\"\n    speaker_id: str\n    start_s: float\n    end_s: float\n    confidence: float | None = None\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#8-adapter-architecture","title":"8. Adapter Architecture","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#81-adapter-role-responsibilities","title":"8.1 Adapter Role &amp; Responsibilities","text":"<p>Adapters encapsulate all external I/O logic. Each adapter:</p> <ul> <li>Implements exactly one <code>Protocol</code> (port)</li> <li>Acts as bi-directional translator: domain \u2194 transport \u2194 SDK</li> <li>Isolates SDK-specific code from domain logic</li> <li>Handles provider-specific error mapping</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#82-bi-directional-mapping-contract","title":"8.2 Bi-Directional Mapping Contract","text":"<p>Every adapter implements request and response mapping:</p> <pre><code>class ProviderRequestMapper:\n    \"\"\"Maps between domain and SDK request formats.\"\"\"\n\n    def to_sdk_request(self, req: ProviderRequest) -&gt; SDKRequest:\n        \"\"\"Domain \u2192 SDK request transformation.\"\"\"\n        ...\n\n    def from_sdk_response(self, resp: SDKResponse) -&gt; ProviderResponse:\n        \"\"\"SDK \u2192 domain response transformation.\"\"\"\n        ...\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#83-adapter-boundary-diagram","title":"8.3 Adapter Boundary Diagram","text":"<pre><code>graph LR\n  A[Domain Model] --&gt; B[ProviderRequest]\n  B --&gt; C[Mapper.to_sdk_request]\n  C --&gt; D[SDK Request]\n  D --&gt; E[External Provider]\n  E --&gt; F[SDK Response]\n  F --&gt; G[Mapper.from_sdk_response]\n  G --&gt; H[ProviderResponse]\n  H --&gt; I[Domain Model]</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#84-sync-adapter-pattern-genai","title":"8.4 Sync Adapter Pattern (GenAI)","text":"<pre><code># providers/openai_adapter.py\nfrom openai import OpenAI\n\nclass OpenAIClient:\n    \"\"\"Adapter for OpenAI chat completions API.\"\"\"\n    PROVIDER = \"openai\"\n\n    def __init__(\n        self,\n        api_key: str,\n        organization: str,\n        mapper: OpenAIRequestMapper | None = None\n    ):\n        self._client = OpenAI(api_key=api_key, organization=organization)\n        self._mapper = mapper or OpenAIRequestMapper()\n\n    def generate(self, req: ProviderRequest) -&gt; ProviderResponse:\n        \"\"\"Implement ProviderClient protocol.\"\"\"\n        # 1. Map domain \u2192 SDK\n        mapped = self._mapper.to_sdk_request(req)\n\n        # 2. Call SDK\n        try:\n            sdk_resp = self._client.chat.completions.create(\n                **mapped.model_dump()\n            )\n        except Exception as e:\n            raise ProviderError(f\"OpenAI API error: {e}\") from e\n\n        # 3. Map SDK \u2192 domain\n        return self._mapper.from_sdk_response(sdk_resp)\n\nclass OpenAIRequestMapper:\n    \"\"\"Bi-directional mapper for OpenAI API.\"\"\"\n\n    def to_sdk_request(self, req: ProviderRequest) -&gt; dict:\n        \"\"\"Convert ProviderRequest to OpenAI API format.\"\"\"\n        messages = [{\"role\": m.role, \"content\": m.content} for m in req.messages]\n        if req.system:\n            messages.insert(0, {\"role\": \"system\", \"content\": req.system})\n\n        return {\n            \"model\": req.model,\n            \"messages\": messages,\n            \"temperature\": req.temperature,\n            \"max_tokens\": req.max_output_tokens,\n            \"seed\": req.seed,\n            \"top_p\": req.top_p,\n        }\n\n    def from_sdk_response(self, resp) -&gt; ProviderResponse:\n        \"\"\"Convert OpenAI response to ProviderResponse.\"\"\"\n        return ProviderResponse(\n            text=resp.choices[0].message.content,\n            usage=Usage(\n                input_tokens=resp.usage.prompt_tokens,\n                output_tokens=resp.usage.completion_tokens,\n            ),\n            model=resp.model,\n            provider=\"openai\",\n            finish_reason=resp.choices[0].finish_reason,\n        )\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#85-async-adapter-pattern-diarization","title":"8.5 Async Adapter Pattern (Diarization)","text":"<pre><code># adapters/assemblyai_adapter.py\nfrom pathlib import Path\n\nclass AssemblyAIClient:\n    \"\"\"Adapter for AssemblyAI async transcription API.\"\"\"\n\n    def __init__(self, cfg: TransportConfig, mapper: AssemblyAIMapper):\n        self.client = VendorClient(cfg)  # Transport layer\n        self.mapper = mapper\n        self.backend = \"assemblyai\"\n\n    def start(self, input_path: Path, params: FeatureParams | None = None) -&gt; str:\n        \"\"\"Start transcription job.\"\"\"\n        media_id = self.client.upload(input_path)\n        job_id = self.client.start(media_id, params.model_dump() if params else None)\n        return job_id\n\n    def get_response(\n        self, \n        job_id: str, \n        *, \n        wait_until_complete: bool = False\n    ) -&gt; Envelope:\n        \"\"\"Get job status/result.\"\"\"\n        if wait_until_complete:\n            jsr = self.client.poll_until_done(job_id)\n        else:\n            jsr = self.client.job_status(job_id)\n\n        return self.mapper.to_envelope(jsr)\n\n    def generate(\n        self,\n        input_path: Path,\n        params: FeatureParams | None = None,\n        *,\n        wait_until_complete: bool = True\n    ) -&gt; Envelope:\n        \"\"\"Convenience: start + wait.\"\"\"\n        job_id = self.start(input_path, params)\n        return self.get_response(job_id, wait_until_complete=wait_until_complete)\n\nclass AssemblyAIMapper:\n    \"\"\"Maps AssemblyAI responses to domain envelopes.\"\"\"\n\n    def __init__(self, backend: str = \"assemblyai\", model: str | None = None):\n        self.backend = backend\n        self.model = model\n\n    def to_envelope(self, jsr: JobStatusResponse) -&gt; Envelope:\n        \"\"\"Convert JobStatusResponse to Envelope.\"\"\"\n        prov = Provenance(\n            backend=self.backend,\n            model=self.model,\n            job_id=jsr.job_id,\n            params={},\n        )\n\n        if jsr.status == \"succeeded\":\n            result = self._map_result(jsr)\n            return Envelope(status=\"succeeded\", result=result, provenance=prov)\n        elif jsr.status == \"failed\":\n            return Envelope(status=\"failed\", error=jsr.error, provenance=prov)\n        elif jsr.status == \"timeout\":\n            return Envelope(status=\"timeout\", error=\"Job exceeded deadline\", provenance=prov)\n        else:\n            return Envelope(status=jsr.status, provenance=prov)\n\n    def _map_result(self, jsr: JobStatusResponse) -&gt; DiarizationResult:\n        \"\"\"Extract domain result from vendor payload.\"\"\"\n        # Parse vendor-specific payload \u2192 domain model\n        segments = [\n            SpeakerBlock(\n                speaker_id=seg[\"speaker\"],\n                start_s=seg[\"start\"],\n                end_s=seg[\"end\"]\n            )\n            for seg in jsr.payload.get(\"utterances\", [])\n        ]\n        return DiarizationResult(\n            segments=segments,\n            num_speakers=len(set(s.speaker_id for s in segments)),\n            duration_s=jsr.elapsed_s\n        )\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#86-adapter-mapping-rules","title":"8.6 Adapter Mapping Rules","text":"<ul> <li>No literals or untyped dicts in mapping code; use TypedDicts or SDK models</li> <li>All defaults come from <code>Settings</code> or <code>Policy</code></li> <li>Mapper code is pure (no I/O, no side-effects)</li> <li>Client code handles exceptions and wraps them as typed errors</li> <li>Map vendor errors \u2192 <code>ProviderError</code> subclasses for uniform handling</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#87-error-handling","title":"8.7 Error Handling","text":"<pre><code># models/errors.py\nclass ProviderError(Exception):\n    \"\"\"Base error for all provider failures.\"\"\"\n    pass\n\nclass RateLimitError(ProviderError):\n    \"\"\"Rate limit exceeded.\"\"\"\n    pass\n\nclass AuthenticationError(ProviderError):\n    \"\"\"Invalid credentials.\"\"\"\n    pass\n\nclass TimeoutError(ProviderError):\n    \"\"\"Request exceeded deadline.\"\"\"\n    pass\n</code></pre> <p>All adapters must raise typed errors for uniform error handling by orchestration layers.</p>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#88-internal-layer-adapters-boundary-mapping-inside-our-own-system","title":"8.8 Internal Layer Adapters (Boundary Mapping Inside Our Own System)","text":"<p>Adapters and mappers are not just for translating vendor payloads\u2014they are equally critical for translating between our own internal abstraction layers. This principle applies whenever there is a semantic boundary or abstraction shift, even within our own codebase.</p>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#rationale-for-internal-mapping","title":"Rationale for Internal Mapping","text":"<ul> <li>Semantic Shifts: As data moves from transport to domain to orchestration, the meaning, required fields, or invariants of objects often change. Mapping makes these changes explicit.</li> <li>Attaching Policy, Safety, Provenance: Internal mappers are the right place to inject policy decisions, provenance metadata, or safety checks as data crosses boundaries.</li> <li>Orchestration Cleanliness: Keeping orchestration code free of manual stitching, dict access, or ad hoc conversions avoids \u201cgod services\u201d and keeps logic testable.</li> <li>Future-Proofing: By isolating mapping, we can later support multiple result types, partial results, or envelope variants without rewriting orchestration.</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#example-mapping-between-internal-layers","title":"Example: Mapping Between Internal Layers","text":"<ul> <li> <p>ProviderResponse \u2192 CompletionResult (GenAI)</p> <pre><code># mappers/completion_mapper.py\ndef to_completion_result(resp: ProviderResponse, fingerprint: str) -&gt; CompletionResult:\n    return CompletionResult(\n        text=resp.text,\n        usage=resp.usage,\n        model=resp.model,\n        provider=resp.provider,\n        prompt_fingerprint=fingerprint,\n    )\n</code></pre> </li> <li> <p>JobStatusResponse \u2192 Envelope (Async Jobs)</p> <pre><code># mappers/job_status_mapper.py\ndef to_envelope(jsr: JobStatusResponse, backend: str, model: str | None = None) -&gt; Envelope:\n    prov = Provenance(\n        backend=backend,\n        model=model,\n        job_id=jsr.job_id,\n        params={},\n    )\n    if jsr.status == \"succeeded\":\n        result = ...  # map jsr.payload to the domain result\n        return Envelope(status=\"succeeded\", result=result, provenance=prov)\n    elif jsr.status == \"failed\":\n        return Envelope(status=\"failed\", error=jsr.error, provenance=prov)\n    elif jsr.status == \"timeout\":\n        return Envelope(status=\"timeout\", error=\"Job exceeded deadline\", provenance=prov)\n    else:\n        return Envelope(status=jsr.status, provenance=prov)\n</code></pre> </li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#explicit-rules-for-internal-mapping","title":"Explicit Rules for Internal Mapping","text":"<ol> <li>All cross-layer translations (even internal) must happen in mappers.</li> <li>Mappers are pure: No I/O, no side effects, no logging, no exception handling.</li> <li>Orchestrators never manually stitch domain objects or reach into dicts.</li> <li>Naming convention: Use <code>to_&lt;DomainType&gt;()</code> or <code>&lt;DomainType&gt;Mapper</code> for class mappers.</li> <li>Class vs Function: Use a class if mapping depends on injected config or policy; use a pure function if stateless.</li> </ol>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#benefits","title":"Benefits","text":"<ul> <li>Consistency: All cross-boundary data flows are explicit and uniform.</li> <li>Testability: Mappers can be tested in isolation, including edge cases and invariants.</li> <li>Evolution: Adding new result types, partials, or shape changes is localized to mappers.</li> <li>Prevents \u201cGod Services\u201d: Orchestration remains free of ad hoc data stitching, keeping logic readable and maintainable.</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#9-transport-layer-clients-polling","title":"9. Transport Layer: Clients &amp; Polling","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#91-transport-client-http-operations","title":"9.1 Transport Client (HTTP operations)","text":"<pre><code># transport/client.py\nimport time\nfrom pathlib import Path\nfrom .models import JobStatusResponse\n\nclass VendorClient:\n    \"\"\"Pure transport/HTTP client for async job APIs.\"\"\"\n\n    def __init__(self, cfg: TransportConfig):\n        self.cfg = cfg\n        self._session = self._create_session()\n\n    def _create_session(self):\n        \"\"\"Create HTTP session with retries and rate limiting.\"\"\"\n        # Implementation details...\n        pass\n\n    def upload(self, path: Path) -&gt; str:\n        \"\"\"Upload file to vendor, return media_id.\"\"\"\n        # POST file \u2192 return ID\n        return \"media_123\"\n\n    def start(self, media_id: str, params: dict | None = None) -&gt; str:\n        \"\"\"Start processing job, return job_id.\"\"\"\n        # POST /jobs \u2192 return job_id\n        return \"job_abc\"\n\n    def job_status(self, job_id: str) -&gt; JobStatusResponse:\n        \"\"\"Get current job status.\"\"\"\n        # GET /jobs/{job_id} \u2192 JobStatusResponse\n        return JobStatusResponse(status=\"running\", job_id=job_id)\n\n    def poll_until_done(\n        self,\n        job_id: str,\n        deadline_s: float | None = None\n    ) -&gt; JobStatusResponse:\n        \"\"\"Poll job until completion with exponential backoff.\"\"\"\n        backoff = self.cfg.backoff_base_s\n        start = time.time()\n\n        while True:\n            jsr = self.job_status(job_id)\n\n            # Terminal states\n            if jsr.status in {\"succeeded\", \"failed\"}:\n                return jsr\n\n            # Timeout check\n            if deadline_s and (time.time() - start) &gt; deadline_s:\n                return JobStatusResponse(\n                    status=\"timeout\",\n                    job_id=job_id,\n                    elapsed_s=time.time() - start\n                )\n\n            # Exponential backoff with jitter\n            time.sleep(backoff + random.uniform(0, backoff * 0.1))\n            backoff = min(backoff * 1.6, self.cfg.backoff_max_s)\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#92-job-poller-reusable","title":"9.2 Job Poller (Reusable)","text":"<pre><code># transport/poller.py\nfrom typing import Callable, TypeVar\nimport time\nimport random\n\nT = TypeVar('T')\n\nclass JobPoller:\n    \"\"\"Reusable polling logic with backoff and jitter.\"\"\"\n\n    def __init__(\n        self,\n        check_fn: Callable[[str], T],\n        is_terminal: Callable[[T], bool],\n        backoff_base: float = 0.5,\n        backoff_max: float = 8.0,\n        deadline_s: float | None = None\n    ):\n        self.check_fn = check_fn\n        self.is_terminal = is_terminal\n        self.backoff_base = backoff_base\n        self.backoff_max = backoff_max\n        self.deadline_s = deadline_s\n\n    def poll(self, job_id: str) -&gt; T:\n        \"\"\"Poll until terminal state or timeout.\"\"\"\n        backoff = self.backoff_base\n        start = time.time()\n\n        while True:\n            result = self.check_fn(job_id)\n\n            if self.is_terminal(result):\n                return result\n\n            if self.deadline_s and (time.time() - start) &gt; self.deadline_s:\n                raise TimeoutError(f\"Job {job_id} exceeded deadline\")\n\n            jitter = random.uniform(0, backoff * 0.1)\n            time.sleep(backoff + jitter)\n            backoff = min(backoff * 1.6, self.backoff_max)\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#10-testing-strategy","title":"10. Testing Strategy","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#101-unit-testing-mock-protocols","title":"10.1 Unit Testing (Mock Protocols)","text":"<pre><code># tests/test_service.py\nclass FakeProviderClient:\n    \"\"\"Mock implementation of ProviderClient protocol.\"\"\"\n\n    def generate(self, req: ProviderRequest) -&gt; ProviderResponse:\n        return ProviderResponse(\n            text=\"mocked response\",\n            usage=Usage(input_tokens=10, output_tokens=20),\n            model=req.model,\n            provider=\"fake\"\n        )\n\ndef test_generate():\n    \"\"\"Test service orchestration with mocked provider.\"\"\"\n    fake_provider = FakeProviderClient()\n    settings = Settings(default_provider=\"fake\")\n    service = GenAIService(settings, provider=fake_provider)\n\n    req = RenderRequest(pattern_key=\"test\", context={})\n    result = service.generate(req)\n\n    assert result.text == \"mocked response\"\n    assert result.provider == \"fake\"\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#102-integration-testing","title":"10.2 Integration Testing","text":"<pre><code># tests/integration/test_openai.py\ndef test_openai_integration():\n    \"\"\"End-to-end test with real OpenAI API.\"\"\"\n    settings = Settings.from_env()\n    service = GenAIService(settings)\n\n    req = RenderRequest(\n        pattern_key=\"summarize\",\n        context={\"text\": \"Long document...\"}\n    )\n    result = service.generate(req)\n\n    assert result.succeeded()\n    assert len(result.text) &gt; 0\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#103-contract-testing-envelope-shape","title":"10.3 Contract Testing (Envelope Shape)","text":"<pre><code>def test_envelope_contract():\n    \"\"\"Verify envelope always contains required fields.\"\"\"\n    env = service.get_response(job_id)\n\n    assert hasattr(env, \"status\")\n    assert hasattr(env, \"provenance\")\n    assert env.provenance.backend is not None\n    assert env.provenance.schema_version is not None\n\n    if env.succeeded():\n        assert env.result is not None\n        assert env.error is None\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#104-golden-testing-policy-variants","title":"10.4 Golden Testing (Policy Variants)","text":"<p>For each policy profile, snapshot the envelope shape to detect regressions.</p>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#11-interface-modalities-integration-patterns","title":"11. Interface Modalities &amp; Integration Patterns","text":"<p>Design for a range of service shapes. Choose the smallest that fits; keep the envelope consistent.</p>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#111-modality-decision-tree","title":"11.1 Modality Decision Tree","text":"<ol> <li>Is the task fast (&lt;2s)? \u2192 Synchronous request/response</li> <li>Long-running and vendor can't call back? \u2192 Async + polling</li> <li>Vendor supports webhooks and you can host an endpoint? \u2192 Async + webhook</li> <li>You need incremental UX? \u2192 Streaming</li> <li>You need decoupling/scale? \u2192 Message queue / Pub-Sub</li> <li>It's a local library? \u2192 Synchronous or streaming</li> </ol>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#112-synchronous-requestresponse","title":"11.2 Synchronous Request/Response","text":"<p>When to use: Small/fast tasks; local libraries; remote endpoints with low latency.</p> <ul> <li>Contract: <code>run(params) -&gt; Envelope</code> (no <code>start()</code>/<code>get_response()</code>)</li> <li>Failure model: Exceptions for programmer errors; <code>failed</code> envelope for domain/remote errors</li> <li>Examples: Local Whisper inference; lightweight text cleaners; GenAI completions</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#113-asynchronous-jobs-with-polling","title":"11.3 Asynchronous Jobs with Polling","text":"<p>When to use: Long-running tasks without push callbacks.</p> <ul> <li>Contract: <code>start() -&gt; job_id</code>, <code>get_response(job_id, wait|snapshot) -&gt; Envelope</code></li> <li>Failure model: Timeouts \u2192 <code>timeout</code> envelope (optionally partials); network errors retried</li> <li>Examples: Diarization, transcription, video processing</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#114-asynchronous-jobs-with-webhooks","title":"11.4 Asynchronous Jobs with Webhooks","text":"<p>When to use: Vendor can push results; you control an HTTP endpoint.</p> <ul> <li>Contract: <code>start(callback_url) -&gt; job_id</code>; callback sends transport payload; adapter maps to envelope</li> <li>Failure model: At-least-once delivery; verify signatures; be idempotent</li> </ul> <pre><code># app/webhooks.py\nfrom fastapi import FastAPI, Request, HTTPException\n\napp = FastAPI()\n\n@app.post(\"/callbacks/vendor\")\nasync def vendor_callback(req: Request):\n    sig = req.headers.get(\"X-Vendor-Signature\")\n    body = await req.json()\n\n    if not verify_signature(sig, body):\n        raise HTTPException(status_code=401)\n\n    jsr = JobStatusResponse.model_validate({\n        **body,\n        \"job_id\": body.get(\"job_id\")\n    })\n    env = adapter.to_envelope(jsr)\n    save_result(env)  # Persist; notify UI\n\n    return {\"ok\": True}\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#115-streaming-sse-websocket-grpc","title":"11.5 Streaming (SSE / WebSocket / gRPC)","text":"<p>When to use: Incremental results (tokens, segments, progress).</p> <ul> <li>Contract: Open stream \u2192 receive <code>Envelope</code> deltas or <code>Progress</code> events; final <code>Envelope</code> closes stream</li> <li>Failure model: Reconnect with resume tokens; dedupe by <code>event_id</code></li> <li>Notes: Wrap stream events into mini-envelopes for consistent shape</li> </ul> <pre><code># Example: async streaming client\nasync def consume_stream(uri: str):\n    async with websockets.connect(uri) as ws:\n        await ws.send(json.dumps({\"auth\": token, \"params\": {...}}))\n        async for msg in ws:\n            evt = json.loads(msg)\n            env = to_envelope(evt)  # Mini-envelopes or progress events\n            handle_event(env)\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#116-message-queue-pub-sub-integration","title":"11.6 Message Queue / Pub-Sub Integration","text":"<p>When to use: Decouple producers/consumers; batch pipelines; fan-out/fan-in.</p> <ul> <li>Contract: Publish <code>Params</code> + correlation IDs; consumers emit <code>Envelope</code>s on result topic</li> <li>Failure model: Retries with DLQ; ensure idempotency via keys</li> </ul> <pre><code>@mq.consume(\"transcribe.jobs\")\ndef worker(msg):\n    params = FeatureParams.model_validate(msg[\"params\"])\n    env = service.generate(Path(msg[\"input\"]), params)\n    mq.publish(\"transcribe.results\", env.model_dump())\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#117-batch-pipelines-orchestration","title":"11.7 Batch Pipelines (Orchestration)","text":"<p>When to use: Multi-stage processing: diarize \u2192 chunk \u2192 transcribe \u2192 summarize.</p> <ul> <li>Contract: Each stage returns an <code>Envelope</code>; orchestrator composes; pass along <code>provenance</code> and <code>policy</code></li> <li>Failure model: Saga-style compensation/skip policies; partial results acceptable per policy</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#12-tracing-metrics-observability","title":"12. Tracing, Metrics &amp; Observability","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#121-instrumentation-boundaries","title":"12.1 Instrumentation Boundaries","text":"Layer Responsibility Tools Domain Trace spans, usage accounting <code>Tracer</code>, <code>UsageAccounting</code> Adapter Metrics (latency, cost, throughput) Prometheus, DataDog Transport HTTP metrics, retry counts Built-in client metrics"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#122-provenance-integration","title":"12.2 Provenance Integration","text":"<p>Always record in <code>provenance.params</code>:</p> <ul> <li>Policy and policy version used</li> <li>System version (git SHA/tag) for reproducibility</li> <li>Upstream IDs (job_id/request_id) and correlation IDs</li> <li>When combining stages, aggregate provenance: keep list of stage-level blocks</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#13-policy-evolution-strategy","title":"13. Policy Evolution Strategy","text":""},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#131-principles","title":"13.1 Principles","text":"<ol> <li>Policy \u2260 Config: Policies affect semantics; configs describe environment</li> <li>Additive first: Prefer adding new policy fields with sensible defaults over changing semantics</li> <li>Version the policy surface: Include <code>policy_version</code> in provenance</li> <li>Document intent: Every policy field gets one-line rationale in docstrings</li> <li>Record used policy: Echo effective policy into <code>provenance.policy</code> on every response</li> </ol>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#132-versioning","title":"13.2 Versioning","text":"<ul> <li>Add <code>policy_version: str = \"1.0\"</code> to policy model</li> <li>Bump minor (<code>1.1</code>) for additive fields; bump major (<code>2.0</code>) for behavior changes</li> <li>Store version in <code>provenance.params[\"policy_version\"]</code></li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#133-migration-patterns","title":"13.3 Migration Patterns","text":"<p>Expand \u2192 Contract:</p> <ul> <li>Ship new policy fields with defaults that preserve old behavior</li> <li>Gather metrics; then flip defaults in major version</li> </ul> <p>Feature flags:</p> <ul> <li>Use boolean or enum policy switches for new behaviors (e.g., <code>overlap_mode = \"ignore|merge|separate\"</code>)</li> </ul> <p>Policy profiles:</p> <ul> <li>Provide named presets (e.g., <code>\"dharma_talks\"</code>, <code>\"interviews\"</code>) that resolve to concrete policy objects</li> </ul> <p>Deprecation path:</p> <ul> <li>Mark fields as deprecated in docstrings</li> <li>Log deprecation warnings when used</li> <li>Remove only after major version bump</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#134-testing-governance","title":"13.4 Testing &amp; Governance","text":"<ul> <li>Golden tests for policy variants: snapshot envelope shape for each profile</li> <li>Contract tests to verify provenance always includes policy version and effective values</li> <li>Review checklist for policy PRs: rationale, defaults, migration notes, docstrings, provenance echo</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#14-key-implementation-rules","title":"14. Key Implementation Rules","text":"<ol> <li>Use <code>Protocol</code> for interfaces; use <code>ABC</code> only when enforcing init-time invariants or providing mixins</li> <li>Every service has exactly one orchestrator class</li> <li>Every adapter implements exactly one protocol</li> <li>Mappers are pure and side-effect-free</li> <li>No dicts except at SDK boundaries (use TypedDict or Pydantic models)</li> <li>All literals replaced by settings/policy parameters</li> <li>Exceptions wrapped in typed error classes</li> <li>Config at init, params per call, response envelope always</li> <li>Domain logic never depends on infrastructure</li> <li>All side-effects isolated to adapters and transport clients</li> </ol>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#15-reference-implementation-index","title":"15. Reference Implementation Index","text":"Concept GenAI Service Diarization Service Orchestrator <code>service.py</code> (GenAIService) <code>diarization_service.py</code> Provider Client <code>providers/openai_adapter.py</code> <code>adapters/pyannote_client.py</code> Mapper <code>OpenAIRequestMapper</code> <code>PyannoteMapper</code> Domain Models <code>models/domain.py</code> <code>models/audio_domain.py</code> Transport Models <code>models/transport.py</code> <code>models/audio_transport.py</code> Errors <code>models/errors.py</code> <code>models/errors.py</code> Transport Client (direct SDK call) <code>transport/vendor_client.py</code> Job Poller (N/A - sync) <code>transport/poller.py</code>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#16-filemodule-scaffold-cookie-ready","title":"16. File/Module Scaffold (Cookie-Ready)","text":"<pre><code>&lt;feature&gt;/\n  config/\n    settings.py          # Application-wide settings\n    transport.py         # Transport configuration\n    policy.py            # Domain/service policies\n  domain/\n    models.py            # Domain models (Result, Envelope)\n    service.py           # Service protocol definition\n  transport/\n    client.py            # HTTP client (upload, start, poll)\n    poller.py            # Reusable polling logic\n    models.py            # Transport models (JobStatusResponse)\n  adapters/  (or providers/)\n    &lt;vendor&gt;_client.py   # Adapter implementing protocol\n    &lt;vendor&gt;_mapper.py   # Bi-directional mapper\n  service/\n    &lt;vendor&gt;_service.py  # Service orchestrator (composes ports)\n  app/\n    processor.py         # Application fa\u00e7ade\n    cli.py               # CLI entry point\n  models/\n    errors.py            # Typed error hierarchy\n</code></pre>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#17-design-review-questions","title":"17. Design Review Questions","text":"<ol> <li>Error Granularity: Should <code>ProviderError</code> be subclassed per provider or remain generic?</li> <li>Tracing Interface: Centralized or service-local?</li> <li>Policy Loading: Should YAML policy files be lazy-loaded or pre-loaded at startup?</li> <li>Composite PatternCatalog: Should multiple pattern sources (file, legacy, DB) be supported concurrently?</li> <li>RetryPolicy Abstraction: Should retries be unified across audio + text services or service-specific?</li> <li>Mappers as Classes vs Functions: Currently class-based; function-based mappers could simplify small providers</li> <li>Streaming: Should streaming adapters implement an async interface (<code>ProviderClientAsync</code>)?</li> <li>Webhook Security: Standardize signature verification across all webhook-based integrations?</li> <li>Rate Limiting: Client-side vs server-enforced; per-provider or global?</li> <li>Partial Results: How to handle and represent partial results in envelopes for long-running jobs?</li> </ol>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#18-glossary","title":"18. Glossary","text":"Term Definition Adapter Component that implements a protocol; maps between domain and transport Anti-corruption layer Boundary that converts vendor shapes to domain shapes Envelope Universal response wrapper containing status, diagnostics, provenance, and optional result Mapper Pure component performing bi-directional type translation Policy Opinionated settings that change behavior but not capability Port Abstract interface defined as Protocol Provenance Metadata about how/when/with what the result was produced Protocol Structural type interface (no inheritance required) Transport Layer handling HTTP, polling, streaming, and retries"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#19-evolution-versioning","title":"19. Evolution &amp; Versioning","text":"<ul> <li>Version the envelope (<code>schema_version</code>) and keep changes additive where possible</li> <li>Be adapter-tolerant to unknown transport fields</li> <li>For breaking API shifts, add new adapter/service and migrate gradually</li> <li>Include <code>policy_version</code> and <code>system_version</code> in provenance for reproducibility</li> <li>Aggregate provenance when combining stages in pipelines</li> </ul>"},{"location":"architecture/object-service/adr/adr-os01-object-service-architecture-v3/#20-closing-notes","title":"20. Closing Notes","text":"<p>This blueprint is modality-agnostic, policy-forward, and strongly-typed. It unifies patterns for:</p> <ul> <li>Synchronous request/response (GenAI)</li> <li>Asynchronous job polling (diarization, transcription)</li> <li>Webhooks, streaming, message queues</li> </ul> <p>Core principles:</p> <ul> <li>Use the smallest viable integration pattern</li> <li>Keep envelopes consistent across modalities</li> <li>Let policy express domain decisions</li> <li>Maintain strong typing with no literals</li> <li>Persist provenance for debugging and reproducibility</li> </ul> <p>Key success metrics:</p> <ul> <li>Can add new provider in &lt;1 day</li> <li>Can swap providers without changing application code</li> <li>Can trace any result back to exact parameters/version used</li> <li>Can test services in isolation with mocked protocols</li> </ul> <p>Copy the scaffolds and stubs from this blueprint to start new features quickly while maintaining architectural consistency.</p>"},{"location":"architecture/project-policies/","title":"Project Policies","text":"<p>Cross-cutting architectural policies and decisions affecting the entire TNH Scholar codebase.</p>"},{"location":"architecture/project-policies/#overview","title":"Overview","text":"<p>This directory contains Architecture Decision Records (ADRs) and policy documents for project-wide concerns that span multiple subsystems. Unlike feature-specific ADRs (GenAI, prompt system, etc.), these policies establish patterns and principles that apply across the entire codebase.</p>"},{"location":"architecture/project-policies/#scope","title":"Scope","text":"<p>Project Policies cover: - Versioning and release strategies - Testing and quality standards (future) - CI/CD architecture (future) - Cross-cutting design patterns (future) - Repository organization (future)</p> <p>Not covered here: - Feature-specific architecture (see architecture/ subdirectories) - Code style (see development/style-guide.md) - Documentation standards (see docs-system/)</p>"},{"location":"architecture/project-policies/#current-policies","title":"Current Policies","text":""},{"location":"architecture/project-policies/#versioning-releases","title":"Versioning &amp; Releases","text":"<ul> <li>ADR-PP01: Rapid Prototype Versioning Policy - Establishes versioning policy for 0.x releases, allowing breaking changes in any release to enable fast iteration</li> <li>Implementation Summary - Documentation updates implementing ADR-PP01</li> </ul> <p>Status: Active (0.x phase) Key Principle: Breaking changes acceptable in ANY 0.x release to prioritize architectural consistency</p> <p>See VERSIONING.md for user-facing policy documentation.</p>"},{"location":"architecture/project-policies/#future-policies-planned","title":"Future Policies (Planned)","text":""},{"location":"architecture/project-policies/#testing-strategy-planned","title":"Testing Strategy (Planned)","text":"<ul> <li>Project-wide testing standards</li> <li>Coverage requirements</li> <li>Test infrastructure patterns</li> <li>Integration test strategies</li> </ul>"},{"location":"architecture/project-policies/#cicd-architecture-planned","title":"CI/CD Architecture (Planned)","text":"<ul> <li>Build pipeline design</li> <li>Release automation</li> <li>Deployment strategies</li> <li>Quality gates</li> </ul>"},{"location":"architecture/project-policies/#monorepo-vs-multi-repo-planned","title":"Monorepo vs Multi-repo (Planned)","text":"<ul> <li>Repository organization strategy</li> <li>Package boundaries</li> <li>Versioning across packages</li> </ul>"},{"location":"architecture/project-policies/#related-documentation","title":"Related Documentation","text":"<ul> <li>VERSIONING.md - User-facing versioning policy</li> <li>CONTRIBUTING.md - Contribution guidelines</li> <li>ADR-OS01 - Object-service architecture (cross-cutting pattern)</li> <li>Architecture Overview - High-level architecture</li> </ul>"},{"location":"architecture/project-policies/#contributing","title":"Contributing","text":"<p>Project policies are decided by the Architecture Working Group with input from maintainers and contributors. To propose a new policy:</p> <ol> <li>Open a GitHub issue with <code>[Policy Proposal]</code> prefix</li> <li>Discuss approach with maintainers</li> <li>Draft ADR using ADR template</li> <li>Submit PR for review</li> </ol> <p>Last Updated: 2025-12-06 Active Policies: 1 (ADR-PP01) Status: Rapid Prototype Phase (0.x)</p>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/","title":"Versioning Policy Documentation Additions","text":"<p>This document summarizes the documentation updates made to clarify TNH Scholar's rapid prototype versioning policy across key developer-facing files.</p>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#background","title":"Background","text":"<p>ADR-PP01 (Rapid Prototype Versioning Policy) establishes that breaking changes are acceptable in ANY 0.x release, including patch versions. This enables fast iteration and architectural improvements (e.g., ADR-PT04 prompt system refactor).</p> <p>This deviates from typical semver expectations where 0.x patch bumps avoid breaking changes. To prevent confusion, we've documented this policy explicitly across all key entry points where developers encounter the project.</p>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#files-updated","title":"Files Updated","text":""},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#1-readmemd","title":"1. README.md","text":"<p>Section: Project Status</p> <p>Added:</p> <ul> <li>Explicit \"rapid prototype phase\" designation</li> <li>Versioning notice callout box explaining breaking change policy</li> <li>Link to VERSIONING.md for full policy</li> </ul> <p>Impact: First point of contact for all users and contributors</p>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#2-contributingmd","title":"2. CONTRIBUTING.md","text":"<p>Section: New section \"Versioning &amp; Breaking Changes\" (\u00a72)</p> <p>Added:</p> <ul> <li>Clear statement that breaking changes are expected and acceptable</li> <li>Guidance for contributors: update dependents immediately</li> <li>Link to VERSIONING.md</li> <li>Renumbered subsequent sections (3-6)</li> </ul> <p>Impact: Sets expectations for contributors making breaking changes</p>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#3-versioningmd-new-file","title":"3. VERSIONING.md - NEW FILE","text":"<p>Created: Comprehensive versioning policy document</p> <p>Contents:</p> <ul> <li>Rapid prototype phase (0.x) policy</li> <li>Core principles (breaking changes acceptable, force refactors, immediate removal)</li> <li>Why rapid prototype versioning (benefits over strict semver)</li> <li>Version numbering scheme</li> <li>Examples of acceptable breaking changes</li> <li>User impact and communication</li> <li>Post-prototype phase (1.0+) policy</li> <li>Transition to semantic versioning</li> <li>Pre-1.0 checklist</li> <li>FAQ section</li> <li>Related documentation links</li> </ul> <p>Impact: Canonical reference for versioning policy</p>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#4-release_checklistmd","title":"4. release_checklist.md","text":"<p>Added:</p> <ul> <li>Frontmatter update timestamp</li> <li>Note at top linking to VERSIONING.md</li> <li>Breaking changes reminder in CHANGELOG step</li> <li>New checklist item: \"If breaking changes: update migration guides/ADRs\"</li> <li>Organized into Pre-Release, Release, Post-Release sections</li> <li>Added: \"Highlight breaking changes\" in GitHub release notes</li> </ul> <p>Impact: Ensures breaking changes are properly documented in releases</p>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#5-contributing-prototype-phasemd","title":"5. contributing-prototype-phase.md","text":"<p>Section: New section \"Rapid Prototype Versioning Policy\" (at top)</p> <p>Added:</p> <ul> <li>IMPORTANT callout explaining breaking change policy</li> <li>Bullet points summarizing key principles</li> <li>Link to VERSIONING.md</li> </ul> <p>Impact: Reinforces policy in developer-focused contributing guide</p>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#key-messages-across-all-files","title":"Key Messages Across All Files","text":""},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#consistent-messaging","title":"Consistent Messaging","text":"<p>All updated files communicate:</p> <ol> <li>\"Breaking changes acceptable in ANY 0.x release\" - including patches</li> <li>\"No backward compatibility guarantees during 0.x\"</li> <li>\"Force refactors in dependents\" - no dual API support</li> <li>\"See VERSIONING.md for complete policy\" - single source of truth</li> </ol>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#rationale-provided","title":"Rationale Provided","text":"<p>VERSIONING.md explains:</p> <ul> <li>Why: Fast iteration &gt; compatibility during prototyping</li> <li>What: Specific examples of acceptable breaking changes</li> <li>How: Operational guidelines for contributors</li> <li>When: 1.0.0 transition to strict semver</li> </ul>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#coverage-analysis","title":"Coverage Analysis","text":""},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#where-policy-is-now-documented","title":"Where Policy Is Now Documented","text":"Location Audience Coverage README.md All users High-level notice CONTRIBUTING.md Contributors Policy + guidelines VERSIONING.md All Complete policy release_checklist.md Maintainers Process integration contributing-prototype-phase.md Developers Reinforcement"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#entry-points-covered","title":"Entry Points Covered","text":"<p>\u2705 pip install users: README.md (first thing they read) \u2705 GitHub visitors: README.md (rendered on repo home) \u2705 Contributors: CONTRIBUTING.md + contributing-prototype-phase.md \u2705 Release managers: release_checklist.md \u2705 Architects/designers: VERSIONING.md (detailed rationale)</p>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#additional-locations-considered","title":"Additional Locations Considered","text":"<p>Not Updated (but could add if needed):</p> <ul> <li><code>DEV_SETUP.md</code> - Could add brief note, but less critical (focuses on environment setup)</li> <li><code>docs/getting-started/*</code> - User-facing, less relevant (users already see README)</li> <li><code>pyproject.toml</code> - No natural place for prose; classifiers already show \"Alpha\"</li> <li>GitHub issue/PR templates - Could add reminder, but contributors already see CONTRIBUTING.md</li> </ul>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#recommendations","title":"Recommendations","text":""},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#immediate","title":"Immediate","text":"<p>\u2705 Complete - All critical entry points updated</p>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#future-before-10","title":"Future (Before 1.0)","text":"<ol> <li>Add to DEV_SETUP.md if developers report confusion</li> <li>Create GitHub PR template with versioning policy reminder</li> <li>Add to release notes template for consistent messaging</li> <li>Consider GitHub Actions bot to flag PRs with breaking changes</li> </ol>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#10-transition","title":"1.0 Transition","text":"<p>When releasing 1.0.0:</p> <ul> <li> Update all references to \"rapid prototype phase\" \u2192 \"stable release\"</li> <li> Update VERSIONING.md to mark 0.x policy as historical</li> <li> Update README.md to remove versioning warning</li> <li> Add semver badge to README</li> <li> Update CONTRIBUTING.md to remove breaking change guidance</li> </ul>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#testing","title":"Testing","text":""},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#verification-steps","title":"Verification Steps","text":"<p>To verify policy is discoverable:</p> <ol> <li>New user flow: README \u2192 VERSIONING.md \u2705</li> <li>Contributor flow: CONTRIBUTING.md \u2192 VERSIONING.md \u2705</li> <li>Release flow: release_checklist.md \u2192 VERSIONING.md \u2705</li> <li>GitHub browse: Visible in root README \u2705</li> </ol>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#user-scenarios","title":"User Scenarios","text":"Scenario Finds Policy? Path Install from PyPI, check README \u2705 Yes README.md \u00a7 Project Status Browse GitHub repo \u2705 Yes README.md (rendered) Want to contribute \u2705 Yes CONTRIBUTING.md \u00a7 2 Create PR with breaking change \u2705 Yes CONTRIBUTING.md + review Release new version \u2705 Yes release_checklist.md note Understand rationale \u2705 Yes VERSIONING.md (linked everywhere)"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#related-work","title":"Related Work","text":"<p>This documentation work directly supports:</p> <ul> <li>ADR-PP01 - Rapid prototype versioning policy</li> <li>ADR-PT04 - Prompt system refactor with breaking changes</li> <li>ADR-OS01 - Object-service architecture (breaking refactors needed)</li> <li>Future rapid prototype refactors during 0.x phase</li> </ul>"},{"location":"architecture/project-policies/versioning-policy-implementation-summary/#conclusion","title":"Conclusion","text":"<p>The rapid prototype versioning policy is now:</p> <ul> <li>\u2705 Discoverable at all key entry points</li> <li>\u2705 Comprehensive with detailed rationale</li> <li>\u2705 Consistent across all files</li> <li>\u2705 Actionable with clear guidelines for contributors</li> <li>\u2705 Linked to single source of truth (VERSIONING.md)</li> </ul> <p>Developers encountering TNH Scholar will understand:</p> <ol> <li>Breaking changes are expected during 0.x</li> <li>Why this approach is taken (fast iteration)</li> <li>How to handle breaking changes (force refactors)</li> <li>Where to find complete policy (VERSIONING.md)</li> </ol> <p>Files Modified: 5 Files Created: 2 (VERSIONING.md, this summary) Total Documentation Pages: 7</p>"},{"location":"architecture/project-policies/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-PP01: Rapid Prototype Versioning Policy - Establishes versioning policy for TNH Scholar during 0.x releases, allowing breaking changes in any release to enable fast iteration and architectural improvements.</p> <p>This file auto-generated.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/","title":"ADR-PP01: Rapid Prototype Versioning Policy","text":"<p>Establish versioning policy for TNH Scholar during rapid prototype phase (0.x) that prioritizes architectural consistency and fast iteration over backward compatibility.</p> <ul> <li>Status: Accepted</li> <li>Date: 2025-12-06</li> <li>Owner: TNH Scholar Architecture Working Group</li> <li>Authors: Aaron Solomon, Claude Sonnet 4.5</li> <li>Related: ADR-PT04, ADR-OS01, VERSIONING.md</li> </ul>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#context","title":"Context","text":""},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#problem","title":"Problem","text":"<p>TNH Scholar is in rapid prototype phase (0.x) where architectural patterns are still evolving. Recent work includes: - Object-service architecture refactors (ADR-OS01) - Prompt system refactor (ADR-PT04) - GenAI service improvements - Transport layer isolations</p> <p>Challenge: Standard semantic versioning (semver) creates pressure to maintain backward compatibility, slowing architectural improvements: - Compatibility shims accumulate technical debt - Dual API support increases maintenance burden - Developer time spent on migration paths instead of core features - Inconsistent codebase with old and new patterns coexisting</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#current-state","title":"Current State","text":"<ul> <li>Version: 0.1.4 (alpha)</li> <li>Standard semver interpretation: \"0.x is unstable, anything may change\"</li> <li>In practice: Unclear expectations on when breaking changes are acceptable</li> <li>Result: Hesitation to make needed architectural improvements</li> </ul>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#use-case-adr-pt04-prompt-system-refactor","title":"Use Case: ADR-PT04 Prompt System Refactor","text":"<p>The prompt system refactor requires: - \u2705 Remove <code>ai_text_processing/prompts.py</code> entirely - \u2705 Change imports: <code>tnh_scholar.ai_text_processing.prompts</code> \u2192 <code>tnh_scholar.prompt_system</code> - \u2705 Remove <code>TNH_PATTERN_DIR</code> environment variable - \u2705 Change CLI flags: <code>--pattern</code> \u2192 <code>--prompt</code> - \u2705 Refactor GenAI service adapter interfaces</p> <p>Question: Can we ship this in 0.1.4 \u2192 0.1.5 (patch), or must we wait for 0.2.0 (minor)?</p> <p>Standard semver for 0.x says \"maybe,\" but unclear. We need explicit policy.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#decision","title":"Decision","text":""},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#rapid-prototype-versioning-for-0x-releases","title":"Rapid Prototype Versioning for 0.x Releases","text":"<p>During 0.x releases only, TNH Scholar adopts rapid prototype versioning with the following principles:</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#1-breaking-changes-acceptable-in-any-0x-release","title":"1. Breaking Changes Acceptable in ANY 0.x Release","text":"<ul> <li>Minor bumps (0.1.x \u2192 0.2.0): MAY include breaking changes</li> <li>Patch bumps (0.1.3 \u2192 0.1.4): MAY include breaking changes</li> <li>No distinction between patch and minor for breaking change policy</li> </ul> <p>Rationale: During prototyping, architectural consistency matters more than stability. Delaying breaking changes to minor bumps creates artificial constraints.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#2-no-backward-compatibility-guarantees","title":"2. No Backward Compatibility Guarantees","text":"<ul> <li>No compatibility shims maintained</li> <li>No dual API support</li> <li>Legacy code removed immediately when replaced</li> <li>Example: <code>TNH_PATTERN_DIR</code> removed entirely, no migration period</li> </ul> <p>Rationale: Shims and dual systems create technical debt and slow iteration. Clean breaks keep codebase consistent.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#3-force-refactors-in-dependent-code","title":"3. Force Refactors in Dependent Code","text":"<ul> <li>When core systems change, ALL dependent modules MUST be updated in same PR/release</li> <li>No gradual migration strategies</li> <li>All code pushed forward to latest patterns</li> </ul> <p>Rationale: Ensures entire codebase stays architecturally consistent. Prevents fragmentation.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#4-clear-communication-documentation","title":"4. Clear Communication &amp; Documentation","text":"<p>While not guaranteeing compatibility, we provide: - Migration guides for major architectural changes (ADRs) - Breaking changes documented in CHANGELOG - Comprehensive test suites to validate changes - ADRs explaining architectural decisions</p> <p>Rationale: Developers need context even without compatibility guarantees.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#version-numbering-scheme-0x","title":"Version Numbering Scheme (0.x)","text":"<p>Use <code>MAJOR.MINOR.PATCH</code> notation with 0.x semantics:</p> <ul> <li>MAJOR: Stays at 0 during prototype phase</li> <li>MINOR bump (0.1.x \u2192 0.2.0): Significant feature additions or major refactors (may break)</li> <li>PATCH bump (0.1.3 \u2192 0.1.4): Bug fixes, small features, or refactors (may break)</li> </ul> <p>Key Difference from Semver: Both PATCH and MINOR bumps can include breaking changes.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#transition-to-semver-at-100","title":"Transition to Semver at 1.0.0","text":"<p>When releasing 1.0.0 (post-prototype): - Adopt strict semantic versioning - MAJOR = breaking changes only - MINOR = new features, backward-compatible - PATCH = bug fixes, backward-compatible - Deprecation policy with migration periods</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#implementation","title":"Implementation","text":""},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#documentation-updates","title":"Documentation Updates","text":"<p>Created/updated the following files: 1. \u2705 VERSIONING.md - Comprehensive policy document (single source of truth) 2. \u2705 README.md - Project Status section with versioning notice 3. \u2705 CONTRIBUTING.md - New section \"Versioning &amp; Breaking Changes\" 4. \u2705 release_checklist.md - Breaking change documentation reminders 5. \u2705 contributing-prototype-phase.md - Policy callout</p> <p>See implementation summary for details.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#communication-strategy","title":"Communication Strategy","text":"<p>For Users (pip install tnh-scholar): - Prominent notice in README - CHANGELOG documents all breaking changes - Pin to specific version if stability needed: <code>pip install tnh-scholar==0.1.4</code></p> <p>For Contributors: - CONTRIBUTING.md sets expectations - When making breaking changes: update dependents in same PR - No need to maintain backward compatibility during 0.x</p> <p>For Release Managers: - release_checklist.md includes breaking change checks - GitHub release notes highlight breaking changes</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#consequences","title":"Consequences","text":""},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#positive","title":"Positive","text":"<ul> <li>\u2705 Faster iteration: Architectural improvements can be implemented immediately</li> <li>\u2705 Consistent codebase: All code follows latest patterns, no legacy cruft</li> <li>\u2705 Simpler maintenance: No compatibility shims or dual APIs</li> <li>\u2705 Clear expectations: Developers know breaking changes are normal during 0.x</li> <li>\u2705 Better architecture: Can refactor toward object-service compliance without hesitation</li> </ul>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#negative","title":"Negative","text":"<ul> <li>\u274c User disruption: Updates may break existing code</li> <li>\u274c Migration burden: Users must update code when upgrading</li> <li>\u274c Perception risk: May deter users seeking stability</li> <li>\u274c Documentation overhead: Each breaking change needs clear communication</li> </ul>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#mitigations","title":"Mitigations","text":"<ul> <li>Clear documentation of breaking changes in CHANGELOG</li> <li>Migration guides (ADRs) for major architectural changes</li> <li>Version pinning guidance for users needing stability</li> <li>Comprehensive test suites to validate changes</li> <li>Explicit communication: \"0.x = rapid prototype, expect breaking changes\"</li> </ul>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#risks","title":"Risks","text":"Risk Likelihood Impact Mitigation Users pin to old versions, miss improvements Medium Medium Clear upgrade guides, compelling features Breaking changes poorly documented Low High release_checklist.md enforces documentation Perception as \"unstable project\" Medium Medium Explicit 0.x = prototype messaging Contributors forget to update dependents Low Medium PR reviews, CI tests catch breakage"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#examples","title":"Examples","text":""},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#acceptable-in-014-015-patch-bump","title":"Acceptable in 0.1.4 \u2192 0.1.5 (Patch Bump)","text":"<p>\u2705 Remove <code>ai_text_processing/prompts.py</code> entirely \u2705 Change import paths \u2705 Remove <code>TNH_PATTERN_DIR</code> environment variable \u2705 Change CLI flags <code>--pattern</code> \u2192 <code>--prompt</code> \u2705 Refactor GenAI service adapter interfaces</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#acceptable-in-01x-020-minor-bump","title":"Acceptable in 0.1.x \u2192 0.2.0 (Minor Bump)","text":"<p>\u2705 All of the above PLUS: \u2705 Major architecture refactor (e.g., prompt system \u2192 object-service) \u2705 Complete rewrite of subsystems \u2705 New CLI tools or major features</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#not-acceptable-never","title":"NOT Acceptable (Never)","text":"<p>\u274c Breaking changes without documentation \u274c Breaking changes without updating dependents \u274c Breaking changes without test updates</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#alternative-1-strict-semver-for-0x","title":"Alternative 1: Strict Semver for 0.x","text":"<p>Rejected: Standard semver allows breaking changes in 0.x minors but discourages in patches. This still creates pressure to batch changes, slowing iteration.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#alternative-2-trunk-based-development-without-releases","title":"Alternative 2: Trunk-Based Development Without Releases","text":"<p>Rejected: Need versioned releases for PyPI distribution and user communication.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#alternative-3-maintain-compatibility-shims","title":"Alternative 3: Maintain Compatibility Shims","text":"<p>Rejected: Creates technical debt and dual-system complexity. Violates rapid prototype principle.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#alternative-4-wait-until-10-for-all-breaking-changes","title":"Alternative 4: Wait Until 1.0 for All Breaking Changes","text":"<p>Rejected: Would delay critical architectural improvements by months/years. Not viable for rapid prototyping.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#success-metrics","title":"Success Metrics","text":""},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#short-term-0x-phase","title":"Short Term (0.x phase)","text":"<ul> <li>\u2705 Breaking changes documented in CHANGELOG (100%)</li> <li>\u2705 Dependent code updated in same release (100%)</li> <li>\u2705 No compatibility shims added</li> <li>\u2705 Test suites pass after breaking changes (100%)</li> </ul>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#long-term-10-transition","title":"Long Term (1.0 transition)","text":"<ul> <li>Stable public API surface defined</li> <li>Comprehensive documentation</li> <li>Production usage validation</li> <li> <p>80% test coverage</p> </li> <li>Security review complete</li> </ul>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#related-documentation","title":"Related Documentation","text":"<ul> <li>VERSIONING.md - Complete versioning policy</li> <li>ADR-PT04 - Example breaking change (prompt system)</li> <li>ADR-OS01 - Architecture requiring breaking refactors</li> <li>Implementation Summary - Documentation updates</li> </ul>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#approval","title":"Approval","text":"<ul> <li>Architecture Team: Accepted 2025-12-06</li> <li>Maintainers: Accepted 2025-12-06</li> <li>Implementation: Complete (documentation updated)</li> </ul>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#appendix-faq","title":"Appendix: FAQ","text":""},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#q-why-not-follow-standard-semver-during-0x","title":"Q: Why not follow standard semver during 0.x?","text":"<p>A: Standard semver says \"0.x is for initial development, anything may change\" but doesn't provide operational guidelines. We make expectations explicit: - Force dependent refactors (no compatibility shims) - Immediate deprecation and removal - Breaking changes acceptable in patches, not just minors</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#q-when-will-10-be-released","title":"Q: When will 1.0 be released?","text":"<p>A: When core architecture is stable, public APIs are finalized, and the project is production-ready. No specific timeline. Follow GitHub milestones.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#q-how-do-i-avoid-breaking-changes","title":"Q: How do I avoid breaking changes?","text":"<p>A: Pin to a specific version: <code>pip install tnh-scholar==0.1.4</code>. Review CHANGELOG before upgrading.</p>"},{"location":"architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning/#q-what-if-im-building-on-top-of-tnh-scholar","title":"Q: What if I'm building on top of TNH Scholar?","text":"<p>A: During 0.x, expect to update your code when upgrading. Consider: - Pin to specific version until 1.0 - Subscribe to release notifications - Review ADRs for architectural changes - Contribute to design discussions</p> <p>Last Updated: 2025-12-06 Current Version: 0.1.4 Current Phase: Rapid Prototype (0.x) Next Review: Before 1.0.0 release</p>"},{"location":"architecture/prompt-system/","title":"Prompt System","text":"<p>Table of Contents:</p> <p>Prompt System Architecture - Current and planned architecture for the TNH Scholar prompt system, including VS Code integration and PromptCatalog service design.</p> <p>Adr - Table of contents for architecture/prompt-system/adr</p> <p>Archive - Table of contents for architecture/prompt-system/archive</p> <p>This file auto-generated.</p>"},{"location":"architecture/prompt-system/prompt-system-architecture/","title":"Prompt System Architecture","text":"<p>Current and planned architecture for the TNH Scholar prompt system, including VS Code integration and PromptCatalog service design.</p>"},{"location":"architecture/prompt-system/prompt-system-architecture/#overview","title":"Overview","text":"<p>The TNH Scholar prompt system provides engineered text inputs (prompts) for AI models, enabling consistent, reproducible, and customizable text processing. This document describes the current implementation and planned enhancements.</p> <p>Current Status: See ADR-PT03: Current Status &amp; Roadmap for implementation details.</p> <p>Historical Context: See Core Pattern Architecture for earlier architectural explorations (uses legacy \"Pattern\" terminology).</p>"},{"location":"architecture/prompt-system/prompt-system-architecture/#design-goals","title":"Design Goals","text":"<ol> <li>Simplicity: Git-based storage, human-readable Jinja2 templates</li> <li>Transparency: All prompts are plain text files, version-controlled</li> <li>Versioning: Git lineage provides collaborative editing and reproducibility</li> <li>Flexibility: Template-based customization with context variables</li> <li>Discoverability: Structured metadata enables browsing and selection</li> <li>Reproducibility: Fingerprinting ensures identical inputs produce identical outputs</li> </ol>"},{"location":"architecture/prompt-system/prompt-system-architecture/#current-architecture-v1","title":"Current Architecture (V1)","text":""},{"location":"architecture/prompt-system/prompt-system-architecture/#storage-and-versioning","title":"Storage and Versioning","text":"<p>Implementation:</p> <ul> <li>Prompts stored as Jinja2 <code>.md</code> files in <code>~/.config/tnh_scholar/patterns/</code></li> <li>Git version control for prompt history</li> <li>Simple file-based discovery</li> </ul> <p>Access Pattern:</p> <pre><code>from tnh_scholar.ai_text_processing.patterns import LocalPatternManager\n\n# Singleton access (prototype phase)\nmanager = LocalPatternManager()\nprompt = manager.get_pattern(\"default_punctuate\")\nrendered = prompt.apply_template(context={\"language\": \"en\"})\n</code></pre>"},{"location":"architecture/prompt-system/prompt-system-architecture/#components","title":"Components","text":"<pre><code>graph TD\n    A[CLI Tools] --&gt; B[LocalPatternManager]\n    B --&gt; C[PatternManager]\n    C --&gt; D[Prompt Files&lt;br/&gt;*.md]\n    D --&gt; E[Git Repository]\n\n    style B fill:#f9f,stroke:#333\n    style C fill:#bbf,stroke:#333\n    style D fill:#bfb,stroke:#333</code></pre> <p>Key Classes:</p> <ul> <li><code>LocalPatternManager</code>: Singleton wrapper for global access</li> <li><code>PatternManager</code>: Core prompt loading and rendering</li> <li>Jinja2 templates: Prompt files with variable substitution</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#limitations","title":"Limitations","text":"<ul> <li>No structured metadata: Limited discoverability</li> <li>Singleton access: Difficult to test, hard to inject</li> <li>No fingerprinting: Can't verify reproducibility</li> <li>Limited validation: No enforcement of required variables</li> <li>No caching: Repeated renders re-parse templates</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#planned-architecture-v2","title":"Planned Architecture (V2)","text":""},{"location":"architecture/prompt-system/prompt-system-architecture/#promptcatalog-service","title":"PromptCatalog Service","text":"<p>Following the Object-Service Design pattern:</p> <pre><code>graph TD\n    A[Application Layer] --&gt; B[PromptCatalog Protocol]\n    B --&gt; C[GitPromptCatalog]\n    C --&gt; D[PromptRepository]\n    D --&gt; E[PromptLoader]\n    D --&gt; F[PromptCache]\n    E --&gt; G[Prompt Files]\n    G --&gt; H[Git Storage]\n\n    style B fill:#bbf,stroke:#333\n    style C fill:#bfb,stroke:#333\n    style D fill:#fbb,stroke:#333</code></pre> <p>Service Protocol:</p> <pre><code>from typing import Protocol\n\nclass PromptCatalog(Protocol):\n    \"\"\"Protocol for prompt catalog implementations.\"\"\"\n\n    def get_prompt(self, name: str, version: Optional[str] = None) -&gt; Prompt:\n        \"\"\"Retrieve prompt by name and optional version.\"\"\"\n        ...\n\n    def list_prompts(\n        self,\n        task_type: Optional[TaskType] = None,\n        tags: Optional[list[str]] = None\n    ) -&gt; list[PromptMetadata]:\n        \"\"\"List available prompts with optional filtering.\"\"\"\n        ...\n\n    def render_prompt(\n        self,\n        name: str,\n        context: dict[str, Any],\n        version: Optional[str] = None\n    ) -&gt; RenderedPrompt:\n        \"\"\"Render prompt with context and return fingerprinted result.\"\"\"\n        ...\n\n    def validate_prompt(self, name: str) -&gt; PromptValidation:\n        \"\"\"Validate prompt template and metadata.\"\"\"\n        ...\n</code></pre>"},{"location":"architecture/prompt-system/prompt-system-architecture/#prompt-metadata-model","title":"Prompt Metadata Model","text":"<pre><code>from pydantic import BaseModel, Field\nfrom enum import Enum\n\nclass TaskType(str, Enum):\n    \"\"\"Types of processing tasks.\"\"\"\n    PUNCTUATE = \"punctuate\"\n    TRANSLATE = \"translate\"\n    SECTION = \"section\"\n    CUSTOM = \"custom\"\n\nclass PromptMetadata(BaseModel):\n    \"\"\"Metadata for a prompt template.\"\"\"\n\n    name: str = Field(..., description=\"Unique prompt identifier\")\n    version: str = Field(..., description=\"Semantic version (e.g., '1.0.0')\")\n    task_type: TaskType = Field(..., description=\"Type of processing task\")\n    description: str = Field(..., description=\"Human-readable description\")\n\n    # Variable requirements\n    required_variables: list[str] = Field(default_factory=list)\n    optional_variables: dict[str, Any] = Field(default_factory=dict)\n\n    # Model constraints\n    recommended_model: Optional[str] = None\n    min_context_window: Optional[int] = None\n    max_tokens: Optional[int] = None\n\n    # Organizational\n    tags: list[str] = Field(default_factory=list)\n    author: Optional[str] = None\n    created: datetime\n    modified: datetime\n\n    # Git provenance\n    git_commit: Optional[str] = None\n    git_branch: Optional[str] = None\n</code></pre>"},{"location":"architecture/prompt-system/prompt-system-architecture/#prompt-fingerprinting","title":"Prompt Fingerprinting","text":"<pre><code>from dataclasses import dataclass\nfrom hashlib import sha256\n\n@dataclass(frozen=True)\nclass PromptFingerprint:\n    \"\"\"Immutable fingerprint for rendered prompt.\"\"\"\n\n    template_hash: str  # Hash of template content\n    metadata_hash: str  # Hash of metadata\n    context_hash: str   # Hash of context variables\n    git_commit: Optional[str]  # Git commit of template\n\n    @property\n    def full_hash(self) -&gt; str:\n        \"\"\"Combined hash of all components.\"\"\"\n        combined = f\"{self.template_hash}:{self.metadata_hash}:{self.context_hash}\"\n        return sha256(combined.encode()).hexdigest()\n\n@dataclass(frozen=True)\nclass RenderedPrompt:\n    \"\"\"Rendered prompt with provenance.\"\"\"\n\n    content: str\n    metadata: PromptMetadata\n    fingerprint: PromptFingerprint\n    rendered_at: datetime\n</code></pre> <p>Use Case: Caching AI responses by fingerprint:</p> <pre><code># Check cache before expensive AI call\nfingerprint = catalog.render_prompt(\"translate\", context).fingerprint\ncached_response = response_cache.get(fingerprint.full_hash)\n\nif cached_response:\n    return cached_response\n\n# Make AI call and cache by fingerprint\nresponse = ai_service.generate(prompt)\nresponse_cache.set(fingerprint.full_hash, response)\n</code></pre>"},{"location":"architecture/prompt-system/prompt-system-architecture/#vs-code-integration-requirements","title":"VS Code Integration Requirements","text":"<p>See ADR-VSC01 and ADR-VSC02 (pending) for full context.</p>"},{"location":"architecture/prompt-system/prompt-system-architecture/#interactive-prompt-selection","title":"Interactive Prompt Selection","text":"<p>Command Palette Integration:</p> <pre><code>// VS Code command: \"TNH Scholar: Select Prompt\"\nconst prompts = await promptCatalog.listPrompts({\n  taskType: \"translate\",\n  tags: [\"dharma\", \"vietnamese\"]\n});\n\nconst selected = await vscode.window.showQuickPick(\n  prompts.map(p =&gt; ({\n    label: p.name,\n    description: p.description,\n    detail: `v${p.version} | ${p.tags.join(\", \")}`\n  }))\n);\n</code></pre>"},{"location":"architecture/prompt-system/prompt-system-architecture/#prompt-preview-and-editing","title":"Prompt Preview and Editing","text":"<p>Requirements:</p> <ul> <li>Real-time rendering with sample context</li> <li>Syntax highlighting for Jinja2 templates</li> <li>Variable validation and autocomplete</li> <li>Git diff view for prompt versions</li> </ul> <p>Preview Workflow:</p> <ol> <li>User selects prompt in VS Code</li> <li>Extension loads prompt metadata</li> <li>User provides context variables (or uses defaults)</li> <li>Extension renders preview with current values</li> <li>User can edit template and see live updates</li> </ol>"},{"location":"architecture/prompt-system/prompt-system-architecture/#prompt-authoring-workflow","title":"Prompt Authoring Workflow","text":"<p>Create New Prompt:</p> <ol> <li>User runs command: \"TNH Scholar: Create Prompt\"</li> <li>VS Code prompts for metadata (name, task type, description)</li> <li>Extension generates scaffold with metadata frontmatter:</li> </ol> <pre><code>---\nname: custom_summarize\nversion: 1.0.0\ntask_type: custom\ndescription: Summarize dharma talk with key insights\nrequired_variables: [text, max_words]\noptional_variables: {language: en, style: concise}\ntags: [summarize, dharma]\n---\n\n# Summarize Dharma Talk\n\nPlease summarize the following dharma talk in {{max_words}} words or less.\n\n**Language**: {{language}}\n**Style**: {{style}}\n\n## Text to Summarize\n\n{{text}}\n\n## Summary Instructions\n\nFocus on:\n- Core dharma teachings\n- Practical applications\n- Key insights for practitioners\n</code></pre> <ol> <li>User edits template</li> <li>Extension validates on save (required variables, Jinja2 syntax)</li> <li>User commits to Git</li> </ol>"},{"location":"architecture/prompt-system/prompt-system-architecture/#migration-path","title":"Migration Path","text":""},{"location":"architecture/prompt-system/prompt-system-architecture/#phase-1-enhanced-metadata-current","title":"Phase 1: Enhanced Metadata (Current)","text":"<ul> <li>\u2705 Add YAML frontmatter to existing prompts</li> <li>\u2705 Document required/optional variables</li> <li>\u2705 Add task_type and tags</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#phase-2-promptcatalog-service-q1-2026","title":"Phase 2: PromptCatalog Service (Q1 2026)","text":"<ul> <li>Implement <code>PromptCatalog</code> protocol</li> <li>Create <code>GitPromptCatalog</code> implementation</li> <li>Add fingerprinting and caching</li> <li>Maintain backwards compatibility with <code>LocalPatternManager</code></li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#phase-3-vs-code-integration-q1-q2-2026","title":"Phase 3: VS Code Integration (Q1-Q2 2026)","text":"<ul> <li>Implement command palette integration</li> <li>Add prompt preview/editing UI</li> <li>Create authoring workflow</li> <li>Enable git-based versioning UI</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#phase-4-production-hardening-q2-2026","title":"Phase 4: Production Hardening (Q2 2026)","text":"<ul> <li>Replace <code>LocalPatternManager</code> singleton with dependency injection</li> <li>Full prompt validation and error reporting</li> <li>Usage analytics and cost tracking</li> <li>Performance optimization</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#implementation-examples","title":"Implementation Examples","text":""},{"location":"architecture/prompt-system/prompt-system-architecture/#current-usage-v1","title":"Current Usage (V1)","text":"<pre><code># CLI tool using singleton\nfrom tnh_scholar.ai_text_processing.patterns import LocalPatternManager\n\nmanager = LocalPatternManager()\nprompt = manager.get_pattern(\"default_translate\")\nrendered = prompt.apply_template(context={\n    \"text\": input_text,\n    \"target_language\": \"en\",\n    \"source_language\": \"vi\"\n})\n\n# Pass to AI service\nresult = ai_service.process(rendered)\n</code></pre>"},{"location":"architecture/prompt-system/prompt-system-architecture/#planned-usage-v2","title":"Planned Usage (V2)","text":"<pre><code># Service with dependency injection\nfrom tnh_scholar.gen_ai_service import GenAIService\nfrom tnh_scholar.prompt_system import GitPromptCatalog\n\n# Initialize with config\ncatalog = GitPromptCatalog(config=prompt_config)\nai_service = GenAIService(config=ai_config)\n\n# Render with fingerprinting\nrendered = catalog.render_prompt(\n    name=\"default_translate\",\n    context={\n        \"text\": input_text,\n        \"target_language\": \"en\",\n        \"source_language\": \"vi\"\n    }\n)\n\n# Check cache\ncached = response_cache.get(rendered.fingerprint.full_hash)\nif cached:\n    return cached\n\n# Process with AI\nresult = ai_service.generate(rendered.content)\n\n# Cache by fingerprint\nresponse_cache.set(rendered.fingerprint.full_hash, result)\n</code></pre>"},{"location":"architecture/prompt-system/prompt-system-architecture/#security-and-safety","title":"Security and Safety","text":""},{"location":"architecture/prompt-system/prompt-system-architecture/#prompt-injection-prevention","title":"Prompt Injection Prevention","text":"<ul> <li>Variable validation: Only allow expected variables</li> <li>Template sandboxing: Restrict Jinja2 capabilities (no code execution)</li> <li>Input sanitization: Escape special characters in context values</li> <li>Audit logging: Track prompt modifications and usage</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#secrets-management","title":"Secrets Management","text":"<ul> <li>Never store secrets in prompts</li> <li>Use environment variables for API keys</li> <li>Git-ignore sensitive context files</li> <li>Prompt templates are public artifacts</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/prompt-system/prompt-system-architecture/#caching-strategy","title":"Caching Strategy","text":"<ul> <li>Template cache: Parsed Jinja2 templates (in-memory)</li> <li>Metadata cache: Prompt metadata (in-memory, TTL: 5 min)</li> <li>Response cache: AI responses by fingerprint (Redis/disk, TTL: 24 hours)</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#lazy-loading","title":"Lazy Loading","text":"<ul> <li>Load prompts on demand, not at startup</li> <li>Cache only frequently-used prompts</li> <li>Invalidate cache on Git updates</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/prompt-system/prompt-system-architecture/#unit-tests","title":"Unit Tests","text":"<ul> <li>Prompt rendering with various contexts</li> <li>Metadata validation</li> <li>Fingerprint uniqueness and stability</li> <li>Variable substitution edge cases</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#integration-tests","title":"Integration Tests","text":"<ul> <li>Git repository operations</li> <li>Cache invalidation</li> <li>VS Code extension integration</li> <li>End-to-end authoring workflow</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>ADR-PT03: Current Status &amp; Roadmap - Current implementation</li> <li>ADR-DD03: Terminology Standardization - Pattern\u2192Prompt shift</li> <li>ADR-VSC01: VS Code Integration Strategy</li> <li>Object-Service Design - Architecture pattern</li> <li>Archive: Pattern Core Design - Historical architecture (legacy terminology)</li> </ul>"},{"location":"architecture/prompt-system/prompt-system-architecture/#references","title":"References","text":"<ul> <li>Jinja2 Template Designer Documentation</li> <li>Prompt Engineering Guide</li> <li>LangChain Prompt Templates</li> </ul>"},{"location":"architecture/prompt-system/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-PT03: Prompt System Current Status &amp; Roadmap - Current as-built status of the TNH Scholar prompt system, documentation terminology standardization, and planned enhancements.</p> <p>ADR-PT04: Prompt System Refactor Plan - Refactors the legacy pattern-based prompt system into a modular, object-service compliant PromptCatalog with validation, transport isolation, and clean dependency injection seams.</p> <p>This file auto-generated.</p>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/","title":"ADR-PT03: Prompt System Current Status &amp; Roadmap","text":"<p>Documents the current state of the TNH Scholar prompt system, recent documentation standardization work, and planned enhancements for VS Code integration and gen-ai-service upgrades.</p> <ul> <li>Filename: <code>adr-pt03-current-status-roadmap.md</code></li> <li>Heading: <code># ADR-PT03: Prompt System Current Status &amp; Roadmap</code></li> <li>Status: Proposed</li> <li>Date: 2025-11-29</li> <li>Author: Claude Sonnet 4.5</li> <li>Owner: TNH Scholar Architecture Working Group</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#purpose","title":"Purpose","text":"<p>This ADR provides a current snapshot of the prompt system architecture, terminology standardization efforts, and planned work. It serves as the primary entry point for understanding the prompt system's current state and future direction.</p>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#current-system-status","title":"Current System Status","text":""},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#implementation-overview","title":"Implementation Overview","text":"<p>The TNH Scholar prompt system currently uses:</p> <ul> <li>Git-based Storage: Prompts stored as versioned text files in <code>~/.config/tnh_scholar/patterns/</code> (directory name retained for backwards compatibility)</li> <li>Jinja2 Templates: Prompts are Jinja2 templates rendered with context variables before AI processing</li> <li>LocalPatternManager: Singleton pattern manager providing global access to prompts (prototype phase; see historical notes in Core Pattern Architecture)</li> <li>CLI Integration: Command-line tools (<code>tnh-fab</code>) use prompts via <code>--pattern</code> flag</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#key-components","title":"Key Components","text":"<ol> <li>Pattern/Prompt Files: Git-versioned Jinja2 templates</li> <li>PatternManager: Manages prompt discovery, loading, and rendering</li> <li>LocalPatternManager: Singleton wrapper for global access (prototype architecture)</li> <li>Prompt Templates: Core prompts include sectioning, translation, punctuation, and custom processing</li> </ol>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#code-status","title":"Code Status","text":"<ul> <li>Core Implementation: <code>src/tnh_scholar/ai_text_processing/</code> (uses legacy \"pattern\" terminology)</li> <li>CLI Tools: <code>src/tnh_scholar/cli_tools/tnh_fab/</code> (uses <code>--pattern</code> flag)</li> <li>Configuration: <code>TNH_PATTERN_DIR</code> environment variable (legacy naming)</li> </ul> <p>Note: Code refactoring to use \"Prompt\" terminology is tracked separately. Many legacy modules are scheduled for deprecation/deletion as part of the gen-ai-service refactor and tnh-gen CLI redesign.</p>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#recent-work-documentation-terminology-standardization","title":"Recent Work: Documentation Terminology Standardization","text":""},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#adr-dd03-patternprompt-terminology-shift","title":"ADR-DD03: Pattern\u2192Prompt Terminology Shift","text":"<p>Completed: 2025-11-28 to 2025-11-29</p> <p>All user-facing documentation has been updated to use \"Prompt\" instead of \"Pattern\" to align with: - Industry standard terminology (Prompt Engineering, Prompt Catalog) - Refactored gen-ai-service architecture (Prompt/PromptCatalog classes) - External stakeholder expectations (Parallax Press, new users)</p> <p>Scope: Documentation only (code refactoring tracked separately)</p> <p>Key Changes: - Updated docs/index.md, README.md, getting-started/, user-guide/ - Renamed <code>docs/user-guide/patterns.md</code> \u2192 <code>prompts.md</code> - Renamed <code>docs/architecture/pattern-system/</code> \u2192 <code>prompt-system/</code> - Added historical terminology note to docs/index.md explaining Pattern\u2192Prompt shift - Retained legacy naming for backwards compatibility: <code>TNH_PATTERN_DIR</code>, <code>--pattern</code> CLI flags</p> <p>Reference: ADR-DD03: Pattern to Prompt Terminology Standardization</p>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#historical-context","title":"Historical Context","text":"<p>Earlier architectural explorations (now archived): - Core Pattern Architecture: Legacy prompt/pattern design notes (uses historical \"Pattern\" terminology)</p> <p>These archived notes document the evolution from prototype singleton architecture toward production dependency-injection patterns.</p>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#planned-work-roadmap","title":"Planned Work &amp; Roadmap","text":""},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#1-vs-code-integration-requirements","title":"1. VS Code Integration Requirements","text":"<p>Upcoming: ADR-VSC02 (tnh-gen CLI implementation) will require prompt system enhancements:</p> <ul> <li>Interactive Prompt Selection: VS Code command palette integration for browsing/selecting prompts</li> <li>Prompt Preview: Real-time rendering of prompts with context variables</li> <li>Custom Prompt Authoring: In-editor prompt creation and testing workflow</li> <li>Prompt Versioning UI: Git integration for prompt history and diffs</li> </ul> <p>Dependencies: - Enhanced PromptCatalog with metadata queries - Prompt validation and preview rendering - Structured prompt metadata (task type, model constraints, variables)</p> <p>Status: Requirements gathering phase; detailed ADR pending</p> <p>Reference: ADR-VSC01: VS Code Integration Strategy</p>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#2-gen-ai-service-refactor-integration","title":"2. Gen-AI-Service Refactor Integration","text":"<p>Planned: Transition to modern Prompt/PromptCatalog architecture:</p> <ul> <li>Replace LocalPatternManager singleton with dependency-injected PromptCatalog</li> <li>Implement PromptFingerprint for reproducibility and caching</li> <li>Add structured prompt metadata (task type, model constraints, safety requirements)</li> <li>Support deterministic rendering and caching keyed by fingerprint</li> <li>Enable usage analytics (cost/latency tracking)</li> </ul> <p>Status: Design phase; awaiting gen-ai-service ADRs</p> <p>Historical Reference: Core Pattern Architecture outlined early dependency-injection transition ideas</p>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#3-tnh-gen-cli-redesign","title":"3. tnh-gen CLI Redesign","text":"<p>Planned: Refactor tnh-fab \u2192 tnh-gen with prompt-first architecture:</p> <ul> <li>Unified <code>tnh-gen</code> command with prompt-based subcommands</li> <li>Improved prompt discovery and selection UX</li> <li>Interactive prompt authoring workflow</li> <li>Better integration with VS Code extension</li> </ul> <p>Status: Requirements phase; ADR pending</p>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#design-principles","title":"Design Principles","text":"<p>Current prompt system design emphasizes:</p> <ol> <li>Simplicity: Git-based storage, minimal abstractions</li> <li>Transparency: Human-readable Jinja2 templates</li> <li>Versioning: Git lineage for collaborative editing and reproducibility</li> <li>Flexibility: Template-based customization with context variables</li> </ol> <p>Future enhancements will preserve these principles while adding: - Structured metadata for discoverability - Reproducibility via fingerprinting - Better tooling integration (VS Code, CLI)</p>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#migration-path","title":"Migration Path","text":"<p>Phase 1 (Completed): Documentation terminology standardization - \u2705 User-facing docs use \"Prompt\" terminology - \u2705 Historical note added to docs/index.md - \u2705 Legacy config/CLI naming documented</p> <p>Phase 2 (Upcoming): VS Code integration foundation - Enhance PromptCatalog with metadata queries - Implement prompt preview/rendering - Add structured prompt metadata</p> <p>Phase 3 (Future): Production architecture transition - Replace singleton with dependency injection (per ADR-PT01) - Implement PromptFingerprint - Add usage analytics and caching</p> <p>Phase 4 (Future): Code terminology updates - Refactor code to use \"Prompt\" terminology - Update CLI flags (with backwards compatibility) - Deprecate legacy modules</p>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#references","title":"References","text":""},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#current-documentation","title":"Current Documentation","text":"<ul> <li>Prompt System User Guide</li> <li>Prompt System Overview</li> <li>Configuration Guide</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#related-adrs","title":"Related ADRs","text":"<ul> <li>ADR-DD03: Pattern to Prompt Terminology Standardization - Documentation terminology shift</li> <li>ADR-VSC01: VS Code Integration Strategy - VS Code extension requirements</li> <li>Archive: Core Pattern Architecture - Historical architecture decisions</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#development-documentation","title":"Development Documentation","text":"<ul> <li>Core Pattern Architecture - Detailed architectural concepts (uses legacy \"Pattern\" terminology)</li> <li>System Design - High-level system architecture</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt03-prompt-system-status-roadmap/#status","title":"Status","text":"<p>Current Phase: Documentation standardization complete; VS Code integration requirements gathering</p> <p>Next Steps: 1. Draft ADR-VSC02 (tnh-gen CLI implementation) with prompt system requirements 2. Begin gen-ai-service refactor ADRs with PromptCatalog design 3. Update development documentation with terminology notes (see ADR-DD03 Phase 2)</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/","title":"ADR-PT04: Prompt System Refactor Plan (Revised)","text":"<p>Retire the monolithic pattern-era prompt code and replace it with a modular, object-service compliant <code>prompt_system</code> package aligned to ADR-A12 and ADR-OS01.</p> <ul> <li>Status: Accepted (Revised)</li> <li>Date: 2025-12-05 (Updated: 2025-12-06)</li> <li>Owner: TNH Scholar Architecture Working Group</li> <li>Authors: Codex (GPT-5), Aaron Solomon, Claude Sonnet 4.5</li> <li>Related: ADR-PT03, ADR-A12, ADR-OS01, ADR-VSC02, TODO</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#context","title":"Context","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#legacy-system-limitations","title":"Legacy System Limitations","text":"<ul> <li>Legacy <code>src/tnh_scholar/ai_text_processing/prompts.py</code> (~34 KB) violates separation of concerns: mixes Jinja rendering, git commits, file locking, CLI UX helpers, and dotenv loading.</li> <li>Pattern-era naming (<code>TNH_PATTERN_DIR</code>, <code>--pattern</code> flags) persists despite terminology migration to \"prompts\" (ADR-DD03/ADR-PT03).</li> <li>No transport layer isolation: git operations, file I/O, and caching are embedded in domain logic.</li> <li>Missing validation: no schema enforcement for prompt metadata (TODO #11, #16).</li> <li>Poor testability: monolithic structure prevents mocking and protocol-based testing.</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#service-contract-requirements","title":"Service Contract Requirements","text":"<ul> <li>ADR-A12: GenAI service expects <code>PromptsAdapter</code> returning <code>(RenderedPrompt, Fingerprint)</code> with provenance constructed in the service layer.</li> <li>ADR-OS01: All services must follow object-service architecture\u2014domain protocols, adapters with mappers, transport isolation, strong typing, no literals.</li> <li>Prompt-first tooling: <code>tnh-gen</code> CLI and VS Code integration (ADR-VSC02) require discoverability, validation, and structured metadata.</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#rapid-prototype-operating-principles","title":"Rapid Prototype Operating Principles","text":"<p>IMPORTANT: TNH Scholar is in rapid prototype phase (0.x). We prioritize:</p> <ol> <li>Breaking changes are acceptable: No backward compatibility guarantees during 0.x; breaking changes push all dependent modules forward rather than maintain legacy shims.</li> <li>Force refactors in dependents: When prompt system changes, GenAI service and CLI tools MUST refactor\u2014this ensures architectural consistency.</li> <li>Remove legacy immediately: Deprecate and remove <code>TNH_PATTERN_DIR</code> and old APIs now; no migration timeline needed.</li> <li>Single implementation: New prompt system replaces GenAI's current implementation completely; no dual catalog support.</li> </ol>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#decision","title":"Decision","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#1-package-structure-object-service-compliant","title":"1. Package Structure (Object-Service Compliant)","text":"<p>Create <code>src/tnh_scholar/prompt_system/</code> with clean layer separation:</p> <pre><code>src/tnh_scholar/prompt_system/\n  config/\n    settings.py              # Settings: env vars (TNH_PROMPT_DIR, defaults)\n    prompt_catalog_config.py # Config: construction-time catalog config\n    policy.py                # Policy: render precedence, validation strictness\n\n  domain/\n    models.py                # Domain models: Prompt, PromptMetadata, RenderedPrompt\n    protocols.py             # Protocols: PromptCatalogPort, PromptRendererPort, etc.\n\n  transport/\n    models.py                # Transport models: PromptFileRequest/Response, GitRefreshRequest/Response\n    git_client.py            # GitTransportClient: pure git operations\n    cache_client.py          # CacheTransport: in-memory caching\n\n  adapters/\n    git_catalog_adapter.py   # GitPromptCatalog: implements PromptCatalogPort\n    filesystem_catalog_adapter.py  # FilesystemPromptCatalog: offline mode\n\n  mappers/\n    prompt_mapper.py         # PromptMapper: bi-directional file \u2194 domain translation\n\n  service/\n    renderer.py              # PromptRenderer: Jinja environment + precedence\n    validator.py             # PromptValidator: schema validation\n    loader.py                # PromptLoader: front-matter parsing\n\n  infra/\n    locks.py                 # File locking utilities (if still needed)\n</code></pre> <p>Migration:</p> <ul> <li>Remove <code>ai_text_processing/prompts.py</code> entirely (no shim).</li> <li>Update all imports to <code>tnh_scholar.prompt_system</code>.</li> <li>Break existing code\u2014force refactors.</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#2-configuration-taxonomy-adr-os01-compliant","title":"2. Configuration Taxonomy (ADR-OS01 Compliant)","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#settings-application-wide-from-environment","title":"Settings (Application-Wide, from Environment)","text":"<pre><code># config/settings.py\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom pathlib import Path\n\nclass PromptSystemSettings(BaseSettings):\n    \"\"\"Application-wide prompt system settings (from environment).\"\"\"\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        extra=\"ignore\"\n    )\n\n    # Prompt repository location\n    tnh_prompt_dir: Path = Path(\"prompts/\")  # NEW: only this env var supported\n\n    # Defaults\n    default_validation_mode: str = \"strict\"\n    cache_enabled: bool = True\n    cache_ttl_seconds: int = 300\n\n    # Safety/security\n    enable_safety_validation: bool = True\n\n    @classmethod\n    def from_env(cls) -&gt; \"PromptSystemSettings\":\n        return cls()\n</code></pre> <p>BREAKING CHANGE: <code>TNH_PATTERN_DIR</code> no longer supported. Use <code>TNH_PROMPT_DIR</code> only.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#config-construction-time","title":"Config (Construction-Time)","text":"<pre><code># config/prompt_catalog_config.py\nfrom pydantic import BaseModel\nfrom pathlib import Path\n\nclass PromptCatalogConfig(BaseModel):\n    \"\"\"Construction-time configuration for PromptCatalog.\"\"\"\n    repository_path: Path\n    enable_git_refresh: bool = True\n    cache_ttl_s: int = 300\n    validation_on_load: bool = True\n\nclass GitTransportConfig(BaseModel):\n    \"\"\"Git transport layer configuration.\"\"\"\n    repository_path: Path\n    auto_pull: bool = False\n    pull_timeout_s: float = 30.0\n    default_branch: str = \"main\"\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#params-per-call","title":"Params (Per-Call)","text":"<pre><code># domain/models.py\nfrom pydantic import BaseModel\nfrom typing import Any, Literal\n\nclass RenderParams(BaseModel):\n    \"\"\"Per-call rendering parameters.\"\"\"\n    variables: dict[str, Any] = {}\n    strict_undefined: bool = True\n    preserve_whitespace: bool = False\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#policy-behavior-control","title":"Policy (Behavior Control)","text":"<pre><code># config/policy.py\nfrom pydantic import BaseModel\nfrom typing import Literal\n\nclass PromptRenderPolicy(BaseModel):\n    \"\"\"Policy for prompt rendering precedence and behavior.\"\"\"\n    policy_version: str = \"1.0\"\n\n    # Precedence order (highest to lowest)\n    precedence_order: list[str] = [\n        \"caller_context\",       # RenderParams.variables\n        \"frontmatter_defaults\", # Prompt metadata defaults\n        \"settings_defaults\"     # Settings defaults\n    ]\n\n    # Behavior toggles\n    allow_undefined_vars: bool = False\n    merge_strategy: Literal[\"override\", \"merge_deep\"] = \"override\"\n\nclass ValidationPolicy(BaseModel):\n    \"\"\"Validation behavior policy.\"\"\"\n    policy_version: str = \"1.0\"\n    mode: Literal[\"strict\", \"warn\", \"permissive\"] = \"strict\"\n    fail_on_missing_required: bool = True\n    allow_extra_variables: bool = False\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#3-domain-models-protocols","title":"3. Domain Models &amp; Protocols","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#domain-models","title":"Domain Models","text":"<pre><code># domain/models.py\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\nclass PromptMetadata(BaseModel):\n    \"\"\"Prompt frontmatter metadata (validated schema).\"\"\"\n    # Required\n    key: str                    # Unique identifier (e.g., \"translate\", derived from filename)\n    name: str\n    version: str\n    description: str\n    task_type: str\n    required_variables: list[str]\n\n    # Optional\n    optional_variables: list[str] = Field(default_factory=list)\n    tags: list[str] = Field(default_factory=list)\n    default_model: str | None = None  # Model recommendation (e.g., \"gpt-4o\")\n    output_mode: Literal[\"text\", \"json\", \"structured\"] | None = None  # Output format hint\n\n    # Safety/security (stubs for future)\n    safety_level: Literal[\"safe\", \"moderate\", \"sensitive\"] | None = None\n    pii_handling: Literal[\"none\", \"anonymize\", \"explicit_consent\"] | None = None\n    content_flags: list[str] = Field(default_factory=list)\n\n    # Provenance\n    schema_version: str = \"1.0\"\n    created_at: str | None = None\n    updated_at: str | None = None\n\nclass Prompt(BaseModel):\n    \"\"\"Domain model for a prompt.\"\"\"\n    name: str\n    version: str\n    template: str  # Jinja2 template body (without frontmatter)\n    metadata: PromptMetadata\n\nclass RenderedPrompt(BaseModel):\n    \"\"\"Rendered prompt ready for provider.\"\"\"\n    system: str | None = None\n    messages: list[Message]\n\nclass Message(BaseModel):\n    \"\"\"Single message in a conversation.\"\"\"\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n\nclass ValidationIssue(BaseModel):\n    \"\"\"Single validation issue.\"\"\"\n    level: Literal[\"error\", \"warning\", \"info\"]\n    code: str\n    message: str\n    field: str | None = None\n    line: int | None = None\n\nclass PromptValidationResult(BaseModel):\n    \"\"\"Result of prompt validation.\"\"\"\n    valid: bool\n    errors: list[ValidationIssue] = Field(default_factory=list)\n    warnings: list[ValidationIssue] = Field(default_factory=list)\n    fingerprint_data: dict[str, Any] = Field(default_factory=dict)\n\n    def succeeded(self) -&gt; bool:\n        return self.valid and len(self.errors) == 0\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#protocols-minimal-focused","title":"Protocols (Minimal, Focused)","text":"<pre><code># domain/protocols.py\nfrom typing import Protocol\nfrom .models import Prompt, PromptMetadata, PromptValidationResult\n\nclass PromptCatalogPort(Protocol):\n    \"\"\"Repository for prompt storage/retrieval.\"\"\"\n\n    def get(self, key: str) -&gt; Prompt:\n        \"\"\"Retrieve prompt by key.\"\"\"\n        ...\n\n    def list(self) -&gt; list[PromptMetadata]:\n        \"\"\"List all available prompts.\"\"\"\n        ...\n\nclass PromptRendererPort(Protocol):\n    \"\"\"Renders prompts with variable substitution.\"\"\"\n\n    def render(self, prompt: Prompt, params: RenderParams) -&gt; RenderedPrompt:\n        \"\"\"Render prompt with Jinja2 templating.\"\"\"\n        ...\n\nclass PromptValidatorPort(Protocol):\n    \"\"\"Validates prompt schema and variables.\"\"\"\n\n    def validate(self, prompt: Prompt) -&gt; PromptValidationResult:\n        \"\"\"Validate prompt metadata schema.\"\"\"\n        ...\n\n    def validate_render(\n        self,\n        prompt: Prompt,\n        params: RenderParams\n    ) -&gt; PromptValidationResult:\n        \"\"\"Validate that render params satisfy prompt requirements.\"\"\"\n        ...\n</code></pre> <p>Design Note: 3 focused protocols instead of one 5-method protocol improves testability and single responsibility.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#4-transport-layer-isolation-of-io","title":"4. Transport Layer (Isolation of I/O)","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#transport-models","title":"Transport Models","text":"<pre><code># transport/models.py\nfrom pydantic import BaseModel\nfrom pathlib import Path\n\nclass PromptFileRequest(BaseModel):\n    \"\"\"Transport-level request to load a prompt file.\"\"\"\n    path: Path\n    commit_sha: str | None = None\n\nclass PromptFileResponse(BaseModel):\n    \"\"\"Transport-level prompt file data.\"\"\"\n    content: str              # Raw file content (with frontmatter)\n    metadata_raw: dict        # Parsed YAML frontmatter\n    file_hash: str            # SHA-256 of content\n    loaded_at: str            # ISO timestamp\n\nclass GitRefreshRequest(BaseModel):\n    \"\"\"Request to refresh git repository.\"\"\"\n    repository_path: Path\n    target_ref: str | None = None\n\nclass GitRefreshResponse(BaseModel):\n    \"\"\"Git refresh operation result.\"\"\"\n    current_commit: str\n    branch: str\n    changed_files: list[str]\n    refreshed_at: str\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#git-transport-client","title":"Git Transport Client","text":"<pre><code># transport/git_client.py\nfrom pathlib import Path\nfrom .models import PromptFileResponse, GitRefreshResponse\n\nclass GitTransportClient:\n    \"\"\"Pure git transport operations (no domain knowledge).\"\"\"\n\n    def __init__(self, config: GitTransportConfig):\n        self.config = config\n\n    def get_current_commit(self) -&gt; str:\n        \"\"\"Get current commit SHA.\"\"\"\n        # git rev-parse HEAD\n        ...\n\n    def pull_latest(self) -&gt; GitRefreshResponse:\n        \"\"\"Pull latest changes from remote.\"\"\"\n        # git pull\n        ...\n\n    def read_file_at_commit(\n        self,\n        path: Path,\n        commit: str | None = None\n    ) -&gt; PromptFileResponse:\n        \"\"\"Read file content at specific commit.\"\"\"\n        # git show &lt;commit&gt;:&lt;path&gt; or read from working tree\n        ...\n\n    def list_files(self, pattern: str = \"**/*.md\") -&gt; list[Path]:\n        \"\"\"List files matching pattern.\"\"\"\n        # git ls-files or filesystem glob\n        ...\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#cache-transport","title":"Cache Transport","text":"<pre><code># transport/cache_client.py\nfrom typing import Protocol, TypeVar, Generic\nimport time\n\nT = TypeVar('T')\n\nclass CacheTransport(Protocol, Generic[T]):\n    \"\"\"Abstract cache transport.\"\"\"\n    def get(self, key: str) -&gt; T | None: ...\n    def set(self, key: str, value: T, ttl_s: int | None = None): ...\n    def invalidate(self, key: str): ...\n    def clear(): ...\n\nclass InMemoryCacheTransport(Generic[T]):\n    \"\"\"In-memory cache implementation with TTL.\"\"\"\n\n    def __init__(self, default_ttl_s: int = 300):\n        self._cache: dict[str, tuple[T, float]] = {}\n        self._default_ttl = default_ttl_s\n\n    def get(self, key: str) -&gt; T | None:\n        if key not in self._cache:\n            return None\n        value, expires_at = self._cache[key]\n        if time.time() &gt; expires_at:\n            del self._cache[key]\n            return None\n        return value\n\n    def set(self, key: str, value: T, ttl_s: int | None = None):\n        ttl = ttl_s if ttl_s is not None else self._default_ttl\n        expires_at = time.time() + ttl\n        self._cache[key] = (value, expires_at)\n\n    def invalidate(self, key: str):\n        self._cache.pop(key, None)\n\n    def clear():\n        self._cache.clear()\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#5-mappers-bi-directional-translation","title":"5. Mappers (Bi-Directional Translation)","text":"<pre><code># mappers/prompt_mapper.py\nfrom pathlib import Path\nfrom ..transport.models import PromptFileRequest, PromptFileResponse\nfrom ..domain.models import Prompt, PromptMetadata\nimport yaml\n\nclass PromptMapper:\n    \"\"\"Bi-directional mapper for prompt files \u2194 domain models.\"\"\"\n\n    def to_file_request(self, key: str, base_path: Path) -&gt; PromptFileRequest:\n        \"\"\"Map prompt key to transport file request.\"\"\"\n        # Key -&gt; file path resolution (e.g., \"summarize\" -&gt; \"summarize.md\")\n        prompt_path = base_path / f\"{key}.md\"\n        return PromptFileRequest(path=prompt_path, commit_sha=None)\n\n    def to_domain_prompt(self, file_resp: PromptFileResponse) -&gt; Prompt:\n        \"\"\"Map transport file response to domain Prompt.\"\"\"\n        # Parse frontmatter and body\n        content_without_fm = self._strip_frontmatter(file_resp.content)\n        metadata = self._parse_metadata(file_resp.metadata_raw)\n\n        return Prompt(\n            name=metadata.name,\n            version=metadata.version,\n            template=content_without_fm,\n            metadata=metadata\n        )\n\n    def _parse_metadata(self, raw: dict) -&gt; PromptMetadata:\n        \"\"\"Parse raw YAML frontmatter to domain PromptMetadata.\"\"\"\n        return PromptMetadata.model_validate(raw)\n\n    def _strip_frontmatter(self, content: str) -&gt; str:\n        \"\"\"Remove YAML frontmatter from content.\"\"\"\n        # Extract body after '---\\n...\\n---\\n'\n        ...\n</code></pre> <p>Design Note: Mappers are pure (no I/O, no side effects), making them easily testable in isolation.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#6-adapters-protocol-implementations","title":"6. Adapters (Protocol Implementations)","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#git-catalog-adapter","title":"Git Catalog Adapter","text":"<pre><code># adapters/git_catalog_adapter.py\nfrom pathlib import Path\nfrom ..domain.protocols import PromptCatalogPort\nfrom ..domain.models import Prompt, PromptMetadata\nfrom ..transport.git_client import GitTransportClient\nfrom ..transport.cache_client import CacheTransport\nfrom ..mappers.prompt_mapper import PromptMapper\nfrom ..service.loader import PromptLoader\n\nclass GitPromptCatalog:\n    \"\"\"Git-backed prompt catalog adapter (implements PromptCatalogPort).\"\"\"\n\n    def __init__(\n        self,\n        config: PromptCatalogConfig,\n        transport: GitTransportClient,\n        cache: CacheTransport[Prompt],\n        mapper: PromptMapper,\n        loader: PromptLoader\n    ):\n        self._config = config\n        self._transport = transport\n        self._cache = cache\n        self._mapper = mapper\n        self._loader = loader\n\n    def get(self, key: str) -&gt; Prompt:\n        \"\"\"Retrieve prompt by key (with caching).\"\"\"\n        # 1. Check cache\n        cache_key = self._make_cache_key(key)\n        cached = self._cache.get(cache_key)\n        if cached:\n            return cached\n\n        # 2. Load via transport\n        file_req = self._mapper.to_file_request(key, self._config.repository_path)\n        file_resp = self._transport.read_file_at_commit(\n            file_req.path,\n            file_req.commit_sha\n        )\n\n        # 3. Map to domain\n        prompt = self._mapper.to_domain_prompt(file_resp)\n\n        # 4. Validate if enabled\n        if self._config.validation_on_load:\n            validation = self._loader.validate(prompt)\n            if not validation.succeeded():\n                raise ValueError(f\"Invalid prompt: {validation.errors}\")\n\n        # 5. Cache and return\n        self._cache.set(cache_key, prompt, ttl_s=self._config.cache_ttl_s)\n        return prompt\n\n    def list(self) -&gt; list[PromptMetadata]:\n        \"\"\"List all available prompts.\"\"\"\n        # Use transport to list files, then map to metadata\n        files = self._transport.list_files(pattern=\"**/*.md\")\n        prompts = [self.get(self._path_to_key(f)) for f in files]\n        return [p.metadata for p in prompts]\n\n    def refresh(self) -&gt; None:\n        \"\"\"Refresh from git (pull latest).\"\"\"\n        if not self._config.enable_git_refresh:\n            return\n\n        refresh_resp = self._transport.pull_latest()\n        # Invalidate cache for changed files\n        for changed_file in refresh_resp.changed_files:\n            key = self._path_to_key(Path(changed_file))\n            self._cache.invalidate(self._make_cache_key(key))\n\n    def _make_cache_key(self, prompt_key: str) -&gt; str:\n        \"\"\"Create cache key from prompt key + commit.\"\"\"\n        commit = self._transport.get_current_commit()\n        return f\"{prompt_key}@{commit[:8]}\"\n\n    def _path_to_key(self, path: Path) -&gt; str:\n        \"\"\"Convert file path to prompt key.\"\"\"\n        return path.stem  # \"summarize.md\" -&gt; \"summarize\"\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#filesystem-catalog-adapter-offline-mode","title":"Filesystem Catalog Adapter (Offline Mode)","text":"<pre><code># adapters/filesystem_catalog_adapter.py\nfrom pathlib import Path\nfrom ..domain.protocols import PromptCatalogPort\nfrom ..domain.models import Prompt, PromptMetadata\n\nclass FilesystemPromptCatalog:\n    \"\"\"Filesystem-backed catalog for offline/packaged distributions.\"\"\"\n\n    def __init__(self, root_path: Path):\n        self._root = root_path\n\n    def get(self, key: str) -&gt; Prompt:\n        \"\"\"Load prompt from filesystem (no git).\"\"\"\n        # Direct file read + parse\n        ...\n\n    def list(self) -&gt; list[PromptMetadata]:\n        \"\"\"List prompts from filesystem.\"\"\"\n        # Glob for *.md files\n        ...\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#7-services-renderer-validator-loader","title":"7. Services (Renderer, Validator, Loader)","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#prompt-renderer","title":"Prompt Renderer","text":"<pre><code># service/renderer.py\nfrom jinja2 import Environment, StrictUndefined\nfrom ..domain.models import Prompt, RenderedPrompt, RenderParams, Message, Role\nfrom ..config.policy import PromptRenderPolicy\n\nclass PromptRenderer:\n    \"\"\"Renders prompts with Jinja2 and variable precedence.\"\"\"\n\n    def __init__(self, policy: PromptRenderPolicy):\n        self._policy = policy\n        self._env = Environment(\n            undefined=StrictUndefined,\n            trim_blocks=True,\n            lstrip_blocks=True\n        )\n\n    def render(self, prompt: Prompt, params: RenderParams) -&gt; RenderedPrompt:\n        \"\"\"Render prompt with variable substitution.\"\"\"\n        # Merge variables per policy precedence\n        merged_vars = self._merge_variables(prompt, params)\n\n        # Render template\n        template = self._env.from_string(prompt.template)\n        system_content = template.render(**merged_vars)\n\n        # Build messages (system + user)\n        return RenderedPrompt(\n            system=system_content,\n            messages=[Message(role=Role.user, content=params.user_input)]\n        )\n\n    def _merge_variables(self, prompt: Prompt, params: RenderParams) -&gt; dict:\n        \"\"\"Merge variables according to policy precedence.\"\"\"\n        # precedence_order: [\"caller_context\", \"frontmatter_defaults\", \"settings_defaults\"]\n        merged = {}\n        # Implement precedence logic\n        ...\n        return merged\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#prompt-validator","title":"Prompt Validator","text":"<pre><code># service/validator.py\nfrom ..domain.models import Prompt, PromptValidationResult, ValidationIssue, RenderParams\nfrom ..config.policy import ValidationPolicy\n\nclass PromptValidator:\n    \"\"\"Validates prompt schema and rendering requirements.\"\"\"\n\n    def __init__(self, policy: ValidationPolicy):\n        self._policy = policy\n\n    def validate(self, prompt: Prompt) -&gt; PromptValidationResult:\n        \"\"\"Validate prompt metadata schema.\"\"\"\n        errors = []\n        warnings = []\n\n        # Check required fields\n        if not prompt.metadata.name:\n            errors.append(ValidationIssue(\n                level=\"error\",\n                code=\"MISSING_NAME\",\n                message=\"Prompt name is required\",\n                field=\"name\"\n            ))\n\n        # Version format\n        if not self._is_valid_version(prompt.metadata.version):\n            errors.append(ValidationIssue(\n                level=\"error\",\n                code=\"INVALID_VERSION\",\n                message=\"Version must be semver format\",\n                field=\"version\"\n            ))\n\n        # Template syntax\n        try:\n            self._validate_jinja_syntax(prompt.template)\n        except Exception as e:\n            errors.append(ValidationIssue(\n                level=\"error\",\n                code=\"INVALID_TEMPLATE\",\n                message=str(e),\n                field=\"template\"\n            ))\n\n        return PromptValidationResult(\n            valid=len(errors) == 0,\n            errors=errors,\n            warnings=warnings\n        )\n\n    def validate_render(\n        self,\n        prompt: Prompt,\n        params: RenderParams\n    ) -&gt; PromptValidationResult:\n        \"\"\"Validate that params satisfy prompt requirements.\"\"\"\n        errors = []\n\n        # Check required variables\n        missing = set(prompt.metadata.required_variables) - set(params.variables.keys())\n        if missing and self._policy.fail_on_missing_required:\n            errors.append(ValidationIssue(\n                level=\"error\",\n                code=\"MISSING_REQUIRED_VARS\",\n                message=f\"Missing required variables: {missing}\",\n                field=\"variables\"\n            ))\n\n        # Check extra variables\n        if not self._policy.allow_extra_variables:\n            all_allowed = (\n                set(prompt.metadata.required_variables) |\n                set(prompt.metadata.optional_variables)\n            )\n            extra = set(params.variables.keys()) - all_allowed\n            if extra:\n                errors.append(ValidationIssue(\n                    level=\"warning\" if self._policy.mode == \"warn\" else \"error\",\n                    code=\"EXTRA_VARIABLES\",\n                    message=f\"Unexpected variables: {extra}\",\n                    field=\"variables\"\n                ))\n\n        return PromptValidationResult(\n            valid=len(errors) == 0,\n            errors=errors\n        )\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#8-integration-with-genai-service","title":"8. Integration with GenAI Service","text":"<p>BREAKING CHANGE: GenAI service's <code>prompts_adapter.py</code> must be refactored to use new <code>prompt_system</code>.</p> <pre><code># gen_ai_service/pattern_catalog/adapters/prompts_adapter.py (REFACTORED)\nfrom tnh_scholar.prompt_system.domain.protocols import (\n    PromptCatalogPort,\n    PromptRendererPort,\n    PromptValidatorPort\n)\nfrom tnh_scholar.prompt_system.domain.models import RenderParams\nfrom tnh_scholar.gen_ai_service.models.domain import (\n    Fingerprint,\n    RenderedPrompt,\n    RenderRequest\n)\nfrom tnh_scholar.gen_ai_service.infra.tracking.fingerprint import (\n    hash_prompt_bytes,\n    hash_user_string,\n    hash_vars\n)\n\nclass PromptsAdapter:\n    \"\"\"Adapter bridging prompt_system to GenAI service contract (ADR-A12, ADR-VSC02).\"\"\"\n\n    def __init__(\n        self,\n        catalog: PromptCatalogPort,\n        renderer: PromptRendererPort,\n        validator: PromptValidatorPort\n    ):\n        self._catalog = catalog\n        self._renderer = renderer\n        self._validator = validator\n\n    def list_all(self) -&gt; list[PromptMetadata]:\n        \"\"\"List all available prompts (ADR-VSC02 requirement for CLI/VS Code).\"\"\"\n        return self._catalog.list()\n\n    def introspect(self, prompt_key: str) -&gt; PromptMetadata:\n        \"\"\"Get detailed metadata for a prompt (ADR-VSC02 requirement for CLI/VS Code).\"\"\"\n        prompt = self._catalog.get(prompt_key)\n        return prompt.metadata\n\n    def render(self, request: RenderRequest) -&gt; tuple[RenderedPrompt, Fingerprint]:\n        \"\"\"Render prompt and produce fingerprint (ADR-A12 contract).\"\"\"\n        # 1. Get prompt from catalog\n        prompt = self._catalog.get(request.instruction_key)\n\n        # 2. Build render params\n        params = RenderParams(\n            variables=request.variables or {},\n            user_input=request.user_input\n        )\n\n        # 3. Validate before rendering\n        validation = self._validator.validate_render(prompt, params)\n        if not validation.succeeded():\n            raise ValueError(f\"Validation failed: {validation.errors}\")\n\n        # 4. Render\n        rendered = self._renderer.render(prompt, params)\n\n        # 5. Compute fingerprint\n        fingerprint = Fingerprint(\n            prompt_key=request.instruction_key,\n            prompt_name=prompt.name,\n            prompt_base_path=str(self._catalog._config.repository_path),  # FIXME: expose via protocol?\n            prompt_content_hash=hash_prompt_bytes(prompt.template.encode()),\n            variables_hash=hash_vars(params.variables),\n            user_string_hash=hash_user_string(params.user_input)\n        )\n\n        return rendered, fingerprint\n</code></pre> <p>Note: This forces GenAI service to inject <code>PromptCatalogPort</code>, <code>PromptRendererPort</code>, <code>PromptValidatorPort</code> instead of using legacy <code>PromptManager</code>. This is the rapid prototype operating principle\u2014break and refactor dependents.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#82-integration-with-tnh-gen-cli-adr-vsc02","title":"8.2 Integration with <code>tnh-gen</code> CLI (ADR-VSC02)","text":"<p>The <code>tnh-gen</code> CLI bridges user input to the prompt system via <code>PromptsAdapter</code>. This section shows how CLI variable precedence aligns with prompt system's rendering policy.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#cli-variable-mapping","title":"CLI Variable Mapping","text":"<pre><code># In tnh-gen CLI implementation (commands/run.py):\nfrom tnh_scholar.prompt_system.domain.models import RenderParams\nfrom tnh_scholar.gen_ai_service.models.domain import RenderRequest\n\ndef run_prompt(prompt_key: str, input_file: Path, vars_file: Path, var: list[str]):\n    \"\"\"Execute a prompt with CLI-provided variables.\"\"\"\n\n    # 1. Build variables dict with CLI precedence (lowest to highest)\n    variables = {}\n\n    # Lowest precedence: input file content (auto-injected as input_text)\n    if input_file:\n        variables[\"input_text\"] = input_file.read_text()\n\n    # Medium precedence: JSON vars file (--vars)\n    if vars_file:\n        variables.update(json.loads(vars_file.read_text()))\n\n    # Highest precedence: inline --var parameters\n    for v in var:\n        k, val = v.split('=', 1)\n        variables[k] = val\n\n    # 2. Build RenderParams (feeds into prompt_system's caller_context)\n    # This becomes the highest precedence in PromptRenderPolicy\n    params = RenderParams(\n        variables=variables,\n        strict_undefined=True\n    )\n\n    # 3. Call PromptsAdapter\n    adapter = PromptsAdapter(catalog, renderer, validator)\n    rendered, fingerprint = adapter.render(\n        RenderRequest(\n            instruction_key=prompt_key,\n            variables=variables,\n            user_input=variables.get(\"input_text\", \"\")\n        )\n    )\n\n    return rendered, fingerprint\n</code></pre> <p>Precedence Alignment:</p> CLI Layer (ADR-VSC02) Prompt System Layer (ADR-PT04) <code>--var</code> inline params <code>caller_context</code> (highest) <code>--vars</code> JSON file <code>caller_context</code> (highest) <code>--input-file</code> content <code>caller_context</code> (highest) (not applicable) <code>frontmatter_defaults</code> (medium) (not applicable) <code>settings_defaults</code> (lowest) <p>All CLI-provided variables merge into a single <code>variables</code> dict that becomes <code>caller_context</code> in the prompt system's precedence order.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#cli-list-command-integration","title":"CLI List Command Integration","text":"<pre><code># In tnh-gen CLI implementation (commands/list.py):\nfrom tnh_scholar.gen_ai_service.pattern_catalog.adapters.prompts_adapter import PromptsAdapter\n\ndef list_prompts(tag: str | None = None, search: str | None = None):\n    \"\"\"List all available prompts with optional filtering.\"\"\"\n\n    # Initialize adapter\n    adapter = PromptsAdapter(catalog, renderer, validator)\n\n    # Get all prompts via new list_all() method\n    all_prompts = adapter.list_all()\n\n    # Apply filters\n    filtered = [\n        p for p in all_prompts\n        if (not tag or tag in p.tags)\n        and (not search or search.lower() in p.name.lower()\n             or search.lower() in p.description.lower())\n    ]\n\n    # Format output for CLI/VS Code consumption\n    return {\n        \"prompts\": [\n            {\n                \"key\": p.key,\n                \"name\": p.name,\n                \"description\": p.description,\n                \"tags\": p.tags,\n                \"required_variables\": p.required_variables,\n                \"optional_variables\": p.optional_variables,\n                \"default_model\": p.default_model,\n                \"output_mode\": p.output_mode,\n                \"version\": p.version\n            }\n            for p in filtered\n        ],\n        \"count\": len(filtered)\n    }\n</code></pre> <p>Design Note: The <code>list_all()</code> and <code>introspect()</code> methods added to <code>PromptsAdapter</code> enable prompt discoverability without exposing internal prompt system implementation to the CLI layer. This maintains clean separation of concerns.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#9-testing-strategy","title":"9. Testing Strategy","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#unit-tests-mock-protocols","title":"Unit Tests (Mock Protocols)","text":"<pre><code># tests/unit/test_prompt_renderer.py\nfrom tnh_scholar.prompt_system.service.renderer import PromptRenderer\nfrom tnh_scholar.prompt_system.domain.models import Prompt, PromptMetadata, RenderParams\nfrom tnh_scholar.prompt_system.config.policy import PromptRenderPolicy\n\ndef test_render_with_variables():\n    \"\"\"Test rendering with variable substitution.\"\"\"\n    prompt = Prompt(\n        name=\"test\",\n        version=\"1.0\",\n        template=\"Hello {{name}}!\",\n        metadata=PromptMetadata(\n            name=\"test\",\n            version=\"1.0\",\n            description=\"Test prompt\",\n            task_type=\"test\",\n            required_variables=[\"name\"]\n        )\n    )\n\n    policy = PromptRenderPolicy()\n    renderer = PromptRenderer(policy)\n\n    params = RenderParams(\n        variables={\"name\": \"World\"},\n        user_input=\"Test input\"\n    )\n\n    result = renderer.render(prompt, params)\n    assert result.system == \"Hello World!\"\n    assert len(result.messages) == 1\n    assert result.messages[0].content == \"Test input\"\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#integration-tests-real-git-catalog","title":"Integration Tests (Real Git Catalog)","text":"<pre><code># tests/integration/test_git_catalog.py\nfrom pathlib import Path\nfrom tnh_scholar.prompt_system.adapters.git_catalog_adapter import GitPromptCatalog\nfrom tnh_scholar.prompt_system.config.prompt_catalog_config import (\n    PromptCatalogConfig,\n    GitTransportConfig\n)\n\ndef test_git_catalog_loads_from_disk(tmp_path):\n    \"\"\"Integration test with real git repo.\"\"\"\n    # Setup temp git repo with test prompts\n    repo_path = tmp_path / \"prompts\"\n    repo_path.mkdir()\n    (repo_path / \"test.md\").write_text(\"\"\"---\nname: test\nversion: 1.0\ndescription: Test prompt\ntask_type: test\nrequired_variables: []\n---\nTest template\n\"\"\")\n\n    # Initialize catalog\n    config = PromptCatalogConfig(\n        repository_path=repo_path,\n        enable_git_refresh=False,\n        validation_on_load=True\n    )\n\n    catalog = GitPromptCatalog.from_config(config)\n    prompt = catalog.get(\"test\")\n\n    assert prompt.name == \"test\"\n    assert prompt.template.strip() == \"Test template\"\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#contract-tests-fingerprint-invariants","title":"Contract Tests (Fingerprint Invariants)","text":"<pre><code># tests/contract/test_fingerprint_contract.py\ndef test_fingerprint_contains_all_inputs(prompts_adapter):\n    \"\"\"Verify Fingerprint captures all render inputs.\"\"\"\n    request = RenderRequest(\n        instruction_key=\"test\",\n        user_input=\"Test input\",\n        variables={\"foo\": \"bar\"}\n    )\n\n    rendered, fingerprint = prompts_adapter.render(request)\n\n    # Contract assertions per ADR-A12\n    assert fingerprint.prompt_key == \"test\"\n    assert fingerprint.prompt_content_hash is not None\n    assert len(fingerprint.prompt_content_hash) == 64  # SHA-256\n    assert fingerprint.variables_hash is not None\n    assert fingerprint.user_string_hash is not None\n</code></pre>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#10-migration-plan-rapid-prototype","title":"10. Migration Plan (Rapid Prototype)","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#phase-1-implement-new-system-week-1","title":"Phase 1: Implement New System (Week 1)","text":"<ul> <li> Create <code>prompt_system</code> package structure</li> <li> Implement domain models, protocols</li> <li> Implement transport layer (git, cache)</li> <li> Implement mappers</li> <li> Implement adapters (git, filesystem)</li> <li> Implement services (renderer, validator, loader)</li> <li> Write unit tests for all components</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#phase-2-break-genai-service-week-1-2","title":"Phase 2: Break GenAI Service (Week 1-2)","text":"<ul> <li> Refactor <code>prompts_adapter.py</code> to use new <code>prompt_system</code></li> <li> Update GenAI service DI to inject new protocols</li> <li> Remove all references to <code>ai_text_processing.prompts</code></li> <li> Fix all breaking tests</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#phase-3-break-cli-tools-week-2","title":"Phase 3: Break CLI Tools (Week 2)","text":"<ul> <li> Update <code>tnh-gen</code> CLI to use new catalog</li> <li> Update all <code>--pattern</code> flags to <code>--prompt</code></li> <li> Remove <code>TNH_PATTERN_DIR</code> env var support (use <code>TNH_PROMPT_DIR</code> only)</li> <li> Update VS Code extension integration</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#phase-4-delete-legacy-week-2","title":"Phase 4: Delete Legacy (Week 2)","text":"<ul> <li> Delete <code>ai_text_processing/prompts.py</code> entirely</li> <li> Delete legacy test fixtures</li> <li> Update all documentation</li> <li> Verify no remaining imports</li> </ul> <p>No backward compatibility shims. Break everything. Force refactors.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#consequences","title":"Consequences","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#positive","title":"Positive","text":"<ul> <li>Object-service compliant: Clean layer separation (domain, transport, adapters, mappers).</li> <li>Testable: Protocol-based design enables easy mocking and unit testing.</li> <li>Injectable: DI-friendly construction supports tooling (CLI, VS Code).</li> <li>Validated: Schema enforcement prevents invalid prompts.</li> <li>Fingerprinted: Complete provenance tracking per ADR-A12.</li> <li>Offline-ready: Filesystem adapter supports packaged distributions.</li> <li>Safety-ready: Metadata stubs for safety/security tags.</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#negative","title":"Negative","text":"<ul> <li>Breaking changes: All dependent code must refactor (GenAI service, CLI tools).</li> <li>Short-term disruption: Rapid prototype phase accepts this trade-off.</li> <li>Migration effort: 2-week sprint to migrate all dependents.</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#risks","title":"Risks","text":"<ul> <li>Incomplete migration: If any module is missed, builds break. Mitigation: comprehensive grep for legacy imports.</li> <li>Test coverage gaps: New system needs full test suite before deleting legacy. Mitigation: contract tests enforce invariants.</li> </ul>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#minimal-patch-to-promptspy","title":"Minimal Patch to <code>prompts.py</code>","text":"<p>Rejected: Retains monolith, tight coupling, and transport/domain mixing. Cannot meet ADR-OS01 requirements.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#gradual-migration-with-shims","title":"Gradual Migration with Shims","text":"<p>Rejected: Violates rapid prototype operating principle. Maintaining dual systems wastes time and creates inconsistency.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#standalone-package-now","title":"Standalone Package Now","text":"<p>Deferred: Premature to extract before internal architecture stabilizes. Can extract post-1.0 if needed.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#open-questions-resolved","title":"Open Questions (Resolved)","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#q1-when-to-deprecate-tnh_pattern_dir","title":"Q1: When to deprecate <code>TNH_PATTERN_DIR</code>?","text":"<p>RESOLVED: Remove immediately. No backward compatibility in rapid prototype phase.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#q2-include-safety-tags-in-metadata-schema","title":"Q2: Include safety tags in metadata schema?","text":"<p>RESOLVED: YES, include stubs now (<code>safety_level</code>, <code>pii_handling</code>, <code>content_flags</code>). Better to have schema upfront.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#q3-support-offline-mode","title":"Q3: Support offline mode?","text":"<p>RESOLVED: YES, via <code>FilesystemPromptCatalog</code> adapter. Enables packaged distributions without git.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#q4-how-to-handle-genai-adapter-mismatch","title":"Q4: How to handle GenAI adapter mismatch?","text":"<p>RESOLVED: New prompt system replaces GenAI's implementation. Force GenAI service refactor to use new protocols.</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#appendix-adr-os01-compliance-checklist","title":"Appendix: ADR-OS01 Compliance Checklist","text":"<ul> <li> Domain models defined (pure, no I/O)</li> <li> Transport models defined (file, git, cache)</li> <li> Protocols defined (minimal 3 protocols: Catalog, Renderer, Validator)</li> <li> Adapters implement protocols (GitPromptCatalog, FilesystemPromptCatalog)</li> <li> Mappers handle bi-directional translation (PromptMapper)</li> <li> Service orchestrators compose protocols (PromptRenderer, PromptValidator)</li> <li> Settings (env vars) defined (PromptSystemSettings)</li> <li> Config (construction-time) defined (PromptCatalogConfig, GitTransportConfig)</li> <li> Params (per-call) defined (RenderParams)</li> <li> Policy (behavior) defined with versioning (PromptRenderPolicy, ValidationPolicy)</li> <li> Precedence order documented</li> <li> Git operations in transport layer (GitTransportClient)</li> <li> File I/O in transport layer</li> <li> Cache in transport layer (InMemoryCacheTransport)</li> <li> All transport ops use typed models</li> <li> Unit test patterns defined</li> <li> Integration test patterns defined</li> <li> Contract tests for provenance defined</li> <li> Mock protocol fixtures provided</li> <li> Migration guide complete</li> <li> API examples provided</li> <li> Test patterns documented</li> <li> CLI integration patterns defined (ADR-VSC02)</li> <li> PromptsAdapter.list_all() implemented</li> <li> PromptsAdapter.introspect() implemented</li> <li> PromptMetadata includes key, default_model, output_mode fields</li> <li> Variable precedence alignment documented (CLI \u2192 prompt_system)</li> </ul> <p>Compliance Score: 31/31 \u2705</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#addendum-as-built-implementation-notes","title":"Addendum: As-Built Implementation Notes","text":""},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#2025-12-07-metadata-infrastructure-integration","title":"2025-12-07: Metadata Infrastructure Integration","text":"<p>Context: During implementation of <code>PromptMapper</code> (step 2 of the migration sequence), we identified that TNH Scholar already has a foundational metadata infrastructure (<code>tnh_scholar.metadata</code>) that provides:</p> <ul> <li><code>Frontmatter.extract()</code> / <code>Frontmatter.embed()</code> for YAML frontmatter handling</li> <li><code>Metadata</code> class (JSON-serializable, dict-like, type-safe)</li> <li><code>ProcessMetadata</code> for transformation provenance tracking</li> <li>JSON-LD support (ADR-MD01) for semantic relationships</li> </ul> <p>Decision: Rather than implementing custom frontmatter parsing in <code>PromptMapper</code>, we reused the existing metadata infrastructure.</p> <p>As-Built Implementation:</p> <pre><code># src/tnh_scholar/prompt_system/mappers/prompt_mapper.py\nfrom tnh_scholar.metadata.metadata import Frontmatter\n\nclass PromptMapper:\n    def _split_frontmatter(self, content: str) -&gt; tuple[dict[str, Any], str]:\n        \"\"\"Split YAML front matter from markdown content using shared Frontmatter helper.\"\"\"\n        cleaned = content.lstrip(\"\\ufeff\")  # Strip BOM if present\n        metadata_obj, body = Frontmatter.extract(cleaned)\n        metadata_raw = metadata_obj.to_dict() if metadata_obj else {}\n        if not metadata_raw:\n            raise ValueError(\"Prompt file missing or invalid YAML front matter.\")\n        return metadata_raw, body.lstrip()\n</code></pre> <p>Benefits Realized:</p> <ol> <li>No duplication: Avoided reimplementing YAML frontmatter parsing logic</li> <li>Consistent behavior: All .md files in TNH Scholar (prompts, corpus, derivatives) use same parsing</li> <li>Future-ready: JSON-LD support available when needed for semantic prompt relationships</li> <li>Provenance support: <code>ProcessMetadata</code> ready for multi-stage prompt transformation tracking</li> </ol> <p>Architectural Insight: This implementation revealed that metadata is foundational infrastructure in TNH Scholar, not a service-specific concern. Prompts are just one type of .md file with metadata; corpus documents, derivative data, and documentation all share this pattern. See ADR-MD02 for metadata's role in the object-service architecture.</p> <p>Related: ADR-MD01, ADR-MD02, ADR-OS01</p>"},{"location":"architecture/prompt-system/adr/adr-pt04-prompt-system-refactor/#2025-12-07-incomplete-tnh-gen-cli-integration-and-dependencies","title":"2025-12-07: Incomplete <code>tnh-gen</code> CLI Integration and Dependencies","text":"<p>Context: Section 8.2 (\"Integration with <code>tnh-gen</code> CLI\") describes the CLI variable mapping and command structure, but implementation was deferred due to broader architectural dependencies.</p> <p>Decision: The <code>tnh-gen</code> CLI implementation and <code>ai_text_processing</code> refactor are tracked in separate ADR series:</p> <ul> <li>ADR-TG01 (CLI Architecture): Core <code>tnh-gen</code> CLI design (commands, error handling, configuration)</li> <li>ADR-TG02 (Prompt Integration): CLI \u2194 prompt system integration (implements PT04 \u00a78.2)</li> <li>ADR-AT03 (AI Text Processing Refactor): Comprehensive 3-tier refactor:</li> <li>Tier 1: Object-service compliance (ADR-OS01, ADR-AT01)</li> <li>Tier 2: GenAIService integration (ADR-A13)</li> <li>Tier 3: Prompt system integration (ADR-PT04)</li> </ul> <p>Rationale:</p> <ol> <li>Scope Separation: <code>tnh-gen</code> is a standalone CLI tool consuming multiple refactored systems</li> <li>Dependency Complexity: CLI requires completed <code>ai_text_processing</code> refactor (AT03) before full implementation</li> <li>Domain Ownership: Text processing refactor belongs in <code>ai-text-processing/adr/</code> series, not <code>tnh-gen/</code></li> <li>Parallel Development: Enables prompt system refinement while dependent systems mature</li> </ol> <p>Status:</p> <ul> <li>\u2705 Section 8.2 variable mapping design is complete and authoritative for ADR-TG02</li> <li>\u2705 <code>PromptsAdapter.list_all()</code> and <code>introspect()</code> methods are implemented</li> <li>\u23f3 CLI implementation blocked pending ADR-AT03 (ai_text_processing refactor)</li> <li>\u23f3 <code>tnh-fab</code> remains active until ADR-TG01/TG02 implementation complete</li> </ul> <p>Migration Path: Once ADR-TG01/TG02/AT03 are implemented, <code>tnh-fab</code> will be deprecated and archived under <code>docs/architecture/tnh-gen/design/archive/</code>.</p> <p>Related: ADR-VSC02, ADR-AT03, ADR-TG01, ADR-TG02</p> <p>Approval Path: Architecture review \u2192 Implementation spike \u2192 Full implementation</p>"},{"location":"architecture/prompt-system/archive/","title":"Archive","text":"<p>Table of Contents:</p> <p>Core Pattern Architecture: Meta-patterns, Textual Expansion Processing - ```mermaid</p> <p>Adr - Table of contents for architecture/prompt-system/archive/adr</p> <p>This file auto-generated.</p>"},{"location":"architecture/prompt-system/archive/core-pattern-architecture/","title":"Core Pattern Architecture: Meta-patterns, Textual Expansion Processing","text":"<p>Terminology Note: This document uses historical \"Pattern\" terminology. In current TNH Scholar documentation, \"Pattern\" has been replaced with \"Prompt\" to align with industry standards. See ADR-DD03 for details on the terminology standardization.</p> <p>When reading this document:</p> <ul> <li>\"Pattern\" \u2192 \"Prompt\" (engineered text inputs for AI models)</li> <li>\"Pattern System\" \u2192 \"Prompt System\"</li> <li>\"Meta-Pattern\" \u2192 \"Meta-Prompt\" (prompts that generate or coordinate other prompts)</li> </ul> <p>This document preserves the original terminology for historical accuracy while serving as valuable architectural reference.</p> <pre><code>flowchart TD\n    subgraph Input\n        A[Raw Text/File] --&gt; |\"initialization\"| B[TextObject]\n        M[Metadata] --&gt; B\n    end\n\n    subgraph \"Stage 1: Sectioning\"\n        B --&gt; |\"Pattern 1\"| C[AI Processor]\n        C --&gt; D[Sectioned TextObject]\n        D --&gt; |\"metadata enrichment\"| E[Context-Aware Sections]\n    end\n\n    subgraph \"Stage 2: Processing\"\n        E --&gt; F{Parallel Processing}\n        F --&gt; |\"Pattern 2\"| P1[Section 1 Process]\n        F --&gt; |\"Pattern 2\"| P2[Section 2 Process]\n        F --&gt; |\"Pattern 2\"| P3[Section n Process]\n        P1 --&gt; G[Final TextObject]\n        P2 --&gt; G\n        P3 --&gt; G\n    end\n\n    classDef meta fill:#a9f,stroke:#333,stroke-width:2px;\n    class M meta;</code></pre> <p>This captures your basic flow, but let's riff on some interesting dimensions:</p> <ol> <li>Information Flow Architecture</li> </ol> <pre><code>flowchart LR\n    subgraph \"Information Layers\"\n        direction TB\n        M1[Explicit Metadata] --&gt; |merge| CM[Combined Metadata]\n        M2[Implicit Metadata] --&gt; |merge| CM\n        M3[Generated Metadata] --&gt; |merge| CM\n        CM --&gt; |enrich| TO[TextObject]\n    end\n\n    subgraph \"Processing Context\"\n        direction TB\n        TO --&gt; |section| S1[Section 1]\n        TO --&gt; |section| S2[Section 2]\n        GC[Global Context] --&gt; S1\n        GC --&gt; S2\n    end</code></pre> <ol> <li>Pattern-Process Relationship</li> </ol> <pre><code>flowchart TD\n    subgraph \"Pattern System\"\n        P1[Pattern] --&gt; |governs| T1[Transformation]\n        P1 --&gt; |generates| M1[Metadata]\n        P1 --&gt; |maintains| C1[Context]\n\n        T1 --&gt; |produces| O1[Output]\n        M1 --&gt; O1\n        C1 --&gt; O1\n    end</code></pre> <p>Some interesting conceptual dimensions to explore:</p> <ol> <li> <p>Metadata Evolution</p> </li> <li> <p>Metadata as a living entity that grows/evolves through processing</p> </li> <li>Metadata inheritance between processing stages</li> <li> <p>Context preservation through metadata</p> </li> <li> <p>Pattern Flexibility</p> </li> <li> <p>Patterns as declarative transformations</p> </li> <li>Pattern composition and chaining</li> <li> <p>Pattern-specific metadata requirements</p> </li> <li> <p>Context Management</p> </li> <li> <p>Bidirectional context flow (section \u2194 whole)</p> </li> <li>Context preservation through transformations</li> <li> <p>Dynamic context generation</p> </li> <li> <p>Processing Granularity</p> </li> <li> <p>Optimal chunk size determination</p> </li> <li>Context window optimization</li> <li>Processing boundary detection</li> </ol> <p>This framework suggests some interesting possibilities:</p> <ol> <li> <p>Recursive Processing</p> </li> <li> <p>Sections could themselves be further sectioned</p> </li> <li>Processing patterns could be hierarchical</li> <li> <p>Context could flow up and down the hierarchy</p> </li> <li> <p>Parallel Processing Potential</p> </li> <li> <p>Section processing could be parallelized</p> </li> <li>Metadata could be processed independently</li> <li> <p>Context could be shared across parallel processes</p> </li> <li> <p>Flexible Pattern Application</p> </li> <li> <p>Patterns could be conditionally applied</p> </li> <li>Pattern selection could be dynamic</li> <li>Pattern composition could be automated</li> </ol>"},{"location":"architecture/prompt-system/archive/core-pattern-architecture/#meta-pattern-concept","title":"meta-pattern concept","text":"<p>This gets into almost self-reflective territory, where patterns could help generate or evolve other patterns. Let me visualize this idea:</p> <pre><code>flowchart TD\n    subgraph \"Meta-Pattern System\"\n        MP[Meta-Pattern] --&gt; |generates| P1[Pattern 1]\n        MP --&gt; |generates| P2[Pattern 2]\n        MP --&gt; |generates| P3[Pattern 3]\n\n        subgraph \"Pattern Coordination\"\n            PC[Pattern Coordinator] --&gt; |orchestrates| P1\n            PC --&gt; |orchestrates| P2\n            PC --&gt; |orchestrates| P3\n            MP --&gt; |informs| PC\n        end\n    end\n\n    subgraph \"Pattern Evolution\"\n        P1 --&gt; |feedback| MP\n        P2 --&gt; |feedback| MP\n        P3 --&gt; |feedback| MP\n    end</code></pre> <p>And here's an interesting view of how meta-patterns could coordinate complex transformations:</p> <pre><code>flowchart LR\n    subgraph \"Meta-Pattern Orchestration\"\n        direction TB\n        MP1[Meta-Pattern] --&gt; |generates| WF[Workflow Pattern]\n        MP1 --&gt; |generates| VP[Validation Pattern]\n        MP1 --&gt; |generates| TP[Transform Pattern]\n\n        WF --&gt; |coordinates| Process\n        VP --&gt; |validates| Process\n        TP --&gt; |transforms| Process\n    end\n\n    subgraph Process\n        direction TB\n        S1[Step 1] --&gt; S2[Step 2] --&gt; S3[Step 3]\n    end</code></pre> <p>This opens up some fascinating possibilities:</p> <ol> <li> <p>Pattern Generation</p> </li> <li> <p>Meta-patterns that create specialized patterns based on content type</p> </li> <li>Dynamic pattern adaptation based on processing results</li> <li> <p>Pattern evolution through feedback loops</p> </li> <li> <p>Pattern Orchestration</p> </li> <li> <p>High-level patterns that coordinate pattern application</p> </li> <li>Context-aware pattern selection</li> <li> <p>Pattern composition and decomposition</p> </li> <li> <p>Pattern Learning</p> </li> <li> <p>Meta-patterns that learn from pattern effectiveness</p> </li> <li>Pattern optimization through usage analytics</li> <li> <p>Pattern adaptation to content characteristics</p> </li> <li> <p>Pattern Ecosystems</p> </li> <li> <p>Interrelated patterns that work together</p> </li> <li>Pattern dependencies and relationships</li> <li>Pattern conflict resolution</li> </ol> <p>This could lead to some really interesting capabilities:</p> <ol> <li> <p>Self-Improving Systems</p> </li> <li> <p>Patterns that evolve based on results</p> </li> <li>Meta-patterns that optimize pattern generation</li> <li> <p>Learning systems for pattern effectiveness</p> </li> <li> <p>Content-Adaptive Processing</p> </li> <li> <p>Dynamic pattern generation based on content</p> </li> <li>Adaptive processing strategies</li> <li> <p>Context-sensitive pattern application</p> </li> <li> <p>Pattern Intelligence</p> </li> <li> <p>Smart pattern selection</p> </li> <li>Pattern effectiveness analysis</li> <li>Pattern optimization strategies</li> </ol>"},{"location":"architecture/prompt-system/archive/core-pattern-architecture/#ecosystem-perspective","title":"Ecosystem Perspective","text":"<p>Let me explore this ecosystem perspective with a focus on evolution and parallelism. Here's a visualization of these interacting systems:</p> <pre><code>flowchart TB\n    subgraph \"Content Evolution\"\n        C1[Raw Content] --&gt; |enrichment| C2[Processed Content]\n        C2 --&gt; |refinement| C3[Enhanced Content]\n\n        M1[Basic Metadata] --&gt; |expansion| M2[Enriched Metadata]\n        M2 --&gt; |learning| M3[Evolved Metadata]\n\n        C1 -.-&gt;|generates| M1\n        C2 -.-&gt;|generates| M2\n        C3 -.-&gt;|generates| M3\n    end\n\n    subgraph \"Pattern Evolution\"\n        P1[Basic Patterns] --&gt; |adaptation| P2[Refined Patterns]\n        P2 --&gt; |optimization| P3[Specialized Patterns]\n\n        K1[Pattern Knowledge] --&gt; |growth| K2[Pattern Intelligence]\n        K2 --&gt; |synthesis| K3[Pattern Wisdom]\n\n        P1 -.-&gt;|builds| K1\n        P2 -.-&gt;|builds| K2\n        P3 -.-&gt;|builds| K3\n    end\n\n    subgraph \"Processing System Evolution\"\n        PS1[Basic Processing] --&gt; |enhancement| PS2[Smart Processing]\n        PS2 --&gt; |learning| PS3[Adaptive Processing]\n\n        PS1 -.-&gt;|influences| P1\n        PS2 -.-&gt;|influences| P2\n        PS3 -.-&gt;|influences| P3\n\n        M1 -.-&gt;|informs| PS1\n        M2 -.-&gt;|informs| PS2\n        M3 -.-&gt;|informs| PS3\n    end</code></pre> <p>Key insights about each component:</p> <p>Content + Metadata Evolution</p> <ul> <li>Content is never just content - it's always paired with metadata</li> <li>Metadata grows richer through processing</li> <li>New metadata can reveal new content relationships</li> <li>Content understanding deepens through metadata evolution</li> <li>Metadata becomes a form of institutional memory</li> </ul> <p>Pattern Evolution</p> <ul> <li>Patterns learn from their application</li> <li>Pattern effectiveness creates feedback loops</li> <li>Patterns develop specializations</li> <li>Patterns can merge or split based on usage</li> <li>Pattern relationships form naturally</li> </ul> <p>Processing System Evolution</p> <ul> <li>Processing systems adapt to content characteristics</li> <li>They learn optimal pattern applications</li> <li>They develop content-specific strategies</li> <li>They build processing intelligence</li> <li>They optimize resource usage</li> </ul> <p>The fascinating thing is how these systems interact:</p> <ol> <li> <p>Content influences Patterns</p> </li> <li> <p>New content types inspire new patterns</p> </li> <li>Content complexity drives pattern evolution</li> <li> <p>Content relationships suggest pattern relationships</p> </li> <li> <p>Patterns influence Processing</p> </li> <li> <p>Pattern effectiveness guides processing strategies</p> </li> <li>Pattern relationships inform processing flow</li> <li> <p>Pattern evolution drives processing adaptation</p> </li> <li> <p>Processing influences Content</p> </li> <li> <p>Processing reveals content relationships</p> </li> <li>Processing generates new metadata</li> <li>Processing deepens content understanding</li> </ol> <p>This ecosystem view suggests some important principles:</p> <ol> <li> <p>Organic Growth</p> </li> <li> <p>Systems evolve naturally through use</p> </li> <li>Relationships form based on effectiveness</li> <li> <p>Evolution follows actual needs</p> </li> <li> <p>Interdependence</p> </li> <li> <p>Each system supports the others</p> </li> <li>Changes ripple through the ecosystem</li> <li> <p>Balance maintains system health</p> </li> <li> <p>Emergent Intelligence</p> </li> <li> <p>System wisdom emerges from interaction</p> </li> <li>Learning happens at multiple levels</li> <li>Intelligence is distributed</li> </ol>"},{"location":"architecture/prompt-system/archive/core-pattern-architecture/#architectural-implications","title":"Architectural implications","text":"<p>Let me reflect on these ideas through a different lens, focusing on pattern roles and architectural implications.</p> <pre><code>flowchart TD\n    subgraph \"Pattern Roles\"\n        direction TB\n        P1[Extraction Patterns] --&gt; |generates| M1[Primary Metadata]\n        P2[Synthesis Patterns] --&gt; |collates| M2[Derived Metadata]\n        P3[Relational Patterns] --&gt; |discovers| M3[Contextual Metadata]\n        P4[Evolutionary Patterns] --&gt; |optimizes| M4[System Metadata]\n\n        M1 --&gt; |informs| P2\n        M2 --&gt; |informs| P3\n        M3 --&gt; |informs| P4\n        M4 -.-&gt;|influences| P1\n    end\n\n    subgraph \"Content Collections\"\n        C1[Content Unit 1] --&gt; |relates to| C2[Content Unit 2]\n        C2 --&gt; |relates to| C3[Content Unit 3]\n\n        H1[Hierarchy 1] --&gt; |contains| C1\n        H1 --&gt; |contains| C2\n        H2[Hierarchy 2] --&gt; |contains| C2\n        H2 --&gt; |contains| C3\n    end\n\n    M1 -.-&gt;|annotates| C1\n    M2 -.-&gt;|annotates| H1\n    M3 -.-&gt;|annotates| C2</code></pre> <p>Key insights about pattern roles:</p> <ol> <li> <p>Information Generation Patterns</p> </li> <li> <p>Extract implicit information</p> </li> <li>Generate new metadata layers</li> <li>Create derivative content</li> <li> <p>Identify relationships</p> </li> <li> <p>Synthesis Patterns</p> </li> <li> <p>Collate related metadata</p> </li> <li>Combine information across sources</li> <li>Generate higher-level insights</li> <li> <p>Create metadata hierarchies</p> </li> <li> <p>Relationship Patterns</p> </li> <li> <p>Discover content connections</p> </li> <li>Map information networks</li> <li>Build context webs</li> <li>Track information flow</li> </ol> <p>Architectural considerations for prototyping:</p> <ol> <li>Flexible Data Structures</li> </ol> <pre><code>class ContentNode:\n    \"\"\"Represents a unit of content with associated metadata\"\"\"\n    content: Any\n    metadata: Dict[str, MetadataLayer]\n    relationships: List[Relationship]\n\nclass MetadataLayer:\n    \"\"\"A specific layer/type of metadata\"\"\"\n    type: str\n    data: Dict[str, Any]\n    source_pattern: str\n    timestamp: datetime\n\nclass Relationship:\n    \"\"\"Represents a connection between content units\"\"\"\n    type: str\n    target: ContentNode\n    metadata: MetadataLayer\n</code></pre> <ol> <li>Pattern Interface Evolution</li> </ol> <pre><code>class Pattern(ABC):\n    \"\"\"Base pattern interface that can evolve\"\"\"\n    @abstractmethod\n    def apply(self, context: ProcessingContext) -&gt; ProcessingResult:\n        pass\n\n    def generate_metadata(self) -&gt; MetadataLayer:\n        \"\"\"Patterns can generate their own metadata\"\"\"\n        pass\n\n    def learn(self, result: ProcessingResult):\n        \"\"\"Patterns can learn from their application\"\"\"\n        pass\n</code></pre> <ol> <li>Dynamic Processing System</li> </ol> <pre><code>class ProcessingSystem:\n    \"\"\"Evolving processing system\"\"\"\n    patterns: Dict[str, Pattern]\n    context_manager: ContextManager\n    metadata_store: MetadataStore\n\n    def select_pattern(self, task: Task) -&gt; Pattern:\n        \"\"\"Dynamic pattern selection\"\"\"\n        pass\n\n    def learn_from_processing(self, result: ProcessingResult):\n        \"\"\"System-level learning\"\"\"\n        pass\n</code></pre> <p>Key architectural principles for prototyping:</p> <ol> <li> <p>Loose Coupling</p> </li> <li> <p>Content and metadata separation</p> </li> <li>Pattern independence</li> <li> <p>Flexible relationships</p> </li> <li> <p>Extensible Foundations</p> </li> <li> <p>Abstract base classes</p> </li> <li>Plugin architecture</li> <li> <p>Interface evolution</p> </li> <li> <p>Learning Support</p> </li> <li> <p>Feedback mechanisms</p> </li> <li>Pattern adaptation</li> <li> <p>System evolution</p> </li> <li> <p>Information Flow</p> </li> <li> <p>Metadata propagation</p> </li> <li>Relationship tracking</li> <li>Context preservation</li> </ol> <p>Prototyping priorities:</p> <ol> <li> <p>Core Framework</p> </li> <li> <p>Basic content/metadata structure</p> </li> <li>Simple pattern interface</li> <li> <p>Essential processing system</p> </li> <li> <p>Pattern Foundation</p> </li> <li> <p>Basic pattern types</p> </li> <li>Pattern execution</li> <li> <p>Metadata generation</p> </li> <li> <p>Relationship Handling</p> </li> <li> <p>Content connections</p> </li> <li>Metadata relationships</li> <li>Context management</li> </ol> <p>This suggests an iterative development approach:</p> <ol> <li>Start with simple content + metadata structures</li> <li>Add basic pattern execution</li> <li>Introduce relationship handling</li> <li>Develop metadata synthesis</li> <li>Add learning mechanisms</li> <li>Evolve pattern roles</li> </ol>"},{"location":"architecture/prompt-system/archive/core-pattern-architecture/#toward-a-detailed-understanding-of-sectioning","title":"Toward a detailed understanding of Sectioning","text":"<p>Let me try to untangle and visualize this specific domain of sectioning, focusing on the key relationships.</p> <pre><code>classDiagram\n    %% Core Components\n    class SectioningPattern {\n        +instructions: str\n        +template_fields: Dict\n        +apply_template() str\n    }\n\n    class SectioningProcessor {\n        +process(text_object, instructions) TextObjectResponse\n    }\n\n    class TextObjectResponse {\n        +document_summary: str\n        +document_metadata: str\n        +key_concepts: str\n        +narrative_context: str\n        +language: str\n        +sections: List[LogicalSection]\n    }\n\n    class LogicalSection {\n        +start_line: int\n        +title: str\n    }\n\n    class TextObject {\n        +content: NumberedText\n        +language: str\n        +sections: List[SectionObject]\n        +metadata: Metadata\n    }\n\n    class SectionObject {\n        +section_range: SectionRange\n        +section_metadata: Metadata\n    }\n\n    %% Relationships\n    SectioningProcessor --&gt; SectioningPattern : uses\n    SectioningProcessor --&gt; TextObjectResponse : produces\n    TextObjectResponse --&gt; LogicalSection : contains\n    TextObject --&gt; SectionObject : contains\n    TextObjectResponse --&gt; TextObject: transforms</code></pre> <p>Key insights about the relationships:</p> <ol> <li> <p>Pattern-Processor Coupling</p> </li> <li> <p>Pattern must produce instructions that processor understands</p> </li> <li>Processor must interpret pattern output correctly</li> <li>Both must align with AI model capabilities</li> <li> <p>Both must understand response format</p> </li> <li> <p>Information Flow</p> </li> </ol> <pre><code>flowchart LR\n    TO[TextObject] --&gt; |Pattern Instructions| SP[SectioningProcessor]\n    SP --&gt; |AI Processing| TR[TextObjectResponse]\n    TR --&gt; |Transformation| NTO[New TextObject]\n\n    subgraph \"Metadata Flow\"\n        M1[Original Metadata] --&gt; |Enrichment| M2[AI Generated Metadata]\n        M2 --&gt; |Synthesis| M3[Combined Metadata]\n    end</code></pre> <ol> <li>Dependencies</li> </ol> <pre><code>flowchart TD\n    subgraph \"Processing Dependencies\"\n        P[Pattern] --&gt; |must match| API[AI API Capabilities]\n        API --&gt; |determines| R[Response Format]\n        R --&gt; |influences| P\n    end\n\n    subgraph \"Data Dependencies\"\n        TO[TextObject] --&gt; |requires| MT[Metadata Types]\n        MT --&gt; |informs| R\n        R --&gt; |extends| MT\n    end</code></pre> <p>Let's break down some key considerations:</p> <ol> <li> <p>Pattern-Processor Relationship</p> </li> <li> <p>Pattern defines \"what\" to do</p> </li> <li>Processor handles \"how\" to do it</li> <li>Both must understand the same \"language\"</li> <li> <p>Both must work within API constraints</p> </li> <li> <p>Response Format Role</p> </li> <li> <p>Bridges pattern and processor</p> </li> <li>Defines metadata structure</li> <li>Captures sectioning logic</li> <li> <p>Preserves original context</p> </li> <li> <p>Metadata Evolution</p> </li> <li> <p>Original metadata informs sectioning</p> </li> <li>Sectioning generates new metadata</li> <li>Response format structures metadata</li> <li>TextObject combines metadata layers</li> </ol> <p>Some architectural implications:</p> <ol> <li>Decoupling Opportunities</li> </ol> <pre><code>class SectioningStrategy(ABC):\n    \"\"\"Abstraction of sectioning logic\"\"\"\n    @abstractmethod\n    def create_sections(\n        self, \n        content: str, \n        metadata: Metadata\n    ) -&gt; List[Section]:\n        pass\n\nclass AIBasedSectioning(SectioningStrategy):\n    \"\"\"Implementation using AI\"\"\"\n    def __init__(self, pattern: Pattern, processor: Processor):\n        self.pattern = pattern\n        self.processor = processor\n</code></pre> <ol> <li>Metadata Management</li> </ol> <pre><code>class MetadataTransformer:\n    \"\"\"Handles metadata evolution through processing\"\"\"\n    def combine_metadata(\n        self,\n        original: Metadata,\n        generated: AIGeneratedMetadata\n    ) -&gt; EnrichedMetadata:\n        pass\n</code></pre> <ol> <li>Response Handling</li> </ol> <pre><code>class ResponseTransformer:\n    \"\"\"Transforms AI response into system objects\"\"\"\n    def transform_response(\n        self,\n        response: TextObjectResponse,\n        original: TextObject\n    ) -&gt; TextObject:\n        pass\n</code></pre> <p>Some potential clarifying principles:</p> <ol> <li> <p>Separation of Concerns</p> </li> <li> <p>Pattern defines sectioning criteria</p> </li> <li>Processor handles AI interaction</li> <li>Response format defines structure</li> <li> <p>TextObject manages content/metadata</p> </li> <li> <p>Information Flow</p> </li> <li> <p>Clear transformation path</p> </li> <li>Metadata preservation</li> <li>Context maintenance</li> <li> <p>Structure evolution</p> </li> <li> <p>Flexibility Points</p> </li> <li> <p>Pattern variation</p> </li> <li>Processor implementation</li> <li>Response format extension</li> <li>Metadata enrichment</li> </ol>"},{"location":"architecture/prompt-system/archive/core-pattern-architecture/#meta-patterns-for-airesponse-generation","title":"Meta-patterns for AIResponse generation?","text":"<p>This is a fascinating direction - using meta-patterns to generate the response models. Let me explore this relationship between patterns and response formats.</p> <p>Let's start with the minimal requirements for an AIResponse:</p> <pre><code>classDiagram\n    class BaseAIResponse {\n        +metadata: Dict[str, Any]\n        +language: str\n    }\n\n    class SectioningResponse {\n        +sections: List[Section]\n        +document_context: str\n    }\n\n    BaseAIResponse &lt;|-- SectioningResponse\n\n    class TranslationResponse {\n        +translated_content: str\n        +translation_notes: str\n    }\n\n    BaseAIResponse &lt;|-- TranslationResponse</code></pre> <p>The interesting challenge is deriving these models from patterns. A sectioning pattern might look like:</p> <pre><code>class SectioningMetaPattern:\n    \"\"\"Generates both instruction pattern and response model\"\"\"\n\n    def analyze_pattern(self, pattern_text: str) -&gt; Dict[str, str]:\n        \"\"\"Extract expected response fields from pattern instructions\"\"\"\n        # Pattern parsing logic to identify:\n        # - Required fields\n        # - Field types\n        # - Field descriptions\n        pass\n\n    def generate_response_model(self, fields: Dict[str, str]) -&gt; Type[BaseModel]:\n        \"\"\"Create Pydantic model from analyzed fields\"\"\"\n        model_fields = {\n            name: (str, Field(..., description=desc))\n            for name, desc in fields.items()\n        }\n\n        return create_model(\n            'DynamicResponse',\n            **model_fields,\n            __base__=BaseAIResponse\n        )\n</code></pre> <p>The processor could be adapted to handle dynamic response types:</p> <pre><code>class FlexibleProcessor:\n    \"\"\"Processor that adapts to different response models\"\"\"\n\n    def process_with_response(\n        self,\n        text: str,\n        pattern: Pattern,\n        response_model: Type[BaseModel]\n    ) -&gt; BaseAIResponse:\n        \"\"\"Process text using pattern and specified response model\"\"\"\n        # Generate instructions from pattern\n        instructions = pattern.apply_template()\n\n        # Process with OpenAI using dynamic response type\n        return openai_process_text(\n            text,\n            instructions,\n            response_format=response_model\n        )\n\n    def validate_pattern_response_compatibility(\n        self,\n        pattern: Pattern,\n        response_model: Type[BaseModel]\n    ) -&gt; bool:\n        \"\"\"Verify pattern outputs match response model requirements\"\"\"\n        # Validation logic\n        pass\n</code></pre> <p>Some key considerations:</p> <ol> <li> <p>Pattern Analysis</p> </li> <li> <p>Patterns need clear markers for expected outputs</p> </li> <li>Pattern language needs to indicate field types</li> <li> <p>Pattern instructions must align with response fields</p> </li> <li> <p>Response Model Requirements</p> </li> <li> <p>Must extend BaseAIResponse</p> </li> <li>Must handle validation</li> <li>Must support transformation to TextObject</li> <li> <p>Must preserve essential metadata</p> </li> <li> <p>Processor Flexibility</p> </li> <li> <p>Must validate pattern-response compatibility</p> </li> <li>Must handle dynamic response types</li> <li>Must ensure consistent TextObject transformation</li> </ol>"},{"location":"architecture/prompt-system/archive/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-PT01: Pattern Access Strategy - Introduces a two-phase plan for accessing PromptTemplate metadata, moving from a singleton to injected managers.</p> <p>ADR-PT02: Adopt Pattern and PatternCatalog as Core Concepts - Keeps the Pattern terminology while clarifying how PatternCatalog functions in TNH Scholar.</p> <p>This file auto-generated.</p>"},{"location":"architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy/","title":"ADR-PT01: Pattern Access Strategy","text":"<p>Defines a phased transition from the prototype LocalPatternManager singleton to dependency-injected managers for PromptTemplates.</p> <ul> <li>Status: Accepted</li> </ul>"},{"location":"architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy/#context","title":"Context","text":"<p>The TNH Scholar system needs a pattern management strategy that:</p> <ul> <li>Supports rapid prototyping now</li> <li>Provides clean transition to production architecture</li> <li>Maintains system modularity</li> <li>Enables proper testing</li> </ul> <p>Currently using a singleton LocalPatternManager for global pattern access.</p>"},{"location":"architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy/#decision","title":"Decision","text":"<p>Implement pattern access in two phases:</p>"},{"location":"architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy/#phase-1-prototype","title":"Phase 1 (Prototype)","text":"<p>Keep LocalPatternManager in patterns.py for simple global access:</p> <pre><code>class LocalPatternManager:\n    _instance: Optional[\"LocalPatternManager\"] = None\n\n    @property\n    def pattern_manager(self) -&gt; PatternManager:\n        if self._pattern_manager is None:\n            self._pattern_manager = PatternManager(TNH_DEFAULT_PATTERN_DIR)\n        return self._pattern_manager\n</code></pre>"},{"location":"architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy/#phase-2-production","title":"Phase 2 (Production)","text":"<p>Transition to configuration-based pattern access:</p> <pre><code>@dataclass\nclass ProcessConfig:\n    pattern_manager: PatternManager\n    model_config: Optional[Dict[str, Any]] = None\n\nclass TextProcessor:\n    def __init__(self, config: ProcessConfig):\n        self.pattern_manager = config.pattern_manager\n</code></pre>"},{"location":"architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy/#rationale","title":"Rationale","text":"<ul> <li>Phase 1 prioritizes rapid development and testing</li> <li>Phase 2 enables proper dependency injection</li> <li>Staged approach balances immediate needs with good architecture</li> <li>Clear transition path maintains system stability</li> </ul>"},{"location":"architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy/#consequences","title":"Consequences","text":""},{"location":"architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy/#positive","title":"Positive","text":"<ul> <li>Simple pattern access during prototyping</li> <li>Clean path to proper dependency injection</li> <li>Better testing in Phase 2</li> <li>Maintains system modularity</li> </ul>"},{"location":"architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy/#negative","title":"Negative","text":"<ul> <li>Temporary acceptance of global state</li> <li>Will require coordinated transition</li> <li>Some refactoring needed in Phase 2</li> </ul>"},{"location":"architecture/prompt-system/archive/adr/adr-pt01-pattern-access-strategy/#migration-strategy","title":"Migration Strategy","text":"<ol> <li>Keep pattern access centralized in patterns.py</li> <li>Document singleton usage as temporary</li> <li>Implement ProcessConfig when transitioning key components</li> <li>Gradually migrate processors to configuration-based pattern access</li> </ol>"},{"location":"architecture/prompt-system/archive/adr/adr-pt02-adopt-pattern-and-patterncatalog/","title":"ADR-PT02: Adopt Pattern and PatternCatalog as Core Concepts","text":"<p>Retains the Pattern nomenclature and codifies PatternCatalog responsibilities even as industry shifts to \u201cprompt\u201d language.</p> <ul> <li>Status: Rejected  </li> <li>Date: 2025-09-22  </li> <li>Context: TNH Scholar \u2014 Pattern system design evolution</li> </ul>"},{"location":"architecture/prompt-system/archive/adr/adr-pt02-adopt-pattern-and-patterncatalog/#context","title":"Context","text":"<p>In the current TNH Scholar system, Patterns are simple, Git-versioned text templates (rendered with Jinja2 into Markdown). They allow reproducible and transparent use of structured prompts. At present, the implementation is minimal but effective.</p> <p>Industry tooling (LangChain, Hugging Face, OpenAI examples) often uses the term Prompt Catalog to refer to a curated library of reusable prompt templates. Aligning with this convention helps external contributors and new users orient quickly.  </p> <p>At the same time, Pattern is a deliberate term within TNH Scholar\u2019s ontology, reflecting both:</p> <ul> <li>Its grounding in software design language (design patterns, architectural patterns).  </li> <li>Its aspiration to be more than \u201cjust a prompt string\u201d\u2014a reusable, versioned artifact of practice.</li> </ul>"},{"location":"architecture/prompt-system/archive/adr/adr-pt02-adopt-pattern-and-patterncatalog/#decision","title":"Decision","text":"<ol> <li>Terminology </li> <li>Retain Pattern as the core artifact type in TNH Scholar.  </li> <li>Introduce PatternCatalog as the managing service (lookup, versioning, fingerprinting, resolution).  </li> <li> <p>In documentation, explicitly state: \u201cPatterns correspond to what the industry commonly calls prompt templates.\u201d</p> </li> <li> <p>Structure </p> </li> <li>Pattern = single artifact (template text + metadata).  </li> <li> <p>PatternCatalog = collection + index + retrieval service.  </p> </li> <li> <p>Fingerprinting </p> </li> <li>Add a PatternFingerprint: a hash of the rendered output + metadata.  </li> <li> <p>Use for reproducibility, caching, observability.  </p> </li> <li> <p>Versioning </p> </li> <li>Continue to store Patterns in Git for lineage and collaborative editing.  </li> <li>Treat Git commit hashes as part of the provenance chain for each Pattern.</li> </ol>"},{"location":"architecture/prompt-system/archive/adr/adr-pt02-adopt-pattern-and-patterncatalog/#consequences","title":"Consequences","text":"<ul> <li>Contributors will encounter PatternCatalog, which maps cleanly to the widely used \u201cPrompt Catalog\u201d concept, while retaining TNH Scholar\u2019s distinctive ontology.  </li> <li>Patterns gain stronger identity and reproducibility via fingerprints.  </li> <li>This design creates a bridge: newcomers can orient with familiar terms, while the TNH Scholar system continues to evolve its unique conceptual architecture.  </li> </ul>"},{"location":"architecture/prompt-system/archive/adr/adr-pt02-adopt-pattern-and-patterncatalog/#future-work","title":"Future Work","text":"<ul> <li>Expand PatternCatalog into a full service, including:</li> <li>Template validation (required variables, length bounds).  </li> <li>Deterministic rendering and caching keyed by fingerprint.  </li> <li>Structured metadata (task type, model constraints, safety requirements).  </li> <li>Usage analytics (which Patterns are used, cost/latency tracking).  </li> <li> <p>Integration with the forthcoming OpenAIService (so <code>CompletionRequest</code> can reference Patterns directly).  </p> </li> <li> <p>Support multi-modal Patterns (templates that include references to image attachments or structured JSON scaffolds).  </p> </li> <li>Provide a lightweight Pattern authoring workflow: local editing + preview + fingerprint check + commit.  </li> <li>Document conventions for external contributors who may wish to repurpose or extend the Pattern system.  </li> </ul>"},{"location":"architecture/prompt-system/archive/adr/adr-pt02-adopt-pattern-and-patterncatalog/#status","title":"Status","text":"<p>This ADR is a preliminary step to prepare the Pattern system for integration with the upcoming OpenAI Object Service design. It formalizes naming and clarifies intentions, while leaving room for incremental improvements.</p>"},{"location":"architecture/setup-tnh/","title":"Setup Tnh","text":"<p>Table of Contents:</p> <p>Design - Table of contents for architecture/setup-tnh/design</p> <p>This file auto-generated.</p>"},{"location":"architecture/setup-tnh/design/","title":"Design","text":"<p>Table of Contents:</p> <p>minimal but extensible setup tool for the prototyping phase - Core Requirements:</p> <p>This file auto-generated.</p>"},{"location":"architecture/setup-tnh/design/setup-tnh-minimal-extensible-tool/","title":"minimal but extensible setup tool for the prototyping phase","text":"<p>Core Requirements:</p> <ol> <li>Configuration directory setup (~/.config/tnh_scholar)</li> <li>Basic .env file for API keys</li> <li>Pattern directory preparation</li> <li>Simple validation checks</li> </ol>"},{"location":"architecture/setup-tnh/design/setup-tnh-minimal-extensible-tool/#high-level-design-structure-iteration-1","title":"high-level design structure (iteration 1)","text":"<pre><code>tnh-setup/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 setup.py           # Main CLI entry point\n\u251c\u2500\u2500 config.py          # Configuration handling\n\u251c\u2500\u2500 patterns.py        # Pattern management \n\u2514\u2500\u2500 validators.py      # Basic validation\n\nKey Functions:\n1. Directory Setup\n   - Create ~/.config/tnh_scholar\n   - Create patterns subdirectory\n   - Create logs subdirectory\n\n2. Environment Setup\n   - Create/update .env file\n   - Basic OpenAI key validation\n   - (Optional) Google Cloud key setup\n\n3. Pattern Setup\n   - Create patterns directory\n   - Minimal set of default patterns\n   - (Future) Pattern repository integration\n\n4. Validation\n   - Check directory permissions\n   - Verify API key format\n   - Test pattern loading\n</code></pre> <p>For prototyping, suggested implementation these core functions:</p> <ol> <li>Directory creation</li> <li>Basic .env file with OpenAI key</li> <li>Pattern directory preparation</li> <li>Simple CLI interface</li> </ol>"},{"location":"architecture/setup-tnh/design/setup-tnh-minimal-extensible-tool/#high-level-design-structure-iteration-2","title":"high-level design structure (iteration 2)","text":"<pre><code>TNH-SETUP TOOL DESIGN\n\n1. Core Functions\n\n   - Create config directory (~/.config/tnh_scholar)\n\n   - Set up OpenAI API key in .env\n\n   - Optional pattern download\n\n2. User Flow\n\n   A. Config Directory\n\n      - Create ~/.config/tnh_scholar\n\n      - Create logs subdirectory\n\n   B. Environment Setup \n\n      - Prompt for OpenAI API key\n\n      - Create/update .env file\n\n   C. Pattern Setup (Optional)\n\n      - Show confirmation message:\n\n        \"Public pattern files (markdown templates) will be downloaded from:\n\n         https://github.com/aaronksolomon/patterns \n\n         and installed to: ~/.config/tnh_scholar/patterns\"\n\n      - If confirmed, download and extract patterns\n\n3. Command Line Interface\n\n   tnh-setup [OPTIONS]\n\n   Options:\n\n   --skip-env        Skip API key setup\n\n   --skip-patterns   Skip pattern download\n\n4. Files Structure\n\n   setup/\n\n   \u251c\u2500\u2500 init.py\n\n   \u251c\u2500\u2500 setup.py      # Main CLI tool\n\n   \u2514\u2500\u2500 download.py   # Pattern download function\n\n5. Dependencies\n\n   - click\n\n   - requests\n\n   - python-dotenv\n</code></pre>"},{"location":"architecture/tnh-gen/","title":"TNH-Gen CLI Architecture","text":"<p>The <code>tnh-gen</code> CLI is TNH Scholar's unified command-line interface for GenAI-powered text processing operations. It replaces the legacy <code>tnh-fab</code> tool with a modern, object-service compliant architecture.</p>"},{"location":"architecture/tnh-gen/#purpose","title":"Purpose","text":"<p><code>tnh-gen</code> provides:</p> <ul> <li>Prompt Discovery: Browse and search available prompts with rich metadata</li> <li>Text Processing: Execute AI-powered transformations (translation, sectioning, summarization, etc.)</li> <li>Configuration Management: Hierarchical configuration with clear precedence rules</li> <li>VS Code Integration: Stable CLI contract for editor extension consumption</li> <li>Batch Operations: Process multiple files with consistent provenance tracking</li> </ul>"},{"location":"architecture/tnh-gen/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        tnh-gen CLI                          \u2502\n\u2502  (Typer-based, JSON output, stable interface)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502                        \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Prompt System \u2502          \u2502 AI Text       \u2502\n         \u2502 (ADR-PT04)    \u2502          \u2502 Processing    \u2502\n         \u2502               \u2502          \u2502 (ADR-AT03)    \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502                          \u2502\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502  GenAI Service    \u2502\n                   \u2502  (ADR-A13)        \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/tnh-gen/#adr-series","title":"ADR Series","text":""},{"location":"architecture/tnh-gen/#core-adrs","title":"Core ADRs","text":"<ul> <li>ADR-TG01: CLI Architecture - Command structure, error handling, configuration</li> <li>ADR-TG02: Prompt System Integration - CLI \u2194 prompt system integration</li> </ul>"},{"location":"architecture/tnh-gen/#related-adrs","title":"Related ADRs","text":"<ul> <li>ADR-AT03: AI Text Processing Refactor - 3-tier refactor (object-service, GenAI, prompts)</li> <li>ADR-PT04: Prompt System Refactor - New prompt system architecture</li> <li>ADR-VSC02: VS Code Extension - VS Code integration strategy</li> </ul>"},{"location":"architecture/tnh-gen/#migration-from-tnh-fab","title":"Migration from tnh-fab","text":"<p>The <code>tnh-gen</code> CLI supersedes the legacy <code>tnh-fab</code> tool:</p> Aspect tnh-fab (Legacy) tnh-gen (Current) Architecture Monolithic, mixed concerns Object-service compliant Prompt System Legacy <code>ai_text_processing.prompts</code> New <code>prompt_system</code> (ADR-PT04) GenAI Integration Direct OpenAI calls GenAIService (ADR-A13) Configuration Ad-hoc, <code>TNH_PATTERN_DIR</code> Hierarchical, <code>TNH_PROMPT_DIR</code> VS Code Support None First-class citizen"},{"location":"architecture/tnh-gen/#design-archive","title":"Design Archive","text":"<p>Legacy <code>tnh-fab</code> design documents are archived for historical reference:</p> <ul> <li>tnh-fab CLI Specification - Original CLI design (archived)</li> <li>tnh-fab Design Document - First-generation design (archived)</li> </ul> <p>These documents informed the <code>tnh-gen</code> design but are superseded by the ADR-TG series.</p>"},{"location":"architecture/tnh-gen/#status","title":"Status","text":"<ul> <li>Current Phase: ADR drafting</li> <li>Implementation: Blocked pending ADR-AT03 (ai_text_processing refactor)</li> <li>Active CLI: <code>tnh-fab</code> (legacy, to be deprecated)</li> </ul>"},{"location":"architecture/tnh-gen/#command-overview","title":"Command Overview","text":"<pre><code># List available prompts with metadata\ntnh-gen list [--tag TAG] [--search QUERY]\n\n# Execute a prompt with variables\ntnh-gen run --prompt KEY --input-file PATH [--var KEY=VALUE]\n\n# Manage configuration\ntnh-gen config show|get|set [KEY] [VALUE]\n\n# Show version information\ntnh-gen version\n</code></pre> <p>See ADR-TG01 for complete command specification.</p>"},{"location":"architecture/tnh-gen/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Stable Interface: CLI contract remains stable for VS Code and scripting consumption</li> <li>Structured Output: JSON-formatted responses for programmatic parsing</li> <li>Clear Errors: Descriptive error messages with actionable suggestions</li> <li>Provenance First: All outputs include generation metadata and fingerprints</li> <li>Configuration Precedence: CLI flags &gt; workspace &gt; user &gt; environment &gt; defaults</li> </ol>"},{"location":"architecture/tnh-gen/#references","title":"References","text":""},{"location":"architecture/tnh-gen/#documentation","title":"Documentation","text":"<ul> <li>User Guide: tnh-gen CLI (planned)</li> <li>Getting Started: Quick Start Guide</li> </ul>"},{"location":"architecture/tnh-gen/#related-architecture","title":"Related Architecture","text":"<ul> <li>Prompt System Overview</li> <li>GenAI Service Strategy</li> <li>Object-Service Architecture</li> </ul> <p>Last Updated: 2025-12-07</p>"},{"location":"architecture/tnh-gen/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-TG01: tnh-gen CLI Architecture - Core command structure, error handling, and configuration for the unified TNH Scholar CLI tool</p> <p>ADR-TG02: TNH-Gen CLI Prompt System Integration - Integration pattern for tnh-gen CLI with prompt system via PromptsAdapter</p> <p>This file auto-generated.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/","title":"ADR-TG01: tnh-gen CLI Architecture","text":"<p>Defines the core architecture, command structure, error handling, and configuration system for the <code>tnh-gen</code> CLI tool\u2014TNH Scholar's unified command-line interface for GenAI-powered text processing.</p> <ul> <li>Filename: <code>adr-tg01-cli-architecture.md</code></li> <li>Heading: <code># ADR-TG01: tnh-gen CLI Architecture</code></li> <li>Status: Proposed</li> <li>Date: 2025-12-07</li> <li>Authors: Aaron Solomon, Claude Sonnet 4.5</li> <li>Owner: aaronksolomon</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#context","title":"Context","text":""},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#background","title":"Background","text":"<p>TNH Scholar requires a unified command-line interface to:</p> <ol> <li>Replace tnh-fab: Consolidate scattered CLI tools into a single, coherent interface</li> <li>Support VS Code Integration: Provide stable CLI contract for editor extension (ADR-VSC02)</li> <li>Enable Batch Processing: Process multiple files with consistent provenance tracking</li> <li>Improve Discoverability: Browse and search prompts with rich metadata</li> <li>Object-Service Compliance: Align with TNH Scholar architectural patterns (ADR-OS01)</li> </ol>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#requirements","title":"Requirements","text":"<p>The CLI must:</p> <ul> <li>Expose prompt system capabilities (ADR-PT04) for discovery and execution</li> <li>Integrate with refactored <code>ai_text_processing</code> module (ADR-AT03)</li> <li>Provide structured JSON output for programmatic consumption</li> <li>Handle errors gracefully with clear exit codes and diagnostics</li> <li>Support hierarchical configuration with clear precedence rules</li> <li>Maintain stable interface across rapid prototype iterations</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#design-principles","title":"Design Principles","text":"<ol> <li>CLI-First Integration: VS Code and other clients communicate exclusively via CLI (no Python imports)</li> <li>Structured I/O: Consistent JSON output for machine readability</li> <li>Progressive Disclosure: Simple cases simple; complex cases possible</li> <li>Clear Errors: Actionable error messages with specific suggestions</li> <li>Stable Contract: Interface remains stable for clients even as internals evolve</li> </ol>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#decision","title":"Decision","text":""},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#1-cli-tool-name-and-entry-point","title":"1. CLI Tool Name and Entry Point","text":"<p>Tool Name: <code>tnh-gen</code></p> <p>Poetry Configuration:</p> <pre><code>[tool.poetry.scripts]\ntnh-gen = \"tnh_scholar.cli_tools.tnh_gen.tnh_gen:main\"\n</code></pre> <p>Installation:</p> <pre><code>poetry install\ntnh-gen --version  # Verify installation\n</code></pre> <p>Rationale: <code>tnh-gen</code> (TNH Generate) clearly indicates generative AI operations while maintaining TNH Scholar branding.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#2-command-structure","title":"2. Command Structure","text":""},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#21-top-level-commands","title":"2.1 Top-Level Commands","text":"<pre><code>tnh-gen list       # List available prompts with metadata\ntnh-gen run        # Execute a prompt with variable substitution\ntnh-gen config     # Manage configuration settings\ntnh-gen version    # Show version information\ntnh-gen help       # Display help information\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#22-global-flags","title":"2.2 Global Flags","text":"<pre><code>--config &lt;path&gt;    # Override config file location\n--format &lt;format&gt;  # Output format (json, yaml, text)\n--verbose, -v      # Enable verbose logging\n--quiet, -q        # Suppress non-error output\n--no-color         # Disable colored terminal output\n</code></pre> <p>Design Note: Global flags apply to all commands and follow standard Unix CLI conventions.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#3-command-tnh-gen-list","title":"3. Command: <code>tnh-gen list</code>","text":"<p>Lists all available prompts with metadata for discovery and selection.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#31-signature","title":"3.1 Signature","text":"<pre><code>tnh-gen list [OPTIONS]\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#32-options","title":"3.2 Options","text":"<pre><code>--format &lt;format&gt;     # Output format: json (default), yaml, table\n--tag &lt;tag&gt;           # Filter by tag (repeatable)\n--search &lt;query&gt;      # Search in names/descriptions (case-insensitive)\n--keys-only           # Output only prompt keys (one per line)\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#33-output-format-json","title":"3.3 Output Format (JSON)","text":"<pre><code>{\n  \"prompts\": [\n    {\n      \"key\": \"translate\",\n      \"name\": \"Vietnamese-English Translation\",\n      \"description\": \"Translate Vietnamese dharma texts to English\",\n      \"tags\": [\"translation\", \"dharma\"],\n      \"required_variables\": [\"source_lang\", \"target_lang\", \"input_text\"],\n      \"optional_variables\": [\"context\"],\n      \"default_model\": \"gpt-4o\",\n      \"output_mode\": \"text\",\n      \"version\": \"1.0\"\n    },\n    {\n      \"key\": \"summarize\",\n      \"name\": \"Summarize Teaching\",\n      \"description\": \"Generate concise summary of dharma teaching\",\n      \"tags\": [\"summarization\", \"dharma\"],\n      \"required_variables\": [\"input_text\"],\n      \"optional_variables\": [\"max_length\"],\n      \"default_model\": \"gpt-4o-mini\",\n      \"output_mode\": \"text\",\n      \"version\": \"1.0\"\n    }\n  ],\n  \"count\": 2\n}\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#34-output-format-table","title":"3.4 Output Format (Table)","text":"<pre><code>KEY         NAME                               TAGS                  MODEL\ntranslate   Vietnamese-English Translation     translation, dharma   gpt-4o\nsummarize   Summarize Teaching                 summarization         gpt-4o-mini\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#35-examples","title":"3.5 Examples","text":"<pre><code># List all prompts as JSON\ntnh-gen list --format json\n\n# List prompts with \"translation\" tag\ntnh-gen list --tag translation\n\n# Search for summarization prompts\ntnh-gen list --search summarize\n\n# Get just the keys for scripting\ntnh-gen list --keys-only\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#36-implementation-notes","title":"3.6 Implementation Notes","text":"<ul> <li>Calls <code>PromptsAdapter.list_all()</code> to retrieve all prompts (see ADR-TG02)</li> <li>Metadata schema defined in ADR-PT04 \u00a72 (PromptMetadata)</li> <li>Filtering and search performed client-side for simplicity</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#4-command-tnh-gen-run","title":"4. Command: <code>tnh-gen run</code>","text":"<p>Executes a prompt with variable substitution and AI processing.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#41-signature","title":"4.1 Signature","text":"<pre><code>tnh-gen run --prompt &lt;key&gt; [OPTIONS]\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#42-required-options","title":"4.2 Required Options","text":"<pre><code>--prompt &lt;key&gt;         # Prompt key to execute (from tnh-gen list)\n--input-file &lt;path&gt;    # Input file (content auto-injected as input_text)\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#43-variable-passing-options","title":"4.3 Variable Passing Options","text":"<p>Style 1: JSON file (preferred for complex variables)</p> <pre><code>--vars &lt;path&gt;          # JSON file with variable definitions\n</code></pre> <p>Style 2: Inline parameters (convenient for simple cases)</p> <pre><code>--var &lt;key&gt;=&lt;value&gt;    # Variable assignment (repeatable)\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#44-model-and-parameter-overrides","title":"4.4 Model and Parameter Overrides","text":"<pre><code>--model &lt;model_name&gt;       # Override prompt's default model\n--intent &lt;intent&gt;          # Routing hint (translation, summarization, etc.)\n--max-tokens &lt;int&gt;         # Max output tokens\n--temperature &lt;float&gt;      # Model temperature (0.0-2.0)\n--top-p &lt;float&gt;            # Nucleus sampling parameter\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#45-output-options","title":"4.5 Output Options","text":"<pre><code>--output-file &lt;path&gt;       # Write result to file\n--format &lt;format&gt;          # Output format: json (default), text\n--no-provenance            # Omit provenance markers from output\n--streaming                # Enable streaming output (future)\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#46-variable-precedence","title":"4.6 Variable Precedence","text":"<p>Variables are merged in this precedence order (highest to lowest):</p> <ol> <li>Inline <code>--var</code> parameters (highest precedence)</li> <li>JSON file via <code>--vars</code></li> <li>Input file content (auto-injected as <code>input_text</code>) (lowest precedence)</li> </ol> <p>Example:</p> <pre><code>tnh-gen run --prompt translate \\\n  --input-file teaching.md \\\n  --var source_lang=vi \\\n  --var target_lang=en \\\n  --var context=\"Dharma talk\" \\\n  --output-file teaching.translate.md\n</code></pre> <p>Variable Resolution:</p> <pre><code>{\n  \"input_text\": \"&lt;contents of teaching.md&gt;\",\n  \"source_lang\": \"vi\",\n  \"target_lang\": \"en\",\n  \"context\": \"Dharma talk\"\n}\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#47-success-output-json","title":"4.7 Success Output (JSON)","text":"<p>Exit code: <code>0</code></p> <pre><code>{\n  \"status\": \"succeeded\",\n  \"result\": {\n    \"text\": \"...\",\n    \"model\": \"gpt-4o\",\n    \"usage\": {\n      \"prompt_tokens\": 1234,\n      \"completion_tokens\": 567,\n      \"total_tokens\": 1801,\n      \"estimated_cost_usd\": 0.08\n    },\n    \"latency_ms\": 3456,\n    \"correlation_id\": \"01HQXYZ123ABC\"\n  },\n  \"provenance\": {\n    \"backend\": \"openai\",\n    \"model\": \"gpt-4o\",\n    \"prompt_key\": \"translate\",\n    \"prompt_fingerprint\": \"sha256:abc123...\",\n    \"prompt_version\": \"1.0\",\n    \"started_at\": \"2025-12-07T10:30:00Z\",\n    \"completed_at\": \"2025-12-07T10:30:03Z\",\n    \"schema_version\": \"1.0\"\n  }\n}\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#48-file-output-handling","title":"4.8 File Output Handling","text":"<p>When <code>--output-file</code> is specified:</p> <ol> <li>Write result text to file</li> <li>Prepend provenance markers (unless <code>--no-provenance</code>)</li> <li>Use appropriate format (markdown, JSON, etc.)</li> <li>Print success message to stderr</li> <li>Print JSON response to stdout (for client parsing)</li> </ol> <p>Provenance Marker Format:</p> <pre><code>&lt;!--\nTNH-Scholar Generated Content\nPrompt: translate (v1.0)\nModel: gpt-4o\nFingerprint: sha256:abc123...\nCorrelation ID: 01HQXYZ123ABC\nGenerated: 2025-12-07T10:30:03Z\n--&gt;\n\n[Generated content follows...]\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#49-examples","title":"4.9 Examples","text":"<pre><code># Simple translation with inline variables\ntnh-gen run --prompt translate \\\n  --input-file teaching.md \\\n  --var source_lang=vi \\\n  --var target_lang=en \\\n  --output-file teaching.en.md\n\n# Complex variables via JSON file\ntnh-gen run --prompt summarize \\\n  --input-file lecture.md \\\n  --vars config.json \\\n  --output-file lecture.summary.md\n\n# Override model\ntnh-gen run --prompt translate \\\n  --input-file teaching.md \\\n  --vars vars.json \\\n  --model gpt-4o \\\n  --output-file teaching.translate.md\n\n# JSON output without file\ntnh-gen run --prompt extract_quotes \\\n  --input-file teaching.md \\\n  --format json\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#5-error-handling-and-exit-codes","title":"5. Error Handling and Exit Codes","text":""},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#51-exit-code-taxonomy","title":"5.1 Exit Code Taxonomy","text":"Exit Code Error Type Description <code>0</code> Success Operation completed successfully <code>1</code> Policy Error Budget exceeded, size limits, validation failed <code>2</code> Transport Error API failure, timeout, network issues <code>3</code> Provider Error Model unavailable, rate limit, auth failure <code>4</code> Format Error JSON parse failure, schema validation failed <code>5</code> Input Error Invalid arguments, missing required variables <p>Rationale: Exit codes follow semantic categories to enable programmatic error handling by clients (VS Code, scripts).</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#52-error-message-format","title":"5.2 Error Message Format","text":"<p>All errors output structured JSON to stdout:</p> <pre><code>{\n  \"status\": \"failed\",\n  \"error\": \"&lt;human-readable error message&gt;\",\n  \"diagnostics\": {\n    \"error_type\": \"&lt;ErrorClass&gt;\",\n    \"error_code\": \"&lt;MACHINE_READABLE_CODE&gt;\",\n    \"&lt;context-specific fields&gt;\": \"...\",\n    \"suggestion\": \"&lt;actionable suggestion for user&gt;\"\n  },\n  \"correlation_id\": \"&lt;ulid&gt;\"\n}\n</code></pre> <p>Design Note: Structured error format enables VS Code extension to display context-appropriate error messages and suggestions.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#53-error-mapping","title":"5.3 Error Mapping","text":"<pre><code># Map GenAI Service errors to exit codes\nERROR_CODE_MAP = {\n    PolicyError: 1,\n    TransportError: 2,\n    ProviderError: 3,\n    FormatError: 4,\n    ValueError: 5,\n    KeyError: 5,\n}\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#54-example-error-messages","title":"5.4 Example Error Messages","text":"<p>Policy Error (exit 1):</p> <pre><code>{\n  \"status\": \"failed\",\n  \"error\": \"Budget exceeded: estimated cost $0.15 exceeds maximum $0.10\",\n  \"diagnostics\": {\n    \"error_type\": \"PolicyError\",\n    \"error_code\": \"BUDGET_EXCEEDED\",\n    \"estimated_cost\": 0.15,\n    \"max_cost\": 0.10,\n    \"suggestion\": \"Increase 'max_dollars' in config or reduce input size\"\n  },\n  \"correlation_id\": \"01HQXYZ789DEF\"\n}\n</code></pre> <p>Input Error (exit 5):</p> <pre><code>{\n  \"status\": \"failed\",\n  \"error\": \"Missing required variable: 'source_lang'\",\n  \"diagnostics\": {\n    \"error_type\": \"InputError\",\n    \"error_code\": \"MISSING_REQUIRED_VARIABLE\",\n    \"missing_variable\": \"source_lang\",\n    \"required_variables\": [\"source_lang\", \"target_lang\", \"input_text\"],\n    \"suggestion\": \"Provide --var source_lang=&lt;value&gt; or include in --vars JSON file\"\n  },\n  \"correlation_id\": \"01HQXYZ456GHI\"\n}\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#6-command-tnh-gen-config","title":"6. Command: <code>tnh-gen config</code>","text":"<p>Manages configuration settings with hierarchical precedence.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#61-subcommands","title":"6.1 Subcommands","text":"<pre><code>tnh-gen config show              # Display current configuration\ntnh-gen config get &lt;key&gt;         # Get specific config value\ntnh-gen config set &lt;key&gt; &lt;value&gt; # Set config value\ntnh-gen config list              # List all config keys\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#62-configuration-sources-and-precedence","title":"6.2 Configuration Sources and Precedence","text":"<p>Configuration loaded in this order (highest to lowest precedence):</p> <ol> <li>CLI flags (e.g., <code>--model gpt-4o</code>)</li> <li>Workspace config (<code>.vscode/tnh-scholar.json</code> or local project config)</li> <li>User config (<code>~/.config/tnh-scholar/config.json</code>)</li> <li>Environment variables (<code>TNH_GENAI_MODEL</code>, <code>OPENAI_API_KEY</code>, <code>TNH_PROMPT_DIR</code>)</li> <li>Defaults (defined in GenAI Service and prompt system)</li> </ol>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#63-configuration-schema","title":"6.3 Configuration Schema","text":"<pre><code>{\n  \"prompt_catalog_dir\": \"/path/to/prompts\",\n  \"default_model\": \"gpt-4o-mini\",\n  \"max_dollars\": 0.10,\n  \"max_input_chars\": 50000,\n  \"default_temperature\": 0.2,\n  \"api_key\": \"$OPENAI_API_KEY\",\n  \"cli_path\": null\n}\n</code></pre> <p>Note: <code>api_key</code> can reference environment variables using <code>$VAR_NAME</code> syntax.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#64-examples","title":"6.4 Examples","text":"<pre><code># Show all configuration\ntnh-gen config show --format json\n\n# Get specific value\ntnh-gen config get default_model\n\n# Set value (writes to user config)\ntnh-gen config set max_dollars 0.25\n\n# Set workspace-level config\ntnh-gen config set --workspace prompt_catalog_dir ./prompts\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#7-command-tnh-gen-version","title":"7. Command: <code>tnh-gen version</code>","text":"<p>Displays version information for debugging and compatibility verification.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#71-output-format","title":"7.1 Output Format","text":"<pre><code>{\n  \"tnh_scholar\": \"0.2.0\",\n  \"tnh_gen\": \"0.1.0\",\n  \"python\": \"3.12.1\",\n  \"platform\": \"darwin\",\n  \"prompt_system_version\": \"1.0.0\",\n  \"genai_service_version\": \"1.0.0\"\n}\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#72-example","title":"7.2 Example","text":"<pre><code>tnh-gen version --format json\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#8-implementation-architecture","title":"8. Implementation Architecture","text":""},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#81-module-structure","title":"8.1 Module Structure","text":"<pre><code>src/tnh_scholar/cli_tools/tnh_gen/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 tnh_gen.py               # Main entry point, CLI argument parsing\n\u251c\u2500\u2500 commands/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 list.py              # tnh-gen list implementation\n\u2502   \u251c\u2500\u2500 run.py               # tnh-gen run implementation\n\u2502   \u251c\u2500\u2500 config.py            # tnh-gen config implementation\n\u2502   \u2514\u2500\u2500 version.py           # tnh-gen version implementation\n\u251c\u2500\u2500 output/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 formatter.py         # JSON/YAML/table output formatting\n\u2502   \u2514\u2500\u2500 provenance.py        # Provenance marker generation\n\u251c\u2500\u2500 errors.py                # Error handling and exit code mapping\n\u2514\u2500\u2500 config_loader.py         # Configuration discovery and loading\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#82-cli-framework","title":"8.2 CLI Framework","text":"<p>Use Typer for argument parsing with rich type hints and automatic help generation:</p> <pre><code># tnh_gen.py\nimport typer\nfrom typing import Optional\nfrom pathlib import Path\nfrom enum import Enum\nfrom tnh_scholar.cli_tools.tnh_gen.commands import list_cmd, run_cmd, config_cmd\n\nclass OutputFormat(str, Enum):\n    \"\"\"Output format options.\"\"\"\n    json = \"json\"\n    yaml = \"yaml\"\n    text = \"text\"\n\napp = typer.Typer(\n    name=\"tnh-gen\",\n    help=\"TNH-Gen: Unified CLI for TNH Scholar GenAI operations\",\n    add_completion=False\n)\n\n# Global state for shared options\nclass CLIContext:\n    def __init__(self):\n        self.config_path: Optional[Path] = None\n        self.format: OutputFormat = OutputFormat.json\n        self.verbose: bool = False\n\nctx = CLIContext()\n\n@app.callback()\ndef main(\n    config: Optional[Path] = typer.Option(None, \"--config\", help=\"Override config file location\"),\n    format: OutputFormat = typer.Option(OutputFormat.json, \"--format\", help=\"Output format\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Enable verbose logging\"),\n    quiet: bool = typer.Option(False, \"--quiet\", \"-q\", help=\"Suppress non-error output\"),\n):\n    \"\"\"Global options for all tnh-gen commands.\"\"\"\n    ctx.config_path = config\n    ctx.format = format\n    ctx.verbose = verbose\n\n# Register subcommands\napp.add_typer(list_cmd.app, name=\"list\")\napp.add_typer(run_cmd.app, name=\"run\")\napp.add_typer(config_cmd.app, name=\"config\")\napp.command()(version_cmd.version)\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>Rationale: Typer provides:</p> <ul> <li>Better Type Hints: Automatic validation and conversion based on Python type annotations</li> <li>Rich Help: Automatic generation of beautiful help text with colors and formatting</li> <li>Fewer Decorators: Cleaner code with type-based argument inference</li> <li>Pydantic Integration: Native support for Pydantic models (aligns with TNH Scholar object-service patterns)</li> <li>Modern Python: Built on top of Click but with Python 3.6+ features</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#83-integration-points","title":"8.3 Integration Points","text":"<p>The CLI integrates with:</p> <ul> <li>Prompt System (ADR-PT04, ADR-TG02): Via <code>PromptsAdapter</code> for discovery and rendering</li> <li>AI Text Processing (ADR-AT03): Via refactored <code>TextProcessor</code> pipeline</li> <li>GenAI Service (ADR-A13): Via <code>GenAIService.generate()</code> for completions</li> <li>Configuration (ADR-OS01): Via hierarchical settings/config/params pattern</li> </ul> <p>Design Note: CLI acts as a thin orchestration layer, delegating domain logic to appropriate services.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#consequences","title":"Consequences","text":"<ul> <li>Positive:</li> <li>Stable CLI contract for VS Code and scripting clients</li> <li>Structured JSON output enables programmatic consumption and error handling</li> <li>Semantic exit codes and detailed diagnostics improve debugging</li> <li>Progressive disclosure: simple commands (<code>list</code>) and complex commands (<code>run</code>) coexist</li> <li>Hierarchical configuration with clear precedence rules</li> <li>Typer provides type-safe arguments, rich help generation, and Pydantic integration</li> <li> <p>Unified interface consolidates scattered CLI tools (<code>tnh-fab</code>, etc.)</p> </li> <li> <p>Negative:</p> </li> <li>JSON-first output may be verbose for human-only CLI usage (mitigated by <code>--format text</code>)</li> <li>Hierarchical configuration precedence requires clear documentation</li> <li>Error mapping requires synchronization between GenAI Service errors and CLI exit codes</li> <li>Typer dependency adds to project dependencies (though built on Click, already present)</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#click-rejected","title":"Click (Rejected)","text":"<p><code>tnh-fab</code> currently uses Click, which is mature and widely adopted.</p> <p>Rejected because:</p> <ul> <li>Less type-safe: Requires manual type conversion and validation</li> <li>More verbose: Requires explicit decorators for all arguments</li> <li>Less modern: Predates Python 3.6+ type hints</li> <li>No Pydantic integration: Harder to align with TNH Scholar object-service patterns</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#argparse-rejected","title":"argparse (Rejected)","text":"<p>Standard library solution, no external dependencies.</p> <p>Rejected because:</p> <ul> <li>Extremely verbose: Requires manual parser configuration</li> <li>Poor composability: Subcommands require boilerplate</li> <li>No automatic help generation: Must manually format help text</li> <li>Low-level API: Requires significant code for basic CLI</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#fire-rejected","title":"Fire (Rejected)","text":"<p>Google's Fire library auto-generates CLIs from Python objects.</p> <p>Rejected because:</p> <ul> <li>Too magical: Exposes Python internals directly to CLI (poor contract stability)</li> <li>Limited control: Hard to customize argument parsing behavior</li> <li>Poor error messages: Generic Python exceptions exposed to users</li> <li>Not designed for multi-command CLIs</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#migration-from-tnh-fab","title":"Migration from tnh-fab","text":""},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#deprecation-strategy","title":"Deprecation Strategy","text":"<ol> <li>Phase 1 (v0.2.0): Introduce <code>tnh-gen</code>, keep <code>tnh-fab</code> functional</li> <li>Phase 2 (v0.3.0): Add deprecation warnings to <code>tnh-fab</code></li> <li>Phase 3 (v0.4.0): Remove <code>tnh-fab</code> from Poetry scripts</li> </ol>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#feature-parity","title":"Feature Parity","text":"<p>Map existing <code>tnh-fab</code> commands to <code>tnh-gen</code>:</p> <code>tnh-fab</code> Command <code>tnh-gen</code> Equivalent <code>tnh-fab run &lt;pattern&gt;</code> <code>tnh-gen run --prompt &lt;pattern&gt;</code> (no equivalent) <code>tnh-gen list</code> (new) (no equivalent) <code>tnh-gen config</code> (new) <p>Migration Guide: Provide side-by-side examples in user documentation.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#open-questions","title":"Open Questions","text":"<ol> <li>Streaming Output: How should <code>--streaming</code> work? Line-by-line? Token-by-token? (Deferred to future ADR)</li> <li>Batch Operations: Should <code>tnh-gen run</code> support multiple input files? (Deferred to v0.2.0)</li> <li>Dry-Run Mode: Add <code>--dry-run</code> to preview requests without API calls? (Deferred)</li> <li>Caching: Should CLI cache <code>list</code> results locally? (Deferred)</li> </ol>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#unit-tests","title":"Unit Tests","text":"<pre><code># tests/cli_tools/tnh_gen/test_run.py\nfrom typer.testing import CliRunner\nfrom tnh_scholar.cli_tools.tnh_gen.tnh_gen import app\n\nrunner = CliRunner()\n\ndef test_run_with_inline_vars():\n    \"\"\"Test run command with inline variable parameters.\"\"\"\n    result = runner.invoke(app, [\n        'run',\n        '--prompt', 'translate',\n        '--input-file', 'test.md',\n        '--var', 'source_lang=vi',\n        '--var', 'target_lang=en'\n    ])\n\n    assert result.exit_code == 0\n    output = json.loads(result.stdout)\n    assert output['status'] == 'succeeded'\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#integration-tests","title":"Integration Tests","text":"<pre><code>@pytest.mark.integration\ndef test_run_with_real_prompt_system():\n    \"\"\"Test against live prompt system (requires TNH_PROMPT_DIR).\"\"\"\n    # Run only when RUN_INTEGRATION_TESTS=1\n    result = runner.invoke(app, ['list', '--format', 'json'])\n    assert result.exit_code == 0\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#golden-tests","title":"Golden Tests","text":"<ul> <li>Store expected JSON outputs for known prompts</li> <li>Validate output schema against golden files</li> <li>Ensure backward compatibility across versions</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#references","title":"References","text":""},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#related-adrs","title":"Related ADRs","text":"<ul> <li>ADR-TG02: Prompt System Integration - CLI \u2194 prompt system</li> <li>ADR-AT03: AI Text Processing Refactor - 3-tier refactor</li> <li>ADR-PT04: Prompt System Refactor - Prompt architecture</li> <li>ADR-VSC02: VS Code Integration - VS Code client</li> <li>ADR-OS01: Object-Service Architecture - Architecture patterns</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg01-cli-architecture/#external-references","title":"External References","text":"<ul> <li>Click Documentation - CLI framework</li> <li>12 Factor CLI Apps - CLI design principles</li> </ul> <p>Approval Path: Architecture review \u2192 Implementation \u2192 Testing \u2192 Documentation</p>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/","title":"ADR-TG02: TNH-Gen CLI Prompt System Integration","text":"<p>This ADR defines how the <code>tnh-gen</code> CLI integrates with the prompt system (ADR-PT04) through the <code>PromptsAdapter</code>, establishing variable precedence rules and command implementation patterns.</p> <ul> <li>Status: Draft</li> <li>Date: 2025-12-07</li> <li>Owner: Aaron Solomon</li> <li>Author: Aaron Solomon, Claude Sonnet 4.5</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#context","title":"Context","text":"<p>The <code>tnh-gen</code> CLI must bridge user input (files, variables, flags) to the prompt system while maintaining clean separation of concerns. Key requirements include:</p> <ol> <li>Variable Precedence: CLI flags must override file-based variables, which override defaults</li> <li>Prompt Discovery: Users need to list and search available prompts without exposing internal catalog structure</li> <li>Transport Isolation: CLI layer should not depend on prompt storage implementation (git, filesystem, etc.)</li> <li>VS Code Integration: Command outputs must be JSON-formatted for programmatic consumption</li> <li>Consistency: Variable handling must align with prompt system's rendering policy (ADR-PT04 \u00a75)</li> </ol> <p>The <code>PromptsAdapter</code> (defined in ADR-PT04 \u00a78.1) provides the contract boundary between CLI and prompt system, offering <code>list_all()</code>, <code>introspect()</code>, and <code>render()</code> methods.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#decision","title":"Decision","text":""},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#1-cli-variable-precedence-model","title":"1. CLI Variable Precedence Model","text":"<p>The CLI collects variables from three sources with clear precedence:</p> <pre><code># In tnh-gen CLI implementation (commands/run.py):\nfrom tnh_scholar.prompt_system.domain.models import RenderParams\nfrom tnh_scholar.gen_ai_service.models.domain import RenderRequest\n\ndef run_prompt(prompt_key: str, input_file: Path, vars_file: Path, var: list[str]):\n    \"\"\"Execute a prompt with CLI-provided variables.\"\"\"\n\n    # 1. Build variables dict with CLI precedence (lowest to highest)\n    variables = {}\n\n    # Lowest precedence: input file content (auto-injected as input_text)\n    if input_file:\n        variables[\"input_text\"] = input_file.read_text()\n\n    # Medium precedence: JSON vars file (--vars)\n    if vars_file:\n        variables.update(json.loads(vars_file.read_text()))\n\n    # Highest precedence: inline --var parameters\n    for v in var:\n        k, val = v.split('=', 1)\n        variables[k] = val\n\n    # 2. Build RenderParams (feeds into prompt_system's caller_context)\n    # This becomes the highest precedence in PromptRenderPolicy\n    params = RenderParams(\n        variables=variables,\n        strict_undefined=True\n    )\n\n    # 3. Call PromptsAdapter\n    adapter = PromptsAdapter(catalog, renderer, validator)\n    rendered, fingerprint = adapter.render(\n        RenderRequest(\n            instruction_key=prompt_key,\n            variables=variables,\n            user_input=variables.get(\"input_text\", \"\")\n        )\n    )\n\n    return rendered, fingerprint\n</code></pre> <p>Precedence Alignment:</p> CLI Layer (ADR-TG02) Prompt System Layer (ADR-PT04) <code>--var</code> inline params <code>caller_context</code> (highest) <code>--vars</code> JSON file <code>caller_context</code> (highest) <code>--input-file</code> content <code>caller_context</code> (highest) (not applicable) <code>frontmatter_defaults</code> (medium) (not applicable) <code>settings_defaults</code> (lowest) <p>All CLI-provided variables merge into a single <code>variables</code> dict that becomes <code>caller_context</code> in the prompt system's precedence order.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#2-list-command-implementation","title":"2. List Command Implementation","text":"<p>The <code>list</code> command uses <code>PromptsAdapter.list_all()</code> for prompt discovery:</p> <pre><code># In tnh-gen CLI implementation (commands/list.py):\nfrom tnh_scholar.gen_ai_service.pattern_catalog.adapters.prompts_adapter import PromptsAdapter\n\ndef list_prompts(tag: str | None = None, search: str | None = None):\n    \"\"\"List all available prompts with optional filtering.\"\"\"\n\n    # Initialize adapter\n    adapter = PromptsAdapter(catalog, renderer, validator)\n\n    # Get all prompts via new list_all() method\n    all_prompts = adapter.list_all()\n\n    # Apply filters\n    filtered = [\n        p for p in all_prompts\n        if (not tag or tag in p.tags)\n        and (not search or search.lower() in p.name.lower()\n             or search.lower() in p.description.lower())\n    ]\n\n    # Format output for CLI/VS Code consumption\n    return {\n        \"prompts\": [\n            {\n                \"key\": p.key,\n                \"name\": p.name,\n                \"description\": p.description,\n                \"tags\": p.tags,\n                \"required_variables\": p.required_variables,\n                \"optional_variables\": p.optional_variables,\n                \"default_model\": p.default_model,\n                \"output_mode\": p.output_mode,\n                \"version\": p.version\n            }\n            for p in filtered\n        ],\n        \"count\": len(filtered)\n    }\n</code></pre> <p>Design Note: The <code>list_all()</code> and <code>introspect()</code> methods added to <code>PromptsAdapter</code> enable prompt discoverability without exposing internal prompt system implementation to the CLI layer. This maintains clean separation of concerns.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#3-promptsadapter-dependency-injection","title":"3. PromptsAdapter Dependency Injection","text":"<p>The CLI uses dependency injection to configure the <code>PromptsAdapter</code>:</p> <pre><code># In tnh-gen CLI initialization (cli.py):\nfrom tnh_scholar.prompt_system.adapters.git_catalog_adapter import GitPromptCatalog\nfrom tnh_scholar.prompt_system.service.renderer import PromptRenderer\nfrom tnh_scholar.prompt_system.service.validator import PromptValidator\nfrom tnh_scholar.gen_ai_service.pattern_catalog.adapters.prompts_adapter import PromptsAdapter\nfrom tnh_scholar.gen_ai_service.services.genai_service import GenAIService\n\ndef initialize_app() -&gt; tuple[PromptsAdapter, GenAIService]:\n    \"\"\"Initialize application dependencies.\"\"\"\n\n    # 1. Configure prompt catalog\n    catalog_config = PromptCatalogConfig.from_env()\n    catalog = GitPromptCatalog.from_config(catalog_config)\n\n    # 2. Configure renderer and validator\n    render_policy = PromptRenderPolicy.from_config(catalog_config.render_policy)\n    renderer = PromptRenderer(render_policy)\n    validator = PromptValidator()\n\n    # 3. Build PromptsAdapter\n    prompts_adapter = PromptsAdapter(\n        catalog=catalog,\n        renderer=renderer,\n        validator=validator\n    )\n\n    # 4. Configure GenAIService\n    genai_service = GenAIService.from_config(GenAIConfig.from_env())\n\n    return prompts_adapter, genai_service\n</code></pre>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#4-error-handling-alignment","title":"4. Error Handling Alignment","text":"<p>CLI error codes (ADR-TG01 \u00a75) map to prompt system exceptions:</p> Prompt System Exception CLI Exit Code Error Type <code>PromptNotFoundError</code> <code>5</code> Input Error <code>VariableValidationError</code> <code>5</code> Input Error <code>RenderError</code> (template syntax) <code>4</code> Format Error <code>ValidationError</code> (schema) <code>1</code> Policy Error <code>GitTransportError</code> <code>2</code> Transport Error"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#5-configuration-integration","title":"5. Configuration Integration","text":"<p>The CLI respects prompt system configuration via hierarchical precedence:</p> <pre><code># ~/.config/tnh-scholar/tnh-gen.yaml\nprompt_system:\n  repository_path: /path/to/prompts\n  enable_git_refresh: true\n  validation_on_load: true\n  cache_ttl_s: 3600\n  render_policy:\n    strict_undefined: true\n    max_output_size_kb: 100\n</code></pre> <p>CLI flags override config file values (see ADR-TG01 \u00a74 for precedence).</p>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#consequences","title":"Consequences","text":""},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#positive","title":"Positive","text":"<ul> <li>Clean Separation: CLI layer depends only on <code>PromptsAdapter</code> contract, not prompt system internals</li> <li>Consistent Precedence: Variable handling aligns across CLI and prompt system layers</li> <li>Discoverability: <code>list_all()</code> and <code>introspect()</code> enable rich prompt exploration without leaking implementation</li> <li>Testability: Adapter contract allows mocking entire prompt system in CLI tests</li> <li>VS Code Integration: JSON output format enables seamless editor consumption</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#negative","title":"Negative","text":"<ul> <li>Adapter Maintenance: Changes to prompt system require coordinated updates to <code>PromptsAdapter</code></li> <li>Variable Complexity: Three-level CLI precedence (file \u2192 vars \u2192 flags) may confuse users unfamiliar with override patterns</li> <li>Validation Boundaries: Prompt variable validation happens after CLI parsing, delaying error feedback</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#risks","title":"Risks","text":"<ul> <li>Breaking Changes: Prompt system refactors (e.g., new metadata fields) may break CLI expectations</li> <li>Performance: Listing all prompts via <code>list_all()</code> may be slow for large catalogs (mitigated by caching in ADR-PT04 \u00a76)</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#alternative-1-direct-prompt-system-access","title":"Alternative 1: Direct Prompt System Access","text":"<p>Approach: CLI directly imports and uses <code>PromptCatalog</code>, <code>PromptRenderer</code>, etc.</p> <p>Rejected: Violates object-service architecture (ADR-OS01). CLI should not depend on domain internals.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#alternative-2-separate-cli-variable-layer","title":"Alternative 2: Separate CLI Variable Layer","text":"<p>Approach: Create <code>CLIVariableResolver</code> to handle precedence, separate from <code>RenderParams</code>.</p> <p>Rejected: Adds unnecessary abstraction. Merging variables into <code>caller_context</code> is simpler and aligns with existing prompt system precedence.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#alternative-3-graphql-api-between-cli-and-prompt-system","title":"Alternative 3: GraphQL API Between CLI and Prompt System","text":"<p>Approach: Expose prompt system via GraphQL API that CLI queries.</p> <p>Rejected: Overengineering for single-process CLI. Useful for future web UI but premature for MVP.</p>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#open-questions","title":"Open Questions","text":"<ol> <li>Lazy Loading: Should <code>list_all()</code> return full metadata or just keys/names for performance? (See ADR-PT04 \u00a78.1)</li> <li>Variable Validation Feedback: Should CLI pre-validate variables against prompt schema before calling <code>render()</code>?</li> <li>Cache Control: Should CLI expose <code>--no-cache</code> flag to bypass prompt caching?</li> </ol>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#references","title":"References","text":""},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#related-adrs","title":"Related ADRs","text":"<ul> <li>ADR-TG01: CLI Architecture - Command structure, error codes, configuration</li> <li>ADR-PT04: Prompt System Refactor - Prompt system architecture, <code>PromptsAdapter</code> contract</li> <li>ADR-AT03: AI Text Processing Refactor - <code>ai_text_processing</code> module refactor</li> <li>ADR-VSC02: VS Code Extension - VS Code integration strategy</li> </ul>"},{"location":"architecture/tnh-gen/adr/adr-tg02-prompt-integration/#external-resources","title":"External Resources","text":"<ul> <li>Typer Documentation</li> <li>Jinja2 Template Syntax</li> <li>Pydantic Validation</li> </ul> <p>This ADR implements prompt system integration patterns from ADR-PT04 \u00a78.2.</p>"},{"location":"architecture/tnh-gen/design/archive/","title":"Archive","text":"<p>Table of Contents:</p> <p>TNH-FAB Command Line Tool Specification - Expanded specification for the <code>tnh-fab</code> CLI with refined command behaviors, options, and workflow integration.</p> <p>TNH FAB Design Document - First-generation design of the <code>tnh-fab</code> CLI covering core commands, usage patterns, and processing goals.</p> <p>This file auto-generated.</p>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/","title":"TNH-FAB Command Line Tool Specification","text":"<p>Expanded specification for the <code>tnh-fab</code> CLI with refined command behaviors, options, and workflow integration.</p>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#overview","title":"Overview","text":"<p><code>tnh-fab</code> is a command-line text processing tool providing standalone but pipeable operations for Buddhist text processing. It is part of the <code>tnh-scholar</code> suite of tools, focusing on simplicity, flexibility, and consistent behavior.</p>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#core-functionality","title":"Core Functionality","text":"<ul> <li>Text punctuation and formatting</li> <li>Section creation and management</li> <li>Translation (line-by-line and block)</li> <li>Pattern-based text processing</li> <li>XML/structured output generation</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#command-structure","title":"Command Structure","text":"<pre><code>tnh-fab &lt;command&gt; [options] [input_file]\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#global-options","title":"Global Options","text":"<pre><code>Input/Output:\n  [input_file]                Input file (optional, uses STDIN if not provided)\n  -o, --output FILE          Output file (default: STDOUT)\n  -d, --output-dir DIR       Output directory (default: current)\n  -f, --format FORMAT        Output format: txt/json/yaml/xml (default varies by command)\n\nConfiguration:\n  -l, --language LANG        Source language code (auto-detect if not specified)\n  -t, --template FILE        Template values file (YAML format)\n  -k, --key-values PAIRS     Space-separated key:value pairs (e.g., speaker:\"Name\")\n  -p, --pattern NAME         Pattern name (command-specific default if not specified)\n  -c, --review-count NUM       Number of review passes (default: 3)\n\nLogging:\n  -v, --verbose             Enable detailed logging\n  --debug                   Enable debug output\n  --quiet                   Suppress all non-error output\n\nOther:\n  -h, --help               Show command-specific help\n  --version                Show version information\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#commands","title":"Commands","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#punctuate","title":"punctuate","text":"<p>Add punctuation and structure to text.</p> <pre><code>Additional Options:\n  -y, --style STYLE            Punctuation style (default: configuration file)\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#section","title":"section","text":"<p>Create and manage text sections.</p> <pre><code>Additional Options:\n  -n, --num-sections NUM   Target number of sections (default: auto)\n  --target-section-size NUM   Target section size in tokens (default: configuration file)\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#translate","title":"translate","text":"<p>Perform line-by-line or block translation.</p> <pre><code>Additional Options:\n  -r, --target LANG        Target language code (default: en)\n  -y, --style STYLE            Translation style (default: configuration file)\n  --context-lines NUM      Number of context lines (default: 3)\n  --segment-size NUM       Lines per translation segment (default: auto)\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#process","title":"process","text":"<p>Execute pattern-based text processing. Can work on sections of data or on the whole input stream.</p> <pre><code>Additional Options:\n  -s, --sections FILE      JSON file containing section data\n  -g, --paragraph         Use line-separated paragraphs as sections\n  --xml                   Wrap output in XML style document tags\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#inputoutput-handling","title":"Input/Output Handling","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#input-sources","title":"Input Sources","text":"<ul> <li>File specified as argument</li> <li>STDIN (piped input)</li> <li>Section data (JSON format)</li> <li>Template files (YAML format)</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#input-processing","title":"Input Processing","text":"<ol> <li>Text content priority:</li> <li>Named input file</li> <li>STDIN if no file specified</li> <li> <p>Error if neither available</p> </li> <li> <p>Section data priority:</p> </li> <li>JSON file specified with -s</li> <li>STDIN when paired with input file</li> <li> <p>Auto-generated sections if no pattern specified</p> </li> <li> <p>Template values priority:</p> </li> <li>Command line key-values (-k)</li> <li>Template file values (-t)</li> <li>Default values from pattern</li> <li>Environment variables (TNH_FAB_*)</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#output-handling","title":"Output Handling","text":"<ol> <li>Output destination priority:</li> <li>File specified by -o</li> <li> <p>STDOUT if no file specified</p> </li> <li> <p>Format determination:</p> </li> <li>Format specified by -f</li> <li>Default format by command:<ul> <li>punctuate: txt</li> <li>section: json</li> <li>translate: txt</li> <li>process: txt</li> </ul> </li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#pattern-management","title":"Pattern Management","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#pattern-resolution","title":"Pattern Resolution","text":"<ol> <li>Pattern name sources (in order):</li> <li>Command line (-p)</li> <li> <p>Command defaults:</p> <ul> <li>punctuate: default_punctuate</li> <li>section: default_section</li> <li>translate: default_translate</li> <li>process: NO DEFAULT (must be specified)</li> </ul> </li> <li> <p>Pattern search paths:</p> </li> <li>Path specified in configuration</li> <li>~/.config/tnh_scholar/patterns/</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#template-value-processing","title":"Template Value Processing","text":"<ol> <li>Key-Value Format:    <pre><code>key:value key2:\"value with spaces\"\n</code></pre></li> <li>Keys must be valid identifiers</li> <li>Values with spaces must be quoted</li> <li> <p>Invalid formats raise error</p> </li> <li> <p>Template File Format (YAML):    <pre><code>key1: value1\nkey2: value2\n</code></pre></p> </li> <li> <p>Environment Variables:</p> </li> <li>Format: TNH_FAB_{KEY}</li> <li>Lowest priority in template resolution</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#pipeline-behavior","title":"Pipeline Behavior","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#data-flow","title":"Data Flow","text":"<ul> <li>All commands accept STDIN</li> <li>All commands can output to STDOUT</li> <li>Section data can flow through pipeline</li> <li>Binary data not supported</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#pipeline-examples","title":"Pipeline Examples","text":"<pre><code># Punctuate and section\ncat input.txt | tnh-fab punctuate | tnh-fab section &gt; sections.json\n\n# Section and process\ntnh-fab section input.txt | tnh-fab process -p format_xml &gt; output.xml\n\n# Complete pipeline\ncat input.txt | \\\n  tnh-fab punctuate -l vi | \\\n  tnh-fab section -n 5 | \\\n  tnh-fab process -p format_xml -k speaker:\"Thay\" &gt; output.xml\n</code></pre> <ol> <li>Single File Input <pre><code>tnh-fab process -p format_xml input.txt\n</code></pre></li> <li> <p>Processes input.txt directly. No sectioning is performed.</p> </li> <li> <p>STDIN Only <pre><code>cat input.txt | tnh-fab process -p format_xml\ncat input.txt | tnh-fab process -g -p format_xml  # process by paragraphs\n</code></pre></p> </li> <li> <p>Processes text from STDIN</p> </li> <li> <p>File + Sections File <pre><code>tnh-fab process -p format_xml -s sections.json input.txt\n</code></pre></p> </li> <li> <p>Processes input.txt using sections from sections.json</p> </li> <li> <p>STDIN + Sections File <pre><code>cat input.txt | tnh-fab process -p format_xml -s sections.json\n</code></pre></p> </li> <li> <p>Processes STDIN text using sections from sections.json</p> </li> <li> <p>Section Stream + Input File <pre><code>tnh-fab section input.txt | tnh-fab process -p format_xml input.txt\n</code></pre></p> </li> <li>Processes input.txt using sections from STDIN</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#input-validation","title":"Input Validation","text":"<ul> <li>When sections are provided (via -s or STDIN):</li> <li>Validates JSON format matches TextObject schema</li> <li>Checks source_file field in TextObject if present</li> <li>Warns if source_file doesn't match input file name</li> <li>Validates section line ranges against input text</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#configuration","title":"Configuration","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#configuration-files","title":"Configuration Files","text":"<ol> <li>User: ~/.config/tnh_scholar/tnh-fab/config.yaml</li> <li>Project: ./.tnh-fab.yaml</li> <li>Priority: Project &gt; User</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#configuration-format","title":"Configuration Format","text":"<pre><code>defaults:\n  language: auto\n  output_format: txt\n\npunctuate:\n  pattern: default_punctuate\n  style: APA\n  review_count: 3\n\nsection:\n  pattern: default_section\n  review_count: 3\n\ntranslate:\n  pattern: default_translate\n  target_language: en\n  style: \"American Dharma Teaching\"\n  context_lines: 3\n  review_count: 3\n\nprocess:\n  wrap_document: true\n\npatterns:\n  path: ~/.config/tnh_scholar/patterns\n\nlogging:\n  level: INFO\n  file: ~/.tnh-fab.log\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#error-handling","title":"Error Handling","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#error-categories","title":"Error Categories","text":"<ol> <li>Input Errors</li> <li>Missing required input</li> <li>Invalid file formats</li> <li>Encoding issues</li> <li> <p>Section/input mismatch</p> </li> <li> <p>Pattern Errors</p> </li> <li>Missing required pattern</li> <li>Pattern not found</li> <li> <p>Invalid pattern format</p> </li> <li> <p>Template Errors</p> </li> <li>Invalid template format</li> <li>Missing required values</li> <li> <p>Invalid key-value syntax</p> </li> <li> <p>Processing Errors</p> </li> <li>AI service errors</li> <li>Timeout errors</li> <li>Validation failures</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#error-reporting","title":"Error Reporting","text":"<ul> <li>Standard error format</li> <li>Error codes for scripting</li> <li>Detailed logging with -v</li> <li>Stack traces with --debug</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#exit-codes","title":"Exit Codes","text":"<pre><code>0  Success\n1  General error\n2  Input error\n3  Pattern error\n4  Template error\n5  Processing error\n64-73  Command-specific errors\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#development-notes","title":"Development Notes","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#key-decision-points","title":"Key Decision Points","text":"<ol> <li>Pattern Management:</li> <li>Consider pattern versioning</li> <li>Pattern validation requirements</li> <li> <p>Pattern update mechanism</p> </li> <li> <p>Pipeline Handling:</p> </li> <li>Memory management for large files</li> <li>Progress indication in pipelines</li> <li> <p>Error propagation in pipelines</p> </li> <li> <p>Configuration:</p> </li> <li>Environment variable handling</li> <li>Configuration validation</li> <li> <p>Configuration migration</p> </li> <li> <p>Testing Requirements:</p> </li> <li>Unit test coverage requirements</li> <li>Integration test scenarios</li> <li>Performance benchmarks</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-cli-spec/#future-considerations","title":"Future Considerations","text":"<ol> <li>Additional Commands:</li> <li>Format validation</li> <li>Pattern management</li> <li> <p>Batch processing</p> </li> <li> <p>Extensions:</p> </li> <li>Plugin system</li> <li>Custom pattern repositories</li> <li> <p>API integration</p> </li> <li> <p>Integration:</p> </li> <li>CI/CD requirements</li> <li>Packaging requirements</li> <li>Documentation generation</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/","title":"TNH FAB Design Document","text":"<p>First-generation design of the <code>tnh-fab</code> CLI covering core commands, usage patterns, and processing goals.</p>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#overview","title":"Overview","text":"<p><code>tnh-fab</code> is a command-line text processing tool that provides standalone but pipeable text processing operations, with a focus on simplicity and flexibility. It is part of the <code>tnh-scholar</code> suite of tools.</p>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#core-functionality","title":"Core Functionality","text":"<ul> <li>Text punctuation</li> <li>Section creation and management </li> <li>Translation (line-by-line and block)</li> <li>General text processing with patterns</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#command-structure","title":"Command Structure","text":"<pre><code>tnh-fab &lt;command&gt; [options] [input_file]\n</code></pre> <p>Commands: - <code>punctuate</code>: Add punctuation and structure to text - <code>section</code>: Create text sections - <code>translate</code>: Perform line-by-line translation - <code>process</code>: Execute pattern-based text processing (typically outputting XML format)</p>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#global-options","title":"Global Options","text":"<pre><code>-d, --output-dir DIR     Output directory (default: current)\n-v, --verbose            Detailed logging\n-l, --language LANG     Source language code of input text (auto-detect if not specified)\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#command-specific-options","title":"Command-Specific Options","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#punctuate","title":"Punctuate","text":"<pre><code>-p, --pattern NAME       Pattern name for punctuation (uses default if not specified)\n-y, --style STYLE       Punctuation style (default: APA)\n-o, --output FILE       Output file (default: stdout or FILE_punct.txt)\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#section","title":"Section","text":"<pre><code>-p, --pattern NAME      Pattern name for sectioning\n-n, --num NUM          Target number of sections\n-o, --output FILE      Output JSON file (default: stdout or FILE_sections.json)\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#translate","title":"Translate","text":"<pre><code>-p, --pattern NAME      Pattern for translation (uses default if not specified)\n-t, --template FILE     Template values file\n-o, --output FILE      Output file (default: stdout)\n-r, --target LANG       The target output language code (default is 'en,' English)\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#process","title":"Process","text":"<pre><code>-p, --pattern NAME      Pattern for processing (REQUIRED)\n-g, --paragraph         Use line separated paragraphs as sections.\n-s, --sections FILE     JSON file containing section data\n-t, --template FILE     Template values file\n-f, --format FORMAT     Output format: txt/json/yaml (default: txt)\n-o, --output FILE      Output file (default: stdout)\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#usage-examples","title":"Usage Examples","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#basic-usage","title":"Basic Usage","text":"<pre><code># Punctuate text\ntnh-fab punctuate input.txt\ntnh-fab punctuate -l vi input.txt &gt; punctuated.txt\n\n# Create sections\ntnh-fab section input.txt -n 5 -o sections.json\ncat input.txt | tnh-fab section &gt; sections.json\n\n# Translate text\ntnh-fab translate input.txt -p vi_en\ncat input.txt | tnh-fab translate -p vi_en &gt; translated.txt\n\n# Process with pattern\ntnh-fab process -p format_xml -s sections.json input.txt\ncat sections.json | tnh-fab process -p format_xml input.txt\n\n# Process by paragraphs\ntnh-fab process -g -p format_xml input.txt\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#pipeline-examples","title":"Pipeline Examples","text":"<pre><code># Punctuate and section\ncat input.txt | tnh-fab punctuate | tnh-fab section &gt; sections.json\n\n# Section and process\ntnh-fab section input.txt | tnh-fab process -p format_xml &gt; output.xml\n\n# Translate and process\ncat input.txt | tnh-fab translate | tnh-fab process -p format_md\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#configuration","title":"Configuration","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#directory-structure","title":"Directory Structure","text":"<pre><code>~/.config/tnh_scholar/\n\u251c\u2500\u2500 patterns/           # Pattern files\n\u2502   \u251c\u2500\u2500 punctuate/\n\u2502   \u251c\u2500\u2500 section/\n\u2502   \u251c\u2500\u2500 translate/\n\u2502   \u2514\u2500\u2500 process/\n\u2514\u2500\u2500 tnh-fab/\n    \u2514\u2500\u2500 settings.yaml   # Default configurations\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#default-configuration-settingsyaml","title":"Default Configuration (settings.yaml)","text":"<pre><code>defaults:\n  punctuate:\n    pattern: default_punctuate\n    style: APA\n    language: auto\n  section:\n    pattern: default_section\n    num_sections: auto\n    review_count: 3\n  translate:\n    pattern: default_translate\n  process:\n    format: txt\n\npattern_path: ~/.config/tnh_scholar/patterns\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#inputoutput-handling","title":"Input/Output Handling","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#input-sources","title":"Input Sources","text":"<ul> <li>File specified as argument</li> <li>STDIN (piped input)</li> <li>Section data (JSON format)</li> <li>Template files (YAML format)</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#output-handling","title":"Output Handling","text":"<ul> <li>STDOUT (default for piping)</li> <li>Specified output file (-o)</li> <li>Default file naming (if no -o): input_stage.ext</li> <li>JSON output for sections</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#pattern-management","title":"Pattern Management","text":"<ul> <li>Uses existing PatternManager class for pattern resolution</li> <li>Uses configured Pattern path</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#special-notes","title":"Special Notes","text":"<ol> <li>Translation is implemented as a standalone command (<code>translate</code>) for line-by-line processing, however can also be accomplished as a process pattern option for section translation</li> <li>Each command is standalone but designed for pipeline compatibility</li> <li>All commands default to STDIN/STDOUT unless specific files are provided</li> <li>Section data is always in JSON format for compatibility</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#-","title":"---","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#tnh-fab-process-detailed-specification","title":"TNH-FAB PROCESS: detailed Specification","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#overview_1","title":"Overview","text":"<p>The <code>process</code> command applies pattern-based text processing using optional section data. It can receive input from files and/or STDIN, with flexible output options. Typical usage is XML output.</p>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#command-format","title":"Command Format","text":"<pre><code>tnh-fab process [options] [input_file]\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#options","title":"Options","text":"<pre><code>-p, --pattern NAME      Pattern name for processing (REQUIRED)\n-s, --sections FILE     JSON file containing section data\n-g, --paragraph         Process text by newliine separated paragraphs\n-t, --template FILE     Template values file (YAML format)\n-k, --key-values PAIRS  Space-separated key:value pairs (e.g., speaker:\"Name\" title:\"Title\")\n-f, --format FORMAT     Output format: XML/txt (default: XML)\n-o, --output FILE      Output file (default: stdout)\n</code></pre>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#input-handling","title":"Input Handling","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#input-sources_1","title":"Input Sources","text":"<ul> <li>Text content can come from:</li> <li>File specified as argument</li> <li>STDIN</li> <li>Section data can come from:</li> <li>JSON file specified with -s</li> <li>STDIN when paired with input file</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#input-scenarios","title":"Input Scenarios","text":"<ol> <li>Single File Input <pre><code>tnh-fab process -p format_xml input.txt\n</code></pre></li> <li> <p>Processes input.txt directly</p> </li> <li> <p>STDIN Only <pre><code>cat input.txt | tnh-fab process -p format_xml\ncat input.txt | tnh-fab process -g -p format_xml  # process by paragraphs\n</code></pre></p> </li> <li> <p>Processes text from STDIN</p> </li> <li> <p>File + Sections File <pre><code>tnh-fab process -p format_xml -s sections.json input.txt\n</code></pre></p> </li> <li> <p>Processes input.txt using sections from sections.json</p> </li> <li> <p>STDIN + Sections File <pre><code>cat input.txt | tnh-fab process -p format_xml -s sections.json\n</code></pre></p> </li> <li> <p>Processes STDIN text using sections from sections.json</p> </li> <li> <p>Section Stream + Input File <pre><code>tnh-fab section input.txt | tnh-fab process -p format_xml input.txt\n</code></pre></p> </li> <li>Processes input.txt using sections from STDIN</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#input-validation","title":"Input Validation","text":"<ul> <li>When sections are provided (via -s or STDIN):</li> <li>Validates JSON format matches TextObject schema</li> <li>Checks source_file field in TextObject if present</li> <li>Warns if source_file doesn't match input file name</li> <li>Validates section line ranges against input text</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#template-value-handling","title":"Template Value Handling","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#priority-order-highest-to-lowest","title":"Priority Order (highest to lowest)","text":"<ol> <li>Command line key-values (-k)</li> <li>Template file values (-t)</li> <li>Default values from pattern</li> </ol>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#key-value-format","title":"Key-Value Format","text":"<ul> <li>Space-separated pairs</li> <li>Key and value joined by colon</li> <li>Values with spaces must be quoted</li> <li>Example: <code>speaker:\"Robert Smith\" title:\"My Journey\"</code></li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#output-handling_1","title":"Output Handling","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#output-destinations","title":"Output Destinations","text":"<ul> <li>STDOUT (default)</li> <li>File specified by -o option</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#format-options","title":"Format Options","text":"<ul> <li>txt (default): Plain text output</li> <li>json: JSON formatted output</li> <li>yaml: YAML formatted output</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#error-handling","title":"Error Handling","text":""},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#input-errors","title":"Input Errors","text":"<ul> <li>Missing required pattern</li> <li>Invalid section JSON format</li> <li>Section/input file mismatch</li> <li>Missing input when required</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#template-errors","title":"Template Errors","text":"<ul> <li>Invalid template file format</li> <li>Invalid key-value pair format</li> <li>Missing required template values</li> </ul>"},{"location":"architecture/tnh-gen/design/archive/tnh-fab-design-document/#usage-examples_1","title":"Usage Examples","text":"<pre><code># Basic file processing\ntnh-fab process -p format_pattern input.txt &gt; output.xml\n\n# Process with template values\ntnh-fab process -p format_pattern -k speaker:\"Robert Smith\" input.txt\n\n# Process with sections file\ntnh-fab process -p format_pattern -s sections.json input.txt\n\n# Process STDIN with sections\ncat input.txt | tnh-fab process -p format_pattern -s sections.json\n\n# Pipeline from section command\ntnh-fab section input.txt | tnh-fab process -p format_pattern input.txt\n\n# Complete example with all options\ntnh-fab process -p format_pattern \\\n  -s sections.json \\\n  -t template.yaml \\\n  -k speaker:\"Robert Smith\" \\\n  -f json \\\n  -o output.json \\\n  input.txt\n</code></pre>"},{"location":"architecture/transcription/","title":"Transcription","text":"<p>Table of Contents:</p> <p>Adr - Table of contents for architecture/transcription/adr</p> <p>Design - Table of contents for architecture/transcription/design</p> <p>This file auto-generated.</p>"},{"location":"architecture/transcription/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-TR01: AssemblyAI Integration for Transcription Service - Introduces a pluggable transcription interface with AssemblyAI and Whisper providers.</p> <p>ADR-TR02: Optimized SRT Generation Design - Uses provider-native SRT generation to simplify the transcription pipeline.</p> <p>ADR-TR03: Standardizing Timestamps to Milliseconds - Aligns all transcription providers on millisecond timestamps to avoid float drift.</p> <p>ADR-TR04: AssemblyAI Service Implementation Improvements - Refactors the AssemblyAI adapter to use the official SDK, richer options, and better error handling.</p> <p>This file auto-generated.</p>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/","title":"ADR-TR01: AssemblyAI Integration for Transcription Service","text":"<p>Adds AssemblyAI as a modular transcription backend alongside Whisper by defining a shared interface and provider factory.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-05-01</li> </ul>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#context","title":"Context","text":"<p>The TNH Scholar project currently uses OpenAI's Whisper API for audio transcription services, while using PyAnnote for speaker diarization. We wish to explore alternative transcription providers to potentially improve quality, gain additional features, and create a more modular architecture that allows for multiple transcription backends.</p> <p>AssemblyAI offers a comprehensive speech-to-text API with advanced features including high-quality transcription, support for multiple languages, and additional capabilities such as entity detection that could be valuable in future development. Integrating AssemblyAI as an alternative transcription backend would provide flexibility while allowing us to continue using PyAnnote for language-independent diarization.</p>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#decision-drivers","title":"Decision Drivers","text":"<ol> <li>Need for a pluggable transcription service architecture that supports multiple backends</li> <li>Desire to access AssemblyAI's transcription quality and feature set</li> <li>Requirement to maintain compatibility with the existing diarization system</li> <li>Support for rapid prototyping while enabling future production-quality implementations</li> <li>Consistent authentication and configuration management across services</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#proposed-decision","title":"Proposed Decision","text":"<p>We will create a modular transcription service architecture with AssemblyAI as an additional backend option. The implementation will:</p> <ol> <li>Define an abstract <code>TranscriptionService</code> interface that various providers can implement</li> <li>Create concrete implementations for both OpenAI Whisper (current) and AssemblyAI (new)</li> <li>Implement a factory pattern to select the appropriate service at runtime</li> <li>Support consistent configuration and authentication across services</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#design-details","title":"Design Details","text":""},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#class-structure","title":"Class Structure","text":"<pre><code>TranscriptionService (ABC)\n\u251c\u2500\u2500 WhisperTranscriptionService\n\u2514\u2500\u2500 AssemblyAITranscriptionService\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#interface-definition","title":"Interface Definition","text":"<pre><code>from abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Dict, Optional, Any, BinaryIO, Union\n\nclass TranscriptionService(ABC):\n    \"\"\"Abstract base class defining the interface for transcription services.\"\"\"\n\n    @abstractmethod\n    def transcribe(\n        self,\n        audio_file: Union[Path, BinaryIO],\n        options: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Transcribe audio file to text.\n\n        Args:\n            audio_file: Path to audio file or file-like object\n            options: Provider-specific options for transcription\n\n        Returns:\n            Dictionary containing transcription results with standardized keys\n            and provider-specific data\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_result(self, job_id: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get results for an existing transcription job.\n\n        Args:\n            job_id: ID of the transcription job\n\n        Returns:\n            Dictionary containing transcription results\n        \"\"\"\n        pass\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#factory-implementation","title":"Factory Implementation","text":"<pre><code>class TranscriptionServiceFactory:\n    \"\"\"Factory for creating transcription service instances.\"\"\"\n\n    @staticmethod\n    def create_service(\n        provider: str = \"whisper\",\n        api_key: Optional[str] = None,\n        **kwargs\n    ) -&gt; TranscriptionService:\n        \"\"\"\n        Create a transcription service instance.\n\n        Args:\n            provider: Service provider (\"whisper\" or \"assemblyai\")\n            api_key: API key for the service\n            **kwargs: Additional provider-specific configuration\n\n        Returns:\n            TranscriptionService instance\n        \"\"\"\n        if provider.lower() == \"whisper\":\n            from .whisper_service import WhisperTranscriptionService\n            return WhisperTranscriptionService(api_key=api_key, **kwargs)\n        elif provider.lower() == \"assemblyai\":\n            from .assemblyai_service import AssemblyAITranscriptionService\n            return AssemblyAITranscriptionService(api_key=api_key, **kwargs)\n        else:\n            raise ValueError(f\"Unsupported transcription provider: {provider}\")\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#authentication-management","title":"Authentication Management","text":"<p>Authentication will be managed using environment variables with consistent naming conventions:</p> <ul> <li><code>OPENAI_API_KEY</code> - for OpenAI Whisper API (existing)</li> <li><code>ASSEMBLYAI_API_KEY</code> - for AssemblyAI API (new)</li> </ul> <p>The system will look for these variables directly or in a <code>.env</code> file, with appropriate fallbacks and error messages.</p>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#configuration-design","title":"Configuration Design","text":"<p>Configuration will be managed through a hierarchical approach:</p> <ol> <li>Default configurations defined at the class level</li> <li>Configuration via constructor parameters</li> <li>Per-request options for fine-grained control</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#assemblyai-implementation","title":"AssemblyAI Implementation","text":"<p>The AssemblyAI implementation will use the AssemblyAI REST API with the following components:</p> <ol> <li>File upload endpoint (<code>https://api.assemblyai.com/v2/upload</code>)</li> <li>Transcription endpoint (<code>https://api.assemblyai.com/v2/transcript</code>)</li> <li>Polling mechanism for async job completion</li> <li>Result standardization for compatibility with existing systems</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#consequences","title":"Consequences","text":""},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#advantages","title":"Advantages","text":"<ol> <li>Modularity: Clear separation of concerns with a pluggable architecture</li> <li>Feature Access: Enables use of AssemblyAI's advanced features</li> <li>Flexibility: Allows switching between providers without changing client code</li> <li>Future-proofing: Architecture supports adding additional providers</li> <li>Consistency: Standardized result format regardless of backend</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#disadvantages","title":"Disadvantages","text":"<ol> <li>Complexity: Additional abstraction layer increases system complexity</li> <li>Integration Effort: Requires implementing and testing a new service</li> <li>Maintenance Overhead: Multiple implementations to maintain</li> <li>Dependency Management: New external dependency to manage</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#risks-and-mitigations","title":"Risks and Mitigations","text":"Risk Mitigation API compatibility changes Encapsulate provider-specific code in concrete implementations Authentication failures Clear error messages and validation for API keys Result format inconsistencies Standardized result mapping in each implementation Performance differences Documentation of expected behavior differences"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#implementation-plan","title":"Implementation Plan","text":""},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#phase-1-core-interface-week-1","title":"Phase 1: Core Interface (Week 1)","text":"<ol> <li>Define the <code>TranscriptionService</code> abstract base class</li> <li>Create a simple adapter for the existing Whisper implementation</li> <li>Implement basic factory pattern</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#phase-2-assemblyai-integration-week-2","title":"Phase 2: AssemblyAI Integration (Week 2)","text":"<ol> <li>Implement <code>AssemblyAITranscriptionService</code> for basic transcription</li> <li>Add authentication and configuration management</li> <li>Implement result standardization</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#phase-3-feature-parity-and-testing-week-3","title":"Phase 3: Feature Parity and Testing (Week 3)","text":"<ol> <li>Ensure feature parity with the current Whisper implementation</li> <li>Add comprehensive testing</li> <li>Update documentation</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#phase-4-integration-with-existing-systems-week-4","title":"Phase 4: Integration with Existing Systems (Week 4)","text":"<ol> <li>Modify <code>audio_transcribe.py</code> to use the new abstraction</li> <li>Add configuration options for selecting the transcription provider</li> <li>Validate integration with the diarization system</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr01-assemblyai-integration/#references","title":"References","text":"<ol> <li>AssemblyAI API Documentation</li> <li>Current OpenAI Whisper Integration Code</li> <li>Diarization System Design</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/","title":"ADR-TR02: Optimized SRT Generation Design","text":"<p>Leverages provider-native subtitle generation so SRT output no longer requires a bespoke converter.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-05-01</li> </ul>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#context","title":"Context","text":"<p>After researching the capabilities of both OpenAI Whisper and AssemblyAI transcription services, we discovered that they both offer direct SRT generation capabilities. Our current design included a separate <code>TranscriptionFormatConverter</code> utility for format conversion, but we can streamline the implementation by utilizing these native capabilities.</p>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#decision-drivers","title":"Decision Drivers","text":"<ol> <li>Direct support for SRT generation in both API services</li> <li>Performance considerations when generating subtitles</li> <li>Consistency of outputs across providers</li> <li>Maintenance complexity</li> <li>API efficiency and cost optimization</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#proposed-decision","title":"Proposed Decision","text":"<p>We will optimize the SRT generation by:</p> <ol> <li>Using provider-specific direct SRT output capabilities when available</li> <li>Overriding the <code>transcribe_to_format()</code> method in each provider implementation to take advantage of native format generation</li> <li>Maintaining the fallback to the general <code>TranscriptionFormatConverter</code> for scenarios where direct generation isn't available</li> <li>Simplifying the implementation to reduce unnecessary code paths</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#design-details","title":"Design Details","text":""},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#service-interface-updates","title":"Service Interface Updates","text":"<p>The <code>TranscriptionService</code> base class will remain mostly unchanged, with the existing methods:</p> <pre><code>def transcribe(self, audio_file, options) -&gt; Dict[str, Any]\ndef get_result(self, job_id) -&gt; Dict[str, Any]\ndef export_format(self, result, format_type, options) -&gt; str\ndef transcribe_to_format(self, audio_file, format_type, transcription_options, format_options) -&gt; str\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#provider-specific-optimizations","title":"Provider-Specific Optimizations","text":""},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#openai-whisper","title":"OpenAI Whisper","text":"<p>For Whisper, we'll use the API's native <code>response_format</code> parameter:</p> <pre><code>def transcribe_to_format(self, audio_file, format_type=\"srt\", transcription_options=None, format_options=None):\n    if format_type in [\"srt\", \"vtt\"]:\n        options = transcription_options or {}\n        options[\"response_format\"] = format_type\n        result = self.transcribe(audio_file, options)\n        if \"subtitle_content\" in result:\n            return result[\"subtitle_content\"]\n        # Fallback to raw result if available\n        if isinstance(result[\"raw_result\"], str):\n            return result[\"raw_result\"]\n    # Fallback to general converter\n    return super().transcribe_to_format(...)\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#assemblyai","title":"AssemblyAI","text":"<p>For AssemblyAI, we'll use the dedicated subtitles endpoint:</p> <pre><code>def transcribe_to_format(self, audio_file, format_type=\"srt\", transcription_options=None, format_options=None):\n    if format_type in self.SUBTITLE_FORMATS:\n        audio_url = self.upload_file(audio_file)\n        transcript_id = self.start_transcription(audio_url, transcription_options)\n        self.poll_for_completion(transcript_id)\n        return self.get_subtitles(transcript_id, format_type, format_options)\n    # Fallback to general converter\n    return super().transcribe_to_format(...)\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#format-converter-simplification","title":"Format Converter Simplification","text":"<p>The <code>TranscriptionFormatConverter</code> will still be available as a fallback, but we'll deprioritize its implementation since most SRT generation will use the native capabilities. We'll focus on the most essential format conversion features:</p> <ol> <li>Basic text-only output</li> <li>Simple SRT generation for cases where native support is unavailable</li> <li>Minimal formatting options</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#consequences","title":"Consequences","text":""},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#advantages","title":"Advantages","text":"<ol> <li>Performance: Direct API generation of SRT files will be more efficient</li> <li>Quality: Native SRT generation will produce higher quality subtitles with proper timing</li> <li>Maintenance: Less code to maintain in the format converter</li> <li>Cost: Potentially lower API costs by avoiding redundant processing</li> <li>Accuracy: Provider-specific optimizations will better handle edge cases</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#disadvantages","title":"Disadvantages","text":"<ol> <li>Provider Coupling: Tighter coupling to provider-specific API capabilities</li> <li>Testing Complexity: Need to test both direct generation and fallback paths</li> <li>Configuration Management: More provider-specific options to document and manage</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#implementation-plan","title":"Implementation Plan","text":"<ol> <li>Update the Whisper service implementation to use the <code>response_format</code> parameter</li> <li>Enhance the AssemblyAI implementation to use the subtitles endpoint</li> <li>Simplify the <code>TranscriptionFormatConverter</code> to focus on fallback scenarios</li> <li>Update the integration tests to verify both direct and fallback paths</li> <li>Update documentation to reflect the optimized approach</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr02-optimized-srt-design/#references","title":"References","text":"<ol> <li>OpenAI Whisper API response_format parameter</li> <li>AssemblyAI Subtitles endpoint</li> <li>Original SRT Generation ADR</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/","title":"ADR-TR03: Standardizing Timestamps to Milliseconds","text":"<p>Normalizes every transcription provider to emit integer millisecond timestamps for precision and consistency.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-05-01</li> </ul>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#context","title":"Context","text":"<p>Our transcription service currently uses a mix of timestamp formats across different providers:</p> <ul> <li>AssemblyAI returns timestamps in milliseconds, which we then convert to seconds</li> <li>OpenAI Whisper returns timestamps in seconds</li> </ul> <p>This inconsistency creates several issues:</p> <ol> <li>We need conversion logic in the AssemblyAI adapter</li> <li>We use floating-point numbers to represent seconds, potentially leading to precision issues</li> <li>The inconsistent API contract creates potential confusion for consumers of the service</li> <li>The mixed format approach is more error-prone and less maintainable</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#decision-drivers","title":"Decision Drivers","text":"<ol> <li>Precision Requirements: Accurate timestamps are critical for subtitle synchronization, speaker diarization, and audio analysis</li> <li>Data Type Consistency: Integer representation is more reliable than floating-point for precise timing</li> <li>Provider API Formats: Different providers use different time unit standards</li> <li>Downstream Compatibility: Impact on existing consumers of the transcription service</li> <li>Performance Considerations: Integer operations are typically faster than floating-point operations</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#proposed-decision","title":"Proposed Decision","text":"<p>We will standardize all timestamps in the transcription service to use milliseconds as the base unit, represented as integers, for the following reasons:</p> <ol> <li>Milliseconds provide sufficient precision for all anticipated use cases</li> <li>Integer representation avoids floating-point precision errors</li> <li>AssemblyAI already uses milliseconds, reducing conversion needs</li> <li>Millisecond precision is the common standard in media processing applications</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#design-impacts","title":"Design Impacts","text":""},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#interface-changes","title":"Interface Changes","text":"<p>The <code>TranscriptionService</code> interface will be updated to specify that all timestamps should be in milliseconds:</p> <pre><code>def transcribe(self, audio_file, options) -&gt; Dict[str, Any]:\n    \"\"\"\n    ...\n    Returns:\n        Dictionary containing transcription results with standardized keys:\n            - ...\n            - words: List of words with timing information (timestamps in milliseconds)\n            - utterances: List of utterances by speaker (timestamps in milliseconds)\n            - audio_duration: Duration of audio in milliseconds\n            - ...\n    \"\"\"\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#service-implementation-changes","title":"Service Implementation Changes","text":"<ol> <li>AssemblyAITranscriptionService:</li> <li>Remove timestamp conversions from milliseconds to seconds</li> <li>Update documentation to reflect millisecond timestamps</li> <li> <p>Keep raw API response format unchanged (already uses milliseconds)</p> </li> <li> <p>WhisperTranscriptionService:</p> </li> <li>Convert timestamps from seconds to milliseconds</li> <li>Update documentation to reflect millisecond timestamps</li> <li> <p>For API responses in seconds, multiply values by 1000 and convert to integers</p> </li> <li> <p>Format Converter:</p> </li> <li>Update handling of timestamps in format conversion logic</li> <li>Ensure SRT and VTT generation use correct millisecond timing</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#standardized-timestamp-format","title":"Standardized Timestamp Format","text":"<p>All timestamp-related fields will follow this standard:</p> <ul> <li>Use integer values representing milliseconds</li> <li>Use consistent field names across all services (<code>start_ms</code>, <code>end_ms</code>, <code>duration_ms</code>)</li> <li>Always include timestamp units in field names for clarity</li> </ul>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#consequences","title":"Consequences","text":""},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#positive","title":"Positive","text":"<ol> <li>Consistent Data Type: Integer timestamps avoid floating-point precision issues</li> <li>Reduced Conversion Logic: Less conversion code in the AssemblyAI adapter</li> <li>Higher Precision: Millisecond precision is suitable for all anticipated use cases</li> <li>Clearer API Contract: Standardized format creates consistency for consumers</li> <li>Future Compatibility: Easier integration with additional transcription services</li> <li>Alignment with Industry Standards: Media processing typically uses milliseconds</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#negative","title":"Negative","text":"<ol> <li>Breaking Changes: Existing code that consumes timestamps in seconds will need updates</li> <li>Increased Values: Millisecond values are 1000x larger than second values</li> <li>Initial Complexity: Additional adapter code needed during transition</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#neutral","title":"Neutral","text":"<ol> <li>Storage Impact: Integer milliseconds may use slightly more memory than floating-point seconds, but the difference is negligible</li> <li>Computational Impact: Integer operations are typically faster than floating-point, potentially offsetting the larger values</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#implementation-plan","title":"Implementation Plan","text":"<p>We will follow a phased approach to minimize disruption:</p>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#phase-1-interface-documentation-2-days","title":"Phase 1: Interface Documentation (2 days)","text":"<ol> <li>Update the interface documentation to specify millisecond timestamps</li> <li>Add deprecation notices for second-based timestamps</li> <li>Document the migration path for consumers</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#phase-2-service-implementation-1-week","title":"Phase 2: Service Implementation (1 week)","text":"<ol> <li>Update the <code>AssemblyAITranscriptionService</code> to remove conversions to seconds</li> <li>Update the <code>WhisperTranscriptionService</code> to output milliseconds</li> <li>Update the format converter timestamp handling</li> <li>Add comprehensive tests for timestamp handling</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#phase-3-compatibility-layer-1-week","title":"Phase 3: Compatibility Layer (1 week)","text":"<ol> <li>Add helper methods for clients to convert between timestamp formats</li> <li>Create adapter functions to maintain backward compatibility where needed</li> <li>Provide utility functions for common timestamp operations</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#phase-4-full-migration-2-weeks","title":"Phase 4: Full Migration (2 weeks)","text":"<ol> <li>Update all internal consumers to use millisecond timestamps</li> <li>Remove compatibility helpers for second-based timestamps</li> <li>Finalize documentation and test coverage</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#1-standardize-on-seconds","title":"1. Standardize on Seconds","text":"<p>Pros: - More human-readable values - Smaller numeric values - Already implemented in Whisper service</p> <p>Cons: - Requires conversion from AssemblyAI's native milliseconds - Floating-point precision issues - Less common in media processing systems</p>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#2-nanosecond-precision","title":"2. Nanosecond Precision","text":"<p>Pros: - Higher precision than milliseconds - Future-proof for high-precision applications</p> <p>Cons: - Unnecessary precision for current use cases - Very large integer values - Not supported by our current providers natively</p>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#3-maintain-mixed-format","title":"3. Maintain Mixed Format","text":"<p>Pros: - No immediate changes needed - Each provider uses its native format</p> <p>Cons: - Inconsistent API contract - More complex error handling - Higher cognitive load for developers</p>"},{"location":"architecture/transcription/adr/adr-tr03-ms-timestamps/#references","title":"References","text":"<ol> <li>IEEE 754 floating-point precision issues</li> <li>AssemblyAI API documentation</li> <li>OpenAI Whisper API documentation</li> <li>SRT subtitle format specification</li> <li>VTT subtitle format specification</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/","title":"ADR-TR04: AssemblyAI Service Implementation Improvements","text":"<p>Modernizes the AssemblyAI integration by adopting the official SDK, exposing advanced options, and hardening error handling.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-05-01</li> </ul>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#context","title":"Context","text":"<p>The current implementation of <code>assemblyai_service.py</code> provides basic functionality but fails to leverage many powerful features of the AssemblyAI API. After a thorough review of the API documentation and capabilities, we've identified several limitations in the current implementation:</p> <ol> <li>Limited Configuration Options: The implementation hardcodes many options, including defaulting to English language.</li> <li>Missing Advanced Features: No support for many audio intelligence features like sentiment analysis, entity detection, etc.</li> <li>Inefficient Implementation: Current implementation doesn't use the official SDK and contains redundant code.</li> <li>Lack of Error Handling: Minimal error handling for common failure scenarios.</li> <li>Missing Regional Support: No support for EU region endpoints for data residency compliance.</li> <li>Hardcoded Paths: API endpoints and configuration options are hardcoded.</li> <li>Incomplete Documentation: Documentation doesn't cover all available options.</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#decision-drivers","title":"Decision Drivers","text":"<ol> <li>API Feature Coverage: Desire to expose as much of AssemblyAI's feature set as possible through our service layer.</li> <li>Maintainability: Reduce code complexity and leverage the official SDK.</li> <li>Type Safety: Improve type annotations for better IDE support and error checking.</li> <li>Flexibility: Enable configuration of all major API parameters.</li> <li>Error Recovery: Improve handling of common failure scenarios.</li> <li>Performance: Optimize API interactions.</li> <li>Developer Experience: Make the service intuitive and well-documented.</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#proposed-decision","title":"Proposed Decision","text":"<p>We will significantly refactor the AssemblyAI service implementation to address these limitations using the following approaches:</p> <ol> <li>Use Official SDK: Replace low-level HTTP calls with the official AssemblyAI Python SDK where appropriate.</li> <li>Enhanced Configuration Model: Implement a comprehensive configuration model that exposes all major API options.</li> <li>Complete Feature Coverage: Add support for all major AssemblyAI features.</li> <li>Improved Error Handling: Implement better error handling with specific exception types.</li> <li>Regional Support: Add EU region support with configuration options.</li> <li>Callback Improvements: Add support for all callback patterns.</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#design-details","title":"Design Details","text":""},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#1-configuration-model","title":"1. Configuration Model","text":"<p>Create a nested configuration model that exposes all major API options:</p> <pre><code>@dataclass\nclass AssemblyAIConfig:\n    \"\"\"Configuration for AssemblyAI service.\"\"\"\n\n    # Base configuration\n    api_key: Optional[str] = None\n    base_url: str = \"https://api.assemblyai.com/v2\"  # or EU endpoint\n\n    # Connection configuration\n    polling_interval: int = 3  # seconds\n    max_polling_time: int = 300  # seconds\n\n    # Core transcription configuration\n    model: Literal[\"best\", \"nano\"] = \"best\"\n    language_code: Optional[str] = None\n    language_detection: bool = False\n    language_detection_confidence_threshold: float = 0.5\n\n    # Text formatting options\n    format_text: bool = True\n    punctuate: bool = True\n    disfluencies: bool = False\n    filter_profanity: bool = False\n\n    # Speaker options\n    speaker_labels: bool = False\n    speakers_expected: Optional[int] = None\n\n    # Audio channel options\n    multichannel: bool = False\n\n    # Accuracy enhancement options\n    word_boost: Optional[List[str]] = None\n    boost_param: Optional[Literal[\"low\", \"default\", \"high\"]] = None\n    custom_spelling: Optional[Dict[str, str]] = None\n\n    # Audio intelligence configuration\n    auto_chapters: bool = False\n    auto_highlights: bool = False\n    entity_detection: bool = False\n    iab_categories: bool = False\n    sentiment_analysis: bool = False\n    summarization: bool = False\n\n    # Callback options\n    webhook_url: Optional[str] = None\n    webhook_auth_header_name: Optional[str] = None\n    webhook_auth_header_value: Optional[str] = None\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#2-service-interface-update","title":"2. Service Interface Update","text":"<p>Update the service interface to use the configuration model and support all major features:</p> <pre><code>class AssemblyAITranscriptionService(TranscriptionService):\n    \"\"\"Enhanced AssemblyAI implementation of the TranscriptionService interface.\"\"\"\n\n    def __init__(self, config: Optional[AssemblyAIConfig] = None):\n        \"\"\"Initialize with comprehensive configuration options.\"\"\"\n        self.config = config or AssemblyAIConfig()\n        self._initialize_sdk()\n\n    def _initialize_sdk(self) -&gt; None:\n        \"\"\"Initialize the AssemblyAI SDK with current configuration.\"\"\"\n        import assemblyai as aai\n\n        # Set API key and base URL\n        aai.settings.api_key = self.config.api_key or os.getenv(\"ASSEMBLYAI_API_KEY\")\n        if not aai.settings.api_key:\n            raise ValueError(\"AssemblyAI API key is required\")\n\n        # Set base URL for regional support\n        if \"eu\" in self.config.base_url:\n            aai.settings.base_url = \"https://api.eu.assemblyai.com/v2\"\n\n        # Initialize transcriber\n        self.transcriber = aai.Transcriber()\n\n    def transcribe(\n        self,\n        audio_file: Union[Path, BinaryIO],\n        options: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Transcribe audio with full feature support.\n\n        Args:\n            audio_file: Path or file-like object\n            options: Override configuration options\n\n        Returns:\n            Standardized transcription result\n        \"\"\"\n        # Create transcription config by merging base config with options\n        import assemblyai as aai\n\n        # Prepare configuration\n        tx_config = self._prepare_transcription_config(options)\n\n        # Handle different input types\n        file_path = self._get_file_path(audio_file)\n\n        # Perform transcription\n        transcript = self.transcriber.transcribe(file_path, config=tx_config)\n\n        # Convert to standardized format\n        return self._standardize_result(transcript)\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#3-subtitle-generation-enhancement","title":"3. Subtitle Generation Enhancement","text":"<p>Improve subtitle generation capabilities:</p> <pre><code>def get_subtitles(\n    self,\n    transcript_id: str,\n    format_type: str = \"srt\",\n    chars_per_caption: int = 42,\n    speaker_labels: bool = False\n) -&gt; str:\n    \"\"\"\n    Get subtitles directly from AssemblyAI.\n\n    Args:\n        transcript_id: ID of completed transcript\n        format_type: Format type (\"srt\" or \"vtt\")\n        chars_per_caption: Maximum characters per caption\n        speaker_labels: Include speaker labels in subtitle\n\n    Returns:\n        Formatted subtitle content\n    \"\"\"\n    # Direct SDK method if available\n    if hasattr(self.transcript, f\"export_subtitles_{format_type.lower()}\"):\n        method = getattr(self.transcript, f\"export_subtitles_{format_type.lower()}\")\n        return method(chars_per_caption=chars_per_caption)\n\n    # Fallback to API call\n    params = {\n        \"chars_per_caption\": chars_per_caption,\n        \"speaker_labels\": speaker_labels\n    }\n\n    endpoint = f\"{self.config.base_url}/transcript/{transcript_id}/{format_type.lower()}\"\n    response = requests.get(\n        endpoint,\n        params=params,\n        headers={\"authorization\": aai.settings.api_key}\n    )\n\n    response.raise_for_status()\n    return response.text\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#4-audio-intelligence-methods","title":"4. Audio Intelligence Methods","text":"<p>Add support for audio intelligence features:</p> <pre><code>def get_intelligence_features(\n    self,\n    transcript: Any\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract intelligence features if enabled in configuration.\n\n    Args:\n        transcript: AssemblyAI transcript object\n\n    Returns:\n        Dictionary with available intelligence features\n    \"\"\"\n    features = {}\n\n    # Add sentiment analysis\n    if hasattr(transcript, \"sentiment_analysis\") and transcript.sentiment_analysis:\n        features[\"sentiment_analysis\"] = transcript.sentiment_analysis\n\n    # Add auto chapters\n    if hasattr(transcript, \"chapters\") and transcript.chapters:\n        features[\"chapters\"] = transcript.chapters\n\n    # Add entity detection\n    if hasattr(transcript, \"entities\") and transcript.entities:\n        features[\"entities\"] = transcript.entities\n\n    # Add auto highlights\n    if hasattr(transcript, \"auto_highlights\") and transcript.auto_highlights:\n        features[\"highlights\"] = transcript.auto_highlights\n\n    # Add topic classification\n    if hasattr(transcript, \"iab_categories\") and transcript.iab_categories:\n        features[\"topics\"] = {\n            \"results\": transcript.iab_categories.results,\n            \"summary\": transcript.iab_categories.summary\n        }\n\n    return features\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#5-region-support","title":"5. Region Support","text":"<p>Add EU region support with configuration detection:</p> <pre><code>def _is_eu_region(self) -&gt; bool:\n    \"\"\"Detect if service is configured for EU region.\"\"\"\n    return \"eu\" in self.config.base_url.lower()\n\ndef _get_appropriate_base_url(self) -&gt; str:\n    \"\"\"Get the appropriate base URL based on configuration.\"\"\"\n    if self._is_eu_region():\n        return \"https://api.eu.assemblyai.com/v2\"\n    return \"https://api.assemblyai.com/v2\"\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#6-webhook-support","title":"6. Webhook Support","text":"<p>Add webhook support for asynchronous processing:</p> <pre><code>def _configure_webhook(self, config: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Configure webhook parameters if enabled.\n\n    Args:\n        config: Current configuration dictionary\n\n    Returns:\n        Updated configuration with webhook parameters\n    \"\"\"\n    if self.config.webhook_url:\n        config[\"webhook_url\"] = self.config.webhook_url\n\n        if self.config.webhook_auth_header_name and self.config.webhook_auth_header_value:\n            config[\"webhook_auth_header_name\"] = self.config.webhook_auth_header_name\n            config[\"webhook_auth_header_value\"] = self.config.webhook_auth_header_value\n\n    return config\n</code></pre>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#consequences","title":"Consequences","text":""},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#advantages","title":"Advantages","text":"<ol> <li>Complete API Coverage: Access to all AssemblyAI features through our service layer.</li> <li>Improved Maintainability: Less code through SDK usage and better organization.</li> <li>Type Safety: Enhanced type annotations for better IDE support.</li> <li>Configuration Flexibility: All API options available through configuration.</li> <li>Better Error Handling: Specific exception types for common failures.</li> <li>Regional Compliance: EU region support for data residency requirements.</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#disadvantages","title":"Disadvantages","text":"<ol> <li>Migration Effort: Existing code using the service will need updates.</li> <li>SDK Dependency: Introduces dependency on the official SDK.</li> <li>Complexity: More configuration options increase initial learning curve.</li> <li>Version Tracking: Need to track SDK version compatibility.</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#implementation-plan","title":"Implementation Plan","text":"<ol> <li>Core Service Update: Replace HTTP calls with SDK usage.</li> <li>Configuration Model: Implement comprehensive configuration model.</li> <li>Feature Implementation: Add support for all major features.</li> <li>Testing: Add tests for new functionality.</li> <li>Documentation: Update documentation with new features and examples.</li> <li>Migration Guide: Create guide for transitioning from old implementation.</li> </ol>"},{"location":"architecture/transcription/adr/adr-tr04-assemblyai-improvements/#references","title":"References","text":"<ol> <li>AssemblyAI API Documentation</li> <li>AssemblyAI Python SDK</li> <li>TranscriptionService Interface Design</li> </ol>"},{"location":"architecture/transcription/design/","title":"Design","text":"<p>Table of Contents:</p> <p>Audio Chunking Algorithm Design Document - Design for splitting diarization segments into five-minute audio chunks using greedy accumulation and speaker-aware boundaries.</p> <p>Diarization Algorithms - This document details the key algorithms in the diarization system, focusing on high-level design without implementation details. Each algorithm is presented with its inputs, outputs, and process flow.</p> <p>Diarization Chunker Module Design Strategy - I've analyzed the current system and PoC code to propose a modular, extensible design for integrating the diarization chunking functionality into your tnh-scholar project.</p> <p>Diarization System Design - Detailed architecture for the diarization pipeline, covering segmentation, track extraction, and transcript remapping.</p> <p>Interval-to-Segment Mapping Algorithm - Algorithm for mapping chunk-relative transcription intervals back to diarization segments using overlap and proximity.</p> <p>Simplified Language-Aware Chunking Design - Language-aware chunking strategy that augments diarization splits with practical language detection heuristics.</p> <p>Language-Aware Chunking Orchestrator Notes - Working notes for extending the DiarizationChunker orchestrator with language-aware strategies.</p> <p>Modular Pipeline Design: Best Practices for Audio Transcription and Diarization - This document summarizes a detailed design and refactoring discussion on building a clean, modular, and production-ready audio transcription pipeline, with a focus on diarization chunking and robust system structure. It includes architectural patterns, file organization, and code hygiene practices.</p> <p>Practical Language-Aware Chunking Design - Practical heuristics for detecting language changes during chunking when diarization output is noisy.</p> <p>Speaker Diarization Algorithm Design - This document details the key algorithms referenced in the main diarization system design. Each algorithm is presented with a clear breakdown of its inputs, outputs, and processing steps.</p> <p>Speaker Diarization and Time-Mapped Transcription System Design - System design for mapping diarization outputs to speaker-specific transcriptions with accurate global timelines.</p> <p>TimelineMapper Design Document - Design for the TimelineMapper component that reprojects chunk-level transcripts into the original audio timeline.</p> <p>This file auto-generated.</p>"},{"location":"architecture/transcription/design/audio-chunking-design/","title":"Audio Chunking Algorithm Design Document","text":"<p>Design for splitting diarization segments into five-minute audio chunks using greedy accumulation and speaker-aware boundaries.</p>"},{"location":"architecture/transcription/design/audio-chunking-design/#problem-statement","title":"Problem Statement","text":"<p>We need to split an audio file into approximately 5-minute chunks, using speaker diarization results to determine the split points. The algorithm should:</p> <ol> <li>Use segment boundaries as potential split points</li> <li>Create chunks that are each greater than 5 minutes in duration</li> <li>Use a simple greedy approach to accumulate segments until reaching the target duration</li> </ol>"},{"location":"architecture/transcription/design/audio-chunking-design/#input-data-structure","title":"Input Data Structure","text":"<p>The input is a list of diarization segments, where each segment contains: - <code>speaker</code>: Speaker identifier (e.g., \"SPEAKER_00\") - <code>start</code>: Start time in seconds - <code>end</code>: End time in seconds</p> <p>Example: <pre><code>{\n  'diarization': [\n    {'speaker': 'SPEAKER_00', 'start': 93.745, 'end': 94.525},\n    {'speaker': 'SPEAKER_00', 'start': 95.645, 'end': 97.765},\n    ...\n  ]\n}\n</code></pre></p>"},{"location":"architecture/transcription/design/audio-chunking-design/#algorithm-description","title":"Algorithm Description","text":""},{"location":"architecture/transcription/design/audio-chunking-design/#high-level-approach","title":"High-Level Approach","text":"<ol> <li>Set up initial state:</li> <li>Start with the first segment</li> <li>Initialize first chunk starting point</li> <li> <p>Initialize empty list to store chunk boundaries</p> </li> <li> <p>Greedy Segment Accumulation:</p> </li> <li>Accumulate segments sequentially</li> <li>Track total accumulated duration</li> <li>When accumulated duration exceeds 5 minutes, create a chunk boundary</li> <li>Continue process until all segments are processed</li> </ol>"},{"location":"architecture/transcription/design/audio-chunking-design/#detailed-steps","title":"Detailed Steps","text":"<ol> <li>Initialization:</li> <li>Set current chunk start time to the start of the first segment</li> <li>Initialize empty list to store chunks</li> <li> <p>Initialize empty list to store segments for current chunk</p> </li> <li> <p>Chunking Process:</p> </li> <li>Iterate through segments in order</li> <li> <p>For each segment:</p> <ul> <li>Calculate accumulated running time (current segment end - chunk start time)</li> <li>Add current segment to current chunk segments list</li> <li>If accumulated running time &gt; 5 minutes (300 seconds):</li> <li>Create a chunk containing all accumulated segments</li> <li>Record chunk boundary times (start time of first segment to end time of last segment)</li> <li>Add chunk to chunks list</li> <li>Reset current chunk segments list</li> <li>Update chunk start time to current segment end time</li> <li>Continue to next segment</li> </ul> </li> <li> <p>Finalizing:</p> </li> <li>Add final chunk containing any remaining segments</li> <li>If final chunk is empty, do nothing</li> </ol>"},{"location":"architecture/transcription/design/audio-chunking-design/#flowchart","title":"Flowchart","text":"<pre><code>flowchart TD\n    A[Start] --&gt; B[Set chunk_start = first segment start]\n    B --&gt; C[Initialize empty chunks list]\n    C --&gt; D[For each segment]\n    D --&gt; E{Current segment end - chunk_start &gt; 300s?}\n    E --&gt;|No| D\n    E --&gt;|Yes| F[Create chunk from chunk_start to current segment start]\n    F --&gt; G[Add chunk to chunks list]\n    G --&gt; H[Set chunk_start = current segment start]\n    H --&gt; D\n    D --&gt;|All segments processed| I[Create final chunk from chunk_start to last segment end]\n    I --&gt; J[Add final chunk to chunks list]\n    J --&gt; K[Return chunks list]\n    K --&gt; L[End]</code></pre>"},{"location":"architecture/transcription/design/audio-chunking-design/#data-flow","title":"Data Flow","text":"<pre><code>flowchart LR\n    A[Diarization Result] --&gt; B[Segment Iterator]\n    B --&gt; C[Duration Calculator]\n    C --&gt; D[Chunk Boundary Detector]\n    D --&gt; E[Chunk List Builder]\n    E --&gt; F[Final Chunk List]</code></pre>"},{"location":"architecture/transcription/design/audio-chunking-design/#data-structures","title":"Data Structures","text":""},{"location":"architecture/transcription/design/audio-chunking-design/#input","title":"Input","text":"<pre><code>DiarizationResult {\n    diarization: Array&lt;Segment&gt;\n}\n\nSegment {\n    speaker: String\n    start: Float\n    end: Float\n}\n</code></pre>"},{"location":"architecture/transcription/design/audio-chunking-design/#working-data","title":"Working Data","text":"<pre><code>ChunkBoundary {\n    start_time: Float\n    end_time: Float\n    segments: Array&lt;Segment&gt;\n}\n</code></pre>"},{"location":"architecture/transcription/design/audio-chunking-design/#output","title":"Output","text":"<pre><code>ChunkList {\n    chunks: Array&lt;ChunkBoundary&gt;\n}\n</code></pre>"},{"location":"architecture/transcription/design/audio-chunking-design/#special-considerations","title":"Special Considerations","text":"<ol> <li>Edge Cases:</li> <li>Short Audio: If the total audio duration is less than 5 minutes, create a single chunk</li> <li>Long Single Segment: If a single segment spans more than 5 minutes, use its boundaries</li> <li> <p>Silent Gaps: Long silent gaps between segments are simply part of the running time calculation, not special cases</p> </li> <li> <p>Performance Considerations:</p> </li> <li>The algorithm has O(n) time complexity where n is the number of segments</li> <li>Memory usage is O(n) where n is the number of segments (each segment is stored in exactly one chunk)</li> </ol>"},{"location":"architecture/transcription/design/audio-chunking-design/#addendum-discussion-of-data-structures","title":"Addendum: Discussion of Data Structures","text":"<p>[I proposed using pydantic classes for later easier integration into tnh-scholar system]</p> <p>You've raised an excellent point about balancing simplicity in prototyping with long-term code sustainability. I appreciate your perspective that investing in properly structured data models early can pay dividends later.</p> <p>Here are some reflections on using Pydantic models for this implementation:</p>"},{"location":"architecture/transcription/design/audio-chunking-design/#benefits-of-using-pydantic-from-the-start","title":"Benefits of Using Pydantic from the Start","text":"<ol> <li> <p>Integration with Existing Code: If the rest of the TNH Scholar project already uses Pydantic models, maintaining that consistency would make this component more naturally integrate with the existing ecosystem.</p> </li> <li> <p>Future-Proofing: Starting with Pydantic models means less refactoring when moving from prototype to production. The validation logic would already be in place.</p> </li> <li> <p>Self-Documenting: Pydantic models serve as both data structures and documentation of the expected data shape, making the code more maintainable.</p> </li> <li> <p>Validation Logic: Automatic validation catches errors early, which can save debugging time even in the prototype phase.</p> </li> <li> <p>JSON Serialization: If these chunks need to be stored or transmitted (which seems likely), Pydantic's serialization capabilities are immediately useful.</p> </li> </ol>"},{"location":"architecture/transcription/design/audio-chunking-design/#additional-considerations","title":"Additional Considerations","text":"<ol> <li> <p>Model Hierarchy: We could create a hierarchy where the diarization models extend or relate to other audio processing models in the system.</p> </li> <li> <p>Computed Properties: Things like \"duration\" can be added as computed properties, reducing duplicated logic throughout the codebase.</p> </li> <li> <p>Type Safety: The typing benefits apply throughout the development lifecycle - IDEs can provide better assistance during both prototyping and production.</p> </li> <li> <p>Schema Evolution: If the diarization format changes in the future, Pydantic makes it easier to handle backward compatibility with validators.</p> </li> <li> <p>Standardization: It encourages standardized access patterns across different parts of the system.</p> </li> </ol>"},{"location":"architecture/transcription/design/audio-chunking-design/#my-revised-opinion","title":"My Revised Opinion","text":"<p>Given these considerations, I agree that using Pydantic models from the beginning makes sense, especially if:</p> <ol> <li>Other parts of TNH Scholar use this pattern</li> <li>The implementation will likely move to production</li> <li>The data structures might be reused elsewhere</li> </ol> <p>The cost of setting up these models is fairly low (a few extra lines of code), especially compared to the benefits of consistency, validation, and integration with the rest of the system.</p> <p>If the TNH Scholar codebase is consistently moving toward Pydantic models, then implementing them here maintains that design direction and will make future maintenance and integration simpler.</p>"},{"location":"architecture/transcription/design/audio-chunking-design/#proposed-data-structures","title":"Proposed Data Structures","text":"<pre><code>class Segment(BaseModel):\n    speaker: str\n    start: float\n    end: float\n\nclass Chunk(BaseModel):\n    start_time: float\n    end_time: float\n    segments: List[Segment]\n\n    @property\n    def duration(self) -&gt; float:\n        return self.end_time - self.start_time\n\nclass ChunkResult(BaseModel):\n    chunks: List[Chunk]\n</code></pre>"},{"location":"architecture/transcription/design/audio-chunking-design/#next-steps","title":"Next Steps","text":"<p>As for next steps, I would propose:</p> <ol> <li> <p>Define the Pydantic Models - Create proper models for Segment, Chunk, and ChunkResult with appropriate validation and computed properties</p> </li> <li> <p>Implement the Core Algorithm - Write the chunking function using the models we've defined, following our agreed approach</p> </li> <li> <p>Add Simple Test Cases - Create tests with various patterns of segments (including edge cases like very long segments, short audio, long silent gaps)</p> </li> <li> <p>Create a Visualization - Build a simple visualization showing the original segments and how they've been grouped into chunks, which would help verify the algorithm's behavior</p> </li> <li> <p>Integration Design - Sketch how this would integrate with the existing audio processing pipeline in TNH Scholar</p> </li> </ol> <p>Would you like to proceed with implementing the Pydantic models and algorithm, or should we further refine any aspects of the design?</p>"},{"location":"architecture/transcription/design/diarization-algorithms/","title":"Diarization Algorithms","text":"<p>This document details the key algorithms in the diarization system, focusing on high-level design without implementation details. Each algorithm is presented with its inputs, outputs, and process flow.</p>"},{"location":"architecture/transcription/design/diarization-algorithms/#1-speaker-segment-merging-algorithm","title":"1. Speaker Segment Merging Algorithm","text":"<p>The algorithm merges adjacent segments from the same speaker when they're close enough in time.</p>"},{"location":"architecture/transcription/design/diarization-algorithms/#11-inputs-and-outputs","title":"1.1 Inputs and Outputs","text":"<p>Inputs:</p> <ul> <li>List of speaker segments from diarization API</li> <li>Gap threshold (maximum gap to consider segments contiguous)</li> </ul> <p>Outputs:</p> <ul> <li>Dictionary mapping speaker IDs to lists of merged segments</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#12-algorithm-flowchart","title":"1.2 Algorithm Flowchart","text":"<pre><code>flowchart TD\n    A[Input: Segments + Threshold] --&gt; B[Sort segments by start time]\n    B --&gt; C[Initialize speaker blocks]\n    C --&gt; D[For each segment]\n    D --&gt; E{Is first segment?}\n    E --&gt;|Yes| F[Initialize current block]\n    E --&gt;|No| G{Same speaker as current?}\n    G --&gt;|Yes| H{Gap \u2264 threshold?}\n    G --&gt;|No| I[Finalize current block]\n    I --&gt; J[Start new block with segment]\n    H --&gt;|Yes| K[Extend current block]\n    H --&gt;|No| L[Finalize current block]\n    L --&gt; M[Start new block with same speaker]\n    F --&gt; D\n    J --&gt; D\n    K --&gt; D\n    M --&gt; D\n    D --&gt; N{More segments?}\n    N --&gt;|Yes| D\n    N --&gt;|No| O[Finalize last block]\n    O --&gt; P[Return speaker blocks]</code></pre>"},{"location":"architecture/transcription/design/diarization-algorithms/#13-algorithm-description","title":"1.3 Algorithm Description","text":"<ol> <li>Sort segments: Arrange all segments in chronological order by start time</li> <li>Process sequentially: Examine each segment in order</li> <li>Speaker change detection: Check if current segment has same speaker as previous</li> <li>Gap analysis: For same speaker, check if gap between segments is below threshold</li> <li>Block management:</li> <li>If same speaker and small gap: extend current block</li> <li>If same speaker but large gap: finalize current block and start new one</li> <li>If different speaker: finalize current block and start new one</li> <li>Result generation: Return dictionary mapping speaker IDs to lists of merged segments</li> </ol>"},{"location":"architecture/transcription/design/diarization-algorithms/#14-potential-enhancements","title":"1.4 Potential Enhancements","text":"<ul> <li>Adaptive gap thresholds: Adjust threshold based on speaker characteristics</li> <li>Confidence-based merging: Incorporate confidence scores from API</li> <li>Speech rate analysis: Consider speech rate in gap threshold determination</li> <li>Overlapping segment handling: Intelligently resolve overlapping segments</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#2-audio-extraction-algorithm","title":"2. Audio Extraction Algorithm","text":"<p>This algorithm extracts speaker segments from the original audio and creates speaker-specific tracks.</p>"},{"location":"architecture/transcription/design/diarization-algorithms/#21-inputs-and-outputs","title":"2.1 Inputs and Outputs","text":"<p>Inputs:</p> <ul> <li>Path to original audio file</li> <li>Speaker blocks (dictionary mapping speakers to segments)</li> <li>Output directory</li> <li>Configuration for silence padding</li> </ul> <p>Outputs:</p> <ul> <li>Dictionary of timeline mapping information for each speaker</li> <li>Speaker-specific audio files on disk</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#22-algorithm-flowchart","title":"2.2 Algorithm Flowchart","text":"<pre><code>flowchart TD\n    A[Input: Original Audio + Speaker Blocks] --&gt; B[Load original audio]\n    B --&gt; C[For each speaker]\n    C --&gt; D[Initialize empty audio track]\n    D --&gt; E[For each block]\n    E --&gt; F[Extract segment from original audio]\n    F --&gt; G{Add padding?}\n    G --&gt;|Yes| H[Add silence padding]\n    G --&gt;|No| I[Skip padding]\n    H --&gt; J[Add to speaker track]\n    I --&gt; J\n    J --&gt; K[Store mapping information]\n    K --&gt; L{More blocks?}\n    L --&gt;|Yes| E\n    L --&gt;|No| M[Export speaker track]\n    M --&gt; N{More speakers?}\n    N --&gt;|Yes| C\n    N --&gt;|No| O[Return mapping information]</code></pre>"},{"location":"architecture/transcription/design/diarization-algorithms/#23-algorithm-description","title":"2.3 Algorithm Description","text":"<ol> <li>Load original audio: Read the complete audio file</li> <li>Speaker iteration: Process each speaker independently</li> <li>Track creation: For each speaker:</li> <li>Create an empty audio track</li> <li>Initialize timeline mapping information</li> <li>Segment processing: For each segment:</li> <li>Extract audio from original file</li> <li>Add silence padding if configured</li> <li>Add to speaker track</li> <li>Record mapping information</li> <li>Export: Save each speaker's audio to disk</li> <li>Return mappings: Provide timeline mapping information for later use</li> </ol>"},{"location":"architecture/transcription/design/diarization-algorithms/#24-potential-enhancements","title":"2.4 Potential Enhancements","text":"<ul> <li>Streaming processing: Handle large audio files without loading entirely</li> <li>Parallel extraction: Process multiple speakers simultaneously</li> <li>Audio enhancement: Apply noise reduction or normalization per speaker</li> <li>Adaptive padding: Adjust silence duration based on context</li> <li>Format optimization: Support different output formats with optimized settings</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#3-srt-timeline-mapping-algorithm","title":"3. SRT Timeline Mapping Algorithm","text":"<p>This algorithm transforms SRT timestamps from speaker timeline to original audio timeline.</p>"},{"location":"architecture/transcription/design/diarization-algorithms/#31-inputs-and-outputs","title":"3.1 Inputs and Outputs","text":"<p>Inputs:</p> <ul> <li>Path to SRT file (with timestamps in speaker timeline)</li> <li>Timeline mapping information</li> <li>Output path for transformed SRT</li> </ul> <p>Outputs:</p> <ul> <li>SRT file with timestamps mapped to original timeline</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#32-algorithm-flowchart","title":"3.2 Algorithm Flowchart","text":"<pre><code>flowchart TD\n    A[Input: SRT + Timeline Map] --&gt; B[Parse SRT file]\n    B --&gt; C[Build interval mapping]\n    C --&gt; D[For each subtitle entry]\n    D --&gt; E[Get start/end times]\n    E --&gt; F[Find best matching interval]\n    F --&gt; G[Apply timeline mapping]\n    G --&gt; H[Create new subtitle entry]\n    H --&gt; I{More entries?}\n    I --&gt;|Yes| D\n    I --&gt;|No| J[Write SRT file]\n    J --&gt; K[Return mapped SRT]</code></pre>"},{"location":"architecture/transcription/design/diarization-algorithms/#33-algorithm-description","title":"3.3 Algorithm Description","text":"<ol> <li>Parse SRT: Extract subtitle entries with timestamps and text</li> <li>Build intervals: Create complete mapping intervals for timeline transformation</li> <li>Entry processing: For each subtitle entry:</li> <li>Convert timestamps to seconds</li> <li>Find best matching mapping interval</li> <li>Apply timeline transformation</li> <li>Create new entry with mapped timestamps</li> <li>Output generation: Write transformed entries to SRT file</li> </ol>"},{"location":"architecture/transcription/design/diarization-algorithms/#34-potential-enhancements","title":"3.4 Potential Enhancements","text":"<ul> <li>Non-linear mapping: Support for variable-rate timeline transformations</li> <li>Format preservation: Maintain formatting and styling in transformed SRT</li> <li>Metadata enrichment: Add speaker information to subtitle entries</li> <li>Multi-language support: Handle translated SRTs with correct timing</li> <li>Overlap resolution: Intelligent handling of overlapping subtitle entries</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#4-finding-best-interval-algorithm","title":"4. Finding Best Interval Algorithm","text":"<p>A subcomponent of the SRT mapping algorithm that finds the most appropriate interval for a subtitle.</p>"},{"location":"architecture/transcription/design/diarization-algorithms/#41-inputs-and-outputs","title":"4.1 Inputs and Outputs","text":"<p>Inputs:</p> <ul> <li>List of timeline intervals</li> <li>Start and end time of subtitle (in speaker timeline)</li> </ul> <p>Outputs:</p> <ul> <li>Interval with best overlap with the subtitle</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#42-algorithm-flowchart","title":"4.2 Algorithm Flowchart","text":"<pre><code>flowchart TD\n    A[Input: Intervals + Subtitle Time] --&gt; B[Initialize best overlap]\n    B --&gt; C[For each interval]\n    C --&gt; D[Calculate overlap]\n    D --&gt; E{Overlap &gt; best?}\n    E --&gt;|Yes| F[Update best interval]\n    E --&gt;|No| G[Continue]\n    F --&gt; G\n    G --&gt; H{More intervals?}\n    H --&gt;|Yes| C\n    H --&gt;|No| I[Return best interval]</code></pre>"},{"location":"architecture/transcription/design/diarization-algorithms/#43-algorithm-description","title":"4.3 Algorithm Description","text":"<ol> <li>Initialization: Start with no overlap found</li> <li>Interval iteration: Examine each potential mapping interval</li> <li>Overlap calculation: Determine how much the subtitle overlaps with each interval</li> <li>Best match selection: Choose interval with greatest overlap</li> <li>Result return: Provide best interval for mapping, or None if no overlap</li> </ol>"},{"location":"architecture/transcription/design/diarization-algorithms/#44-potential-enhancements","title":"4.4 Potential Enhancements","text":"<ul> <li>Weighted overlap: Consider duration and content in overlap importance</li> <li>Context awareness: Consider surrounding subtitles in interval selection</li> <li>Confidence scoring: Include confidence metrics in interval selection</li> <li>Multiple interval mapping: Split subtitle across intervals when necessary</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#5-timeline-mapping-function","title":"5. Timeline Mapping Function","text":"<p>Maps time from speaker timeline to original timeline using interval information.</p>"},{"location":"architecture/transcription/design/diarization-algorithms/#51-inputs-and-outputs","title":"5.1 Inputs and Outputs","text":"<p>Inputs:</p> <ul> <li>Mapping interval (original and speaker timeline information)</li> <li>Time to map (in speaker timeline)</li> </ul> <p>Outputs:</p> <ul> <li>Corresponding time in original timeline</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#52-algorithm-flowchart","title":"5.2 Algorithm Flowchart","text":"<pre><code>flowchart TD\n    A[Input: Interval + Time] --&gt; B[Extract interval parameters]\n    B --&gt; C[Calculate position in interval]\n    C --&gt; D[Map to original timeline]\n    D --&gt; E[Return mapped time]</code></pre>"},{"location":"architecture/transcription/design/diarization-algorithms/#53-algorithm-description","title":"5.3 Algorithm Description","text":"<ol> <li>Parameter extraction: Get original and speaker timeline boundaries from interval</li> <li>Position calculation: Determine relative position of time within speaker timeline range</li> <li>Timeline mapping: Apply same relative position to original timeline range</li> <li>Result return: Return the mapped time in original timeline</li> </ol>"},{"location":"architecture/transcription/design/diarization-algorithms/#54-potential-enhancements","title":"5.4 Potential Enhancements","text":"<ul> <li>Non-linear mapping: Support for variable rates within intervals</li> <li>Interpolation: Multi-point interpolation for improved accuracy</li> <li>Boundary handling: Special cases for interval boundaries</li> <li>Error estimation: Include confidence/error metrics with mapped times</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#6-improvements-and-extensions","title":"6. Improvements and Extensions","text":""},{"location":"architecture/transcription/design/diarization-algorithms/#61-enhanced-speaker-merging","title":"6.1 Enhanced Speaker Merging","text":"<ul> <li>Speaker characteristics: Adapt merging based on speech patterns</li> <li>Content awareness: Consider linguistic content in merging decisions</li> <li>Machine learning: Train models to predict optimal merge points</li> <li>Speaker identification: Incorporate voice recognition for improved accuracy</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#62-advanced-timeline-mapping","title":"6.2 Advanced Timeline Mapping","text":"<ul> <li>Content-aware stretching: Vary mapping based on speech content</li> <li>Multi-point anchoring: Use multiple reference points for mapping</li> <li>Adaptive resolution: Higher precision for critical segments</li> <li>Bidirectional optimization: Ensure equal accuracy in both mapping directions</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#63-audio-processing-enhancements","title":"6.3 Audio Processing Enhancements","text":"<ul> <li>Noise profiling: Identify and remove background noise specific to each speaker</li> <li>Volume normalization: Consistent loudness across all speakers</li> <li>Voice enhancement: Improve clarity of speech for each speaker</li> <li>Acoustic environment matching: Maintain consistent room acoustics</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#64-srt-processing-extensions","title":"6.4 SRT Processing Extensions","text":"<ul> <li>Formatting preservation: Maintain styling across timeline mapping</li> <li>Speaker identification: Label subtitles with speaker information</li> <li>Confidence indication: Include reliability markers for uncertain segments</li> <li>Multi-language alignment: Coordinate timing across translated subtitles</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#65-performance-optimizations","title":"6.5 Performance Optimizations","text":"<ul> <li>Progressive processing: Begin processing before complete file download</li> <li>Memory-efficient handling: Process large files without excessive memory use</li> <li>Computation distribution: Distribute processing across multiple cores/machines</li> <li>Caching strategies: Reuse intermediate results when appropriate</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#7-integration-with-audio-transcribe","title":"7. Integration with audio-transcribe","text":""},{"location":"architecture/transcription/design/diarization-algorithms/#71-integration-points","title":"7.1 Integration Points","text":"<ul> <li>Command-line flags: Add diarization-specific options to CLI</li> <li>Configuration file: Support for detailed diarization configuration</li> <li>Pipeline integration: Use diarization in place of or alongside existing chunking</li> <li>Output control: Options for organizing and formatting diarized output</li> </ul>"},{"location":"architecture/transcription/design/diarization-algorithms/#72-workflow-integration","title":"7.2 Workflow Integration","text":"<ul> <li>Preprocessing: Use diarization to guide chunking strategy</li> <li>Speaker-aware transcription: Optimize transcription for each speaker</li> <li>Result combination: Merge speaker transcripts with proper attribution</li> <li>Output generation: Create integrated output with speaker identification</li> </ul>"},{"location":"architecture/transcription/design/diarization-chunker-design/","title":"Diarization Chunker Module Design Strategy","text":"<p>I've analyzed the current system and PoC code to propose a modular, extensible design for integrating the diarization chunking functionality into your tnh-scholar project.</p>"},{"location":"architecture/transcription/design/diarization-chunker-design/#current-architecture-assessment","title":"Current Architecture Assessment","text":"<p>From the provided information, I understand that:</p> <ol> <li>The <code>diarization_chunker</code> module exists with some basic functionality</li> <li>The PoC in <code>diarize_poc3.py</code> contains key algorithms for:</li> <li>Extracting audio segments by speaker</li> <li>Transforming SRT timelines between speaker-specific and original timelines</li> <li>Support modules like <code>timed_text.py</code> handle subtitle formats</li> <li>Transcription services already have a modular structure</li> </ol>"},{"location":"architecture/transcription/design/diarization-chunker-design/#design-goals","title":"Design Goals","text":"<p>The enhanced system should:</p> <ul> <li>Process diarization data into speaker-specific chunks of configurable duration</li> <li>Extract audio segments for each speaker</li> <li>Create timeline mapping for accurate timestamp transformation</li> <li>Support integrating transcriptions back into a unified timeline</li> <li>Maintain modularity with small, single-purpose methods</li> <li>Use Pydantic for serializable data models</li> </ul>"},{"location":"architecture/transcription/design/diarization-chunker-design/#process-flow","title":"Process Flow","text":"<p>The enhanced system would work like this:</p> <ol> <li>Load diarization data and parse into <code>DiarizationSegment</code> objects</li> <li>Use <code>DiarizationChunker</code> to merge segments by speaker</li> <li><code>SpeakerProcessor</code> creates speaker blocks and chunks them to target duration</li> <li><code>AudioExtractor</code> extracts audio for each chunk</li> <li><code>TimelineMapper</code> builds timeline mappings and can transform transcriptions</li> </ol>"},{"location":"architecture/transcription/design/diarization-chunker-design/#integration-with-timed_textpy","title":"Integration with timed_text.py","text":"<p>The <code>TimelineMapper</code> would integrate with <code>timed_text.py</code> by:</p> <ol> <li>Parsing SRT content into <code>TimedTextSegment</code> objects</li> <li>Applying timeline transformations to each segment</li> <li>Generating new SRT content with adjusted timestamps</li> </ol>"},{"location":"architecture/transcription/design/diarization-chunker-design/#refinement-enhanced-diarization-chunker-design-strategy","title":"Refinement: Enhanced Diarization Chunker Design Strategy","text":"<p>This refinement comes from dialog with ChatGPT o3 model</p>"},{"location":"architecture/transcription/design/diarization-chunker-design/#1-improved-package-structure","title":"1. Improved Package Structure","text":"<p>Combining both approaches, I recommend this refined structure:</p> <pre><code>tnh_scholar/\n\u2514\u2500\u2500 audio_processing/\n    \u2514\u2500\u2500 diarization/\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 chunker.py              # Main facade (DiarizationChunker)\n        \u251c\u2500\u2500 models.py               # Pydantic data models\n        \u251c\u2500\u2500 config.py               # Configuration with BaseSettings\n        \u251c\u2500\u2500 speaker_processor.py    # Core speaker processing logic\n        \u251c\u2500\u2500 audio.py                # Audio operations (isolates ffmpeg)\n        \u251c\u2500\u2500 mapping.py              # Timeline mapping utilities\n        \u2514\u2500\u2500 _extractors.py          # Internal helper classes\n</code></pre> <p>Key improvements from ChatGPT o3: - Separate <code>audio.py</code> to isolate external dependencies (ffmpeg) - More granular separation of concerns - Clear distinction between public interfaces and internal helpers</p>"},{"location":"architecture/transcription/design/diarization-chunker-design/#2-enhanced-configuration-model","title":"2. Enhanced Configuration Model","text":"<p>Adopting BaseSettings for environmental flexibility:</p> <pre><code>from pydantic import BaseSettings, Field\n\nclass ChunkerConfig(BaseSettings):\n    \"\"\"Configuration for diarization chunking\"\"\"\n    target_duration_ms: int = Field(300000, env='CHUNKER_TARGET_MS')\n    gap_threshold_ms: int = Field(2000, env='CHUNKER_GAP_MS')\n    min_segment_duration_ms: int = Field(1000)\n    include_silence_padding: bool = True\n    silence_padding_ms: int = 500\n    audio_format: str = \"mp3\"\n    audio_bitrate: str = \"128k\"\n    extract_audio: bool = True\n    cache_temp_files: bool = True\n\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n</code></pre>"},{"location":"architecture/transcription/design/diarization-chunker-design/#3-refined-data-models","title":"3. Refined Data Models","text":"<p>Combining both proposals with clearer separation:</p> <pre><code>class DiarizationSegment(BaseModel):\n    \"\"\"Raw segment from diarization\"\"\"\n    speaker: str\n    start_ms: int\n    end_ms: int\n\nclass Chunk(BaseModel):\n    \"\"\"Processed chunk for extraction\"\"\"\n    speaker: str\n    start_ms: int\n    end_ms: int\n    segments: List[DiarizationSegment] = Field(default_factory=list)\n    audio_data: Optional[bytes] = None\n    timeline_mappings: List[TimelineMapping] = Field(default_factory=list)\n\n    @property\n    def duration_ms(self) -&gt; int:\n        return self.end_ms - self.start_ms\n\nclass TimelineMapping(BaseModel):\n    \"\"\"Maps between original and speaker-specific timelines\"\"\"\n    original_start_ms: int\n    original_end_ms: int\n    speaker_start_ms: int\n    speaker_end_ms: int\n    srt_indices: List[int] = Field(default_factory=list)\n</code></pre>"},{"location":"architecture/transcription/design/diarization-chunker-design/#4-enhanced-public-interface","title":"4. Enhanced Public Interface","text":"<p>Adopting the facade pattern with incremental processing:</p> <pre><code>class DiarizationChunker:\n    \"\"\"Main orchestrator for diarization processing\"\"\"\n\n    def __init__(self, config: Optional[ChunkerConfig] = None):\n        self.config = config or ChunkerConfig()\n        self._processor = SpeakerProcessor(self.config)\n        self._audio_handler = AudioHandler(self.config)\n        self._mapper = TimelineMapper()\n\n    def parse_diarization(self, data: Union[Dict, Path]) -&gt; List[DiarizationSegment]:\n        \"\"\"Parse diarization data from various sources\"\"\"\n\n    def build_chunks(self, segments: List[DiarizationSegment]) -&gt; List[SpeakerChunk]:\n        \"\"\"Create speaker chunks from segments\"\"\"\n\n    def extract_audio(self, chunks: List[SpeakerChunk], audio_file: Path) -&gt; List[SpeakerChunk]:\n        \"\"\"Extract audio for each chunk\"\"\"\n\n    def build_mapping(self, chunks: List[SpeakerChunk], original_text: TimedText) -&gt; List[TimelineMapping]:\n        \"\"\"Create timeline mappings\"\"\"\n\n    def transform_timed_text(self, original: TimedText, mappings: List[TimelineMapping]) -&gt; TimedText:\n        \"\"\"Transform timestamps back to original timeline\"\"\"\n\n    # Convenience wrapper as suggested by ChatGPT o3\n    def process(self, audio_file: Path, diarization_data: Dict, original_srt: Optional[str] = None) -&gt; ProcessingResult:\n        \"\"\"End-to-end processing convenience method\"\"\"\n</code></pre>"},{"location":"architecture/transcription/design/diarization-chunker-design/#5-audio-isolation-layer","title":"5. Audio Isolation Layer","text":"<p>Implementing ChatGPT o3's excellent suggestion:</p> <pre><code>class AudioHandler:\n    \"\"\"Isolates audio operations and external dependencies\"\"\"\n\n    def __init__(self, config: ChunkerConfig):\n        self.config = config\n        self._cache = {} if config.cache_temp_files else None\n\n    def slice_audio(self, path: Path, start_ms: int, end_ms: int) -&gt; bytes:\n        \"\"\"Extract audio segment with caching\"\"\"\n\n    def add_silence(self, audio_data: bytes, duration_ms: int) -&gt; bytes:\n        \"\"\"Add silence padding\"\"\"\n\n    def combine_segments(self, segments: List[bytes]) -&gt; bytes:\n        \"\"\"Combine multiple audio segments\"\"\"\n</code></pre>"},{"location":"architecture/transcription/design/diarization-chunker-design/#6-timeline-mapping-refinements","title":"6. Timeline Mapping Refinements","text":"<p>Separating pure mapping logic from SRT I/O:</p> <pre><code>class TimelineMapper:\n    \"\"\"Pure timeline mapping utilities\"\"\"\n\n    def build_overlap_map(self, chunks: List[SpeakerChunk], timed_text: TimedText) -&gt; List[TimelineMapping]:\n        \"\"\"Create mapping based on overlap algorithm\"\"\"\n\n    def find_best_interval(self, target_start: int, target_end: int, \n                          intervals: List[Tuple[int, int]]) -&gt; Optional[Tuple[int, int]]:\n        \"\"\"Find interval with maximum overlap\"\"\"\n\n    def transform_timestamp(self, timestamp_ms: int, mapping: TimelineMapping) -&gt; int:\n        \"\"\"Apply single timestamp transformation\"\"\"\n</code></pre>"},{"location":"architecture/transcription/design/diarization-chunker-design/#7-testing-strategy-enhanced","title":"7. Testing Strategy (Enhanced)","text":"<p>Incorporating ChatGPT o3's testing suggestions:</p> <ol> <li>Unit tests for each module</li> <li>Property-based testing with Hypothesis for mapping algorithms</li> <li>Golden-file tests for end-to-end validation</li> <li>Mock-based tests for audio operations</li> </ol> <pre><code># Example property test structure\nfrom hypothesis import given, strategies as st\n\n@given(\n    chunks=st.lists(chunk_strategy(), min_size=1),\n    timed_text=timed_text_strategy()\n)\ndef test_mapping_preserves_order(chunks, timed_text):\n    \"\"\"Property: mappings maintain chronological order\"\"\"\n</code></pre>"},{"location":"architecture/transcription/design/diarization-chunker-design/#8-migration-plan","title":"8. Migration Plan","text":"<p>Adopting ChatGPT o3's incremental approach:</p> <ol> <li>Phase 1: Create models.py and config.py</li> <li>Phase 2: Implement audio.py to isolate ffmpeg</li> <li>Phase 3: Move core logic to speaker_processor.py</li> <li>Phase 4: Implement mapping.py with pure functions</li> <li>Phase 5: Create chunker.py facade</li> <li>Phase 6: Add comprehensive tests</li> <li>Phase 7: Write notebook examples</li> </ol>"},{"location":"architecture/transcription/design/diarization-chunker-design/#9-enhanced-integration-example","title":"9. Enhanced Integration Example","text":"<pre><code># Notebook usage with step-by-step inspection\nchunker = DiarizationChunker(ChunkerConfig(\n    target_duration_ms=300000,\n    audio_format=\"mp3\"\n))\n\n# Step-by-step processing\nsegments = chunker.parse_diarization(diarization_data)\nprint(f\"Found {len(segments)} segments\")\n\nchunks = chunker.build_chunks(segments)\nprint(f\"Created {len(chunks)} chunks\")\n\nchunks_with_audio = chunker.extract_audio(chunks, audio_file)\nmappings = chunker.build_mapping(chunks_with_audio, original_timed_text)\n\n# Or use convenience method\nresult = chunker.process(audio_file, diarization_data, original_srt)\n</code></pre>"},{"location":"architecture/transcription/design/diarization-chunker-design/#10-additional-considerations","title":"10. Additional Considerations","text":"<p>Based on ChatGPT o3's open questions:</p> <ol> <li>Multiple diarization sources: Create an abstract <code>DiarizationParser</code> protocol</li> <li>Concurrency: Add optional ThreadPoolExecutor for audio operations</li> <li>Error tolerance: Make chunk merging behavior configurable</li> </ol> <pre><code>from typing import Protocol\n\nclass DiarizationParser(Protocol):\n    \"\"\"Protocol for different diarization sources\"\"\"\n    def parse(self, data: Any) -&gt; List[DiarizationSegment]:\n        ...\n</code></pre>"},{"location":"architecture/transcription/design/diarization-chunker-design/#benefits-of-synthesized-design","title":"Benefits of Synthesized Design","text":"<p>This combined approach offers:</p> <ol> <li>Better isolation: Audio operations cleanly separated</li> <li>Environmental flexibility: BaseSettings enables easy configuration</li> <li>Incremental processing: Step-by-step methods for notebook exploration</li> <li>Robust testing: Multiple testing strategies for reliability</li> <li>Future extensibility: Clear extension points for new features</li> <li>Cache optimization: Optional caching for performance</li> <li>Clean migration path: Incremental steps from PoC to production</li> </ol> <p>This synthesized design takes the best ideas from both approaches, creating a more robust, flexible, and maintainable solution that's still suitable for the alpha development phase.</p>"},{"location":"architecture/transcription/design/diarization-system-design/","title":"Diarization System Design","text":"<p>Detailed architecture for the diarization pipeline, covering segmentation, track extraction, and transcript remapping.</p>"},{"location":"architecture/transcription/design/diarization-system-design/#1-system-overview","title":"1. System Overview","text":"<p>The diarization system identifies and separates different speakers in audio content, enabling speaker-specific processing of transcripts and audio. The system has evolved from the successful proof-of-concept implementation, which demonstrated effective speaker separation, audio track extraction, and timeline mapping for SRT generation.</p> <p>The system follows these high-level steps: 1. Process audio through remote diarization service (PyAnnote) 2. Identify and merge speaker segments 3. Extract speaker-specific audio tracks with timeline mapping 4. Generate and process transcriptions for each speaker 5. Remap transcription timelines to original audio (for integrated presentation)</p>"},{"location":"architecture/transcription/design/diarization-system-design/#core-design-principles","title":"Core Design Principles","text":"<ul> <li>Modularity: Independent components with clear responsibilities</li> <li>Single-Action Methods: Each method performs a single logical operation</li> <li>Timeline Integrity: Precise tracking of timestamps across transformations</li> <li>Extensibility: Support for multiple output formats and processing pipelines</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#2-system-architecture","title":"2. System Architecture","text":"<pre><code>graph TD\n    A[Audio File] --&gt; B[PyAnnote Client]\n    B --&gt; |DiarizationResult| C[Diarization Processor]\n    C --&gt; |Speaker Segments| D[Speaker Track Manager]\n    D --&gt; |Speaker Tracks| E[Audio Extractor]\n    E --&gt; F1[Speaker Audio 1]\n    E --&gt; F2[Speaker Audio 2]\n    F1 --&gt; G1[Transcription Engine]\n    F2 --&gt; G2[Transcription Engine]\n    G1 --&gt; H1[Speaker SRT 1]\n    G2 --&gt; H2[Speaker SRT 2]\n    D --&gt; |Timeline Maps| I[SRT Timeline Mapper]\n    H1 --&gt; I\n    H2 --&gt; I\n    I --&gt; J[Mapped SRTs]\n\n    subgraph \"External Components\"\n        G1\n        G2\n    end</code></pre>"},{"location":"architecture/transcription/design/diarization-system-design/#3-core-components","title":"3. Core Components","text":""},{"location":"architecture/transcription/design/diarization-system-design/#31-pyannote-client","title":"3.1 PyAnnote Client","text":"<p>Handles communication with the PyAnnote API for speaker diarization:</p> <ul> <li>Responsibilities:</li> <li>Authentication and session management</li> <li>Media upload and URL generation</li> <li>Job submission and status tracking</li> <li> <p>Result retrieval and parsing</p> </li> <li> <p>Key Classes:</p> </li> <li><code>PyannoteClient</code>: Main interface to the API</li> <li><code>PyannoteConfig</code>: Configuration constants</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#32-diarization-models","title":"3.2 Diarization Models","text":"<p>Core data structures representing diarization information:</p> <ul> <li>Key Classes:</li> <li><code>DiarizationSegment</code>: Individual speaker segment with timing</li> <li><code>DiarizationResult</code>: Complete result set with metadata</li> <li> <p><code>SpeakerTrack</code>: Collection of segments for a single speaker</p> </li> <li> <p>Responsibilities:</p> </li> <li>Define consistent data structures</li> <li>Maintain segment information</li> <li>Support conversion between formats</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#33-diarization-processor","title":"3.3 Diarization Processor","text":"<p>Coordinates the diarization workflow:</p> <ul> <li>Responsibilities:</li> <li>Orchestrate the diarization process</li> <li>Handle result processing</li> <li>Manage speaker segment merging</li> <li> <p>Generate speaker tracks</p> </li> <li> <p>Key Methods:</p> </li> <li><code>diarize()</code>: Complete diarization process</li> <li><code>process_segments()</code>: Process raw segments</li> <li><code>merge_speaker_segments()</code>: Combine adjacent segments</li> <li><code>build_speaker_tracks()</code>: Generate timeline maps</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#34-speaker-track-manager","title":"3.4 Speaker Track Manager","text":"<p>Manages speaker-specific audio tracks and timeline mapping:</p> <ul> <li>Responsibilities:</li> <li>Create and manage speaker track objects</li> <li>Build timeline mapping tables</li> <li> <p>Export track metadata</p> </li> <li> <p>Key Classes:</p> </li> <li><code>SpeakerTrack</code>: Represents a speaker's audio track</li> <li><code>TimelineMap</code>: Maps between original and track timelines</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#35-audio-extractor","title":"3.5 Audio Extractor","text":"<p>Handles extraction of speaker-specific audio:</p> <ul> <li>Responsibilities:</li> <li>Extract audio segments from original file</li> <li>Combine segments into speaker tracks</li> <li>Handle silence/gap insertion</li> <li> <p>Export audio tracks</p> </li> <li> <p>Key Methods:</p> </li> <li><code>extract_segments()</code>: Extract speaker segments</li> <li><code>build_track()</code>: Combine segments into a track</li> <li><code>save_track()</code>: Export audio track to file</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#36-srt-timeline-mapper","title":"3.6 SRT Timeline Mapper","text":"<p>Transforms SRT files between different timelines:</p> <ul> <li>Responsibilities:</li> <li>Parse SRT files</li> <li>Map timestamps between timelines</li> <li> <p>Generate remapped SRT files</p> </li> <li> <p>Key Methods:</p> </li> <li><code>parse_srt()</code>: Parse SRT file into objects</li> <li><code>map_timeline()</code>: Apply timeline mapping</li> <li><code>generate_srt()</code>: Create SRT file with new timestamps</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#4-process-flow","title":"4. Process Flow","text":""},{"location":"architecture/transcription/design/diarization-system-design/#41-diarization-process","title":"4.1 Diarization Process","text":"<pre><code>sequenceDiagram\n    participant User\n    participant PyannoteClient\n    participant DiarizationProcessor\n    participant SpeakerTrackManager\n    participant AudioExtractor\n\n    User-&gt;&gt;DiarizationProcessor: diarize(audio_file)\n    DiarizationProcessor-&gt;&gt;PyannoteClient: upload_audio(audio_file)\n    PyannoteClient--&gt;&gt;DiarizationProcessor: media_id\n    DiarizationProcessor-&gt;&gt;PyannoteClient: start_diarization(media_id)\n    PyannoteClient--&gt;&gt;DiarizationProcessor: job_id\n    DiarizationProcessor-&gt;&gt;PyannoteClient: poll_until_complete(job_id)\n    PyannoteClient--&gt;&gt;DiarizationProcessor: diarization_result\n\n    DiarizationProcessor-&gt;&gt;DiarizationProcessor: process_segments(result)\n    DiarizationProcessor-&gt;&gt;DiarizationProcessor: merge_speaker_segments()\n    DiarizationProcessor-&gt;&gt;SpeakerTrackManager: build_speaker_tracks()\n    SpeakerTrackManager--&gt;&gt;DiarizationProcessor: speaker_tracks\n\n    DiarizationProcessor-&gt;&gt;AudioExtractor: extract_speaker_audio(speaker_tracks)\n    AudioExtractor--&gt;&gt;DiarizationProcessor: exported_tracks\n    DiarizationProcessor--&gt;&gt;User: speaker_tracks</code></pre>"},{"location":"architecture/transcription/design/diarization-system-design/#42-srt-transformation-process","title":"4.2 SRT Transformation Process","text":"<pre><code>sequenceDiagram\n    participant User\n    participant SRTTimelineMapper\n    participant TranscriptionEngine\n\n    User-&gt;&gt;TranscriptionEngine: transcribe_tracks(speaker_tracks)\n    TranscriptionEngine--&gt;&gt;User: speaker_srt_files\n\n    User-&gt;&gt;SRTTimelineMapper: transform_srt_files(speaker_srt_files, timeline_maps)\n    SRTTimelineMapper-&gt;&gt;SRTTimelineMapper: parse_srt_files()\n    SRTTimelineMapper-&gt;&gt;SRTTimelineMapper: remap_timestamps()\n    SRTTimelineMapper-&gt;&gt;SRTTimelineMapper: generate_output_files()\n    SRTTimelineMapper--&gt;&gt;User: mapped_srt_files</code></pre>"},{"location":"architecture/transcription/design/diarization-system-design/#5-data-models","title":"5. Data Models","text":""},{"location":"architecture/transcription/design/diarization-system-design/#51-diarizationsegment","title":"5.1 DiarizationSegment","text":"<p>Represents a single speaker segment with timing information:</p> <ul> <li><code>speaker</code>: String identifier for the speaker</li> <li><code>start</code>: Start time in seconds</li> <li><code>end</code>: End time in seconds</li> <li><code>duration</code>: Calculated property for segment duration</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#52-speakertrack","title":"5.2 SpeakerTrack","text":"<p>Collection of segments belonging to a single speaker:</p> <ul> <li><code>speaker</code>: Speaker identifier</li> <li><code>segments</code>: List of DiarizationSegment objects</li> <li><code>timeline_map</code>: Maps original timeline to track timeline</li> <li>Methods for adding segments and exporting audio</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#53-timelinemap","title":"5.3 TimelineMap","text":"<p>Maps between original and track timelines:</p> <ul> <li><code>intervals</code>: List of TimelineInterval objects</li> <li>Methods for mapping timestamps and exporting mapping data</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#54-srtentry","title":"5.4 SRTEntry","text":"<p>Represents a single subtitle entry:</p> <ul> <li><code>index</code>: Entry index number</li> <li><code>start_time</code>: Start time in SRT format (HH:MM:SS,mmm)</li> <li><code>end_time</code>: End time in SRT format (HH:MM:SS,mmm)</li> <li><code>text</code>: Subtitle text</li> <li>Properties for time conversion between formats</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#6-integration-points","title":"6. Integration Points","text":""},{"location":"architecture/transcription/design/diarization-system-design/#61-audio-processing-integration","title":"6.1 Audio Processing Integration","text":"<p>The system integrates with existing audio processing tools:</p> <ul> <li>Integration with pydub for audio manipulation</li> <li>Support for various audio formats and codecs</li> <li>Extraction and combination of audio segments</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#62-transcription-integration","title":"6.2 Transcription Integration","text":"<p>The system allows integration with different transcription engines:</p> <ul> <li>Configurable transcription engine selection</li> <li>Support for different output formats</li> <li>Integration with existing transcription workflows</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#63-cli-integration","title":"6.3 CLI Integration","text":"<p>Integration with the audio-transcribe CLI:</p> <ul> <li>Diarization options as command-line flags</li> <li>Configuration for speaker gap handling</li> <li>Output format selection options</li> <li>Integration with existing processing pipelines</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#7-extension-points","title":"7. Extension Points","text":""},{"location":"architecture/transcription/design/diarization-system-design/#71-alternative-diarization-services","title":"7.1 Alternative Diarization Services","text":"<p>Support for different diarization services through an abstract interface:</p> <ul> <li>Common API for different diarization providers</li> <li>Standardized result processing</li> <li>Pluggable service implementations</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#72-custom-speaker-merging-strategies","title":"7.2 Custom Speaker Merging Strategies","text":"<p>Customizable segment merging through strategy pattern:</p> <ul> <li>Configurable merging algorithms</li> <li>Different gap threshold strategies</li> <li>Speaker-specific merging rules</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#73-timeline-mapping-extensions","title":"7.3 Timeline Mapping Extensions","text":"<p>Support for different timeline mapping strategies:</p> <ul> <li>Various mapping algorithms</li> <li>Handling of edge cases</li> <li>Bidirectional mapping support</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#8-implementation-considerations","title":"8. Implementation Considerations","text":""},{"location":"architecture/transcription/design/diarization-system-design/#81-error-handling","title":"8.1 Error Handling","text":"<p>In the production implementation:</p> <ul> <li>Graceful Recovery: Handle API failures with retries</li> <li>Validation: Ensure segment integrity before processing</li> <li>User Feedback: Provide clear error messages</li> <li>Safe Defaults: Use reasonable defaults when possible</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#82-performance-optimization","title":"8.2 Performance Optimization","text":"<p>For handling large audio files:</p> <ul> <li>Streaming Processing: Process large files in chunks</li> <li>Parallel Processing: Extract speaker tracks in parallel</li> <li>Memory Management: Avoid loading entire audio into memory</li> <li>Caching: Cache intermediate results where appropriate</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#83-testing-strategy","title":"8.3 Testing Strategy","text":"<p>Comprehensive testing approach:</p> <ul> <li>Unit Tests: Test individual components in isolation</li> <li>Integration Tests: Test component interactions</li> <li>Acceptance Tests: Test end-to-end workflows</li> <li>Performance Tests: Verify handling of large files</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#84-documentation-strategy","title":"8.4 Documentation Strategy","text":"<p>Documentation requirements:</p> <ul> <li>API Documentation: Document public interfaces</li> <li>Usage Examples: Provide common usage patterns</li> <li>Integration Guide: Document integration points</li> <li>Configuration Reference: Document configuration options</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#9-future-considerations","title":"9. Future Considerations","text":""},{"location":"architecture/transcription/design/diarization-system-design/#91-speaker-identification","title":"9.1 Speaker Identification","text":"<p>Future support for speaker identification:</p> <ul> <li>Integration with voice recognition systems</li> <li>Speaker profile management</li> <li>Consistent speaker labeling across files</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#92-multi-language-support","title":"9.2 Multi-language Support","text":"<p>Enhanced language handling:</p> <ul> <li>Language detection per speaker</li> <li>Language-specific transcription models</li> <li>Cross-language speaker tracking</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#93-advanced-audio-processing","title":"9.3 Advanced Audio Processing","text":"<p>Future audio processing capabilities:</p> <ul> <li>Background noise reduction</li> <li>Audio quality enhancement</li> <li>Speaker audio normalization</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#94-interactive-visualization","title":"9.4 Interactive Visualization","text":"<p>User interface enhancements:</p> <ul> <li>Interactive timeline visualization</li> <li>Speaker track playback controls</li> <li>Waveform display with speaker highlighting</li> </ul>"},{"location":"architecture/transcription/design/diarization-system-design/#95-batch-processing","title":"9.5 Batch Processing","text":"<p>Support for batch operations:</p> <ul> <li>Process multiple files</li> <li>Generate consistent speaker IDs across files</li> <li>Aggregate statistics across multiple recordings</li> </ul>"},{"location":"architecture/transcription/design/interval-to-segment-mapping/","title":"Interval-to-Segment Mapping Algorithm","text":"<p>Algorithm for mapping chunk-relative transcription intervals back to diarization segments using overlap and proximity.</p>"},{"location":"architecture/transcription/design/interval-to-segment-mapping/#terminology","title":"Terminology","text":"<ul> <li>Interval: A TimedTextUnit from the TimedText module, representing a unit of text with start and end times in the chunk-relative timeline</li> <li>Segment: A unit of speech from the diarization process, representing a span of speech in the original audio timeline</li> <li>Mapping: The process of transforming interval timings from chunk-relative coordinates to original audio coordinates</li> </ul>"},{"location":"architecture/transcription/design/interval-to-segment-mapping/#core-algorithm","title":"Core Algorithm","text":"<p>For each interval in the TimedText object, we need to find the appropriate diarization segment to use for mapping:</p> <ol> <li>First attempt direct overlap matching:</li> <li>For a given interval, find all diarization segments that directly overlap with it</li> <li>If any overlaps exist, use the segment with the largest overlap</li> <li> <p>Use this segment for timeline mapping</p> </li> <li> <p>If no overlap found, use proximal segment matching:</p> </li> <li>Find the nearest segment that ends before the interval starts</li> <li>Find the nearest segment that starts after the interval ends</li> <li>Calculate gap distances to each proximal segment</li> <li>Choose the segment with the smaller gap (or the earlier one if equal)</li> <li> <p>Use this segment for timeline mapping</p> </li> <li> <p>Apply the mapping transformation:</p> </li> <li>Once the best segment is identified, apply a timeline shift:<ul> <li>Calculate the offset of the interval start relative to segment's local start</li> <li>Add this offset to the segment's original start time</li> <li>Apply the same transformation to the interval's end time</li> </ul> </li> </ol>"},{"location":"architecture/transcription/design/interval-to-segment-mapping/#finding-best-segment-flowchart","title":"Finding Best Segment Flowchart","text":"<pre><code>flowchart TB\n    A[For a given interval] --&gt; B[Find segments with direct overlap]\n    B --&gt; C{Any overlaps found?}\n    C --&gt;|Yes| D[Use segment with largest overlap]\n    C --&gt;|No| E[Find proximal segments]\n    E --&gt; F[Find nearest segment ending before interval]\n    E --&gt; G[Find nearest segment starting after interval]\n    F --&gt; H{Proximal segments found?}\n    G --&gt; H\n    H --&gt;|Both before and after| I[Calculate gap distances to each]\n    H --&gt;|Only one| J[Use the single segment found]\n    H --&gt;|None| K[Raise exception - no matching segment]\n    I --&gt; L{Equal distances?}\n    L --&gt;|Yes| M[Use the earlier segment]\n    L --&gt;|No| N[Use segment with smaller gap]</code></pre>"},{"location":"architecture/transcription/design/interval-to-segment-mapping/#gap-distance-calculation","title":"Gap Distance Calculation","text":"<p>For intervals with no direct overlap, we calculate gap distances to determine the closest segment:</p> <pre><code>flowchart LR\n    subgraph \"Timeline\"\n        direction LR\n        A[Segment A&lt;br&gt;Ends before] --- |Gap 1| B[Interval&lt;br&gt;to map] --- |Gap 2| C[Segment B&lt;br&gt;Starts after]\n    end\n\n    subgraph \"Gap Distance Calculation\"\n        direction TB\n        D1[Gap 1 = Interval.start - SegmentA.end]\n        D2[Gap 2 = SegmentB.start - Interval.end]\n    end\n\n    B -.-&gt; D1\n    B -.-&gt; D2\n    D1 --&gt; E[Compare&lt;br&gt;distances]\n    D2 --&gt; E\n    E --&gt; F[Choose segment&lt;br&gt;with smaller gap]</code></pre>"},{"location":"architecture/transcription/design/interval-to-segment-mapping/#mapping-transformation","title":"Mapping Transformation","text":"<p>Once the best segment is identified, the mapping applies a timing transformation:</p> <pre><code>flowchart TB\n    A[Identify best segment&lt;br&gt;for mapping] --&gt; B[Calculate relative offset&lt;br&gt;within segment]\n    B --&gt; C[Apply offset to&lt;br&gt;original timeline]\n\n    subgraph \"Offset Calculation\"\n        direction TB\n        D1[offset = interval.start - segment.local_start]\n        D2[new_start = segment.orig_start + offset]\n        D3[new_end = new_start + interval.duration]\n    end\n\n    B --&gt; D1\n    D1 --&gt; D2\n    D2 --&gt; D3\n    D3 --&gt; E[Return remapped interval]</code></pre>"},{"location":"architecture/transcription/design/interval-to-segment-mapping/#modular-algorithm-components","title":"Modular Algorithm Components","text":"<p>To ensure flexibility and maintainability, the algorithm is divided into these modular components:</p> <ol> <li>Overlap Calculation:</li> <li>Calculate overlap between an interval and a segment</li> <li> <p>Return overlap amount in milliseconds (0 if no overlap)</p> </li> <li> <p>Best Overlap Finding:</p> </li> <li>For a given interval, find segment with maximum overlap</li> <li> <p>Return best segment or null if no overlap</p> </li> <li> <p>Proximal Segment Finding:</p> </li> <li>Find nearest segment ending before interval starts</li> <li>Find nearest segment starting after interval ends</li> <li> <p>Return tuple of (before_segment, after_segment)</p> </li> <li> <p>Gap Distance Calculation:</p> </li> <li>For a segment that doesn't overlap with the interval</li> <li>Calculate the gap distance between them</li> <li> <p>Return distance in milliseconds</p> </li> <li> <p>Proximal Segment Selection:</p> </li> <li>Given before and after segments, calculate distances</li> <li>Choose segment with smaller distance (or earlier if equal)</li> <li> <p>Return selected segment</p> </li> <li> <p>Mapping Application:</p> </li> <li>Apply the timeline mapping transformation</li> <li>Create new interval with adjusted timestamps</li> </ol>"},{"location":"architecture/transcription/design/interval-to-segment-mapping/#edge-cases","title":"Edge Cases","text":"<ol> <li> <p>No segments available: If the diarization chunk contains no segments at all, we cannot perform mapping and must raise an exception.</p> </li> <li> <p>No overlapping or proximal segments: This should not occur if there is at least one segment, as each interval will be a either overlap with or be before or after this segment, guaranteeing that there is at least one segment that will be selected. This logic is fairly complex but it can be proven to be the case.</p> </li> <li> <p>Equal gap distances: If the gap distance to the segment before equals the gap distance to the segment after, prefer the earlier segment for consistent behavior.</p> </li> <li> <p>Zero-duration intervals or segments: These should be handled carefully to avoid division-by-zero errors in calculations. for any interval or segment with zero duration, this implies start==end. Therefore we can do a preprocessing step which searches for zero duration intervals/segments and maps the end time to be +1 of the start time. This 1 millisecond accuracy loss will generally not be consequential, but the algorithm should be clear in stating its operation in this regard.</p> </li> </ol>"},{"location":"architecture/transcription/design/interval-to-segment-mapping/#complete-algorithm","title":"Complete Algorithm","text":"<ol> <li>Find best segment for mapping</li> <li> <p>For a given interval:</p> <ul> <li>Find all segments with direct overlap</li> <li>If one or more exists, select the one with maximum overlap</li> <li>If none, find proximal segments (before and after)</li> <li>Choose best proximal segment based on gap distance</li> </ul> </li> <li> <p>If no matching segment found</p> </li> <li>Raise an exception - every interval must map to a segment</li> <li> <p>This indicates no diarization segments. In fact we can raise an error on method entry if there are no segments.</p> </li> <li> <p>Apply mapping transformation</p> </li> <li>Calculate offset = interval.start_ms - segment.local_start</li> <li>new_start_ms = segment.orig_start + offset</li> <li>new_end_ms = new_start_ms + (interval.end_ms - interval.start_ms)</li> <li>Create new interval with transformed timestamps</li> </ol> <p>By maintaining this modular approach, the algorithm can be optimized or modified in specific areas (e.g., implementing bisection search for large segment collections) without affecting the overall logic flow.</p>"},{"location":"architecture/transcription/design/language-aware-chunking-design/","title":"Simplified Language-Aware Chunking Design","text":"<p>Language-aware chunking strategy that augments diarization splits with practical language detection heuristics.</p>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#core-problem","title":"Core Problem","text":"<p>Current chunking only considers time and speaker boundaries. We need to add language boundary detection to prevent mixed-language chunks that degrade ASR performance.</p>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#core-functionality","title":"Core Functionality","text":"<ol> <li>Detect language changes within speaker blocks (when speaker has long continuous speech)</li> <li>Detect language changes between speakers</li> <li>Keep segments together when language is consistent (up to target chunk size)</li> </ol>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#basic-protocols","title":"Basic Protocols","text":"<pre><code>class ChunkingStrategy(Protocol):\n    def chunk(self, segments: List[DiarizationSegment], config: ChunkerConfig) -&gt; List[DiarizationChunk]:\n        ...\n\nclass LanguageDetector(Protocol):\n    def detect(self, audio_chunk: AudioChunk, start_ms: int = 0, duration_ms: int = 5000) -&gt; tuple[str, float]:\n        \"\"\"Returns (language_code, confidence)\"\"\"\n        ...\n\n    def detect_multiple(self, audio_samples: List[tuple[AudioChunk, int, int]]) -&gt; List[tuple[str, float]]:\n        \"\"\"Concurrent detection for multiple samples\"\"\"\n        ...\n</code></pre>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#simple-configuration","title":"Simple Configuration","text":"<pre><code>@dataclass \nclass ChunkerConfig:\n    # Existing fields\n    target_time: int = 300_000\n\n    # Language detection\n    enable_language_detection: bool = False\n    min_speaker_duration_for_probe: int = 20_000  # Only probe long speaker segments\n    sample_duration_ms: int = 5000\n    confidence_threshold: float = 0.7\n</code></pre>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#high-level-algorithm","title":"High-Level Algorithm","text":""},{"location":"architecture/transcription/design/language-aware-chunking-design/#language-aware-chunking-flow","title":"Language-Aware Chunking Flow","text":"<pre><code>flowchart TD\n    A[Diarization Segments] --&gt; B[Group by Speaker]\n    B --&gt; C{Speaker Duration &gt; Threshold?}\n    C --&gt;|Yes| D[Probe Speaker Language]\n    C --&gt;|No| E[Use Previous Language]\n    D --&gt; F[Compare with Previous Language]\n    E --&gt; F\n    F --&gt; G{Language Changed?}\n    G --&gt;|Yes| H[Split Chunk Here]\n    G --&gt;|No| I[Continue Current Chunk]\n    H --&gt; J{Chunk Size &gt; Target?}\n    I --&gt; J\n    J --&gt;|Yes| K[Finalize Chunk]\n    J --&gt;|No| L[Add More Segments]\n    K --&gt; M[Next Segments]\n    L --&gt; M\n    M --&gt; N{More Segments?}\n    N --&gt;|Yes| C\n    N --&gt;|No| O[Final Chunks]</code></pre>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#core-algorithm-logic","title":"Core Algorithm Logic","text":"<ol> <li>Initialize: Start with first speaker's segments</li> <li>For each speaker transition:</li> <li>If speaker duration &gt; threshold: probe language</li> <li>Compare language with current chunk language</li> <li>If different: split chunk, start new chunk</li> <li>If same: continue adding to current chunk</li> <li>Within speaker: Only probe if speaker has very long continuous segments</li> <li>Size check: Split chunk if target size exceeded regardless of language</li> </ol>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#simple-implementation-outline","title":"Simple Implementation Outline","text":"<pre><code>class LanguageAwareStrategy:\n    def __init__(self, detector: LanguageDetector):\n        self.detector = detector\n\n    def chunk(self, segments: List[DiarizationSegment], config: ChunkerConfig) -&gt; List[DiarizationChunk]:\n        chunks = []\n        current_chunk_segments = []\n        current_language = None\n\n        speaker_groups = self._group_by_speaker(segments)\n\n        for speaker_group in speaker_groups:\n            # Probe language if speaker duration is significant\n            if self._should_probe_language(speaker_group, config):\n                detected_language = self._probe_speaker_language(speaker_group, config)\n            else:\n                detected_language = current_language  # Inherit previous\n\n            # Check for language change\n            if (current_language is not None and \n                detected_language != current_language and \n                current_chunk_segments):\n                # Language changed - finalize current chunk\n                chunks.append(self._create_chunk(current_chunk_segments, current_language))\n                current_chunk_segments = []\n\n            # Add speaker segments to current chunk\n            current_chunk_segments.extend(speaker_group)\n            current_language = detected_language\n\n            # Check size limit\n            if self._chunk_too_large(current_chunk_segments, config):\n                chunks.append(self._create_chunk(current_chunk_segments, current_language))\n                current_chunk_segments = []\n\n        # Handle final chunk\n        if current_chunk_segments:\n            chunks.append(self._create_chunk(current_chunk_segments, current_language))\n\n        return chunks\n</code></pre>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#concurrency-requirements","title":"Concurrency Requirements","text":""},{"location":"architecture/transcription/design/language-aware-chunking-design/#whisperservice-thread-safety","title":"WhisperService Thread Safety","text":"<pre><code>class ConcurrentLanguageDetector:\n    def __init__(self, max_workers: int = 3):\n        self.max_workers = max_workers\n        self._thread_local = threading.local()\n\n    def _get_whisper_service(self) -&gt; WhisperTranscriptionService:\n        \"\"\"Thread-local whisper service instance\"\"\"\n        if not hasattr(self._thread_local, 'service'):\n            self._thread_local.service = WhisperTranscriptionService()\n        return self._thread_local.service\n</code></pre> <p>Key Requirements:</p> <ul> <li>Thread-local WhisperService instances (avoid shared state)</li> <li>Concurrent audio sample processing</li> <li>Simple result aggregation</li> </ul>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#decision-points","title":"Decision Points","text":""},{"location":"architecture/transcription/design/language-aware-chunking-design/#when-to-probe-language","title":"When to Probe Language","text":"<ul> <li>Always probe: When speaker duration &gt; <code>min_speaker_duration_for_probe</code></li> <li>Never probe: Short speaker segments (inherit previous language)</li> <li>Future enhancement: Probe within very long single-speaker segments</li> </ul>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#language-change-handling","title":"Language Change Handling","text":"<ul> <li>High confidence change: Split chunk immediately</li> <li>Low confidence: Continue current chunk (avoid false splits)</li> <li>No previous language: Use detected language as baseline</li> </ul>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#simple-data-flow","title":"Simple Data Flow","text":"<pre><code>sequenceDiagram\n    participant C as Chunker\n    participant D as LanguageDetector\n    participant W as WhisperService\n\n    loop For each speaker group\n        C-&gt;&gt;C: Check if should probe\n        alt Duration &gt; threshold\n            C-&gt;&gt;D: Probe language\n            D-&gt;&gt;W: Detect language (concurrent)\n            W-&gt;&gt;D: Language + confidence\n            D-&gt;&gt;C: Detection result\n        else Too short\n            C-&gt;&gt;C: Use previous language\n        end\n\n        C-&gt;&gt;C: Compare with current chunk language\n        alt Language changed\n            C-&gt;&gt;C: Finalize current chunk\n            C-&gt;&gt;C: Start new chunk\n        else Same language\n            C-&gt;&gt;C: Add to current chunk\n        end\n    end</code></pre>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#minimal-extensions","title":"Minimal Extensions","text":""},{"location":"architecture/transcription/design/language-aware-chunking-design/#future-hooks","title":"Future Hooks","text":"<ul> <li>Energy-based detection: Add energy threshold checks before language probing</li> <li>Topic awareness: Add semantic similarity checks</li> <li>Adaptive thresholds: Adjust confidence thresholds based on audio quality</li> </ul>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#configuration-expansion","title":"Configuration Expansion","text":"<pre><code># Future config additions (placeholders)\nadaptive_thresholds: bool = False  # Future: adjust based on audio quality\nprobe_long_segments: bool = False  # Future: probe within long single-speaker segments\n</code></pre>"},{"location":"architecture/transcription/design/language-aware-chunking-design/#success-criteria","title":"Success Criteria","text":"<ul> <li>Functional: Chunks have consistent language when confidence is high</li> <li>Performance: Language detection doesn't significantly slow processing</li> <li>Simple: Easy to understand and modify algorithm</li> <li>Extensible: Clear hooks for future enhancements</li> </ul> <p>This simplified approach focuses on the core language boundary detection while maintaining the extensibility needed for future improvements.</p>"},{"location":"architecture/transcription/design/language-aware-chunking-orchestrator-notes/","title":"Language-Aware Chunking Orchestrator Notes","text":"<p>Working notes for extending the DiarizationChunker orchestrator with language-aware strategies.</p> <p>Design Proposal: Language-Aware Chunking Extension for diarization_chunker</p> <ol> <li>Background</li> </ol> <p>DiarizationChunker currently supports two implicit strategies     1.  Contiguous-Time (no speaker split)     2.  Speaker-Aware (split at speaker change)</p> <p>Both rely exclusively on timing / speaker labels and treat language as uniform.</p> <p>\u2e3b</p> <ol> <li>Objectives</li> </ol> <p>Goal    Why it matters Language-Purity per Chunk   Whisper (and most ASR models) transcribe best when each input clip is single-language. Pluggable Strategies    Future research (e.g., energy-based chunking, silence detection) should drop in without core refactors. Compact, Single-Purpose Methods Improves testability &amp; maintenance. Config-Driven   Users should tune via settings rather than editing code.</p> <p>\u2e3b</p> <ol> <li> <p>High-Level Architecture</p> <pre><code>     +--------------------+\n</code></pre> <p>audio ----&gt; | DiarizationChunker |  (orchestrator, owns config)          +---------+----------+                    |                    v          +--------------------+                       +----------------+          | IChunkingStrategy  |&lt;--- dependency delve --| LanguageProbe |          +---------+----------+                       +----------------+                    ^                                           ^      (Strategy 1)  |                                           |                    | (Strategy N)                              |        +--------------------+                         +-------------------+        | DefaultStrategy    |                         | LanguageAwareStrategy |        +--------------------+                         +-------------------+</p> <p>\u2022   IChunkingStrategy (Protocol / ABC)</p> </li> </ol> <p>class IChunkingStrategy(Protocol):     def extract(self, segments: list[DiarizationSegment]) -&gt; list[DiarizationChunk]: ...</p> <pre><code>\u2022   DiarizationChunker becomes a light fa\u00e7ade\n\u2022   Accepts strategy: IChunkingStrategy via constructor or config name.\n\u2022   Delegates all chunk creation to that strategy\u2014no branching inside.\n\u2022   LanguageProbe helper\n\u2022   Stateless (or memoized) service:\n\u2022   probe(audio: AudioChunk, *, sample_ms: int, tries: int, randomize: bool) -&gt; str\n\u2022   Wraps Whisper\u2010tiny / faster language ID.\n</code></pre> <p>\u2e3b</p> <ol> <li>New Strategy: LanguageAwareStrategy</li> </ol> <p>Step    Action  Notes 1   Group contiguous segments by speaker    Reuse existing _gap_time, _speaker_change. 2   Compute speaker-block duration  Skip probing if block &lt; min_probe_block_ms. 3   Sample audio from each block    Either deterministic (mid-segment) or random.sample. 4   Detect language via LanguageProbe   Early-exit if all blocks agree on language. 5   Split chunk if mixed languages  Options:\u2022 Hard split at first language boundary\u2022 Greedy accumulate until mismatch\u2022 Recursively bisect noisy blocks 6   Return list[DiarizationChunk]   Each chunk carries a language attribute (extend DiarizationChunk).</p> <p>Config Additions (suggested):</p> <p>Field   Type    Default Description lang_probe_sample_ms    int 2000    Milliseconds per sample. lang_probe_tries    int 3   Number of probes per block. lang_split_enabled  bool    True    Gate for runtime enable/disable. max_speakers_for_probe  int 2   Skip whole strategy if count greater. min_probe_block_ms  int 15 000  Don\u2019t probe very short blocks.</p> <p>\u2e3b</p> <ol> <li>Modifications to Existing Models</li> </ol> <p>Class   Change DiarizationChunk    Add language: Optional[str] = None ChunkerConfig   Extend with new language-probe fields. AudioChunk (if available)   Provide lightweight .slice(start_ms, end_ms) for probe sampling.</p> <p>\u2e3b</p> <ol> <li>Example (Illustrative Only)</li> </ol>"},{"location":"architecture/transcription/design/language-aware-chunking-orchestrator-notes/#orchestrator","title":"orchestrator","text":"<p>chunker = DiarizationChunker(     strategy=LanguageAwareStrategy(config=ChunkerConfig(lang_split_enabled=True)) )</p> <p>chunks = chunker.chunk(diarization_segments) for ch in chunks:     print(ch.language, ch.total_duration_sec)</p> <p>\u2e3b</p> <ol> <li>Open Questions &amp; Options</li> </ol> <p>Topic   Choices &amp; Trade-offs Language detector   Whisper-tiny vs. fasttext vs. pycld3. Whisper is heavier but aligned with final ASR. Granularity of split    Split at block boundary vs. mid-block split when detection disagrees? Parallelism Probe calls can run concurrently; decide between concurrent.futures or asyncio. Caching Same speaker ID across chunks might reuse earlier language result. Testing Need synthetic mixed-language audio fixtures. Fallback    If detector returns \"unknown\" above threshold, skip splitting to avoid over-fragmentation.</p> <p>\u2e3b</p> <ol> <li>Migration Plan<ol> <li>Introduce interfaces.py with IChunkingStrategy, LanguageDetector.</li> <li>Refactor existing logic out of DiarizationChunker into DefaultStrategy.</li> <li>Add unit tests ensuring parity with current outputs for default strategy.</li> <li>Implement LanguageProbe (start with Whisper CLI call wrapper).</li> <li>Develop LanguageAwareStrategy iteratively: \u2022   Phase-0: Probe once per speaker, no splitting\u2014just annotate language. \u2022   Phase-1: Enable optional split on mismatch.</li> <li>Benchmark transcription WER before &amp; after on bilingual test set.</li> <li>Update docs &amp; example notebooks.</li> </ol> </li> </ol> <p>\u2e3b</p> <ol> <li>Future Extensions     \u2022   Energy-AwareStrategy (split on low RMS zones).     \u2022   Topic-AwareStrategy (use embedding similarity between segments).     \u2022   Dynamic-Target-Duration (adjust chunk length based on compute budget).</li> </ol> <p>\u2e3b</p> <ol> <li>Summary</li> </ol> <p>By extracting chunking logic into interchangeable strategies and adding a LanguageAwareStrategy that probes speaker blocks for language shifts, we achieve:     \u2022   Cleaner separation of concerns     \u2022   Config-driven experimentation     \u2022   Improved ASR accuracy on mixed-language recordings     \u2022   Foundation for future chunking heuristics</p> <p>\u2e3b</p>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/","title":"Modular Pipeline Design: Best Practices for Audio Transcription and Diarization","text":"<p>This document summarizes a detailed design and refactoring discussion on building a clean, modular, and production-ready audio transcription pipeline, with a focus on diarization chunking and robust system structure. It includes architectural patterns, file organization, and code hygiene practices.</p>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#1-overview-of-pipeline-structure","title":"1. Overview of Pipeline Structure","text":"<p>The pipeline under design:</p> <pre><code>DiarizationChunker (input: diarization JSON)\n    \u2192 AudioHandler (input: Chunk, output: Chunk + AudioChunk)\n    \u2192 TranscriptionService (input: AudioChunk, output: transcription dict)\n    \u2192 TimedText (canonical timing + text model)\n    \u2192 TimelineMapper (align TimedText with global timeline)\n    \u2192 SRTProcessor (render as SRT/VTT)\n</code></pre>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#2-six-key-modular-design-suggestions","title":"2. Six Key Modular Design Suggestions","text":""},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#21-narrow-interfaces-ports-adapters-hexagonal-architecture","title":"2.1 Narrow Interfaces: Ports &amp; Adapters (Hexagonal Architecture)","text":"<p>Goal: Separate domain logic from infrastructure (e.g., Whisper, AssemblyAI).</p> <ul> <li>Port: A <code>Protocol</code> that defines a required interface.</li> <li>Adapter: A class implementing that interface using a specific backend.</li> </ul> <p>Example:</p> <pre><code>class TranscriptionProvider(Protocol):\n    def transcribe(self, audio: BytesIO) -&gt; Dict[str, Any]: ...\n</code></pre> <p>This allows:</p> <ul> <li>Testing with mocks</li> <li>Easy backend swapping</li> <li>Clear data flow</li> </ul>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#22-pipelinechain-of-responsibility-style","title":"2.2 Pipeline/Chain-of-Responsibility Style","text":"<p>Define a base interface for composable pipeline stages:</p> <pre><code>class PipelineStage(ABC, Generic[I, O]):\n    def __call__(self, item: I) -&gt; O: ...\n</code></pre> <p>Then enable chaining via <code>|</code> or functional composition:</p> <pre><code>pipeline = StageA() | StageB() | StageC()\nresult = pipeline(data)\n</code></pre>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#23-strategy-pattern-for-chunking","title":"2.3 Strategy Pattern for Chunking","text":"<p>Avoid boolean flags like <code>split_on_speaker_change</code>; define chunking behaviors as strategies:</p> <pre><code>class ChunkingStrategy(Protocol):\n    def should_split(self, segment: Segment, chunk: Chunk) -&gt; bool: ...\n</code></pre> <p>Ship <code>TimeBasedStrategy</code>, <code>SpeakerChangeStrategy</code>, etc.</p>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#24-event-hooks-pub-sub-for-observability","title":"2.4 Event Hooks / Pub-Sub for Observability","text":"<p>Use an <code>EventBus</code> or hook system to emit structured events:</p> <pre><code>bus.emit(ChunkCreated(chunk_id, duration))\n</code></pre> <p>This supports: logging, progress bars, tracing, or telemetry later.</p>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#25-standardization-of-time-units","title":"2.5 Standardization of Time Units","text":"<p>Internally, use milliseconds everywhere. Convert to <code>HH:MM:SS,mmm</code> only in renderers.</p> <ul> <li>Reduces rounding bugs</li> <li>Easier arithmetic</li> </ul>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#26-immutable-streamable-design","title":"2.6 Immutable, Streamable Design","text":"<ul> <li>Keep core models immutable (e.g., <code>Chunk</code>, <code>Segment</code>)</li> <li>Let pipeline stages operate on <code>Iterable[T]</code></li> <li>Easier to parallelize or lazily stream</li> </ul>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#3-protocol-definitions-for-pipeline","title":"3. Protocol Definitions for Pipeline","text":"Stage Protocol Name Method Input Output Chunking <code>ChunkExtractor</code> <code>to_chunks</code> <code>dict</code> <code>List[Chunk]</code> Attach Audio <code>AudioProvider</code> <code>attach_audio</code> <code>Chunk</code> <code>Chunk</code> Transcription <code>TranscriptionProvider</code> <code>transcribe</code> <code>BytesIO</code> <code>Dict[str, Any]</code> TimedText Builder <code>TimedTextBuilder</code> <code>build</code> <code>dict</code> <code>TimedText</code> Timeline Mapper <code>TimelineMapper</code> <code>map</code> <code>TimedText, Chunk</code> <code>TimedText</code> Subtitle Render <code>SRTBuilder</code> <code>to_srt</code> <code>TimedText</code> <code>str</code>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#4-recommended-file-structure","title":"4. Recommended File Structure","text":"<p>A light modular structure suitable for the current project phase:</p> <pre><code>audio_processing/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 chunk.py\n\u2502   \u251c\u2500\u2500 segment.py\n\u2502   \u251c\u2500\u2500 audio_chunk.py\n\u2502   \u2514\u2500\u2500 timed_text.py\n\u251c\u2500\u2500 protocols/\n\u2502   \u251c\u2500\u2500 chunker.py\n\u2502   \u251c\u2500\u2500 audio_provider.py\n\u2502   \u251c\u2500\u2500 transcription_provider.py\n\u2502   \u251c\u2500\u2500 timedtext_builder.py\n\u2502   \u251c\u2500\u2500 timeline_mapper.py\n\u2502   \u2514\u2500\u2500 srt_renderer.py\n\u251c\u2500\u2500 adapters/\n\u2502   \u251c\u2500\u2500 whisper_service.py\n\u2502   \u251c\u2500\u2500 assemblyai_service.py\n\u2502   \u2514\u2500\u2500 local_audio_handler.py\n\u251c\u2500\u2500 processors/\n\u2502   \u251c\u2500\u2500 diarization_chunker.py\n\u2502   \u251c\u2500\u2500 srt_processor.py\n\u2502   \u2514\u2500\u2500 timeline_mapper.py\n\u251c\u2500\u2500 services/\n\u2502   \u251c\u2500\u2500 transcription_service.py\n\u2502   \u2514\u2500\u2500 format_converter.py\n\u2514\u2500\u2500 patches/\n    \u2514\u2500\u2500 whisper_security.py\n</code></pre>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#5-obsolete-code-handling","title":"5. Obsolete Code Handling","text":""},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#case-transcriptionpy","title":"Case: <code>transcription.py</code>","text":"<ul> <li>Obsolete Whisper prototype with one CLI dependency.</li> <li>Superseded by <code>transcription_service.py</code> with proper interfaces.</li> </ul> <p>\u2705 Recommended: Port CLI, delete file.</p>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#best-practice","title":"Best Practice:","text":"Case Action Easily replaced Port usage, delete immediately Unclear usage Move to <code>legacy/</code> folder temporarily Used across repos Mark with <code>DeprecationWarning</code>, document plan"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#6-patch-module-handling-whisper_securitypy","title":"6. Patch Module Handling (<code>whisper_security.py</code>)","text":"<p>A patch to fix <code>torch.load</code>'s <code>weights_only=True</code> security option.</p> <p>\u2705 Move to a dedicated folder:</p> <pre><code>patches/\n  \u2514\u2500\u2500 whisper_security.py\n</code></pre> <p>Document patches with:</p> <ul> <li>Library/version patched</li> <li>Reason</li> <li>Link to upstream issue if possible</li> <li>Plan for removal</li> </ul>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#7-final-thoughts","title":"7. Final Thoughts","text":"<p>The user is now building a pipeline with:</p> <ul> <li>Clean, swappable stages (Protocol + Adapter)</li> <li>Typed data contracts</li> <li>Future-proof folder structure</li> <li>Minimal glue logic</li> </ul> <p>This design is robust for research, easy to evolve, and ready to become production-grade.</p>"},{"location":"architecture/transcription/design/modular-pipeline-best-practices/#next-steps-checklist-optional","title":"\u2728 Next Steps Checklist (optional)","text":"<ul> <li> Port CLI to use <code>TranscriptionService</code></li> <li> Delete <code>transcription.py</code></li> <li> Move <code>whisper_security.py</code> to <code>patches/</code></li> <li> Create <code>protocols/</code> and move interfaces in</li> <li> Refactor <code>Segment</code>, <code>Chunk</code>, etc. into <code>models/</code></li> <li> Consider simple pipeline runner with <code>|</code> chaining</li> </ul> <p>\"You are building like a professional systems architect. Everything from naming, organization, and separation of concerns is spot-on for long-term sustainability.\"</p>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/","title":"Practical Language-Aware Chunking Design","text":"<p>Practical heuristics for detecting language changes during chunking when diarization output is noisy.</p>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#core-problem","title":"Core Problem","text":"<p>Current chunking only considers time and speaker boundaries. We need language boundary detection for mixed-language audio, but diarization output is noisy (800+ segments, false speaker detection) requiring practical segment handling.</p>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#real-world-context","title":"Real-World Context","text":"<p>Typical diarization output:</p> <ul> <li>1hr audio with 5 actual speakers \u2192 800+ segments detected</li> <li>Diarizer reports 12 speakers (due to noise/audio artifacts)  </li> <li>Many segments \u22641 second (utterance-based detection)</li> <li>Need minimum 5+ seconds for reliable language detection</li> </ul> <p>Audio scenarios:</p> <ul> <li>Conversational: Multiple speakers, translation, code-switching</li> <li>Formal: Panel discussions, Q&amp;A sessions, prepared statements</li> </ul>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#probing-strategies-for-testing","title":"Probing Strategies for Testing","text":""},{"location":"architecture/transcription/design/practical-language-aware-chunking/#1-individual-segment-threshold","title":"1. Individual Segment Threshold","text":"<p>Strategy: Poll any segment longer than X seconds</p> <ul> <li>Config: <code>min_segment_duration_for_probe: int = 5000  # 5 seconds</code></li> <li>Use case: Catch longer utterances that might contain language switches</li> <li>Pro: Simple, catches obvious language segments</li> <li>Con: Misses short code-switching</li> </ul>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#2-contiguous-speaker-duration","title":"2. Contiguous Speaker Duration","text":"<p>Strategy: Poll when speaker's contiguous segments total &gt; Y seconds</p> <ul> <li>Config: <code>min_speaker_duration_for_probe: int = 10000  # 10 seconds</code></li> <li>Logic: Accumulate consecutive segments from same speaker until interrupted</li> <li>Use case: Speaker delivers longer statement, might switch languages mid-way</li> <li>Special case: Y=0 means probe every speaker appearance (exhaustive)</li> </ul>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#3-fixed-speaker-languages","title":"3. Fixed Speaker Languages","text":"<p>Strategy: Assign known languages to specific speakers</p> <ul> <li>Config: <code>fixed_speaker_languages: Dict[str, str] = {\"SPEAKER_01\": \"en\", \"SPEAKER_02\": \"vi\"}</code></li> <li>Use case: Known multilingual scenarios (interpreter + speaker)</li> <li>Pro: Eliminates false language detection</li> <li>Con: Requires prior knowledge</li> </ul>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#4-contiguous-speaker-consistency","title":"4. Contiguous Speaker Consistency","text":"<p>Strategy: Keep language fixed for uninterrupted speaker segments</p> <ul> <li>Config: <code>lock_language_until_interruption: bool = True</code></li> <li>Logic: Once language detected for speaker, maintain until different speaker talks</li> <li>Use case: Formal presentations where speakers stick to one language per turn</li> <li>Pro: Reduces false language switches</li> <li>Con: Misses mid-turn language changes</li> </ul>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#5-speaker-transition-probing","title":"5. Speaker Transition Probing","text":"<p>Strategy: Always probe language when speaker changes</p> <ul> <li>Config: <code>probe_on_speaker_change: bool = True</code></li> <li>Logic: Check language of new speaker (if sufficient audio available)</li> <li>Use case: Detect language switching between speakers</li> <li>Pro: Catches most language boundaries</li> <li>Con: Expensive with many speaker transitions</li> </ul>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#algorithm-flow","title":"Algorithm Flow","text":"<pre><code>flowchart TD\n    A[Diarization Segments] --&gt; B[Process Sequentially]\n    B --&gt; C{Speaker Changed?}\n    C --&gt;|Yes| D[Apply Speaker Transition Logic]\n    C --&gt;|No| E[Apply Same-Speaker Logic]\n\n    D --&gt; F{Fixed Language for Speaker?}\n    F --&gt;|Yes| G[Use Fixed Language]\n    F --&gt;|No| H{Enough Audio to Probe?}\n    H --&gt;|Yes| I[Probe New Speaker Language]\n    H --&gt;|No| J[Inherit Previous Language]\n\n    E --&gt; K{Strategy Check}\n    K --&gt; L{Segment &gt; Threshold?}\n    L --&gt;|Yes| M[Probe Segment Language]\n    L --&gt;|No| N{Contiguous Duration &gt; Threshold?}\n    N --&gt;|Yes| O[Probe Accumulated Language]\n    N --&gt;|No| P[Keep Current Language]\n\n    G --&gt; Q[Language Decision Made]\n    I --&gt; Q\n    J --&gt; Q\n    M --&gt; Q\n    O --&gt; Q\n    P --&gt; Q\n\n    Q --&gt; R{Language Changed?}\n    R --&gt;|Yes| S[Split Chunk]\n    R --&gt;|No| T[Continue Chunk]\n    S --&gt; U[Add to Chunks]\n    T --&gt; V{Chunk Size OK?}\n    V --&gt;|No| U\n    V --&gt;|Yes| W[Continue Processing]\n    U --&gt; W\n    W --&gt; X{More Segments?}\n    X --&gt;|Yes| B\n    X --&gt;|No| Y[Final Chunks]</code></pre>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#configuration-options","title":"Configuration Options","text":"<pre><code>@dataclass\nclass LanguageProbeConfig:\n    # Individual segment probing\n    min_segment_duration_for_probe: int = 5000  # ms\n\n    # Contiguous speaker probing  \n    min_speaker_duration_for_probe: int = 10000  # ms\n    probe_exhaustively: bool = False  # Y=0 case\n\n    # Fixed assignments\n    fixed_speaker_languages: Dict[str, str] = field(default_factory=dict)\n\n    # Consistency rules\n    lock_language_until_interruption: bool = False\n\n    # Transition probing\n    probe_on_speaker_change: bool = True\n\n    # Detection settings\n    confidence_threshold: float = 0.7\n    sample_duration_ms: int = 5000\n</code></pre>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#segment-combining-logic","title":"Segment Combining Logic","text":"<p>Before language detection, combine very short segments to reach minimum testable duration:</p> <pre><code>flowchart LR\n    A[Short Segments] --&gt; B[Combine by Speaker]\n    B --&gt; C{Combined Duration &gt; Min?}\n    C --&gt;|Yes| D[Probe Combined Audio]\n    C --&gt;|No| E[Skip Probing]\n    D --&gt; F[Language Result]\n    E --&gt; G[Inherit/Default Language]</code></pre> <p>Combining rules:</p> <ul> <li>Combine consecutive segments from same speaker</li> <li>Stop combining when different speaker appears</li> <li>If combined duration still &lt; minimum, skip probing</li> <li>Apply probing strategy to combined segment</li> </ul>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#processing-sequence","title":"Processing Sequence","text":""},{"location":"architecture/transcription/design/practical-language-aware-chunking/#for-each-segment-in-timeline","title":"For Each Segment in Timeline","text":"<ol> <li> <p>Check speaker transition: Has speaker changed from previous segment?</p> </li> <li> <p>Apply appropriate strategy:</p> </li> <li>If speaker changed: Check fixed languages, probe if needed</li> <li> <p>If same speaker: Apply same-speaker probing rules</p> </li> <li> <p>Language detection decision:</p> </li> <li>Use fixed language if configured</li> <li>Probe if thresholds met and audio sufficient</li> <li> <p>Inherit previous language otherwise</p> </li> <li> <p>Chunk management:</p> </li> <li>If language changed (high confidence): Split chunk</li> <li>If chunk too large: Split on size</li> <li>Otherwise: Continue building chunk</li> </ol>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#strategy-testing-plan","title":"Strategy Testing Plan","text":""},{"location":"architecture/transcription/design/practical-language-aware-chunking/#phase-1-basic-implementation","title":"Phase 1: Basic Implementation","text":"<ul> <li>Implement all 5 strategies as configurable options</li> <li>Test with known multilingual audio samples</li> <li>Measure language detection accuracy vs. manual annotation</li> </ul>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#phase-2-optimization","title":"Phase 2: Optimization","text":"<ul> <li>Test different threshold values (3s, 5s, 10s, 15s)</li> <li>Compare exhaustive vs. selective probing performance</li> <li>Evaluate false positive rates for each strategy</li> </ul>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#phase-3-combination-testing","title":"Phase 3: Combination Testing","text":"<ul> <li>Test multiple strategies simultaneously</li> <li>Find optimal combinations for different audio types</li> <li>Measure impact on transcription quality</li> </ul>"},{"location":"architecture/transcription/design/practical-language-aware-chunking/#expected-outcomes","title":"Expected Outcomes","text":"<p>Conversational audio (cooking demo scenario):</p> <ul> <li>Strategy 2 + 5: Probe contiguous speaker duration + speaker transitions</li> <li>Handle 800+ segments efficiently</li> <li>Catch translation segments between speakers</li> </ul> <p>Formal audio (panel discussions):</p> <ul> <li>Strategy 4 + 5: Lock language until interruption + speaker transitions  </li> <li>Reduce false language switches during long statements</li> <li>Probe only at natural speaker boundaries</li> </ul> <p>This practical approach handles real diarization noise while providing multiple testable strategies for different audio contexts.</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/","title":"Speaker Diarization Algorithm Design","text":"<p>This document details the key algorithms referenced in the main diarization system design. Each algorithm is presented with a clear breakdown of its inputs, outputs, and processing steps.</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#1-timeline-mapping-algorithm","title":"1. Timeline Mapping Algorithm","text":"<p>The core process for mapping timestamps between original and consolidated timelines.</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#11-inputs-and-outputs","title":"1.1 Inputs and Outputs","text":"<p>Inputs: - <code>TimeMap</code>: Collection of TimeMapInterval objects - <code>timestamp</code>: Float value representing a time point in the original timeline (seconds)</p> <p>Outputs: - Mapped timestamp in the consolidated timeline (seconds)</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#12-process-flow","title":"1.2 Process Flow","text":"<pre><code>flowchart TD\n    A[Input: Original Timestamp + TimeMap] --&gt; B{Is timestamp in any TimeMapInterval?}\n    B --&gt;|Yes| C[Find containing TimeMapInterval]\n    B --&gt;|No| D[Find surrounding TimeMapIntervals]\n    C --&gt; E[Calculate relative position in interval]\n    E --&gt; F[Calculate equivalent position in transformed interval]\n    D --&gt; G[Determine if timestamp is in a gap]\n    G --&gt;|Yes| H[Apply proportional mapping across gap]\n    G --&gt;|No| I[Handle boundary case]\n    F --&gt; J[Return mapped timestamp]\n    H --&gt; J\n    I --&gt; J</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#13-implementation-approach","title":"1.3 Implementation Approach","text":"<p>This algorithm is implemented in the <code>TimeMap.map_time()</code> method with these steps:</p> <ol> <li> <p>Find the TimeMapInterval containing the timestamp:    <pre><code>def find_containing_interval(time_map, timestamp):\n    \"\"\"Find interval containing the timestamp or None if in a gap.\"\"\"\n    for interval in time_map.intervals:\n        if interval.original_start &lt;= timestamp &lt;= interval.original_end:\n            return interval\n    return None\n</code></pre></p> </li> <li> <p>Calculate the position within an interval:    <pre><code>def calculate_position_in_interval(interval, timestamp):\n    \"\"\"Calculate relative position (0.0-1.0) in interval.\"\"\"\n    if interval.duration == 0:\n        return 0\n    return (timestamp - interval.original_start) / interval.duration\n</code></pre></p> </li> <li> <p>Map the position to the transformed timeline:    <pre><code>def map_within_interval(interval, position):\n    \"\"\"Map a relative position to the transformed timeline.\"\"\"\n    return interval.transformed_start + (position * interval.duration)\n</code></pre></p> </li> <li> <p>Handle gaps between intervals:    <pre><code>def handle_gap_mapping(previous_interval, next_interval, timestamp):\n    \"\"\"Map a timestamp in a gap between intervals.\"\"\"\n    original_gap = next_interval.original_start - previous_interval.original_end\n    transformed_gap = next_interval.transformed_start - previous_interval.transformed_end\n\n    # Position in gap (0.0-1.0)\n    gap_position = (timestamp - previous_interval.original_end) / original_gap\n\n    return previous_interval.transformed_end + (gap_position * transformed_gap)\n</code></pre></p> </li> </ol>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#2-speakertrack-consolidation-algorithm","title":"2. SpeakerTrack Consolidation Algorithm","text":"<p>The process for combining discrete speaker segments into a continuous audio track.</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#21-inputs-and-outputs","title":"2.1 Inputs and Outputs","text":"<p>Inputs: - <code>SpeakerTrack</code>: Object containing a list of DiarizationSegment objects for a single speaker - <code>AudioSegment</code>: Original complete audio containing all speakers - <code>gap_duration</code>: Float value specifying silence duration to insert between non-contiguous segments</p> <p>Outputs: - Consolidated <code>AudioSegment</code> containing only the specified speaker's segments - Updated <code>TimeMap</code> in the SpeakerTrack object</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#22-process-flow","title":"2.2 Process Flow","text":"<pre><code>flowchart TD\n    A[Inputs: SpeakerTrack + Original Audio] --&gt; B[Sort DiarizationSegments by start time]\n    B --&gt; C[Initialize empty output AudioSegment]\n    C --&gt; D[Initialize current_position = 0.0]\n    D --&gt; E[For each DiarizationSegment]\n    E --&gt; F[Extract segment from original audio]\n    F --&gt; G[Add TimeMapInterval to SpeakerTrack.time_map]\n    G --&gt; H[Append segment to output AudioSegment]\n    H --&gt; I[Update current_position += segment.duration]\n    I --&gt; J{More segments?}\n    J --&gt;|Yes| K[Add silence gap if needed]\n    K --&gt; E\n    J --&gt;|No| L[Return consolidated AudioSegment]</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#23-implementation-approach","title":"2.3 Implementation Approach","text":"<p>This algorithm is implemented in the <code>SpeakerTrack.consolidate_audio()</code> method:</p> <ol> <li> <p>Sort segments by original time:    <pre><code>def sort_segments(speaker_track):\n    \"\"\"Sort DiarizationSegments by start time.\"\"\"\n    return sorted(speaker_track.segments, key=lambda s: s.start)\n</code></pre></p> </li> <li> <p>Process each segment:    <pre><code>def process_segment(segment, original_audio, current_position, time_map):\n    \"\"\"Process a single segment and update the time map.\"\"\"\n    # Extract audio segment (ms precision for AudioSegment)\n    start_ms = int(segment.start * 1000)\n    end_ms = int(segment.end * 1000)\n    segment_audio = original_audio[start_ms:end_ms]\n\n    # Record time mapping\n    time_map.add_interval(segment.start, segment.end, current_position)\n\n    return segment_audio, segment.duration\n</code></pre></p> </li> <li> <p>Handle gaps between segments:    <pre><code>def create_gap(duration, format_params=None):\n    \"\"\"Create a silent gap of specified duration.\"\"\"\n    return AudioSegment.silent(duration=int(duration * 1000))\n</code></pre></p> </li> </ol>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#3-srt-remapping-algorithm","title":"3. SRT Remapping Algorithm","text":"<p>The process for converting subtitle timestamps from consolidated to original timeline.</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#31-inputs-and-outputs","title":"3.1 Inputs and Outputs","text":"<p>Inputs: - <code>Path</code> to SRT file with timestamps in the consolidated timeline - <code>TimeMap</code> object for the associated speaker - <code>Path</code> for output remapped SRT file</p> <p>Outputs: - SRT file with timestamps remapped to the original timeline</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#32-process-flow","title":"3.2 Process Flow","text":"<pre><code>flowchart TD\n    A[Inputs: SRT Path + TimeMap] --&gt; B[Parse SRT file into SubtitleEntry objects]\n    B --&gt; C[For each SubtitleEntry]\n    C --&gt; D[Get start and end times in seconds]\n    D --&gt; E[Use TimeMap.reverse_map_time for both timestamps]\n    E --&gt; F[Create new SubtitleEntry with remapped times]\n    F --&gt; G{More entries?}\n    G --&gt;|Yes| C\n    G --&gt;|No| H[Sort entries by start time]\n    H --&gt; I[Reindex entries sequentially]\n    I --&gt; J[Write formatted SRT to output file]</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#33-implementation-approach","title":"3.3 Implementation Approach","text":"<p>The SRT remapping is implemented in the <code>TimingRemapper.remap_srt()</code> method:</p> <ol> <li> <p>Parse SRT file:    <pre><code>def parse_srt_file(file_path):\n    \"\"\"Parse SRT file into SubtitleEntry objects.\"\"\"\n    entries = []\n    # Implementation details for parsing SRT format\n    return entries\n</code></pre></p> </li> <li> <p>Remap individual entries:    <pre><code>def remap_entry(entry, time_map):\n    \"\"\"Remap a single SubtitleEntry using the TimeMap.\"\"\"\n    # Create a new entry to preserve original\n    new_entry = entry.clone()\n\n    # Remap timestamps (reverse mapping: transformed \u2192 original)\n    original_start = time_map.reverse_map_time(entry.start_seconds)\n    original_end = time_map.reverse_map_time(entry.end_seconds)\n\n    # Update the entry\n    new_entry.set_times(original_start, original_end)\n    return new_entry\n</code></pre></p> </li> <li> <p>Write SRT output:    <pre><code>def write_srt_file(entries, output_path):\n    \"\"\"Write SubtitleEntry objects to SRT file.\"\"\"\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for entry in entries:\n            f.write(entry.format_srt())\n            f.write('\\n\\n')\n</code></pre></p> </li> </ol>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#4-timemap-reverse-mapping","title":"4. TimeMap Reverse Mapping","text":"<p>The algorithm for mapping timestamps from transformed timeline back to original timeline.</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#41-inputs-and-outputs","title":"4.1 Inputs and Outputs","text":"<p>Inputs: - <code>TimeMap</code>: Collection of TimeMapInterval objects - <code>timestamp</code>: Float value representing a time point in the transformed timeline (seconds)</p> <p>Outputs: - Mapped timestamp in the original timeline (seconds)</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#42-process-flow","title":"4.2 Process Flow","text":"<pre><code>flowchart TD\n    A[Input: Transformed Timestamp + TimeMap] --&gt; B{Is timestamp in any transformed interval?}\n    B --&gt;|Yes| C[Find containing transformed interval]\n    B --&gt;|No| D[Find surrounding transformed intervals]\n    C --&gt; E[Calculate relative position in transformed interval]\n    E --&gt; F[Calculate equivalent position in original interval]\n    D --&gt; G[Determine if timestamp is in a transformed gap]\n    G --&gt;|Yes| H[Apply reverse proportional mapping across gap]\n    G --&gt;|No| I[Handle boundary case]\n    F --&gt; J[Return original timestamp]\n    H --&gt; J\n    I --&gt; J</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#43-implementation-approach","title":"4.3 Implementation Approach","text":"<p>This is implemented in the <code>TimeMap.reverse_map_time()</code> method:</p> <ol> <li> <p>Find interval containing the transformed timestamp:    <pre><code>def find_containing_transformed_interval(time_map, timestamp):\n    \"\"\"Find interval containing the transformed timestamp.\"\"\"\n    for interval in time_map.intervals:\n        if interval.transformed_start &lt;= timestamp &lt;= interval.transformed_end:\n            return interval\n    return None\n</code></pre></p> </li> <li> <p>Calculate reverse position:    <pre><code>def calculate_position_in_transformed_interval(interval, timestamp):\n    \"\"\"Calculate relative position (0.0-1.0) in transformed interval.\"\"\"\n    return (timestamp - interval.transformed_start) / interval.duration\n</code></pre></p> </li> <li> <p>Map back to original timeline:    <pre><code>def map_to_original(interval, position):\n    \"\"\"Map a relative position back to the original timeline.\"\"\"\n    return interval.original_start + (position * interval.duration)\n</code></pre></p> </li> </ol>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#5-gap-detection-and-speaker-change-analysis","title":"5. Gap Detection and Speaker Change Analysis","text":"<p>The process for detecting speaker changes and silence gaps in the audio.</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#51-inputs-and-outputs","title":"5.1 Inputs and Outputs","text":"<p>Inputs: - List of <code>DiarizationSegment</code> objects from all speakers - Threshold parameters for gap detection</p> <p>Outputs: - List of segments with speaker change and gap annotations</p>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#52-process-flow","title":"5.2 Process Flow","text":"<pre><code>flowchart TD\n    A[Inputs: All DiarizationSegments] --&gt; B[Sort segments by start time]\n    B --&gt; C[For each pair of adjacent segments]\n    C --&gt; D{Same speaker?}\n    D --&gt;|Yes| E[Check for intra-speaker gap]\n    D --&gt;|No| F[Mark as speaker change]\n    E --&gt; G{Gap &gt; threshold?}\n    G --&gt;|Yes| H[Mark significant gap]\n    G --&gt;|No| I[Mark continuous speech]\n    F --&gt; J[Calculate change point]\n    H --&gt; J\n    I --&gt; J\n    J --&gt; K{More segments?}\n    K --&gt;|Yes| C\n    K --&gt;|No| L[Return annotated segments]</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-algorithm-design/#53-implementation-approach","title":"5.3 Implementation Approach","text":"<p>This algorithm helps determine how to handle transitions between segments:</p> <ol> <li> <p>Analyze adjacent segments:    <pre><code>def analyze_segment_transition(seg1, seg2, gap_threshold):\n    \"\"\"Analyze transition between two segments.\"\"\"\n    gap = seg2.start - seg1.end\n    same_speaker = seg1.speaker == seg2.speaker\n\n    if gap &lt; 0:  # Overlapping segments\n        return {\n            'type': 'overlap',\n            'duration': -gap,\n            'speaker_change': not same_speaker\n        }\n\n    return {\n        'type': 'gap',\n        'duration': gap,\n        'speaker_change': not same_speaker,\n        'significant': gap &gt; gap_threshold\n    }\n</code></pre></p> </li> <li> <p>Find optimal split points:    <pre><code>def find_split_point(seg1, seg2):\n    \"\"\"Find optimal point to split between segments.\"\"\"\n    if seg1.end &lt; seg2.start:  # Gap exists\n        return (seg1.end + seg2.start) / 2  # Middle of gap\n    else:  # Overlapping\n        return (seg1.end + seg2.start) / 2  # Middle of overlap\n</code></pre></p> </li> </ol> <p>These algorithms form the core processing logic of the diarization system. They are designed to be modular and focused on single tasks, making them easier to implement and test in the prototype phase.</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/","title":"Speaker Diarization and Time-Mapped Transcription System Design","text":"<p>System design for mapping diarization outputs to speaker-specific transcriptions with accurate global timelines.</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#1-overview","title":"1. Overview","text":"<p>This document outlines a design for enhancing the existing diarization system to support speaker-specific transcription with accurate time mapping. The system will consolidate audio segments by speaker while preserving timing information, generate transcriptions for each speaker's track, and remap the transcription timings back to the original audio timeline.</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#2-design-goals","title":"2. Design Goals","text":"<ul> <li>Minimal Changes: Extend existing functionality with minimal modifications to current code</li> <li>Accurate Timing: Maintain precise timing relationships throughout the pipeline</li> <li>Conceptual Simplicity: Use clear, minimalist data structures that map directly to the problem domain</li> <li>Single-Action Functions: Each function or method should perform a single, well-defined action; for sequences, each element should be implemented as a single-action function</li> <li>Modularity: Create well-defined components with clear responsibilities</li> <li>Code Reuse: Maximize reuse of data structures and helper functions across components</li> <li>Extensibility: Support future enhancements like language detection</li> <li>Usability: Integrate with existing CLI interfaces</li> </ul>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#3-system-architecture","title":"3. System Architecture","text":""},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#31-core-components","title":"3.1 Core Components","text":"<pre><code>graph TD\n    A[Audio File] --&gt; B[Diarization Processor]\n    B --&gt; |DiarizationSegments| C[Speaker Track Generator]\n    C --&gt; D1[Speaker Track 1]\n    C --&gt; D2[Speaker Track 2]\n    C --&gt; D3[Speaker Track n]\n    D1 --&gt; E1[Transcription Engine]\n    D2 --&gt; E2[Transcription Engine]\n    D3 --&gt; E3[Transcription Engine]\n    E1 --&gt; F1[Speaker SRT 1]\n    E2 --&gt; F2[Speaker SRT 2]\n    E3 --&gt; F3[Speaker SRT n]\n    F1 --&gt; G[Timing Remapper]\n    F2 --&gt; G\n    F3 --&gt; G\n    G --&gt; H[Final SRT Files]\n\n    subgraph \"Existing Components\"\n        A\n        B\n    end\n\n    subgraph \"New Components\"\n        C\n        G\n    end\n\n    subgraph \"Per-Speaker Processing\"\n        D1\n        D2\n        D3\n        E1\n        E2\n        E3\n        F1\n        F2\n        F3\n    end</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#32-data-flow-architecture","title":"3.2 Data Flow Architecture","text":"<pre><code>flowchart TB\n    subgraph InputProcess[\"Input Processing\"]\n        direction TB\n        RawAudio[\"Raw Audio File\"]\n        DiarizeJob[\"Pyannote Diarization\"]\n        SpeakerSegments[\"Speaker Segments\"]\n\n        RawAudio --&gt; DiarizeJob\n        DiarizeJob --&gt; SpeakerSegments\n    end\n\n    subgraph ConsolidationProcess[\"Track Consolidation\"]\n        direction TB\n        SpeakerChunks[\"Speaker Chunks&lt;br&gt;(From Diarization)\"]\n        GroupSegments[\"Group Segments&lt;br&gt;By Speaker\"]\n        SpeakerTracks[\"Speaker Tracks\"]\n        ConsolidatedTracks[\"Consolidated Audio Tracks&lt;br&gt;(Per Speaker)\"]\n        TimeMaps[\"Time Mapping Tables\"]\n\n        SpeakerChunks --&gt; GroupSegments\n        GroupSegments --&gt; SpeakerTracks\n        SpeakerTracks --&gt; ConsolidatedTracks\n        SpeakerTracks --&gt; TimeMaps\n    end\n\n    subgraph TranscriptionProcess[\"Transcription\"]\n        direction TB\n        Speaker1Track[\"Speaker 1 Track\"]\n        Speaker2Track[\"Speaker 2 Track\"]\n        SpeakerNTrack[\"Speaker N Track\"]\n        Speaker1SRT[\"Speaker 1 SRT\"]\n        Speaker2SRT[\"Speaker 2 SRT\"]\n        SpeakerNSRT[\"Speaker N SRT\"]\n\n        Speaker1Track --&gt; Speaker1SRT\n        Speaker2Track --&gt; Speaker2SRT\n        SpeakerNTrack --&gt; SpeakerNSRT\n    end\n\n    subgraph RemappingProcess[\"Time Remapping\"]\n        direction TB\n        RawSRTs[\"Speaker SRTs&lt;br&gt;(Transformed Timeline)\"]\n        ParseSRT[\"Parse SRT Files\"]\n        RemapOperation[\"Remap Timestamps\"]\n        WriteSRT[\"Write Remapped SRTs\"]\n        MappedSRTs[\"Remapped SRTs&lt;br&gt;(Original Timeline)\"]\n        FinalOutput[\"Final SRT Files\"]\n\n        RawSRTs --&gt; ParseSRT\n        ParseSRT --&gt; RemapOperation\n        TimeMaps --&gt; RemapOperation\n        RemapOperation --&gt; WriteSRT\n        WriteSRT --&gt; MappedSRTs\n        MappedSRTs --&gt; FinalOutput\n    end\n\n    InputProcess --&gt; ConsolidationProcess\n    ConsolidationProcess --&gt; TranscriptionProcess\n    TranscriptionProcess --&gt; RemappingProcess</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#33-error-handling-strategy","title":"3.3 Error Handling Strategy","text":"<p>For the prototype phase, error handling will be minimal, allowing failures to surface immediately for debugging.</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#331-core-approach","title":"3.3.1 Core Approach","text":"<ul> <li>Focus on logging rather than error handling for most operations</li> <li>Handle only critical expected errors (file access, format errors)</li> <li>Allow other exceptions to propagate and terminate execution for easier debugging</li> <li>Use descriptive log messages for error contexts</li> </ul>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#332-implementation-examples","title":"3.3.2 Implementation Examples","text":"<p>Only handle errors where they commonly occur and are expected:</p> <pre><code>def load_audio_file(file_path):\n    \"\"\"Load audio file or raise appropriate error.\"\"\"\n    if not file_path.exists():\n        logger.error(f\"Audio file not found: {file_path}\")\n        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n\n    # Let any other exceptions propagate up\n    return AudioSegment.from_file(file_path)\n</code></pre> <p>In all other cases, log the operation and let exceptions propagate:</p> <pre><code>def process_speaker_track(track, audio_file):\n    \"\"\"Process a speaker track.\"\"\"\n    logger.info(f\"Processing track for speaker {track.speaker_id}\")\n    return track.consolidate_audio(audio_file)\n</code></pre> <p>This minimal approach is appropriate for the prototype phase, with comprehensive error handling to be implemented later.</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#4-component-specifications","title":"4. Component Specifications","text":""},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#41-speaker-track-generator","title":"4.1 Speaker Track Generator","text":"<p>Purpose: Consolidate speaker segments into continuous audio tracks while maintaining timing relationships</p> <pre><code>classDiagram\n    class SpeakerTrackGenerator {\n        +generate_speaker_tracks(diarization_segments: List[DiarizationSegment], gap_duration: float) List[SpeakerTrack]\n        +group_by_speaker(diarization_segments: List[DiarizationSegment]) Dict[str, List[DiarizationSegment]]\n        +create_track(speaker_id: str, segments: List[DiarizationSegment], gap_duration: float) SpeakerTrack\n        +export_time_maps(tracks: List[SpeakerTrack], output_dir: Path)\n    }\n\n    class SpeakerTrack {\n        +speaker_id: str\n        +segments: List[DiarizationSegment]\n        +time_map: TimeMap\n        +consolidated_path: Path\n        +add_segment(segment: DiarizationSegment, transformed_start: float)\n        +consolidate_audio(original_audio: AudioSegment)\n        +save_audio(output_dir: Path) Path\n    }\n\n    class TimeMap {\n        +speaker_id: str\n        +intervals: List[TimeMapInterval]\n        +add_interval(original_start: float, original_end: float, transformed_start: float)\n        +map_time(original_time: float) float\n        +reverse_map_time(transformed_time: float) float\n        +export_to_json(file_path: Path)\n        +import_from_json(file_path: Path) TimeMap\n    }\n\n    class TimeMapInterval {\n        +original_start: float\n        +original_end: float\n        +transformed_start: float\n        +duration() float\n        +transformed_end() float\n    }\n\n    class DiarizationSegment {\n        +speaker: str\n        +start: float\n        +end: float\n        +duration() float\n    }\n\n    SpeakerTrackGenerator --&gt; SpeakerTrack\n    SpeakerTrack --&gt; TimeMap\n    TimeMap --&gt; TimeMapInterval\n    SpeakerTrackGenerator --&gt; DiarizationSegment\n    SpeakerTrack --&gt; DiarizationSegment</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#42-time-mapping-system","title":"4.2 Time Mapping System","text":"<p>Purpose: Track time relationships between original and consolidated audio</p> <pre><code>graph TB\n    subgraph \"Original Timeline (Mixed Speakers)\"\n        OT1[Speaker 1 &lt;br&gt; 0:00-0:30]\n        OT2[Speaker 2 &lt;br&gt; 0:30-1:15]\n        OT3[Speaker 3 &lt;br&gt; 1:15-2:00]\n        OT4[Speaker 1 &lt;br&gt; 2:00-2:45]\n        OT5[Speaker 2 &lt;br&gt; 2:45-3:10]\n        OT6[Speaker 3 &lt;br&gt; 3:10-3:45]\n        OT7[Speaker 1 &lt;br&gt; 3:45-4:20]\n        OT8[Speaker 3 &lt;br&gt; 4:20-5:00]\n        OT9[Speaker 2 &lt;br&gt; 5:00-5:30]\n    end\n\n    subgraph \"Consolidated Timeline (Speaker 1)\"\n        CT1_1[0:00-0:30]\n        CT1_2[0:31-1:16]\n        CT1_3[1:17-1:52]\n    end\n\n    subgraph \"Consolidated Timeline (Speaker 2)\"\n        CT2_1[0:00-0:45]\n        CT2_2[0:46-1:11]\n        CT2_3[1:12-1:42]\n    end\n\n    subgraph \"Consolidated Timeline (Speaker 3)\"\n        CT3_1[0:00-0:45]\n        CT3_2[0:46-1:21]\n        CT3_3[1:22-2:02]\n    end\n\n    OT1 -- map --&gt; CT1_1\n    OT4 -- map --&gt; CT1_2\n    OT7 -- map --&gt; CT1_3\n\n    OT2 -- map --&gt; CT2_1\n    OT5 -- map --&gt; CT2_2\n    OT9 -- map --&gt; CT2_3\n\n    OT3 -- map --&gt; CT3_1\n    OT6 -- map --&gt; CT3_2\n    OT8 -- map --&gt; CT3_3\n\n    style OT1 fill:#f9f,stroke:#333,stroke-width:2px\n    style OT4 fill:#f9f,stroke:#333,stroke-width:2px\n    style OT7 fill:#f9f,stroke:#333,stroke-width:2px\n\n    style OT2 fill:#bbf,stroke:#333,stroke-width:2px\n    style OT5 fill:#bbf,stroke:#333,stroke-width:2px\n    style OT9 fill:#bbf,stroke:#333,stroke-width:2px\n\n    style OT3 fill:#bfb,stroke:#333,stroke-width:2px\n    style OT6 fill:#bfb,stroke:#333,stroke-width:2px\n    style OT8 fill:#bfb,stroke:#333,stroke-width:2px\n\n    style CT1_1 fill:#f9f,stroke:#333,stroke-width:2px\n    style CT1_2 fill:#f9f,stroke:#333,stroke-width:2px\n    style CT1_3 fill:#f9f,stroke:#333,stroke-width:2px\n\n    style CT2_1 fill:#bbf,stroke:#333,stroke-width:2px\n    style CT2_2 fill:#bbf,stroke:#333,stroke-width:2px\n    style CT2_3 fill:#bbf,stroke:#333,stroke-width:2px\n\n    style CT3_1 fill:#bfb,stroke:#333,stroke-width:2px\n    style CT3_2 fill:#bfb,stroke:#333,stroke-width:2px\n    style CT3_3 fill:#bfb,stroke:#333,stroke-width:2px</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#43-timing-remapper","title":"4.3 Timing Remapper","text":"<p>Purpose: Convert SRT timings from consolidated tracks to original timeline</p> <pre><code>classDiagram\n    class TimingRemapper {\n        +remap_srt(srt_path: Path, time_map: TimeMap, output_path: Path) Path\n        +parse_srt_file(srt_path: Path) List[SubtitleEntry]\n        +remap_timestamp(timestamp: float, time_map: TimeMap) float\n        +remap_entries(entries: List[SubtitleEntry], time_map: TimeMap) List[SubtitleEntry]\n        +write_srt_file(entries: List[SubtitleEntry], output_path: Path)\n    }\n\n    class SubtitleEntry {\n        +index: int\n        +start_time: str\n        +end_time: str\n        +text: str\n        +start_seconds: float\n        +end_seconds: float\n        +parse_timestamp(timestamp: str) float\n        +format_timestamp(seconds: float) str\n        +set_times(start_seconds: float, end_seconds: float)\n        +format_srt() str\n        +clone() SubtitleEntry\n    }\n\n    class SRTUtils {\n        +&lt;&lt;static&gt;&gt; parse_timestamp(timestamp: str) float\n        +&lt;&lt;static&gt;&gt; format_timestamp(seconds: float) str\n        +&lt;&lt;static&gt;&gt; read_srt_file(srt_path: Path) List[SubtitleEntry]\n        +&lt;&lt;static&gt;&gt; write_srt_file(entries: List[SubtitleEntry], output_path: Path)\n    }\n\n    TimingRemapper --&gt; SubtitleEntry\n    TimingRemapper --&gt; SRTUtils\n    SubtitleEntry --&gt; SRTUtils: uses</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#44-cli-integration","title":"4.4 CLI Integration","text":"<p>Purpose: Extend existing CLI interface to support the new functionality</p> <pre><code>classDiagram\n    class AudioTranscribeCLI {\n        +diarize_option: bool\n        +speaker_gap: float\n        +srt_output: bool\n        +handle_diarize()\n        +handle_speaker_tracks()\n        +handle_transcribe_tracks()\n        +handle_remap_timings()\n    }\n\n    class AudioPipeline {\n        +run()\n        +execute_stage(stage_name: str)\n        +get_stage_result(stage_name: str) Any\n    }\n\n    class PipelineStage {\n        &lt;&lt;interface&gt;&gt;\n        +execute(context: Dict[str, Any]) Dict[str, Any]\n        +validate(context: Dict[str, Any]) bool\n    }\n\n    class DiarizationStage {\n        +execute(context: Dict[str, Any]) Dict[str, Any]\n        +validate(context: Dict[str, Any]) bool\n    }\n\n    class SpeakerTrackStage {\n        +execute(context: Dict[str, Any]) Dict[str, Any]\n        +validate(context: Dict[str, Any]) bool\n    }\n\n    class TranscriptionStage {\n        +execute(context: Dict[str, Any]) Dict[str, Any]\n        +validate(context: Dict[str, Any]) bool\n    }\n\n    class RemappingStage {\n        +execute(context: Dict[str, Any]) Dict[str, Any]\n        +validate(context: Dict[str, Any]) bool\n    }\n\n    AudioTranscribeCLI --&gt; AudioPipeline\n    AudioPipeline --&gt; PipelineStage\n    PipelineStage &lt;|-- DiarizationStage\n    PipelineStage &lt;|-- SpeakerTrackStage\n    PipelineStage &lt;|-- TranscriptionStage\n    PipelineStage &lt;|-- RemappingStage</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#5-algorithm-approaches","title":"5. Algorithm Approaches","text":""},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#51-time-mapping-approach","title":"5.1 Time Mapping Approach","text":"<p>The core time mapping functionality can be broken down into simple steps:</p> <pre><code>flowchart TD\n    A[Original Timestamp] --&gt; B{In a segment?}\n    B --&gt;|Yes| C[Calculate relative position in segment]\n    B --&gt;|No| D[Find surrounding segments]\n    C --&gt; E[Map to position in transformed segment]\n    D --&gt; F[Determine if in a gap]\n    F --&gt;|Yes| G[Proportional mapping across gap]\n    F --&gt;|No| H[Handle edge case]\n    G --&gt; I[Return mapped timestamp]\n    E --&gt; I\n    H --&gt; I</code></pre> <p>For implementation, each step should be a simple function:</p> <ol> <li>Find containing segment for a timestamp</li> <li>Calculate relative position within segment</li> <li>Map timestamp from segment to transformed timeline</li> <li>Handle gap mapping with simple proportional scaling</li> </ol>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#52-audio-consolidation-approach","title":"5.2 Audio Consolidation Approach","text":"<p>Audio consolidation can be simplified into these discrete steps:</p> <pre><code>flowchart TD\n    A[Speaker Segments] --&gt; B[Sort by original time]\n    B --&gt; C[Create empty output]\n    C --&gt; D[For each segment]\n    D --&gt; E[Extract segment audio]\n    E --&gt; F[Record time mapping]\n    F --&gt; G[Add to output]\n    G --&gt; H[Add silence gap if needed]\n    H --&gt; D\n    D --&gt;|Done| I[Return consolidated audio]</code></pre> <p>Implementation approach: 1. Extract each segment as a separate function 2. Add segments to output sequentially 3. Track current position for time mapping 4. Insert silence between non-contiguous segments</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#53-srt-remapping-approach","title":"5.3 SRT Remapping Approach","text":"<p>The SRT remapping process follows these simple steps:</p> <pre><code>flowchart TD\n    A[Input SRT File] --&gt; B[Parse SRT]\n    B --&gt; C[For each subtitle]\n    C --&gt; D[Remap start time]\n    D --&gt; E[Remap end time]\n    E --&gt; F[Create new subtitle entry]\n    F --&gt; C\n    C --&gt;|Done| G[Sort by start time]\n    G --&gt; H[Renumber entries]\n    H --&gt; I[Write output SRT]</code></pre> <p>Each step should be implemented as a separate single-purpose function: 1. Parse SRT entries 2. Remap individual timestamps 3. Create new entries with remapped times 4. Sort and renumber entries 5. Write formatted SRT output</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#6-resource-management","title":"6. Resource Management","text":""},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#61-temporary-file-handling","title":"6.1 Temporary File Handling","text":"<p>For the prototype phase, a minimalist approach to file management:</p> <ol> <li>Simple Directory Structure</li> <li>Create timestamped parent directory for each processing run</li> <li>Create subdirectory for each speaker</li> <li> <p>Log all created paths for potential manual cleanup</p> </li> <li> <p>File Naming Convention</p> </li> <li>Use consistent prefixes for file types: <code>speaker_&lt;id&gt;_track.mp3</code></li> <li>Include start/end timestamps in filenames</li> <li> <p>Keep critical metadata in filenames for manual inspection</p> </li> <li> <p>Minimal Cleanup Logic <pre><code># Simple list of temporary files\ntemp_files = []\n\n# Register temporary file\ndef register_temp_file(file_path):\n    temp_files.append(file_path)\n    logger.debug(f\"Registered temporary file: {file_path}\")\n\n# Basic cleanup function - call manually when needed\ndef cleanup_temp_files():\n    for file_path in temp_files:\n        if file_path.exists():\n            file_path.unlink()\n            logger.debug(f\"Removed: {file_path}\")\n</code></pre></p> </li> <li> <p>Development Strategy</p> </li> <li>In prototype phase, prefer leaving files for inspection</li> <li>Add command-line flag for cleanup (<code>--clean-temp</code>) </li> <li>Document created files for manual cleanup when needed</li> </ol>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#7-data-structures","title":"7. Data Structures","text":""},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#71-timemap","title":"7.1 TimeMap","text":"<p>The TimeMap structure is crucial for maintaining timing relationships:</p> <pre><code>class TimeMapInterval:\n    \"\"\"Maps a segment of the original timeline to the transformed timeline.\"\"\"\n    original_start: float  # Start time in original audio (seconds)\n    original_end: float    # End time in original audio (seconds)\n    transformed_start: float  # Start time in transformed audio (seconds)\n\n    @property\n    def duration(self) -&gt; float:\n        \"\"\"Duration of the interval (same in both timelines).\"\"\"\n        return self.original_end - self.original_start\n\n    @property\n    def transformed_end(self) -&gt; float:\n        \"\"\"End time in the transformed timeline.\"\"\"\n        return self.transformed_start + self.duration\n</code></pre> <pre><code>class TimeMap:\n    \"\"\"Maps time points between original and transformed timelines.\"\"\"\n    speaker_id: str\n    intervals: List[TimeMapInterval]  # Ordered list of intervals\n\n    def add_interval(self, original_start: float, original_end: float, \n                    transformed_start: float) -&gt; None:\n        \"\"\"Add a new mapping interval.\"\"\"\n\n    def map_time(self, original_time: float) -&gt; float:\n        \"\"\"Map a time from original timeline to transformed timeline.\"\"\"\n\n    def reverse_map_time(self, transformed_time: float) -&gt; float:\n        \"\"\"Map a time from transformed timeline back to original timeline.\"\"\"\n\n    def export_to_json(self, file_path: Path) -&gt; None:\n        \"\"\"Export the time map to a JSON file.\"\"\"\n\n    @classmethod\n    def import_from_json(cls, file_path: Path) -&gt; 'TimeMap':\n        \"\"\"Import a time map from a JSON file.\"\"\"\n</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#72-speakertrack","title":"7.2 SpeakerTrack","text":"<pre><code>class SpeakerTrack:\n    \"\"\"Represents a consolidated audio track for a single speaker.\"\"\"\n    speaker_id: str\n    segments: List[DiarizationSegment]  # Original segments from diarization\n    consolidated_path: Optional[Path]  # Path to consolidated audio file\n    time_map: TimeMap  # Mapping between original and transformed timelines\n\n    def add_segment(self, segment: DiarizationSegment, transformed_start: float) -&gt; None:\n        \"\"\"Add a segment to the track with its position in the transformed timeline.\"\"\"\n\n    def consolidate_audio(self, original_audio: AudioSegment) -&gt; AudioSegment:\n        \"\"\"Extract and concatenate all segments to create a continuous audio track.\"\"\"\n\n    def save_audio(self, output_dir: Path) -&gt; Path:\n        \"\"\"Save the consolidated audio to a file.\"\"\"\n</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#73-subtitleentry","title":"7.3 SubtitleEntry","text":"<pre><code>class SubtitleEntry:\n    \"\"\"Represents a single subtitle entry in an SRT file.\"\"\"\n    index: int\n    start_time: str  # SRT format time (HH:MM:SS,mmm)\n    end_time: str    # SRT format time (HH:MM:SS,mmm)\n    text: str\n    start_seconds: float  # Time in seconds for calculations\n    end_seconds: float    # Time in seconds for calculations\n\n    @staticmethod\n    def parse_timestamp(timestamp: str) -&gt; float:\n        \"\"\"Convert SRT timestamp to seconds.\"\"\"\n\n    @staticmethod\n    def format_timestamp(seconds: float) -&gt; str:\n        \"\"\"Convert seconds to SRT timestamp.\"\"\"\n\n    def set_times(self, start_seconds: float, end_seconds: float) -&gt; None:\n        \"\"\"Set both timestamps from seconds values.\"\"\"\n\n    def format_srt(self) -&gt; str:\n        \"\"\"Format entry as SRT text.\"\"\"\n\n    def clone(self) -&gt; 'SubtitleEntry':\n        \"\"\"Create a copy of this entry.\"\"\"\n</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#8-process-flow","title":"8. Process Flow","text":""},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#81-speaker-track-generation","title":"8.1 Speaker Track Generation","text":"<pre><code>sequenceDiagram\n    participant AP as AudioPipeline\n    participant DP as DiarizationProcessor\n    participant SG as SpeakerTrackGenerator\n    participant ST as SpeakerTrack\n    participant TM as TimeMap\n\n    AP-&gt;&gt;DP: process_audio()\n    DP-&gt;&gt;AP: diarization_segments\n    AP-&gt;&gt;SG: generate_speaker_tracks(diarization_segments, gap_duration)\n\n    SG-&gt;&gt;SG: group_by_speaker(diarization_segments)\n    Note over SG: Organize segments by speaker ID\n\n    loop For each speaker\n        SG-&gt;&gt;ST: create_track(speaker_id)\n        ST-&gt;&gt;TM: create_time_map()\n\n        loop For each segment\n            ST-&gt;&gt;ST: add_segment(segment, gap_duration)\n            Note over ST: Calculate transformed time position\n            ST-&gt;&gt;TM: add_interval(original_start, original_end, transformed_start)\n            Note over TM: Map original segment to transformed timeline\n        end\n    end\n\n    SG-&gt;&gt;AP: speaker_tracks</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#82-audio-consolidation-and-transcription","title":"8.2 Audio Consolidation and Transcription","text":"<pre><code>sequenceDiagram\n    participant AP as AudioPipeline\n    participant ST as SpeakerTrack\n    participant AS as AudioSegment\n    participant TR as TranscriptionEngine\n    participant TM as TimeMap\n\n    AP-&gt;&gt;AP: load_original_audio()\n\n    loop For each speaker_track\n        AP-&gt;&gt;ST: consolidate_audio(original_audio)\n\n        ST-&gt;&gt;AS: create_empty()\n\n        loop For each segment\n            ST-&gt;&gt;AS: extract_segment(original_audio, segment)\n            AS-&gt;&gt;AS: append(segment_audio)\n        end\n\n        ST-&gt;&gt;ST: save_audio(output_dir)\n        ST-&gt;&gt;TM: export_to_json(output_dir)\n\n        AP-&gt;&gt;TR: transcribe_audio(track.consolidated_path)\n        TR-&gt;&gt;AP: srt_path\n    end</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#83-timing-remapping","title":"8.3 Timing Remapping","text":"<pre><code>sequenceDiagram\n    participant AP as AudioPipeline\n    participant TR as TimingRemapper\n    participant SE as SubtitleEntry\n    participant TM as TimeMap\n\n    loop For each speaker_srt\n        AP-&gt;&gt;TR: remap_srt(srt_path, time_map)\n\n        TR-&gt;&gt;TR: parse_srt_file(srt_path)\n\n        loop For each subtitle_entry\n            TR-&gt;&gt;SE: clone()\n            TR-&gt;&gt;TM: reverse_map_time(entry.start_seconds)\n            TR-&gt;&gt;TM: reverse_map_time(entry.end_seconds)\n            TR-&gt;&gt;SE: set_times(original_start, original_end)\n        end\n\n        TR-&gt;&gt;TR: write_srt_file(remapped_entries, output_path)\n        TR-&gt;&gt;AP: remapped_srt_path\n    end</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#84-complete-pipeline-flow","title":"8.4 Complete Pipeline Flow","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; InitializeContext\n    InitializeContext --&gt; LoadAudio\n    LoadAudio --&gt; Diarization\n    Diarization --&gt; SpeakerSegmentation\n\n    SpeakerSegmentation --&gt; TrackGeneration\n    TrackGeneration --&gt; AudioConsolidation\n    AudioConsolidation --&gt; Transcription\n    Transcription --&gt; TimingRemapping\n\n    TimingRemapping --&gt; GenerateMasterSRT\n    GenerateMasterSRT --&gt; [*]\n\n    state TrackGeneration {\n        [*] --&gt; GroupBySpeaker\n        GroupBySpeaker --&gt; SortSegments\n        SortSegments --&gt; BuildTracks\n        BuildTracks --&gt; BuildTimeMaps\n        BuildTimeMaps --&gt; [*]\n    }\n\n    state AudioConsolidation {\n        [*] --&gt; ProcessSegments\n        ProcessSegments --&gt; SaveTracks\n        SaveTracks --&gt; [*]\n    }\n\n    state TimingRemapping {\n        [*] --&gt; LoadSRTs\n        LoadSRTs --&gt; ParseSRTFiles\n        ParseSRTFiles --&gt; RemapTimestamps\n        RemapTimestamps --&gt; WriteRemappedSRTs\n        WriteRemappedSRTs --&gt; [*]\n    }\n\n    state GenerateMasterSRT {\n        [*] --&gt; MergeSpeakerSRTs\n        MergeSpeakerSRTs --&gt; SortByTimestamp\n        SortByTimestamp --&gt; FormatOutput\n        FormatOutput --&gt; [*]\n    }</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#9-integration-strategy","title":"9. Integration Strategy","text":""},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#91-modular-single-action-functions","title":"9.1 Modular Single-Action Functions","text":"<p>To adhere to the single-action principle, functions in the implementation are designed with the following characteristics:</p> <ol> <li>Focused Responsibility: Each function has one clear purpose</li> <li>Limited Side Effects: Functions minimize state changes</li> <li>Clear Input/Output Boundaries: Well-defined parameters and return values</li> <li>Single Level of Abstraction: Operations within a function are at the same conceptual level</li> </ol> <p>Examples of single-action functions:</p> <pre><code># Time mapping\ndef map_timestamp(timestamp: float, time_map: TimeMap) -&gt; float:\n    \"\"\"Map a single timestamp from one timeline to another.\"\"\"\n\n# SRT processing\ndef parse_srt_timestamp(timestamp: str) -&gt; float:\n    \"\"\"Parse SRT timestamp format to seconds.\"\"\"\n\n# Audio processing\ndef extract_audio_segment(audio: AudioSegment, start: float, end: float) -&gt; AudioSegment:\n    \"\"\"Extract a segment of audio between specified times.\"\"\"\n</code></pre>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#92-simplified-core-concepts","title":"9.2 Simplified Core Concepts","text":"<p>The design centers around three key concepts:</p> <ol> <li>SpeakerTrack: Contains segments for a single speaker throughout the recording</li> <li>TimeMapInterval: Maps an interval of time from the original timeline to the transformed timeline</li> <li>TimeMap: Collection of intervals that collectively define the complete timeline transformation</li> </ol> <p>This minimalist approach makes the design: - More intuitive to understand - Easier to implement and maintain - Directly mapped to real-world entities</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#93-modifications-to-existing-code","title":"9.3 Modifications to Existing Code","text":"<ol> <li>DiarizationProcessor:</li> <li> <p>No changes needed - already produces required data format</p> </li> <li> <p>audio_transcribe.py:</p> </li> <li>Extend CLI options for diarization and SRT generation</li> <li> <p>Add pipeline stage for track generation and remapping</p> </li> <li> <p>Pyannote Client:</p> </li> <li>No changes needed</li> </ol>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#10-extensibility-for-language-detection","title":"10. Extensibility for Language Detection","text":"<p>The design supports future language detection with minimal changes:</p> <pre><code>graph TD\n    A[Audio File] --&gt; B[Diarization]\n    B --&gt; C[Speaker Segmentation]\n    C --&gt; D[Language Detection]\n    D --&gt; E1[Speaker 1 + Language A]\n    D --&gt; E2[Speaker 1 + Language B]\n    D --&gt; E3[Speaker 2 + Language A]\n    E1 --&gt; F1[Transcription Engine A]\n    E2 --&gt; F2[Transcription Engine B]\n    E3 --&gt; F3[Transcription Engine A]\n    F1 --&gt; G[Time Remapping]\n    F2 --&gt; G\n    F3 --&gt; G</code></pre> <p>The speaker track abstraction already accommodates this extension through:</p> <ol> <li>Composable Track Generator:</li> <li>Language detection can be added as a stage between speaker segmentation and track generation</li> <li> <p>Results can feed into extended version of SpeakerTrack</p> </li> <li> <p>Extended TimeMap Structure:</p> </li> <li>TimeMap structure can accommodate additional metadata like language</li> <li> <p>Segments can be tagged with language information</p> </li> <li> <p>Specialized Transcription Engines:</p> </li> <li>SpeakerTrack can be extended with language information</li> <li>Transcription pipeline can select appropriate engine based on language</li> </ol>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#11-implementation-plan","title":"11. Implementation Plan","text":"<ol> <li>Phase 1: Core Infrastructure</li> <li>Implement TimeMap and related utilities</li> <li>Create SpeakerTrack data structures</li> <li> <p>Build SRT parsing/writing utilities</p> </li> <li> <p>Phase 2: Audio Processing</p> </li> <li>Implement track consolidation</li> <li>Add gap insertion logic</li> <li> <p>Build audio segment extraction</p> </li> <li> <p>Phase 3: Integration</p> </li> <li>Extend AudioPipeline with new stages</li> <li>Create CLI integration</li> <li> <p>Add configuration options</p> </li> <li> <p>Phase 4: Testing and Refinement</p> </li> <li>Test with various speaker patterns</li> <li>Fine-tune timing accuracy</li> <li>Optimize performance</li> </ol>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#12-configuration-options","title":"12. Configuration Options","text":"<p>The design supports the following configuration options:</p> <ol> <li>Speaker Gap Duration: Time gap (in seconds) to insert between non-contiguous segments</li> <li>Minimum Segment Duration: Threshold for including a speaker segment</li> <li>Maximum Audio Processing Chunk: Size limit for audio processing to manage memory usage</li> <li>Audio Export Format: Format options for speaker tracks (MP3, WAV, etc.)</li> <li>SRT Output Options: Character encoding, line formatting, etc.</li> <li>Timeline Tolerance: Precision threshold for timestamp mapping operations</li> </ol>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#13-future-development-needs","title":"13. Future Development Needs","text":""},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#131-testing-strategy","title":"13.1 Testing Strategy","text":"<p>A comprehensive testing strategy will need to be developed for the production version. This should include unit tests for core algorithms, integration tests for component interactions, and end-to-end tests with real audio samples. Performance benchmarks and accuracy measurements will be essential for validating the system.</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#132-performance-optimization","title":"13.2 Performance Optimization","text":"<p>While the prototype implementation focuses on correctness, the production version will require performance optimization. This includes memory management for large audio files, potentially implementing streaming processing for segments, and parallelizing independent operations like speaker track processing.</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#133-integration-details","title":"13.3 Integration Details","text":"<p>Detailed integration specifications with existing TNH Scholar components will be required. This includes formal API definitions, compatibility requirements, and version management strategies to ensure smooth integration with the broader system.</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#134-validation-metrics","title":"13.4 Validation Metrics","text":"<p>A formal set of validation metrics needs to be established. This should include measures of diarization accuracy (speaker identification precision/recall), timing accuracy (temporal alignment of speech segments), and overall quality metrics for the final output.</p>"},{"location":"architecture/transcription/design/speaker-diarization-time-mapped-design/#14-conclusion","title":"14. Conclusion","text":"<p>This design provides an elegant and streamlined approach to speaker-specific transcription with accurate time mapping. By focusing on essential concepts and removing unnecessary complexity, the implementation will be both intuitive and maintainable.</p> <p>The core of the design is the simplified TimeMapInterval concept, which maps sections of the original audio timeline to the transformed timeline with minimal complexity. This approach elegantly handles the complex interleaved speaker patterns common in real conversations.</p> <p>The system requires minimal changes to existing code while providing a powerful new capability. By adhering to the single-action function principle and applying Occam's razor to remove unnecessary components, this design achieves conceptual clarity without sacrificing functionality.</p> <p>This streamlined architecture will be easier to implement, test, and extend in the future, providing a solid foundation for enhancements such as language detection.</p>"},{"location":"architecture/transcription/design/timelinemapper-design/","title":"TimelineMapper Design Document","text":"<p>Design for the TimelineMapper component that reprojects chunk-level transcripts into the original audio timeline.</p>"},{"location":"architecture/transcription/design/timelinemapper-design/#purpose","title":"Purpose","text":"<p>In an audio transcription pipeline that integrates speaker diarization and chunked audio processing, transcriptions produced by ASR (Automatic Speech Recognition) are offset relative to each audio chunk rather than the original full audio file.</p> <p>The <code>TimelineMapper</code> solves this by remapping each transcription segment's start and end timecodes from the chunk's local timeline back into the global original audio timeline.</p>"},{"location":"architecture/transcription/design/timelinemapper-design/#context","title":"Context","text":""},{"location":"architecture/transcription/design/timelinemapper-design/#high-level-pipeline","title":"High-Level Pipeline","text":"<pre><code>flowchart LR\n    DiarizationJSON[Diarization JSON] --&gt; Chunker[DiarizationChunker]\n    Chunker --&gt; AudioHandler\n    AudioHandler --&gt; Transcription\n    Transcription --&gt; TimelineMapper\n    TimelineMapper --&gt; SRTProcessor</code></pre> <ol> <li>DiarizationChunker splits diarization segments into manageable processing units</li> <li>AudioHandler extracts corresponding audio for each chunk</li> <li>Transcription produces transcriptions relative to chunk timelines</li> <li>TimelineMapper reprojects the transcription back into original audio coordinates</li> <li>SRTProcessor formats the finalized subtitles</li> </ol>"},{"location":"architecture/transcription/design/timelinemapper-design/#core-problem","title":"Core Problem","text":"<p>When a chunk of audio is built, it splices together diarized segments, sometimes inserting silences for gaps. This causes the chunk's internal time (0 \u2192 chunk duration) to diverge from the original global audio timeline.</p> <p>Since ASR operates on the chunk, its output must be corrected to be meaningful relative to the original full audio.</p>"},{"location":"architecture/transcription/design/timelinemapper-design/#remapping-strategy","title":"Remapping Strategy","text":"Step Description 1 Build a piecewise mapping table from original segment start/end times and their corresponding chunk offsets (<code>audio_map_time</code>). 2 For each subtitle segment (in chunk time), find the best matching interval in the mapping table. 3 Apply a simple shift (no stretch or scale): <code>global_time = orig_start + (chunk_time - local_start)</code> 4 Handle edge cases gracefully, e.g., no matching interval found. 5 Return a new <code>TimedText</code> object with remapped times, keeping all text and metadata intact."},{"location":"architecture/transcription/design/timelinemapper-design/#conceptual-illustration","title":"Conceptual Illustration","text":"<pre><code>flowchart TB\n    TimedTextLocal[\"TimedText&lt;br/&gt;(chunk time)\"] --&gt; TimelineMapper[\"TimelineMapper.remap()\"]\n    DiarizationChunk[\"DiarizationChunk&lt;br/&gt;(original mapping info)\"] --&gt; TimelineMapper\n    TimelineMapper --&gt; TimedTextGlobal[\"TimedText&lt;br/&gt;(global time)\"]</code></pre>"},{"location":"architecture/transcription/design/timelinemapper-design/#detailed-algorithm","title":"Detailed Algorithm","text":""},{"location":"architecture/transcription/design/timelinemapper-design/#mapping-table-construction","title":"Mapping Table Construction","text":"<p>Each diarization segment produces an interval:</p> <ul> <li><code>orig_start</code> \u2192 <code>orig_end</code> (original timeline)</li> <li><code>local_start</code> = <code>audio_map_time</code></li> <li><code>local_end</code> = <code>audio_map_time + segment_duration</code></li> </ul> <p>For inserted silences (gap fill-ins), we can infer synthetic intervals if needed to maintain timeline continuity.</p>"},{"location":"architecture/transcription/design/timelinemapper-design/#remapping-each-subtitle","title":"Remapping Each Subtitle","text":"<p>For each transcription segment:</p> <ol> <li>Identify overlap: Find the mapping interval with the largest overlap (\u2265 1 ms).</li> <li>Shift time:</li> </ol> <pre><code>new_start = orig_start + (subtitle_start - local_start)\nnew_end   = orig_start + (subtitle_end - local_start)\n</code></pre> <ol> <li> <p>Edge Handling:</p> </li> <li> <p>If no matching interval is found, either:</p> <ul> <li>Log a warning and leave unchanged</li> <li>Apply a fallback shift</li> <li>Raise an exception (configurable)</li> </ul> </li> </ol>"},{"location":"architecture/transcription/design/timelinemapper-design/#proposed-timelinemapper-class","title":"Proposed <code>TimelineMapper</code> Class","text":"<pre><code>from typing import List, Optional\nfrom tnh_scholar.audio_processing.transcription_service.diarization_chunker import DiarizationChunk\nfrom tnh_scholar.audio_processing.timed_text import TimedText, TimedTextUnit\n\nclass TimelineMapper:\n    \"\"\"Maps TimedText from chunk-relative timecodes back to global timecodes.\"\"\"\n\n    def remap(self, timed_text: TimedText, chunk: DiarizationChunk) -&gt; TimedText:\n        \"\"\"\n        Remap TimedText from chunk-relative timecodes back to global timecodes.\n\n        Args:\n            timed_text: TimedText with chunk-relative timestamps\n            chunk: DiarizationChunk with mapping information\n\n        Returns:\n            New TimedText with global timestamps\n        \"\"\"\n        if not chunk.segments:\n            raise exception\n\n        ...\n\n        return TimedText(segments=remapped_segments)\n</code></pre>"},{"location":"architecture/transcription/design/timelinemapper-design/#internal-helpers","title":"Internal Helpers","text":"Helper Purpose <code>_build_mapping_intervals(chunk: DiarizationChunk)</code> Constructs ordered list of mapping intervals <code>_find_best_overlap(intervals, start, end)</code> Finds the mapping interval with the greatest overlap for a subtitle <code>_apply_shift(interval, segment)</code> Applies the simple shift formula to subtitle times <p>Internal data structure:</p> <pre><code>@dataclass\nclass MappingInterval:\n    orig_start: int\n    orig_end: int\n    local_start: int\n    local_end: int\n</code></pre>"},{"location":"architecture/transcription/design/timelinemapper-design/#design-considerations","title":"Design Considerations","text":"<ul> <li>Statelessness: <code>TimelineMapper</code> does not hold state between remaps.</li> <li>Immutability: Never mutate the input <code>TimedText</code>; always produce a new instance.</li> <li> <p>Performance:</p> </li> <li> <p>Pre-sort intervals by <code>local_start</code></p> </li> <li>Consider <code>bisect</code> (binary search) for efficient lookup</li> <li> <p>Configurability:</p> </li> <li> <p>Handling of unmatched subtitles (warn/skip/fallback)</p> </li> <li>Optional detailed logging for debug mode</li> <li> <p>Testability:</p> </li> <li> <p>Easily unit-testable with synthetic chunks and known offsets</p> </li> <li>Edge case coverage: overlaps, straddling gaps, missing matches</li> </ul>"},{"location":"architecture/transcription/design/timelinemapper-design/#testing-plan","title":"Testing Plan","text":"Test Case Description Basic remap Straightforward 1-to-1 mapping with no gaps Overlapping segments Subtitle overlaps two mapping intervals Gaps and silences Subtitle falls into an inserted silence No matching interval Subtitle completely outside known mappings Boundary conditions Subtitle exactly starts/ends at interval boundary Performance test 10,000+ segments"},{"location":"architecture/transcription/design/timelinemapper-design/#potential-future-extensions","title":"Potential Future Extensions","text":"<ul> <li>Affine time transforms: for stretch/compression correction (e.g., <code>(t) \u2192 a\u00b7t + b</code>)</li> <li>Speaker-aware remapping: Map only into segments from the same speaker</li> <li>Multi-chunk remapping: Process an entire transcription consisting of multiple chunks in one pass</li> <li>Confidence-weighted remap: If multiple overlaps, prefer higher ASR confidence segments</li> </ul>"},{"location":"architecture/transcription/design/timelinemapper-design/#implementation-plan","title":"Implementation Plan","text":""},{"location":"architecture/transcription/design/timelinemapper-design/#1-core-implementation-1-2-days","title":"1. Core Implementation (1-2 days)","text":"<ul> <li>Build <code>MappingInterval</code> class and internal helpers</li> <li>Implement basic remapping algorithm</li> <li>Handle edge cases for unmatched segments</li> </ul>"},{"location":"architecture/transcription/design/timelinemapper-design/#2-testing-1-day","title":"2. Testing (1 day)","text":"<ul> <li>Create unit tests with synthetic data</li> <li>Test with real audio data from the pipeline</li> </ul>"},{"location":"architecture/transcription/design/timelinemapper-design/#3-pipeline-integration-1-day","title":"3. Pipeline Integration (1 day)","text":"<ul> <li>Add to the transcription processing workflow</li> <li>Optimize for performance with real data</li> </ul>"},{"location":"architecture/transcription/design/timelinemapper-design/#summary","title":"Summary","text":"<p>The <code>TimelineMapper</code> cleanly restores transcriptions into the global audio timeline after diarization and chunked processing, preserving the modular design and clarity of the broader transcription pipeline.</p> <p>Its simplicity, performance, and testability make it a critical bridge from localized ASR output to globally meaningful subtitle and transcript data.</p>"},{"location":"architecture/ui-ux/","title":"Ui Ux","text":"<p>Table of Contents:</p> <p>Design - Table of contents for architecture/ui-ux/design</p> <p>Vs Code Integration - Table of contents for architecture/ui-ux/vs-code-integration</p> <p>This file auto-generated.</p>"},{"location":"architecture/ui-ux/design/","title":"Design","text":"<p>Table of Contents:</p> <p>Design Strategy: VS Code as UI/UX Platform for TNH Scholar - ---</p> <p>This file auto-generated.</p>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/","title":"Design Strategy: VS Code as UI/UX Platform for TNH Scholar","text":""},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#abstract","title":"Abstract","text":"<p>This document proposes a comprehensive UI/UX design strategy for the TNH Scholar project, advocating for Visual Studio Code (VS Code) as the core user experience platform. It outlines the rationale, layering approach, user profiles, and surface strategies that leverage VS Code\u2019s extensibility and ecosystem to deliver a powerful, maintainable, and familiar environment for scholarly editing, reading, and collaboration.</p>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#context-and-motivation","title":"Context and Motivation","text":"<p>UI/UX is central to the TNH Scholar mission: enabling scholars, editors, and readers to interact fluidly with complex, multilingual, and richly annotated textual corpora. Previous approaches using bespoke FastAPI/Tailwind viewers, though valuable for surfacing data issues, demanded significant engineering effort to achieve even basic UI/UX parity with modern editors. These solutions suffered from slow iteration, limited feature sets, and high maintenance overhead. In contrast, VS Code offers a thriving extension ecosystem, robust editor features, and a familiar interface for many users. Its support for extension packs, custom profiles, and webviews makes it uniquely suited to serve as both a deep editorial platform and a customizable reading environment.</p>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#design-strategy","title":"Design Strategy","text":"<p>We propose adopting VS Code as the primary UI/UX base for TNH Scholar. This involves:</p> <ul> <li>Developing a suite of TNH Scholar extensions targeting key layers of functionality.</li> <li>Packaging these extensions into curated profiles and extension packs for different user cohorts.</li> <li>Leveraging VS Code\u2019s webview APIs for custom document viewers and interactive panels.</li> <li>Using VS Code\u2019s settings, keybindings, and workspace configurations to tailor experiences for diverse roles. This strategy ensures rapid feature development, maintainability, and access to a mature ecosystem, while allowing us to focus on domain-specific value rather than reinventing core editor capabilities.</li> </ul>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#layering-approach","title":"Layering Approach","text":"<p>The functional architecture is mapped to discrete VS Code extensions, each corresponding to a layer of the user experience:</p> <ol> <li>Viewer Extension: Parallel text viewer (e.g., Vietnamese/English, images, annotations), leveraging custom editors and webviews.</li> <li>Corpus &amp; TOC Extension: Navigation of corpora, table of contents, and structural metadata.</li> <li>Authoring &amp; Metadata Extension: Editing, annotation, and metadata management tools.</li> <li>Search &amp; Insights Extension: Advanced search, concordance, and data insights panels.</li> <li>Pipelines Extension: Integration with data processing and the TNH pattern system, validation, and export pipelines.</li> <li>Collaboration Extension: Real-time or asynchronous collaboration features, comments, and versioning. Extensions are bundled into extension packs and surfaced via curated profiles.</li> </ol>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#profiles-and-user-cohorts","title":"Profiles and User Cohorts","text":"<p>To accommodate diverse workflows, we define simplified, curated VS Code profiles for major user roles:</p> <ul> <li>Reader/Annotator: Clean, distraction-free interface; parallel viewer; annotation tools.</li> <li>Scholar/Editor: Full editorial features; metadata editing; advanced search; scripting support.</li> <li>Maintainer: Data validation, pipeline integration, and export tools. Profiles are distributed as extension packs with pre-configured settings, keybindings, and UI layouts, reducing onboarding friction and ensuring consistency across user cohorts.</li> </ul>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#web-vs-desktop-surfaces","title":"Web vs Desktop Surfaces","text":"<p>Our dual-surface strategy balances accessibility and power:</p> <ul> <li>VS Code Web (vscode.dev): Instant, installation-free access for lightweight reading, annotation, and review tasks.</li> <li>VS Code Desktop: Full-featured editorial environment, supporting local workflows, advanced scripting, and deep extension integration. This approach enables casual users to engage with the corpus easily while providing power users with the tools needed for high-touch editorial work. Trade-offs include limited local file access and extension support on the web, versus richer capabilities and offline access on desktop.</li> </ul>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#evaluation-test-for-viability","title":"Evaluation Test for Viability","text":"<p>A quick \u201cwalking skeleton\u201d test will validate the approach: implement a custom editor extension that opens a <code>bundle.json</code> file, displaying parallel Vietnamese/English text alongside images in split panes. This minimal prototype exercises the core VS Code APIs (custom editors, webviews, extension activation) and demonstrates the feasibility of the layered, extension-based approach.</p>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#rationale","title":"Rationale","text":"<p>Adopting VS Code as the UI/UX platform is preferable to forking the codebase or persisting with web-only solutions for several reasons:</p> <ul> <li>Maintainability: Leverages ongoing improvements and bug fixes from the VS Code core and extension ecosystem.</li> <li>Ecosystem: Access to a vast library of extensions (e.g., language support, version control, formatting).</li> <li>Speed: Focuses engineering effort on domain-specific features, reducing time-to-value.</li> <li>Familiarity: Many users are already comfortable with VS Code, lowering the learning curve. Forking VS Code would incur significant long-term maintenance costs, while web-only solutions struggle to match the maturity and feature set of VS Code\u2019s editor and extension APIs.</li> </ul>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#historical-discussion-notes","title":"Historical Discussion Notes","text":"<p>Initial work with FastAPI/Tailwind-based viewers surfaced important data and modeling issues but required substantial engineering to approach the UX and productivity of modern editors. Feature iteration was slow, and the gap between custom-built viewers and user expectations remained wide. In contrast, VS Code provides a robust, extensible foundation, allowing us to deliver rich experiences with less effort by building on a mature platform.</p>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#open-questions-and-alternatives","title":"Open Questions and Alternatives","text":"<p>Several open issues and alternatives remain:</p> <ul> <li>Real-time Collaboration: To what extent can VS Code extensions support multi-user, real-time editing? Is Live Share sufficient?</li> <li>Multi-user Editing: How will session management, conflict resolution, and user attribution be handled?</li> <li>Publishing/Export Integration: What is the best way to integrate publishing pipelines\u2014extension, external app, or hybrid?</li> <li>Extension vs External Web App Roles: Where should the boundary lie between VS Code extensions and standalone web apps for specialized tasks or public access?</li> <li>Accessibility and Onboarding: Are there user groups for whom VS Code is a barrier? How can onboarding and accessibility be improved?</li> </ul>"},{"location":"architecture/ui-ux/design/vs-code-as-ui-platform/#next-steps","title":"Next Steps","text":"<p>This document serves as a strategic foundation. The next phase involves authoring detailed Architectural Decision Records (ADRs) to specify and refine key components, such as:</p> <ul> <li>ADR: \u201cParallel Viewer Extension Skeleton\u201d</li> <li>ADR: \u201cProfiles and Extension Pack Packaging\u201d</li> <li>ADR: \u201cCorpus Navigation and TOC Extension\u201d These ADRs will guide the technical implementation and inform ongoing evaluation of the VS Code-based strategy.</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/","title":"Vs Code Integration","text":"<p>Table of Contents:</p> <p>adr-vsc01-vscode-integration-strategy</p> <p>ADR-VSC02: VS Code Extension Integration with tnh-gen CLI - VS Code extension strategy for consuming tnh-gen CLI and providing GenAI text processing UI</p> <p>This file auto-generated.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/","title":"ADR-VSC01: VS Code Integration Strategy (TNH-Scholar Extension v0.1.0)","text":"<p>Status: Proposed Author: Aaron Solomon Date: 2025-01-28 Tags: strategy, vscode, genai, architecture, integration Context: Long-term TNH-Scholar UX roadmap; GenAIService maturity; PromptCatalog stability. Related ADRs: GenAI Service Strategy, UI/UX Strategy (VS Code as Platform)</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#1-context","title":"1. Context","text":"<p>TNH-Scholar is evolving into a multi-layer system with:</p> <ul> <li>a structured corpus,</li> <li>data-processing pipelines,</li> <li>a pattern/prompt-driven GenAIService,</li> <li>provenance-first transformations,</li> <li>and a growing collection of CLI tools and automation flows.</li> </ul> <p>A next logical frontier is developer-facing integration within VS Code, enabling:</p> <ul> <li>fast interaction with patterns,</li> <li>file-level and selection-level transformations,</li> <li>agent-assisted workflows,</li> <li>clearer UX pathways for developers, researchers, and contributors,</li> <li>and eventually: semi-autonomous loops for corpus processing or code maintenance.</li> </ul> <p>This ADR establishes the strategic foundation for VS Code integration and describes the intended shape of v0.1.0 of the TNH-Scholar VS Code Extension.</p> <p>It is not an implementation ADR \u2014 it defines the approach, boundaries, responsibilities, and rationale behind the first integration.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#11-relationship-to-genai-service","title":"1.1 Relationship to GenAI Service","text":"<p>The GenAI Service (see GenAI Service Strategy doc) provides:</p> <ul> <li>Pattern-driven transformations via <code>GenAIService.generate()</code></li> <li>Rich domain model (<code>RenderRequest</code>, <code>CompletionEnvelope</code>, <code>PromptCatalog</code>)</li> <li>Provenance tracking, fingerprinting, policy enforcement</li> <li>Model routing, cost estimation, observability</li> </ul> <p>The VS Code extension is a consumer of GenAI Service capabilities, not a reimplementation. The CLI acts as a transport adapter, exposing GenAI Service functionality through a stable command-line interface.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#2-problem-motivation","title":"2. Problem / Motivation","text":"<p>Developers and contributors currently:</p> <ul> <li>run patterns via scattered CLI tools (<code>tnh-fab</code>, domain-specific scripts) or Python directly,</li> <li>manually open and inspect results,</li> <li>must jump between terminal + editor + documentation,</li> <li>have no discoverability for the PromptCatalog,</li> <li>cannot easily apply transformations to the currently open file or selection,</li> <li>lack real-time feedback during long-running operations.</li> </ul> <p>Desired improvements:</p> <ol> <li> <p>Simple UX slice:    Selecting a prompt \u2192 running it \u2192 producing an output file \u2192 opening it automatically.</p> </li> <li> <p>Discoverability:    QuickPick-like interfaces to browse prompt metadata (names, descriptions, tags, required variables).</p> </li> <li> <p>Developer ergonomics:    Minimize friction; enable quick prototyping and evaluation of patterns.</p> </li> <li> <p>Clear architecture seam:    VS Code should be a thin client \u2014 not aware of Python internals \u2014 and communicate via well-defined, stable interfaces.</p> </li> <li> <p>Long-term extensibility:    Future support for:</p> </li> <li>selections and inline replacements,</li> <li>streaming token-by-token output,</li> <li>multi-file batch operations,</li> <li>progress feedback for long operations,</li> <li>agent-assisted loops,</li> <li>fully autonomous flows.</li> </ol> <p>This ADR sets the direction for achieving these goals.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#3-decision","title":"3. Decision","text":""},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#31-core-architectural-boundary-cli-first-integration","title":"3.1 Core Architectural Boundary: CLI-First Integration","text":"<p>The VS Code Extension will communicate with TNH-Scholar exclusively via a unified CLI interface in v0.1.0.</p> <p>The extension will not:</p> <ul> <li>import Python modules directly,</li> <li>embed Python interpreters,</li> <li>run Python LSP servers,</li> <li>invoke TNH-Scholar internals through ad hoc mechanisms.</li> </ul> <p>Instead, we define a stable CLI integration seam using the new <code>tnh-gen</code> command-line tool.</p> <p>Rationale:</p> <ul> <li>Simplicity: CLI is the fastest path to shipping a working integration.</li> <li>Stability: Isolates VS Code from Python implementation changes.</li> <li>Testability: CLI can be tested independently of the extension.</li> <li>Replaceability: Future transports (HTTP, MCP) can be added without breaking the extension contract.</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#32-the-tnh-gen-cli-tool","title":"3.2 The <code>tnh-gen</code> CLI Tool","text":"<p><code>tnh-gen</code> is a new, unified CLI that:</p> <ul> <li>Replaces the legacy <code>tnh-fab</code> CLI and scattered domain-specific scripts.</li> <li>Wraps the GenAI Service, exposing its rich feature set via command-line interface.</li> <li>Serves as the public contract for VS Code integration.</li> </ul> <p>Design Principles:</p> <ol> <li>GenAI Service parity: Match the GenAI Service's capabilities as closely as possible.</li> <li>Flexible variable passing: Support both JSON file and inline parameter styles.</li> <li>Rich metadata: Expose PromptCatalog metadata for discoverability.</li> <li>Structured output: JSON-formatted responses for programmatic consumption.</li> <li>Error taxonomy: Clear exit codes and error messages aligned with GenAI Service error types.</li> </ol>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#33-the-tnh-gen-cli-core-capabilities","title":"3.3 The <code>tnh-gen</code> CLI: Core Capabilities","text":"<p>The <code>tnh-gen</code> CLI provides three primary commands:</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#tnh-gen-list-discover-prompts","title":"<code>tnh-gen list</code> - Discover Prompts","text":"<p>Lists all available prompts with rich metadata (names, descriptions, tags, required variables).</p> <pre><code>tnh-gen list --format json\n</code></pre> <p>Enables VS Code extension to build dynamic QuickPick interfaces without hardcoding prompt metadata.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#tnh-gen-run-execute-patterns","title":"<code>tnh-gen run</code> - Execute Patterns","text":"<p>Executes a prompt pattern with flexible variable passing:</p> <pre><code># Inline variables\ntnh-gen run --prompt translate \\\n  --input-file teaching.md \\\n  --var source_lang=vi \\\n  --var target_lang=en\n\n# JSON file variables\ntnh-gen run --prompt translate \\\n  --input-file teaching.md \\\n  --vars variables.json\n</code></pre> <p>Key Features:</p> <ul> <li>Supports both JSON file and inline parameter styles</li> <li>Auto-injects file content as <code>input_text</code> variable</li> <li>Outputs structured JSON for programmatic consumption</li> <li>Provides clear exit codes for error handling (0-5)</li> <li>Generates provenance markers in output files</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#tnh-gen-config-manage-configuration","title":"<code>tnh-gen config</code> - Manage Configuration","text":"<p>Configuration discovery with precedence: CLI flags &gt; workspace &gt; user &gt; environment &gt; defaults.</p> <pre><code>tnh-gen config show\ntnh-gen config set max_dollars 0.25\n</code></pre> <p>See ADR-VSC02 for complete CLI implementation details (command signatures, output schemas, error handling, etc.).</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#34-vs-code-extension-commands-v010","title":"3.4 VS Code Extension Commands (v0.1.0)","text":"<p>The extension provides minimal commands that wrap <code>tnh-gen</code>:</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#command-1-tnh-scholar-run-prompt-on-active-file","title":"Command 1: \"TNH Scholar: Run Prompt on Active File\"","text":"<p>Workflow:</p> <ol> <li>Execute <code>tnh-gen list --format json</code></li> <li>Show QuickPick with prompt names + descriptions</li> <li>For selected prompt, show input form for required variables</li> <li>Execute <code>tnh-gen run --prompt &lt;key&gt; --input-file &lt;active_file&gt; --vars &lt;temp.json&gt;</code></li> <li>Parse JSON response</li> <li>Write output to <code>&lt;basename&gt;.&lt;prompt_key&gt;.&lt;ext&gt;</code></li> <li>Open output file in split editor</li> </ol>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#command-2-tnh-scholar-refresh-prompt-catalog","title":"Command 2: \"TNH Scholar: Refresh Prompt Catalog\"","text":"<ul> <li>Re-executes <code>tnh-gen list</code></li> <li>Clears extension cache</li> <li>Shows notification with prompt count</li> </ul> <p>This is a walking skeleton: minimal, end-to-end, testable.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#35-output-strategy-v010","title":"3.5 Output Strategy (v0.1.0)","text":"<p>File Naming:</p> <p>Never overwrite the original file. Use deterministic naming:</p> <pre><code>&lt;basename&gt;.&lt;prompt_key&gt;.&lt;ext&gt;\n\nExamples:\n  teaching.md \u2192 teaching.translate.md\n  teaching.md \u2192 teaching.summarize.md\n  notes.txt \u2192 notes.extract_quotes.txt\n</code></pre> <p>Provenance Markers:</p> <p>GenAI Service automatically prepends provenance metadata to output files:</p> <pre><code>&lt;!--\nTNH-Scholar Generated Content\nPattern: translate (v1.0)\nModel: gpt-4o\nFingerprint: sha256:abc123...\nCorrelation ID: 01HQXYZ123ABC\nGenerated: 2025-01-28T10:30:03Z\n--&gt;\n\n[Generated content follows...]\n</code></pre> <p>Editor Integration:</p> <ul> <li>Open output file automatically in split editor (right pane)</li> <li>Keep original file focused (left pane)</li> <li>Enable side-by-side comparison</li> </ul> <p>Future: Diff view, inline replacements, overwrite confirmations (v0.2.0+).</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#36-error-handling","title":"3.6 Error Handling","text":"<p>VS Code Extension Error Display:</p> Error Type Display Method Policy Error Error notification with budget/limit details Transport Error Error notification + \"Check API key\" hint Provider Error Error notification + \"Try different model\" hint Format Error Warning notification + save partial output with <code>.partial</code> suffix <p>Partial Results:</p> <p>If JSON parsing fails but text is available:</p> <ul> <li>Save to <code>&lt;basename&gt;.&lt;prompt_key&gt;.partial.&lt;ext&gt;</code></li> <li>Show warning notification</li> <li>Log full error to extension output channel</li> </ul> <p>Logging:</p> <p>All CLI invocations and responses are logged to the \"TNH Scholar\" output channel for debugging.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#37-promptcatalog-metadata-source-of-truth","title":"3.7 PromptCatalog Metadata Source of Truth","text":"<p>PromptCatalog (via <code>PromptsAdapter</code>) remains the only authoritative source for:</p> <ul> <li>Prompt keys and versions</li> <li>Human-readable names and descriptions</li> <li>Tags and categorization</li> <li>Required/optional variables</li> <li>Recommended filetypes</li> <li>Default model hints</li> <li>Expected output modes (text/json)</li> </ul> <p>The extension must never duplicate prompt metadata. All metadata is fetched dynamically via <code>tnh-gen list</code>.</p> <p>Note: This requires formalizing the PromptCatalog metadata schema. See ADR-VSC03.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#38-workspace-configuration-discovery","title":"3.8 Workspace Configuration Discovery","text":"<p>For v0.1.0, the extension assumes <code>tnh-gen</code> is on <code>$PATH</code>.</p> <p>Configuration Sources:</p> <pre><code>// .vscode/tnh-scholar.json (workspace-level)\n{\n  \"promptCatalog\": \"./prompts\",\n  \"defaultModel\": \"gpt-4o-mini\",\n  \"maxDollars\": 0.10,\n  \"cliPath\": \"/path/to/tnh-gen\"  // Optional: override CLI location\n}\n</code></pre> <p>Future (v0.2.0+):</p> <ul> <li>Auto-detect Poetry virtualenv</li> <li>Discover in-project venv</li> <li>Fallback to system Python</li> </ul> <p>See ADR-VSC04 for detailed configuration strategy.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#4-rationale","title":"4. Rationale","text":""},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#41-stability","title":"4.1 Stability","text":"<p>The CLI boundary isolates VS Code from Python implementation changes. The GenAI Service can refactor internals without breaking the extension.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#42-replaceability","title":"4.2 Replaceability","text":"<p>The CLI is the v0.1.0 transport. Future versions can add:</p> <ul> <li>HTTP/FastAPI service (v0.2.0) for streaming, progress updates, session management</li> <li>Hybrid approach (v1.0.0) with HTTP preferred, CLI fallback</li> <li>MCP integration (v2.0.0+) for agent-native workflows</li> </ul> <p>The extension can auto-detect and prefer faster transports while maintaining CLI compatibility.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#43-discoverability-ux","title":"4.3 Discoverability &amp; UX","text":"<p>Developers can browse prompt metadata without manual file digging. The QuickPick interface surfaces:</p> <ul> <li>Human-readable names (not just file paths)</li> <li>Descriptions (understanding prompt purpose)</li> <li>Tags (finding related patterns)</li> <li>Required variables (knowing what inputs are needed)</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#44-future-agent-compatibility","title":"4.4 Future-Agent Compatibility","text":"<p>Agent loops require:</p> <ul> <li>Explicit boundaries (CLI provides clear input/output contract)</li> <li>Reproducible inputs/outputs (provenance + fingerprinting)</li> <li>Structured error handling (status envelopes + diagnostics)</li> </ul> <p>The CLI seam provides all three and maps cleanly to future MCP/agent protocols.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#45-genai-service-parity","title":"4.5 GenAI Service Parity","text":"<p>By matching GenAI Service's feature set (<code>tnh-gen</code> as a thin wrapper), we:</p> <ul> <li>Avoid impedance mismatch between Python API and CLI</li> <li>Enable future features (streaming, batch, policy overrides) without redesign</li> <li>Maintain consistency across consumption patterns (Python, CLI, VS Code)</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#46-unified-cli-tnh-gen-replaces-tnh-fab","title":"4.6 Unified CLI (<code>tnh-gen</code> replaces <code>tnh-fab</code>)","text":"<p>Consolidating scattered CLI tools into <code>tnh-gen</code>:</p> <ul> <li>Reduces cognitive load (one tool to learn vs. many)</li> <li>Simplifies documentation and onboarding</li> <li>Provides consistent UX across all GenAI operations</li> <li>Enables future expansion (corpus management, validation, etc.)</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#5-alternatives-considered","title":"5. Alternatives Considered","text":""},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#51-embedding-python-in-the-extension","title":"5.1 Embedding Python in the extension \u274c","text":"<p>Approach: Use <code>python-shell</code> or <code>pyodide</code> to call GenAI Service directly from Node.js.</p> <p>Rejected because:</p> <ul> <li>Too fragile (Python environment discovery, virtualenv management)</li> <li>Platform-specific issues (different Python versions, missing dependencies)</li> <li>Not web-compatible (can't run in vscode.dev)</li> <li>High maintenance burden</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#52-language-server-protocol-lsp","title":"5.2 Language Server Protocol (LSP) \u274c","text":"<p>Approach: Implement TNH-Scholar as a language server.</p> <p>Rejected because:</p> <ul> <li>Protocol mismatch: LSP is designed for language features (diagnostics, hover, completion), not general compute</li> <li>Heavyweight: Requires persistent server + complex protocol implementation</li> <li>Limited discoverability: LSP doesn't have primitives for \"list all patterns\" or \"apply transformation\"</li> <li>Over-engineering for v0.1.0 needs</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#53-direct-http-service-v010","title":"5.3 Direct HTTP service (v0.1.0) \u23f8\ufe0f","text":"<p>Approach: Launch FastAPI service, have extension call HTTP endpoints.</p> <p>Deferred to v0.2.0 because:</p> <ul> <li>Adds daemon management complexity</li> <li>Requires port management, health checks</li> <li>Higher setup friction for users</li> <li>CLI is simpler and sufficient for initial validation</li> </ul> <p>Note: HTTP service is planned for v0.2.0 (see ADR-VSC02).</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#54-direct-node-python-rpc","title":"5.4 Direct Node &lt;-&gt; Python RPC \u274c","text":"<p>Approach: Custom IPC/socket protocol between extension and Python service.</p> <p>Rejected because:</p> <ul> <li>Adds unnecessary coupling</li> <li>Reinvents HTTP without benefits</li> <li>Harder to test and debug than standard protocols</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#6-impact","title":"6. Impact","text":""},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#on-developers","title":"On developers","text":"<p>\u2705 Benefits:</p> <ul> <li>Easy access to GenAI patterns from editor</li> <li>Intuitive prompt discovery and selection</li> <li>Automatic file handling and provenance</li> <li>Reduced context switching (terminal \u2194 editor)</li> </ul> <p>\u26a0\ufe0f Considerations:</p> <ul> <li>Must have <code>tnh-gen</code> installed and on PATH</li> <li>Initial setup requires API key configuration</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#on-codebase","title":"On codebase","text":"<p>\u2705 Required Work:</p> <ul> <li>Implement <code>tnh-gen</code> CLI (new)</li> <li>Expand <code>PromptsAdapter.introspect()</code> for rich metadata</li> <li>Define PromptCatalog metadata schema</li> <li>Create VS Code extension scaffold</li> </ul> <p>\u267b\ufe0f Refactoring:</p> <ul> <li>Migrate <code>tnh-fab</code> functionality to <code>tnh-gen</code></li> <li>Deprecate legacy CLI tools</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#on-documentation","title":"On documentation","text":"<p>\ud83d\udcda New Docs Required:</p> <ul> <li><code>tnh-gen</code> CLI reference</li> <li>VS Code extension user guide</li> <li>Configuration guide</li> <li>Prompt authoring guide (for catalog contributors)</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#7-phased-transport-evolution","title":"7. Phased Transport Evolution","text":"<p>This ADR defines v0.1.0 using CLI. Future versions will evolve the transport:</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#v010-cli-current-adr","title":"v0.1.0: CLI (Current ADR) \u2705","text":"<ul> <li>Transport: <code>tnh-gen</code> CLI subprocess</li> <li>Goal: Ship walking skeleton, validate UX</li> <li>Capabilities: Basic file transformations, prompt discovery</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#v020-add-http-service","title":"v0.2.0: Add HTTP Service \ud83d\udd04","text":"<ul> <li>Transport: FastAPI HTTP service (preferred) + CLI fallback</li> <li>Goal: Rich UX with streaming, progress, sessions</li> <li>Capabilities: Token-by-token output, real-time progress, conversation history</li> <li>See: ADR-VSC02 for HTTP service design</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#v100-hybrid-http-cli","title":"v1.0.0: Hybrid (HTTP + CLI) \ud83c\udfaf","text":"<ul> <li>Transport: Auto-detect HTTP, gracefully fall back to CLI</li> <li>Goal: Best UX when service available, always works offline</li> <li>Capabilities: Full feature parity across transports</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#v200-mcp-integration","title":"v2.0.0+: MCP Integration \ud83d\udd2e","text":"<ul> <li>Transport: Model Context Protocol for agent-native workflows</li> <li>Goal: Enable semi-autonomous corpus processing loops</li> <li>Capabilities: Multi-step agent chains, approval gates, batch orchestration</li> <li>Timing: Evaluate when MCP ecosystem matures (2026+)</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#8-prerequisites-dependencies","title":"8. Prerequisites &amp; Dependencies","text":"<p>Before implementing the VS Code extension, these must be completed:</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#p0-blocking","title":"P0: Blocking","text":"<ol> <li>Implement <code>tnh-gen</code> CLI (ADR-VSC02)</li> <li>Wrap GenAI Service with CLI interface</li> <li>Support JSON and inline variable passing</li> <li> <p>Add to Poetry scripts as entry point</p> </li> <li> <p>Define PromptCatalog Metadata Schema (ADR-VSC03)</p> </li> <li>Formalize metadata fields (name, description, tags, variables)</li> <li>Extend <code>PromptsAdapter.introspect()</code> to return metadata</li> <li> <p>Create metadata validation</p> </li> <li> <p>Define Configuration Strategy (ADR-VSC04)</p> </li> <li>Specify config file formats and locations</li> <li>Define precedence rules</li> <li>Handle API key discovery and secrets management</li> </ol>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#p1-high-priority","title":"P1: High Priority","text":"<ol> <li>Create Error Taxonomy Mapping</li> <li>Map GenAI Service errors to CLI exit codes</li> <li>Define JSON error response schema</li> <li> <p>Document error handling best practices</p> </li> <li> <p>Define Output File Format Strategy</p> </li> <li>Specify provenance marker format</li> <li>Define file extension mapping rules</li> <li>Handle special cases (JSON output, binary formats)</li> </ol>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#9-follow-up-adrs","title":"9. Follow-up ADRs","text":"<p>The following ADRs will detail implementation:</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#adr-vsc02-tnh-gen-cli-implementation-created","title":"ADR-VSC02: <code>tnh-gen</code> CLI Implementation \u2705 Created","text":"<p>Status: Complete (see adr-vsc02-tnh-gen-cli-implementation.md)</p> <p>Scope:</p> <ul> <li>CLI argument parsing and validation (Click framework)</li> <li>Command implementations (<code>list</code>, <code>run</code>, <code>config</code>, <code>version</code>)</li> <li>Variable injection strategies (JSON file, inline params, file content)</li> <li>GenAI Service integration and request building</li> <li>Error handling and exit codes (0-5 taxonomy)</li> <li>Output formatting (JSON, YAML, table)</li> <li>Provenance marker generation</li> <li>Migration plan from <code>tnh-fab</code></li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#adr-vsc03-promptcatalog-metadata-schema-required","title":"ADR-VSC03: PromptCatalog Metadata Schema \ud83d\udd34 Required","text":"<p>Scope:</p> <ul> <li>Metadata field definitions (name, description, tags, variables, etc.)</li> <li>Metadata storage format (YAML frontmatter, JSON sidecar, embedded?)</li> <li><code>introspect()</code> API expansion</li> <li>Validation rules and schema</li> <li>Versioning strategy for prompts</li> <li>Backward compatibility with existing patterns</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#adr-vsc04-configuration-discovery-management-required","title":"ADR-VSC04: Configuration Discovery &amp; Management \ud83d\udd34 Required","text":"<p>Scope:</p> <ul> <li>Configuration file formats (.vscode/tnh-scholar.json, ~/.config/tnh-scholar/)</li> <li>Precedence rules (CLI flags &gt; workspace &gt; user &gt; env &gt; defaults)</li> <li>API key discovery and secrets management</li> <li>Virtualenv/Poetry detection</li> <li>CLI path override mechanisms</li> <li>Environment variable mapping</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#adr-vsc05-vs-code-extension-implementation-implementation","title":"ADR-VSC05: VS Code Extension Implementation \ud83d\udfe1 Implementation","text":"<p>Scope:</p> <ul> <li>Extension architecture and command structure</li> <li>QuickPick UI implementation</li> <li>Variable input forms (dynamic based on prompt metadata)</li> <li>File output handling and editor integration</li> <li>Error display and notification strategy</li> <li>Extension settings and preferences</li> <li>Testing strategy (unit, integration, E2E)</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#adr-vsc06-http-service-for-rich-ux-future-v020","title":"ADR-VSC06: HTTP Service for Rich UX \ud83d\udfe2 Future (v0.2.0)","text":"<p>Scope:</p> <ul> <li>FastAPI service architecture</li> <li>API endpoint design (REST, SSE for streaming)</li> <li>Auto-start mechanism from VS Code extension</li> <li>Health checks and daemon management</li> <li>Session management for multi-turn conversations</li> <li>Hybrid transport strategy (HTTP preferred, CLI fallback)</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#adr-vsc07-selection-level-transformations-future-v030","title":"ADR-VSC07: Selection-Level Transformations \ud83d\udfe2 Future (v0.3.0)","text":"<p>Scope:</p> <ul> <li>Inline text replacement</li> <li>Diff view and preview</li> <li>Undo/redo integration</li> <li>Multi-cursor support</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#adr-vsc08-batch-multi-file-operations-future-v040","title":"ADR-VSC08: Batch &amp; Multi-File Operations \ud83d\udfe2 Future (v0.4.0)","text":"<p>Scope:</p> <ul> <li>Bulk pattern application</li> <li>Progress tracking</li> <li>Cancellation and retry</li> <li>Result aggregation</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#adr-vsc09-agent-assisted-workflows-future-v200","title":"ADR-VSC09: Agent-Assisted Workflows \ud83d\udd2e Future (v2.0.0+)","text":"<p>Scope:</p> <ul> <li>MCP integration</li> <li>Approval gates and human-in-the-loop</li> <li>Semi-autonomous corpus processing</li> <li>Audit logging and provenance chains</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#10-decision-summary","title":"10. Decision Summary","text":"<p>TNH-Scholar will integrate with VS Code via a minimal, stable CLI boundary using the new <code>tnh-gen</code> unified CLI tool.</p> <p>v0.1.0 Walking Skeleton:</p> <ul> <li><code>tnh-gen list</code> \u2014 Discover prompts with rich metadata</li> <li><code>tnh-gen run</code> \u2014 Execute patterns with flexible variable passing</li> <li><code>tnh-gen config</code> \u2014 Manage configuration</li> </ul> <p>Key Principles:</p> <ul> <li>\u2705 CLI-first for simplicity (v0.1.0)</li> <li>\u2705 GenAI Service feature parity (match rich domain model)</li> <li>\u2705 Flexible variable passing (JSON file + inline params)</li> <li>\u2705 Rich error handling (structured responses, exit codes)</li> <li>\u2705 Provenance-first (automatic markers, fingerprinting)</li> <li>\u2705 Transport evolution path (CLI \u2192 HTTP \u2192 Hybrid \u2192 MCP)</li> <li>\u2705 Unified CLI (<code>tnh-gen</code> replaces <code>tnh-fab</code>)</li> </ul> <p>The extension acts as a thin client, delegating all GenAI logic to the CLI/service layer.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#11-acceptance-criteria-v010","title":"11. Acceptance Criteria (v0.1.0)","text":"<p>CLI Implementation:</p> <ul> <li> <code>tnh-gen list --format json</code> returns prompt metadata</li> <li> <code>tnh-gen run</code> supports JSON file variable passing</li> <li> <code>tnh-gen run</code> supports inline <code>--var</code> parameter passing</li> <li> <code>tnh-gen run</code> injects <code>--input-file</code> content as variable</li> <li> CLI outputs structured JSON on success/failure</li> <li> CLI exits with appropriate error codes (0-4)</li> <li> CLI respects configuration precedence (flags &gt; workspace &gt; user &gt; env)</li> <li> <code>tnh-gen</code> is installable via Poetry scripts</li> </ul> <p>VS Code Extension:</p> <ul> <li> \"Run Prompt on Active File\" command works end-to-end</li> <li> QuickPick shows prompt names + descriptions</li> <li> Variable input form dynamically adapts to selected prompt</li> <li> Output file opens automatically in split editor</li> <li> Errors display user-friendly notifications</li> <li> Extension logs CLI invocations to output channel</li> <li> Extension respects workspace configuration</li> </ul> <p>Documentation:</p> <ul> <li> <code>tnh-gen</code> CLI reference published</li> <li> VS Code extension user guide published</li> <li> Configuration guide published</li> <li> Migration guide from <code>tnh-fab</code> published</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#12-status","title":"12. Status","text":"<p>Current: Proposed (awaiting approval)</p> <p>Next Steps:</p> <ol> <li>\u2705 Review and approve ADR-VSC01 (this document)</li> <li>\ud83d\udd34 Write ADR-VSC02 (<code>tnh-gen</code> CLI implementation)</li> <li>\ud83d\udd34 Write ADR-VSC03 (PromptCatalog metadata schema)</li> <li>\ud83d\udd34 Write ADR-VSC04 (Configuration discovery)</li> <li>\ud83d\udfe1 Implement <code>tnh-gen</code> CLI</li> <li>\ud83d\udfe1 Implement VS Code extension</li> <li>\ud83d\udfe2 Ship v0.1.0</li> </ol>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc01-vscode-integration-strategy/#appendix-example-workflow","title":"Appendix: Example Workflow","text":"<p>User Story: Developer wants to translate a Vietnamese teaching to English.</p> <ol> <li>Open <code>teaching.md</code> in VS Code</li> <li>Cmd+Shift+P \u2192 \"TNH Scholar: Run Prompt on Active File\"</li> <li>QuickPick shows:</li> </ol> <pre><code>Vietnamese-English Translation\nTranslate Vietnamese dharma texts to English\nTags: translation, dharma\n</code></pre> <ol> <li>User selects \"Vietnamese-English Translation\"</li> <li>Extension shows input form:</li> </ol> <pre><code>Source Language: [vi]\nTarget Language: [en]\nContext (optional): [Dharma talk on mindfulness]\n</code></pre> <ol> <li>User fills form, clicks \"Run\"</li> <li>Extension executes:</li> </ol> <pre><code>tnh-gen run --prompt translate \\\n  --input-file teaching.md \\\n  --var source_lang=vi \\\n  --var target_lang=en \\\n  --var context=\"Dharma talk on mindfulness\" \\\n  --output-file teaching.translate.md\n</code></pre> <ol> <li>Output file <code>teaching.translate.md</code> opens in split pane with provenance header</li> <li>Success notification: \"Translation completed (gpt-4o, $0.08, 3.4s)\"</li> </ol> <p>Developer experience: 10 seconds from intent to result, zero context switching.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/","title":"ADR-VSC02: VS Code Extension Integration with tnh-gen CLI","text":"<p>This ADR defines how the VS Code extension integrates with the <code>tnh-gen</code> CLI to provide GenAI-powered text processing capabilities within the editor.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-01-28</li> <li>Updated: 2025-12-07</li> <li>Owner: Aaron Solomon</li> <li>Author: Aaron Solomon, Claude Sonnet 4.5</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#context","title":"Context","text":"<p>TNH Scholar users work primarily in VS Code for text editing and translation workflows. The VS Code extension needs to provide:</p> <ol> <li>Prompt Discovery: Browse available prompts without leaving the editor</li> <li>Text Processing: Execute prompts on selected text or open files</li> <li>Configuration Management: Configure prompt directories and GenAI settings</li> <li>Provenance Tracking: Show metadata about generated content</li> </ol>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#design-constraints","title":"Design Constraints","text":"<ul> <li>No Direct GenAI Integration: Extension should not directly call OpenAI/Anthropic APIs</li> <li>CLI as Contract: Extension consumes <code>tnh-gen</code> CLI as stable interface</li> <li>JSON Protocol: Structured JSON I/O enables programmatic consumption</li> <li>Error Handling: Extension must gracefully handle CLI errors with user-friendly messages</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#related-work","title":"Related Work","text":"<ul> <li>ADR-VSC01: VS Code Integration Strategy (establishes CLI-based architecture)</li> <li>ADR-TG01: CLI Architecture (defines <code>tnh-gen</code> command structure, error codes, configuration)</li> <li>ADR-TG02: Prompt System Integration (defines CLI \u2194 prompt system integration patterns)</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#decision","title":"Decision","text":""},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#1-extension-architecture","title":"1. Extension Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   VS Code Extension                         \u2502\n\u2502  (TypeScript, VSCode API, UI components)                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502                        \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 CLI Adapter   \u2502          \u2502 UI Components \u2502\n         \u2502 (spawn tnh-gen\u2502          \u2502 (prompts list,\u2502\n         \u2502  parse JSON)  \u2502          \u2502  progress,    \u2502\n         \u2502               \u2502          \u2502  config)      \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  tnh-gen CLI  \u2502\n         \u2502  (Python)     \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#2-cli-invocation-strategy","title":"2. CLI Invocation Strategy","text":"<p>The extension spawns <code>tnh-gen</code> as a child process and communicates via JSON:</p> <pre><code>// src/cli/CliAdapter.ts\nimport { spawn } from 'child_process';\n\nexport class TnhGenCliAdapter {\n  private cliPath: string;\n\n  constructor(cliPath: string) {\n    this.cliPath = cliPath; // e.g., /path/to/venv/bin/tnh-gen\n  }\n\n  async listPrompts(options?: { tag?: string; search?: string }): Promise&lt;PromptListResponse&gt; {\n    const args = ['list', '--format', 'json'];\n    if (options?.tag) args.push('--tag', options.tag);\n    if (options?.search) args.push('--search', options.search);\n\n    const result = await this.spawnCli(args);\n    return JSON.parse(result.stdout);\n  }\n\n  async runPrompt(request: RunPromptRequest): Promise&lt;RunPromptResponse&gt; {\n    const args = ['run', '--prompt', request.promptKey, '--format', 'json'];\n\n    // Add input file\n    if (request.inputFile) {\n      args.push('--input-file', request.inputFile);\n    }\n\n    // Add variables\n    for (const [key, value] of Object.entries(request.variables)) {\n      args.push('--var', `${key}=${value}`);\n    }\n\n    // Add output file\n    if (request.outputFile) {\n      args.push('--output-file', request.outputFile);\n    }\n\n    const result = await this.spawnCli(args);\n    return JSON.parse(result.stdout);\n  }\n\n  private async spawnCli(args: string[]): Promise&lt;{ stdout: string; stderr: string; exitCode: number }&gt; {\n    return new Promise((resolve, reject) =&gt; {\n      const proc = spawn(this.cliPath, args);\n      let stdout = '';\n      let stderr = '';\n\n      proc.stdout.on('data', (data) =&gt; stdout += data.toString());\n      proc.stderr.on('data', (data) =&gt; stderr += data.toString());\n\n      proc.on('close', (code) =&gt; {\n        if (code === 0) {\n          resolve({ stdout, stderr, exitCode: code });\n        } else {\n          reject(new CliError(stdout, stderr, code || -1));\n        }\n      });\n    });\n  }\n}\n</code></pre>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#3-error-handling","title":"3. Error Handling","text":"<p>Map CLI exit codes (ADR-TG01 \u00a75) to user-friendly messages:</p> <pre><code>// src/cli/CliError.ts\nexport class CliError extends Error {\n  constructor(\n    public stdout: string,\n    public stderr: string,\n    public exitCode: number\n  ) {\n    super(CliError.formatMessage(stdout, exitCode));\n  }\n\n  static formatMessage(stdout: string, exitCode: number): string {\n    try {\n      const response = JSON.parse(stdout);\n      if (response.error) {\n        // Use CLI's structured error message\n        return response.diagnostics?.suggestion\n          ? `${response.error}\\n\\nSuggestion: ${response.diagnostics.suggestion}`\n          : response.error;\n      }\n    } catch {\n      // Fallback to generic message\n      return CliError.genericMessage(exitCode);\n    }\n  }\n\n  static genericMessage(exitCode: number): string {\n    switch (exitCode) {\n      case 1: return 'Policy error: Budget exceeded or validation failed';\n      case 2: return 'Transport error: API failure or network issue';\n      case 3: return 'Provider error: Model unavailable or rate limit exceeded';\n      case 4: return 'Format error: Invalid JSON or schema validation failed';\n      case 5: return 'Input error: Invalid arguments or missing required variables';\n      default: return `Unknown error (exit code ${exitCode})`;\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#4-ui-components","title":"4. UI Components","text":""},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#41-prompt-picker-quick-pick","title":"4.1 Prompt Picker (Quick Pick)","text":"<pre><code>// src/commands/runPrompt.ts\nimport * as vscode from 'vscode';\nimport { TnhGenCliAdapter } from '../cli/CliAdapter';\n\nexport async function runPromptCommand(context: vscode.ExtensionContext) {\n  const cli = new TnhGenCliAdapter(getCliPath(context));\n\n  // 1. List prompts\n  const response = await cli.listPrompts();\n\n  // 2. Show quick pick\n  const selected = await vscode.window.showQuickPick(\n    response.prompts.map(p =&gt; ({\n      label: p.name,\n      description: p.tags.join(', '),\n      detail: p.description,\n      promptKey: p.key,\n      requiredVariables: p.required_variables\n    })),\n    { placeHolder: 'Select a prompt to run' }\n  );\n\n  if (!selected) return;\n\n  // 3. Collect variables\n  const variables: Record&lt;string, string&gt; = {};\n  for (const varName of selected.requiredVariables) {\n    const value = await vscode.window.showInputBox({\n      prompt: `Enter value for ${varName}`,\n      placeHolder: varName\n    });\n    if (!value) return; // User cancelled\n    variables[varName] = value;\n  }\n\n  // 4. Get input file (active document)\n  const editor = vscode.window.activeTextEditor;\n  if (!editor) {\n    vscode.window.showErrorMessage('No active document');\n    return;\n  }\n\n  // Save document to temp file\n  const inputFile = await saveTempFile(editor.document.getText());\n\n  // 5. Execute prompt\n  try {\n    await vscode.window.withProgress(\n      { location: vscode.ProgressLocation.Notification, title: 'Processing...' },\n      async () =&gt; {\n        const result = await cli.runPrompt({\n          promptKey: selected.promptKey,\n          inputFile,\n          variables,\n          outputFile: inputFile + '.out'\n        });\n\n        // 6. Show result\n        const doc = await vscode.workspace.openTextDocument(inputFile + '.out');\n        await vscode.window.showTextDocument(doc);\n      }\n    );\n  } catch (error) {\n    if (error instanceof CliError) {\n      vscode.window.showErrorMessage(error.message);\n    } else {\n      throw error;\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#42-configuration-management","title":"4.2 Configuration Management","text":"<pre><code>// src/config/ConfigManager.ts\nexport class ConfigManager {\n  static getCliPath(context: vscode.ExtensionContext): string {\n    // Precedence: workspace &gt; user &gt; auto-detect\n    const workspaceConfig = vscode.workspace.getConfiguration('tnhScholar');\n    const cliPath = workspaceConfig.get&lt;string&gt;('cliPath');\n\n    if (cliPath) return cliPath;\n\n    // Auto-detect from active Python environment\n    return this.detectCliPath();\n  }\n\n  private static detectCliPath(): string {\n    // Use Python extension API to get active interpreter\n    const pythonExt = vscode.extensions.getExtension('ms-python.python');\n    if (pythonExt?.isActive) {\n      const pythonPath = pythonExt.exports.settings.getExecutionDetails().execCommand[0];\n      // Assume tnh-gen is in same venv\n      return pythonPath.replace(/python$/, 'tnh-gen');\n    }\n\n    // Fallback to $PATH\n    return 'tnh-gen';\n  }\n}\n</code></pre>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#5-cli-discovery-and-version-checking","title":"5. CLI Discovery and Version Checking","text":"<pre><code>// src/cli/CliValidator.ts\nexport class CliValidator {\n  static async validateCli(cliPath: string): Promise&lt;{ valid: boolean; version?: string; error?: string }&gt; {\n    try {\n      const proc = spawn(cliPath, ['version', '--format', 'json']);\n      const stdout = await this.readStream(proc.stdout);\n      const version = JSON.parse(stdout);\n\n      // Check minimum version\n      if (this.compareVersions(version.tnh_gen, '0.1.0') &lt; 0) {\n        return {\n          valid: false,\n          error: `tnh-gen version ${version.tnh_gen} is too old (minimum: 0.1.0)`\n        };\n      }\n\n      return { valid: true, version: version.tnh_gen };\n    } catch (error) {\n      return {\n        valid: false,\n        error: `Failed to execute tnh-gen at ${cliPath}: ${error.message}`\n      };\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#6-extension-configuration-schema","title":"6. Extension Configuration Schema","text":"<pre><code>// package.json (contributes.configuration)\n{\n  \"contributes\": {\n    \"configuration\": {\n      \"title\": \"TNH Scholar\",\n      \"properties\": {\n        \"tnhScholar.cliPath\": {\n          \"type\": \"string\",\n          \"default\": null,\n          \"description\": \"Path to tnh-gen CLI executable (auto-detected if not set)\"\n        },\n        \"tnhScholar.promptDirectory\": {\n          \"type\": \"string\",\n          \"default\": null,\n          \"description\": \"Path to prompt directory (overrides TNH_PROMPT_DIR)\"\n        },\n        \"tnhScholar.defaultModel\": {\n          \"type\": \"string\",\n          \"default\": \"gpt-4o-mini\",\n          \"description\": \"Default GenAI model for prompts\"\n        },\n        \"tnhScholar.maxCostUsd\": {\n          \"type\": \"number\",\n          \"default\": 0.10,\n          \"description\": \"Maximum cost per request (USD)\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#consequences","title":"Consequences","text":""},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#positive","title":"Positive","text":"<ul> <li>Stable Contract: Extension depends only on CLI JSON protocol, not Python internals</li> <li>Version Independence: Extension and CLI can evolve independently</li> <li>Error Transparency: CLI exit codes and structured errors enable rich error handling</li> <li>Testability: CLI can be mocked for extension unit tests</li> <li>Reusability: CLI implementation (ADR-TG01/TG02) serves both VS Code and command-line users</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#negative","title":"Negative","text":"<ul> <li>Process Overhead: Spawning Python process for each operation introduces latency (mitigated by keeping CLI operations fast)</li> <li>Version Synchronization: Extension must validate CLI version compatibility</li> <li>Error Mapping: Extension must parse CLI JSON errors and present user-friendly messages</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#risks","title":"Risks","text":"<ul> <li>CLI Path Discovery: Auto-detection may fail in complex Python environments (mitigated by explicit configuration)</li> <li>Breaking Changes: CLI protocol changes require coordinated extension updates (mitigated by semantic versioning)</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#alternative-1-direct-python-integration-via-python-extension","title":"Alternative 1: Direct Python Integration (via Python Extension)","text":"<p>Approach: Extension imports TNH Scholar Python modules directly via VS Code Python extension API.</p> <p>Rejected: Tight coupling to Python implementation. Extension would need to handle Python environment activation, dependency resolution, and version compatibility.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#alternative-2-language-server-protocol-lsp","title":"Alternative 2: Language Server Protocol (LSP)","text":"<p>Approach: Create TNH Scholar language server that VS Code extension communicates with via LSP.</p> <p>Rejected: Overengineering for initial MVP. LSP is designed for language features (completion, diagnostics), not GenAI operations.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#alternative-3-rest-api","title":"Alternative 3: REST API","text":"<p>Approach: Run TNH Scholar as HTTP server, extension makes REST calls.</p> <p>Rejected: Adds complexity (server lifecycle management, port conflicts). CLI spawn model is simpler for single-user desktop usage.</p>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#open-questions","title":"Open Questions","text":"<ol> <li>Streaming Support: How should extension handle streaming CLI output (future <code>--streaming</code> flag)?</li> <li>Multi-Root Workspaces: How to handle different prompt directories per workspace folder?</li> <li>Offline Mode: Should extension cache prompt list to avoid repeated CLI calls?</li> </ol>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#references","title":"References","text":""},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#related-adrs","title":"Related ADRs","text":"<ul> <li>ADR-VSC01: VS Code Integration Strategy - Overall VS Code strategy</li> <li>ADR-TG01: CLI Architecture - CLI command structure, error codes, configuration</li> <li>ADR-TG02: Prompt System Integration - CLI \u2194 prompt system integration</li> <li>ADR-AT03: AI Text Processing Refactor - ai_text_processing refactor</li> </ul>"},{"location":"architecture/ui-ux/vs-code-integration/adr-vsc02-tnh-gen-cli-implementation/#external-resources","title":"External Resources","text":"<ul> <li>VS Code Extension API</li> <li>VS Code Python Extension</li> <li>Typer CLI Framework</li> </ul> <p>This ADR focuses on VS Code extension strategy. CLI implementation details are defined in ADR-TG01 and ADR-TG02.</p>"},{"location":"architecture/utilities/","title":"Utilities","text":"<p>Table of Contents:</p> <p>Design - Table of contents for architecture/utilities/design</p> <p>This file auto-generated.</p>"},{"location":"architecture/utilities/design/","title":"Design","text":"<p>Table of Contents:</p> <p>Package Version Checker Design Document - Specification for a reusable package version checking utility with flexible strategies and clear reporting.</p> <p>TNH\u2011Scholar Utilities Catalog - This catalog lists core utility modules used across the TNH\u2011Scholar codebase. It provides a quick reference for shared abstractions, their purpose, API highlights, and stability. All modules are currently in Prototype phase, with most stable in active use.</p> <p>This file auto-generated.</p>"},{"location":"architecture/utilities/design/package-version-checker-design/","title":"Package Version Checker Design Document","text":"<p>Specification for a reusable package version checking utility with flexible strategies and clear reporting.</p>"},{"location":"architecture/utilities/design/package-version-checker-design/#overview","title":"Overview","text":"<p>This document outlines the design for a robust, reusable package version checking utility that can verify versions for any Python package against minimum requirements, latest versions, or specific version constraints.</p> <p>The utility provides a simple, flexible API for checking package versions with configurable behavior for different compatibility requirements. It's designed to support both simple version checks and sophisticated version difference monitoring.</p>"},{"location":"architecture/utilities/design/package-version-checker-design/#context","title":"Context","text":"<p>Modern Python applications often depend on rapidly evolving libraries. Ensuring compatibility with these dependencies requires monitoring version differences and enforcing version requirements. This is especially important for libraries with frequent updates and API changes, such as OpenAI's client libraries.</p> <p>Existing approaches often lack flexibility in defining what constitutes a \"compatible\" version, particularly when dealing with semantic versioning differences at various levels (major, minor, patch).</p>"},{"location":"architecture/utilities/design/package-version-checker-design/#goals","title":"Goals","text":"<ul> <li>Create a modular, extensible version checking system that works with any Python package</li> <li>Provide both programmatic and CLI interfaces</li> <li>Support multiple checking strategies (minimum version, exact version, version range)</li> <li>Include robust error handling with configurable failure modes</li> <li>Maintain clean separation of concerns</li> <li>Support caching to minimize network requests</li> <li>Provide clear, actionable feedback to users</li> <li>Allow fine-grained control over version difference thresholds</li> </ul>"},{"location":"architecture/utilities/design/package-version-checker-design/#design","title":"Design","text":""},{"location":"architecture/utilities/design/package-version-checker-design/#core-components","title":"Core Components","text":"<pre><code>classDiagram\n    class PackageVersionChecker {\n        +check_version(package_name, config) Result\n    }\n\n    class VersionCheckerConfig {\n        +fail_on_error: bool\n        +cache_duration: int\n        +network_timeout: int\n        +strategy: VersionStrategy\n        +requirement: str\n    }\n\n    class VersionStrategy {\n        &lt;&lt;enumeration&gt;&gt;\n        MINIMUM\n        EXACT\n        LATEST\n        RANGE\n        VERSION_DIFF\n    }\n\n    class PackageInfo {\n        +name: str\n        +installed_version: Version\n        +required_version: Version\n        +latest_version: Version\n    }\n\n    class Result {\n        +is_compatible: bool\n        +needs_update: bool\n        +package_info: PackageInfo\n        +error: Optional[str]\n        +get_upgrade_command() str\n    }\n\n    class VersionProvider {\n        &lt;&lt;interface&gt;&gt;\n        +get_installed_version(package_name) Version\n        +get_latest_version(package_name) Version\n    }\n\n    class StandardVersionProvider {\n        +get_installed_version(package_name) Version\n        +get_latest_version(package_name) Version\n    }\n\n    class VersionCache {\n        +get(package_name) CachedVersion\n        +set(package_name, version) None\n        +is_valid(package_name) bool\n    }\n\n    PackageVersionChecker --&gt; VersionCheckerConfig : uses\n    PackageVersionChecker --&gt; VersionProvider : uses\n    PackageVersionChecker --&gt; Result : produces\n    Result *-- PackageInfo\n    VersionCheckerConfig *-- VersionStrategy\n    VersionProvider &lt;|-- StandardVersionProvider\n    StandardVersionProvider --&gt; VersionCache : uses</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#key-classes","title":"Key Classes","text":""},{"location":"architecture/utilities/design/package-version-checker-design/#1-packageversionchecker","title":"1. <code>PackageVersionChecker</code>","text":"<p>The main entry point for version checking functionality:</p> <pre><code>class PackageVersionChecker:\n    def __init__(self, \n                 provider: Optional[VersionProvider] = None,\n                 cache: Optional[VersionCache] = None):\n        self.provider = provider or StandardVersionProvider()\n        self.cache = cache or VersionCache()\n\n    def check_version(self, \n                      package_name: str, \n                      config: Optional[VersionCheckerConfig] = None) -&gt; Result:\n        \"\"\"Check if package meets version requirements based on config.\"\"\"\n        # Implementation\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#2-versioncheckerconfig","title":"2. <code>VersionCheckerConfig</code>","text":"<p>Configuration for version checking behavior:</p> <pre><code>class VersionCheckerConfig:\n    def __init__(self,\n                 strategy: VersionStrategy = VersionStrategy.MINIMUM,\n                 requirement: str = \"\",\n                 fail_on_error: bool = False,\n                 cache_duration: int = 3600,  # 1 hour\n                 network_timeout: int = 5,    # seconds\n                 vdiff_warn_matrix: Optional[str] = None,\n                 vdiff_fail_matrix: Optional[str] = None):\n        \"\"\"\n        Initialize version checker configuration.\n\n        Args:\n            strategy: Version checking strategy to use\n            requirement: Version requirement string (for MINIMUM, EXACT, RANGE strategies)\n            fail_on_error: Whether to raise exceptions on errors\n            cache_duration: How long to cache PyPI results in seconds\n            network_timeout: Network request timeout in seconds\n            vdiff_warn_matrix: Version difference matrix for warnings [major.minor.micro]\n                               Use * for any/infinity, e.g. \"1.5.*\" \n            vdiff_fail_matrix: Version difference matrix for failures [major.minor.micro]\n                               Use * for any/infinity, e.g. \"0.20.*\"\n        \"\"\"\n        self.strategy = strategy\n        self.requirement = requirement\n        self.fail_on_error = fail_on_error\n        self.cache_duration = cache_duration\n        self.network_timeout = network_timeout\n\n        # Version difference matrices\n        self.vdiff_warn_matrix = vdiff_warn_matrix\n        self.vdiff_fail_matrix = vdiff_fail_matrix\n\n    def parse_vdiff_matrix(self, matrix_str: Optional[str]) -&gt; Tuple[Optional[int], Optional[int], Optional[int]]:\n        \"\"\"\n        Parse a version difference matrix string into component thresholds.\n\n        Args:\n            matrix_str: String in format \"major.minor.micro\" where each component\n                       is either a number or * for infinity\n\n        Returns:\n            Tuple of (major_diff, minor_diff, micro_diff) thresholds\n\n        Examples:\n            \"1.5.*\" -&gt; (1, 5, None)  # Major diff &gt; 1 or minor diff &gt; 5, ignore micro\n            \"0.0.0\" -&gt; (0, 0, 0)     # Any difference triggers\n            \"*.*.*\" -&gt; (None, None, None)  # No difference triggers\n        \"\"\"\n        if not matrix_str:\n            return None, None, None\n\n        parts = matrix_str.split(\".\")\n        if len(parts) != 3:\n            raise ValueError(f\"Invalid version difference matrix: {matrix_str}. \"\n                            f\"Format should be 'major.minor.micro'\")\n\n        thresholds = []\n        for part in parts:\n            if part == \"*\":\n                thresholds.append(None)  # None represents infinity/any\n            else:\n                try:\n                    thresholds.append(int(part))\n                except ValueError:\n                    raise ValueError(f\"Invalid version component in matrix: {part}. \"\n                                    f\"Must be a number or *\")\n\n        return tuple(thresholds)  # type: Tuple[Optional[int], Optional[int], Optional[int]]\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#3-result","title":"3. <code>Result</code>","text":"<p>Structured result of version check operations:</p> <pre><code>class Result:\n    def __init__(self,\n                 is_compatible: bool,\n                 needs_update: bool,\n                 package_info: PackageInfo,\n                 error: Optional[str] = None,\n                 warning_level: Optional[str] = None,\n                 diff_details: Optional[Dict[str, int]] = None):\n        self.is_compatible = is_compatible\n        self.needs_update = needs_update\n        self.package_info = package_info\n        self.error = error\n        self.warning_level = warning_level  # e.g., \"MAJOR\", \"MINOR\", \"MICRO\"\n        self.diff_details = diff_details    # e.g., {\"major\": 1, \"minor\": 5}\n\n    def get_upgrade_command(self) -&gt; str:\n        \"\"\"Return pip command to upgrade package.\"\"\"\n        # Implementation\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#implementation-phases","title":"Implementation Phases","text":""},{"location":"architecture/utilities/design/package-version-checker-design/#phase-1-core-implementation","title":"Phase 1: Core Implementation","text":"<ol> <li>Define all interfaces and data classes</li> <li>Implement <code>StandardVersionProvider</code> (using importlib.metadata and PyPI API)</li> <li>Implement basic caching mechanism</li> <li>Implement core <code>PackageVersionChecker</code> functionality</li> <li>Add support for minimum version strategy</li> </ol>"},{"location":"architecture/utilities/design/package-version-checker-design/#phase-2-enhanced-functionality","title":"Phase 2: Enhanced Functionality","text":"<ol> <li>Add support for all version strategies</li> <li>Implement robust error handling</li> <li>Add configurable logging</li> <li>Implement cache invalidation</li> <li>Add utility functions for common use cases</li> </ol>"},{"location":"architecture/utilities/design/package-version-checker-design/#phase-3-cli-and-integration","title":"Phase 3: CLI and Integration","text":"<ol> <li>Create CLI interface</li> <li>Add support for requirements.txt parsing</li> <li>Implement environment-aware version checking</li> <li>Add documentation and examples</li> <li>Create integration tests</li> </ol>"},{"location":"architecture/utilities/design/package-version-checker-design/#implementation-plan","title":"Implementation Plan","text":""},{"location":"architecture/utilities/design/package-version-checker-design/#module-structure","title":"Module Structure","text":"<pre><code>src/tnh_scholar/utils/\n\u251c\u2500\u2500 version_check/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 checker.py       # Main PackageVersionChecker class\n\u2502   \u251c\u2500\u2500 config.py        # Configuration classes\n\u2502   \u251c\u2500\u2500 providers.py     # Version provider implementations  \n\u2502   \u251c\u2500\u2500 cache.py         # Caching mechanisms\n\u2502   \u251c\u2500\u2500 models.py        # Data models (Result, PackageInfo)\n\u2502   \u251c\u2500\u2500 strategies.py    # Version comparison strategies\n\u2502   \u2514\u2500\u2500 cli.py           # Command-line interface\n\u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/utilities/design/package-version-checker-design/#version-retrieval","title":"Version Retrieval","text":"<p>We'll build on the existing implementation, but make it more generic:</p> <pre><code>def get_installed_version(package_name: str) -&gt; Version:\n    \"\"\"Get installed package version.\"\"\"\n    try:\n        if version_str := str(importlib.metadata.version(package_name)):\n            return Version(version_str)\n        else:\n            raise InvalidVersion(f\"{package_name} version string is empty\")\n    except importlib.metadata.PackageNotFoundError as e:\n        raise ImportError(f\"{package_name} is not installed\") from e\n    except InvalidVersion as e:\n        raise InvalidVersion(f\"Invalid version for {package_name}: {e}\") from e\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#version-comparison","title":"Version Comparison","text":"<p>Version strategies will use comparison operators from <code>packaging.version</code>:</p> <pre><code>def check_minimum_version(installed: Version, \n                         required: Version) -&gt; bool:\n    \"\"\"Check if installed version meets minimum requirement.\"\"\"\n    return installed &gt;= required\n\ndef check_compatible_version(installed: Version, \n                           required: Version) -&gt; bool:\n    \"\"\"Check if installed version is compatible with requirement.\"\"\"\n    # Example: versions like 2.1.x are compatible with 2.1.0\n    return (installed.major == required.major and \n            installed.minor == required.minor and\n            installed.micro &gt;= required.micro)\n\ndef check_version_diff(installed: Version,\n                      reference: Version,\n                      vdiff_matrix: Optional[str] = None) -&gt; Tuple[bool, Dict[str, int]]:\n    \"\"\"\n    Check if version difference is within specified limits.\n\n    Args:\n        installed: The installed version\n        reference: The reference version to compare against (typically latest)\n        vdiff_matrix: Version difference matrix string in format \"major.minor.micro\"\n\n    Returns:\n        Tuple of (is_within_limits, diff_details)\n        where diff_details is a dict with actual differences\n    \"\"\"\n    # Calculate actual differences\n    major_diff = abs(reference.major - installed.major)\n    minor_diff = abs(reference.minor - installed.minor) if reference.major == installed.major else 0\n    micro_diff = abs(reference.micro - installed.micro) if (reference.major == installed.major and \n                                                            reference.minor == installed.minor) else 0\n\n    diff_details = {\n        \"major\": major_diff,\n        \"minor\": minor_diff,\n        \"micro\": micro_diff\n    }\n\n    # If no matrix provided, differences are acceptable\n    if not vdiff_matrix:\n        return True, diff_details\n\n    # Parse matrix\n    major_limit, minor_limit, micro_limit = parse_vdiff_matrix(vdiff_matrix)\n\n    # Check limits (None means no limit)\n    if major_limit is not None and major_diff &gt; major_limit:\n        return False, diff_details\n\n    if minor_limit is not None and minor_diff &gt; minor_limit:\n        return False, diff_details\n\n    if micro_limit is not None and micro_diff &gt; micro_limit:\n        return False, diff_details\n\n    return True, diff_details\n\ndef parse_vdiff_matrix(matrix_str: str) -&gt; Tuple[Optional[int], Optional[int], Optional[int]]:\n    \"\"\"\n    Parse a version difference matrix string.\n\n    Args:\n        matrix_str: String in format \"major.minor.micro\"\n\n    Returns:\n        Tuple of (major_limit, minor_limit, micro_limit)\n        where None represents no limit\n    \"\"\"\n    parts = matrix_str.split(\".\")\n    if len(parts) != 3:\n        raise ValueError(f\"Invalid version difference matrix: {matrix_str}\")\n\n    limits = []\n    for part in parts:\n        if part == \"*\":\n            limits.append(None)  # No limit\n        else:\n            try:\n                limits.append(int(part))\n            except ValueError:\n                raise ValueError(f\"Invalid version component: {part}\")\n\n    return tuple(limits)  # type: Tuple[Optional[int], Optional[int], Optional[int]]\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#caching-mechanism","title":"Caching Mechanism","text":"<p>Simple time-based caching for PyPI results:</p> <pre><code>class VersionCache:\n    def __init__(self, cache_duration: int = 3600):\n        self.cache = {}\n        self.timestamps = {}\n        self.cache_duration = cache_duration\n\n    def get(self, key: str) -&gt; Optional[Version]:\n        \"\"\"Get cached version if still valid.\"\"\"\n        if not self.is_valid(key):\n            return None\n        return self.cache.get(key)\n\n    def set(self, key: str, value: Version) -&gt; None:\n        \"\"\"Cache version with current timestamp.\"\"\"\n        self.cache[key] = value\n        self.timestamps[key] = time.time()\n\n    def is_valid(self, key: str) -&gt; bool:\n        \"\"\"Check if cached value is still valid.\"\"\"\n        if key not in self.timestamps:\n            return False\n        age = time.time() - self.timestamps[key]\n        return age &lt; self.cache_duration\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#error-handling","title":"Error Handling","text":"<p>Robust error handling with configurable behavior:</p> <pre><code>def handle_error(error: Exception, \n                config: VersionCheckerConfig, \n                package_name: str) -&gt; Result:\n    \"\"\"Handle errors based on configuration.\"\"\"\n    error_message = str(error)\n    logger.error(f\"Error checking {package_name} version: {error_message}\")\n\n    if config.fail_on_error:\n        raise error\n\n    return Result(\n        is_compatible=False,\n        needs_update=False,\n        package_info=PackageInfo(name=package_name),\n        error=error_message\n    )\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#usage-examples","title":"Usage Examples","text":""},{"location":"architecture/utilities/design/package-version-checker-design/#basic-usage","title":"Basic Usage","text":"<pre><code>from tnh_scholar.utils.version_check import PackageVersionChecker, VersionCheckerConfig, VersionStrategy\n\n# Check if package meets minimum version\nchecker = PackageVersionChecker()\nresult = checker.check_version(\n    \"openai\", \n    VersionCheckerConfig(\n        strategy=VersionStrategy.MINIMUM,\n        requirement=\"1.70.0\"\n    )\n)\n\nif not result.is_compatible:\n    print(f\"openai package version {result.package_info.installed_version} \" \n          f\"does not meet minimum requirement {result.package_info.required_version}\")\n    print(f\"Upgrade with: {result.get_upgrade_command()}\")\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#integration-with-whisper-service","title":"Integration with Whisper Service","text":"<pre><code>def __init__(self, api_key: Optional[str] = None, **config_options):\n    # Check OpenAI package version\n    checker = PackageVersionChecker()\n    result = checker.check_version(\n        \"openai\",\n        VersionCheckerConfig(\n            strategy=VersionStrategy.MINIMUM,\n            requirement=\"1.70.0\",\n            fail_on_error=False\n        )\n    )\n\n    if not result.is_compatible:\n        logger.warning(\n            f\"OpenAI package version {result.package_info.installed_version} \"\n            f\"is older than required version {result.package_info.required_version}. \"\n            f\"Some features may not work as expected. \"\n            f\"Please upgrade using: {result.get_upgrade_command()}\"\n        )\n\n    # Continue with initialization\n    self.config = WhisperConfig()\n    # ...\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#command-line-interface","title":"Command-line Interface","text":"<pre><code>usage: version-check [-h] [--min MIN_VERSION] [--exact EXACT_VERSION] \n                     [--vdiff-warn VDIFF_WARN_MATRIX]\n                     [--vdiff-fail VDIFF_FAIL_MATRIX]\n                     [--fail-on-error] [--timeout TIMEOUT] [--no-cache]\n                     PACKAGE_NAME\n\nCheck if a Python package meets version requirements.\n\npositional arguments:\n  PACKAGE_NAME          Name of the package to check\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --min MIN_VERSION     Minimum required version\n  --exact EXACT_VERSION Exact required version\n  --vdiff-warn VDIFF_WARN_MATRIX\n                        Version difference matrix for warnings in format major.minor.micro\n                        Use * for any/infinity, e.g. \"1.5.*\"\n  --vdiff-fail VDIFF_FAIL_MATRIX\n                        Version difference matrix for failures in format major.minor.micro\n                        Use * for any/infinity, e.g. \"0.20.*\"\n  --fail-on-error       Exit with error code on version mismatch\n  --timeout TIMEOUT     Network timeout in seconds\n  --no-cache            Bypass cache and check PyPI directly\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#implementation-examples","title":"Implementation Examples","text":"<p>Here's a simplified sketch of the key implementation components:</p> <pre><code>def check_version(\n    package_name: str, \n    config: VersionCheckerConfig\n) -&gt; Result:\n    \"\"\"Main version checking function.\"\"\"\n    try:\n        installed = get_installed_version(package_name)\n        latest = get_latest_version(package_name, config.network_timeout)\n\n        # Determine compatibility based on strategy\n        is_compatible = True\n        needs_update = False\n        warning_level = None\n        diff_details = None\n\n        if config.strategy == VersionStrategy.MINIMUM:\n            required = Version(config.requirement)\n            is_compatible = installed &gt;= required\n            needs_update = installed &lt; latest\n\n        elif config.strategy == VersionStrategy.EXACT:\n            required = Version(config.requirement)\n            is_compatible = installed == required\n            needs_update = installed != required\n\n        elif config.strategy == VersionStrategy.VERSION_DIFF:\n            # Check warning threshold\n            if config.vdiff_warn_matrix:\n                warn_within_limits, diff_details = check_version_diff(\n                    installed, latest, config.vdiff_warn_matrix)\n                if not warn_within_limits:\n                    warning_level = determine_warning_level(diff_details)\n\n            # Check failure threshold\n            if config.vdiff_fail_matrix:\n                fail_within_limits, diff_details = check_version_diff(\n                    installed, latest, config.vdiff_fail_matrix)\n                is_compatible = fail_within_limits\n\n            needs_update = installed &lt; latest\n\n        # Create package info\n        package_info = PackageInfo(\n            name=package_name,\n            installed_version=str(installed),\n            latest_version=str(latest),\n            required_version=config.requirement if hasattr(config, 'requirement') else None\n        )\n\n        # Create and return result\n        return Result(\n            is_compatible=is_compatible,\n            needs_update=needs_update,\n            package_info=package_info,\n            warning_level=warning_level,\n            diff_details=diff_details\n        )\n\n    except Exception as e:\n        # Handle errors based on configuration\n        if config.fail_on_error:\n            raise\n        return Result(\n            is_compatible=False,\n            needs_update=False,\n            package_info=PackageInfo(name=package_name),\n            error=str(e)\n        )\n</code></pre>"},{"location":"architecture/utilities/design/package-version-checker-design/#extensibility","title":"Extensibility","text":"<p>The design supports these extension points:</p> <ol> <li> <p>New Version Strategies: Adding new comparison strategies is as simple as extending the <code>VersionStrategy</code> enum and implementing the corresponding check in the <code>check_version</code> method.</p> </li> <li> <p>Custom Version Providers: The separation of version retrieval logic allows for swapping in alternative providers for handling private repositories, alternate package indexes, or specialized version parsing.</p> </li> <li> <p>Advanced Caching: The caching system can be extended to provide persistent caching, environment-aware caching, or more sophisticated invalidation strategies.</p> </li> <li> <p>Result Processing: The detailed <code>Result</code> object can be used to generate different types of reports, integrate with monitoring systems, or enforce custom policies.</p> </li> </ol>"},{"location":"architecture/utilities/design/package-version-checker-design/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Support for version ranges with upper bounds</li> <li>Integration with virtual environment detection</li> <li>Cache persistence across runs</li> <li>Support for private PyPI repositories</li> <li>Batch checking of multiple packages</li> <li>Integration with dependency resolution</li> <li>Support for pre-release versions</li> <li>More sophisticated version difference metrics (e.g., weighted differences)</li> <li>Configurable version comparison for non-semver packages</li> <li>Support for checking against specific release channels (stable, beta, etc.)</li> <li>Integration with package security advisories</li> </ul>"},{"location":"architecture/utilities/design/package-version-checker-design/#conclusion","title":"Conclusion","text":"<p>This version checking utility will provide a robust, flexible solution for ensuring package compatibility across the TNH Scholar project. By building on the existing YTDVersionChecker implementation but making it more generic and configurable, it will help prevent runtime errors and improve user experience.</p>"},{"location":"architecture/utilities/design/utilities-catalog/","title":"TNH\u2011Scholar Utilities Catalog","text":"<p>This catalog lists core utility modules used across the TNH\u2011Scholar codebase. It provides a quick reference for shared abstractions, their purpose, API highlights, and stability. All modules are currently in Prototype phase, with most stable in active use.</p> Status Meaning Stable Used widely; interface considered reliable Evolving Actively refined; API may change Experimental Early or niche use; subject to major revision"},{"location":"architecture/utilities/design/utilities-catalog/#timing_utilspy-stable","title":"timing_utils.py \u2014 Stable","text":"<p>Module: <code>tnh_scholar.utils.timing_utils</code></p> <p>Purpose: Small, explicit time type for millisecond-based values. Replaces ad-hoc float timestamps and makes conversions and arithmetic explicit and type-safe.</p> <p>Key APIs:</p> <ul> <li><code>class TimeMs(int)</code> \u2014 lightweight typed integer representing milliseconds. Supports construction from int/float/TimeMs, arithmetic (+, -, radd, rsub), and pydantic core schema integration.</li> <li><code>TimeMs.from_seconds(seconds: float) -&gt; TimeMs</code> \u2014 construct from seconds.</li> <li><code>TimeMs.to_ms() -&gt; int</code> \u2014 return milliseconds as int.</li> <li><code>TimeMs.to_seconds() -&gt; float</code> \u2014 return seconds as float.</li> <li><code>convert_sec_to_ms(val: float) -&gt; int</code> \u2014 helper to convert seconds to ms (rounded).</li> <li><code>convert_ms_to_sec(ms: int) -&gt; float</code> \u2014 helper to convert ms to seconds.</li> </ul> <p>Used by: Any module that needs deterministic millisecond arithmetic or pydantic models (e.g., provenance, latency reporting).</p> <p>Notes: Designed to be small and explicit; does not attempt to provide clocks or wall-time semantics \u2014 that belongs in an Observer/tracer implementation.</p>"},{"location":"architecture/utilities/design/utilities-catalog/#json_utilspy-stable","title":"json_utils.py \u2014 Stable","text":"<p>Module: <code>tnh_scholar.utils.json_utils</code></p> <p>Purpose: Robust JSON helpers for files and pydantic models: read/write JSON, load JSONL, and format files consistently.</p> <p>Key APIs:</p> <ul> <li><code>write_data_to_json_file(file: Path, data: dict | list, indent=4, ensure_ascii=False) -&gt; None</code> \u2014 serialize data to JSON and write to disk, creating parent folders.</li> <li><code>save_model_to_json(file: Path, model: BaseModel, indent=4, ensure_ascii=False) -&gt; None</code> \u2014 dump a Pydantic model (uses <code>model_dump()</code>), writing via <code>write_data_to_json_file</code>.</li> <li><code>load_jsonl_to_dict(file_path: Path) -&gt; list[dict]</code> \u2014 read a JSONL file into a list of dicts.</li> <li><code>load_json_into_model(file: Path, model: type[BaseModel]) -&gt; BaseModel</code> \u2014 load JSON and validate/construct the given Pydantic model type (raises ValueError on failure).</li> <li><code>format_json(file: Path) -&gt; None</code> \u2014 read and re-write JSON file with indentation and ensure_ascii=False.</li> </ul> <p>Used by: Config I/O, model persistence, dataset pre-processing, and simple scripting tasks.</p> <p>Notes: Functions raise informative exceptions (ValueError / IOError) on failure to make error handling explicit to callers.</p>"},{"location":"architecture/utilities/design/utilities-catalog/#langpy-stable","title":"lang.py \u2014 Stable","text":"<p>Module: <code>tnh_scholar.utils.lang</code></p> <p>Purpose: Language detection helpers using <code>langdetect</code> for short samples and <code>pycountry</code> for mapping codes to English names.</p> <p>Key APIs:</p> <ul> <li><code>get_language_code_from_text(text: str) -&gt; str</code> \u2014 returns ISO-639-1 code (e.g., 'en') or 'un' when detection fails; raises ValueError if input is empty.</li> <li><code>get_language_name_from_text(text: str) -&gt; str</code> \u2014 returns English language name for detected code (uses <code>pycountry</code>).</li> <li><code>get_language_from_code(code: str) -&gt; str</code> \u2014 maps ISO code to human name or returns 'Unknown' with a warning.</li> <li><code>_get_sample_text(text: str, words_per_sample: int = 30) -&gt; str</code> \u2014 internal helper that extracts 3 samples (start, \u2153, \u2154) to improve detection for long text.</li> </ul> <p>Used by: Metadata extraction, normalization, and any pipeline that needs a quick language hint.</p> <p>Notes: Defensive for long/short texts; returns 'un' when detection fails to avoid exceptions in pipelines.</p>"},{"location":"architecture/utilities/design/utilities-catalog/#file_utilspy-stable","title":"file_utils.py \u2014 Stable","text":"<p>Module: <code>tnh_scholar.utils.file_utils</code></p> <p>Purpose: Common filesystem helpers used by CLI and batch jobs: ensure directories, write/read strings, copy files matching patterns, and filename sanitization.</p> <p>Key APIs:</p> <ul> <li><code>DEFAULT_MAX_FILENAME_LENGTH: int</code> \u2014 default max length used by sanitizers.</li> <li><code>FileExistsWarning</code> \u2014 custom warning class.</li> <li><code>ensure_directory_exists(dir_path: Path) -&gt; bool</code> \u2014 mkdir -p semantics; returns True on success.</li> <li><code>ensure_directory_writable(dir_path: Path) -&gt; None</code> \u2014 verifies/creates dir and tests writability using a NamedTemporaryFile (raises on failure).</li> <li><code>iterate_subdir(directory: Path, recursive: bool = False) -&gt; Generator[Path, None, None]</code> \u2014 yield subdirectory Paths (one level or recursive).</li> <li><code>path_source_str(path: Path) -&gt; str</code> \u2014 return resolved string path.</li> <li><code>copy_files_with_regex(source_dir: Path, destination_dir: Path, regex_patterns: list[str], preserve_structure: bool = True) -&gt; None</code> \u2014 copy files one level down that match patterns; creates destination directories as needed.</li> <li><code>read_str_from_file(file_path: Path) -&gt; str</code> \u2014 read full text content.</li> <li><code>write_str_to_file(file_path: PathLike, text: str, overwrite: bool = False) -&gt; None</code> \u2014 write text with optional overwrite guard.</li> <li><code>sanitize_filename(filename: str, max_length: int = DEFAULT_MAX_FILENAME_LENGTH) -&gt; str</code> \u2014 normalize/slugify and truncate to safe ascii filename.</li> <li><code>to_slug(string: str) -&gt; str</code> \u2014 produce a URL\u2011friendly slug (lowercase, hyphens).</li> <li><code>path_as_str(path: Path) -&gt; str</code> \u2014 alias for resolved path as string.</li> </ul> <p>Used by: Any code that reads/writes files, prepares artifacts for storage, or needs consistent filename handling.</p> <p>Notes: The module intentionally surfaces IO exceptions for callers to handle; it prefers explicit failures in prototype code.</p>"},{"location":"architecture/utilities/design/utilities-catalog/#user_io_utilspy-stable","title":"user_io_utils.py \u2014 Stable","text":"<p>Module: <code>tnh_scholar.utils.user_io_utils</code></p> <p>Purpose: Small cross-platform console utilities for interactive scripts (single-character input, confirmation prompts), with fallbacks for Jupyter/IPython.</p> <p>Key APIs:</p> <ul> <li><code>get_single_char(prompt: Optional[str] = None) -&gt; str</code> \u2014 read a single character without requiring Enter in terminal environments; falls back to <code>input()</code> in notebooks.</li> <li><code>get_user_confirmation(prompt: str, default: bool = True) -&gt; bool</code> \u2014 prompt for a y/n confirmation using <code>get_single_char</code>; returns default on Enter.</li> </ul> <p>Used by: CLI scripts, interactive tooling, and any dev tooling that wants compact confirmations.</p> <p>Notes: The implementation handles Windows (msvcrt) and Unix (termios/tty) cases and deliberately falls back in interactive notebook environments.</p>"},{"location":"architecture/utilities/design/utilities-catalog/#validatepy-stable","title":"validate.py \u2014 Stable","text":"<p>Module: <code>tnh_scholar.utils.validate</code></p> <p>Purpose: Lightweight environment checks and user-facing error messages for required environment variables and features.</p> <p>Key APIs:</p> <ul> <li><code>get_env_message(missing_vars: List[str], feature: str = \"this feature\") -&gt; str</code> \u2014 human-friendly message explaining how to set missing env vars.</li> <li><code>check_env(required_vars: Set[str], feature: str = \"this feature\", output: bool = True) -&gt; bool</code> \u2014 returns True if all required vars are present; logs/prints helpful message if not.</li> <li><code>check_openai_env(output: bool = True) -&gt; bool</code> \u2014 convenience wrapper checking <code>OPENAI_API_KEY</code>.</li> <li><code>check_ocr_env(output: bool = True) -&gt; bool</code> \u2014 convenience wrapper checking <code>GOOGLE_APPLICATION_CREDENTIALS</code>.</li> </ul> <p>Used by: Startup checks, test harnesses, and preflight validation in scripts.</p>"},{"location":"architecture/utilities/design/utilities-catalog/#logging_configpy-stable","title":"logging_config.py \u2014 Stable","text":"<p>Module: <code>tnh_scholar.logging_config</code></p> <p>Purpose: Centralized, production-grade logging configuration for the entire TNH-Scholar system. Provides color/plain text logs in development, JSON logs in production, queue-based asynchronous logging, file rotation, noise suppression, and Python warnings capture. Designed for library compatibility and app-layer configurability.</p> <p>Key APIs:</p> <ul> <li><code>setup_logging(...)</code> \u2014 main initializer; reads environment variables to configure log level, handlers, formatters, and rotation. Should be called once by the application layer (CLI, Streamlit, API service).</li> <li><code>get_logger(name: str)</code> \u2014 preferred helper to retrieve a logger for a given module or component.</li> <li><code>get_child_logger(name: str, console=False, separate_file=False)</code> \u2014 legacy helper for modules needing ad-hoc console or file handlers; maintained for backward compatibility.</li> <li><code>setup_logging_legacy(...)</code> \u2014 deprecated alias to <code>setup_logging()</code> with a <code>DeprecationWarning</code>.</li> <li><code>priority_info(message, *args, **kwargs)</code> \u2014 legacy helper method on logger instances; emits at custom level 25 with a deprecation warning; prefer <code>logger.info(..., extra={\"priority\": \"high\"})</code>.</li> </ul> <p>Environment Variables:</p> <ul> <li><code>APP_ENV</code>: <code>dev</code> | <code>prod</code> | <code>test</code> (default: <code>dev</code>)</li> <li><code>LOG_JSON</code>: <code>true|false</code> (enable JSON output; default true in prod)</li> <li><code>LOG_STDOUT</code>: <code>true|false</code> (emit to stdout)</li> <li><code>LOG_FILE_ENABLE</code>: <code>true|false</code></li> <li><code>LOG_FILE_PATH</code>: path to log file (default <code>./logs/main.log</code>)</li> <li><code>LOG_ROTATE_BYTES</code>, <code>LOG_ROTATE_WHEN</code>, <code>LOG_BACKUPS</code>: control file rotation</li> <li><code>LOG_USE_QUEUE</code>: <code>true|false</code> (async logging)</li> <li><code>LOG_STREAM</code>: <code>stdout|stderr</code> (default <code>stderr</code>; dev defaults to <code>stdout</code>)</li> <li><code>LOG_COLOR</code>: <code>true|false|auto</code></li> <li><code>LOG_CAPTURE_WARNINGS</code>: <code>true|false</code> (redirect Python warnings)</li> <li><code>LOG_SUPPRESS</code>: comma-separated list of noisy modules to set to WARNING</li> <li><code>LOG_LEVEL</code>: base log level (default <code>INFO</code>)</li> </ul> <p>Usage:</p> <ul> <li>Application entrypoint:</li> </ul> <pre><code>from tnh_scholar.logging_config import setup_logging, get_logger\nsetup_logging()  # read from environment\nlog = get_logger(__name__)\nlog.info(\"app started\", extra={\"service\": \"gen-ai\"})\n</code></pre> <ul> <li>Library/service:</li> </ul> <pre><code>from tnh_scholar.logging_config import get_logger\nlog = get_logger(__name__)\nlog.debug(\"internal operation\")\n</code></pre> <p>Backward Compatibility:</p> <ul> <li>Supports existing modules using <code>get_child_logger(__name__)</code> without change.</li> <li>Legacy custom level <code>PRIORITY_INFO</code> retained for compatibility but deprecated.</li> </ul> <p>Notes:</p> <ul> <li>Does not configure the root logger; uses the project base logger (<code>tnh</code>) for isolation.</li> <li>Default behavior:</li> <li>dev: plain or color text, stdout, no queue.</li> <li>prod: JSON logs to stderr, queue enabled, suitable for structured log collection.</li> <li>Integrates with Python\u2019s <code>logging.captureWarnings()</code> when enabled.</li> </ul>"},{"location":"architecture/utilities/design/utilities-catalog/#progress_utilspy-experimental","title":"progress_utils.py \u2014 Experimental","text":"<p>Module: <code>tnh_scholar.utils.progress_utils</code></p> <p>Purpose: Time-based progress displays. Provides both a tqdm-backed expected-time bar (with delayed start to avoid flicker) and a simple dot/spinner progress for lightweight cases.</p> <p>Key APIs:</p> <ul> <li><code>ExpectedTimeTQDM(expected_time: float, display_interval: float = 0.5, desc: str = \"Time-based Progress\", delay_start: float = 1.0)</code> \u2014 context manager that shows a tqdm bar after an optional delay.</li> <li><code>TimeProgress(expected_time: Optional[float] = None, display_interval: float = 1.0, desc: str = \"\")</code> \u2014 context manager printing a lightweight spinner/dots with elapsed/expected time.</li> </ul> <p>Notes: Both are intended for CLI tooling (not GUI). <code>ExpectedTimeTQDM</code> spawns a background thread to update the bar and avoids creating the bar if the task completes before <code>delay_start</code>.</p>"},{"location":"architecture/utilities/design/utilities-catalog/#tnh_audio_segmentpy-stable","title":"tnh_audio_segment.py \u2014 Stable","text":"<p>Module: <code>tnh_scholar.utils.tnh_audio_segment</code></p> <p>Purpose: Thin, typed wrapper around <code>pydub.AudioSegment</code> to give clearer typing, a small API surface, and convenience constructors used by the audio pipeline.</p> <p>Key APIs:</p> <ul> <li><code>class TNHAudioSegment</code> \u2014 wrapper with methods:</li> <li><code>from_file(file: str | Path | BytesIO, format: str | None = None, **kwargs) -&gt; TNHAudioSegment</code></li> <li><code>export(out_f: str | BinaryIO, format: str, **kwargs) -&gt; None</code></li> <li><code>silent(duration: int) -&gt; TNHAudioSegment</code></li> <li><code>empty() -&gt; TNHAudioSegment</code></li> <li><code>__getitem__(key: int | slice) -&gt; TNHAudioSegment</code></li> <li><code>__add__</code>, <code>__iadd__</code>, <code>__len__</code>, and <code>raw</code> property exposing underlying <code>_AudioSegment</code>.</li> </ul> <p>Used by: Audio ingestion, concatenation, slicing, and export in diarization/transcription flows.</p>"},{"location":"architecture/utilities/design/utilities-catalog/#webhook_serverpy-evolving","title":"webhook_server.py \u2014 Evolving","text":"<p>Module: <code>tnh_scholar.utils.webhook_server</code></p> <p>Purpose: Small development helper to run a local Flask webhook endpoint and (optionally) create a public tunnel via <code>pylt</code> (localtunnel client). Useful for testing provider callbacks and webhooks during development.</p> <p>Key APIs:</p> <ul> <li><code>class WebhookServer</code> \u2014 convenience wrapper providing:</li> <li><code>start_server(host: str = \"127.0.0.1\", port: int = 0) -&gt; str</code> \u2014 starts a local Flask server on an available port; returns server URL.</li> <li><code>wait_for_webhook(timeout: float = 30.0) -&gt; dict | None</code> \u2014 block until a webhook payload arrives or timeout; returns parsed payload.</li> <li><code>create_tunnel(subdomain: Optional[str] = None) -&gt; str</code> \u2014 spawn <code>pylt</code> subprocess to create a tunnel and return the public URL (relies on <code>pylt</code> being installed).</li> <li><code>close_tunnel()</code> \u2014 terminate the tunnel subprocess safely.</li> <li><code>shutdown_server()</code> and <code>cleanup()</code> \u2014 helper methods to stop the server and any background processes.</li> </ul> <p>Notes: This module is primarily for local development. The tunnel creation parses subprocess stdout/stderr and depends on the <code>pylt</code> client; use with care in CI or headless environments.</p>"},{"location":"architecture/utilities/design/utilities-catalog/#observability-obs-doc-only","title":"Observability / obs (doc-only)","text":"<p>The design docs reference a lightweight Observer/ObsSpan protocol used to capture phase timings and attach metadata to spans. The docs include small Protocol examples (ObsSpan as a context manager with <code>duration_ms</code> and <code>Observer.phase(name, **fields) -&gt; ObsSpan</code>) and a <code>NoOpObserver</code> used for tests or when tracing is not needed.</p>"},{"location":"architecture/video-processing/","title":"Video Processing","text":"<p>Table of Contents:</p> <p>Adr - Table of contents for architecture/video-processing/adr</p> <p>This file auto-generated.</p>"},{"location":"architecture/video-processing/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-VP01: Video Processing Return Types and Configuration - Centralizes yt-dlp configuration and return types so video tooling emits consistent metadata.</p> <p>This file auto-generated.</p>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/","title":"ADR-VP01: Video Processing Return Types and Configuration","text":"<p>Unifies yt-dlp configuration and return shapes so download, transcript, and metadata operations stay consistent.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-02-01</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#context","title":"Context","text":"<p>The video processing module currently handles YouTube operations (transcript retrieval, audio download) through separate functions with different return types and repeated configuration settings. Each operation independently manages yt-dlp options and metadata extraction.</p> <p>Current structure:</p> <ul> <li>Independent functions for download, transcript, and metadata operations</li> <li>Repeated yt-dlp configuration in multiple places</li> <li>No standardized metadata collection</li> <li>Different return types per function</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#decision","title":"Decision","text":"<p>We will:</p>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#1-create-centralized-configuration-constants-for-yt-dlp-options","title":"1. Create centralized configuration constants for yt-dlp options","text":"<pre><code>DEFAULT_YDL_OPTIONS = {...}\nTRANSCRIPT_OPTIONS = DEFAULT_YDL_OPTIONS | {...}\nAUDIO_DOWNLOAD_OPTIONS = DEFAULT_YDL_OPTIONS | {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [...],\n    \"noplaylist\": True,\n    ...\n}\n</code></pre>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#2-define-a-standard-return-type-using-dataclass","title":"2. Define a standard return type using dataclass","text":"<pre><code>@dataclass\nclass VideoResult:\n    metadata: Dict[str, Any]\n    content: Optional[str] = None  # For transcripts\n    filepath: Optional[Path] = None  # For downloads\n</code></pre>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#3-establish-configurable-metadata-selection","title":"3. Establish configurable metadata selection","text":"<pre><code>DEFAULT_METADATA_FIELDS = [...]\nDUBLIN_CORE_MAPPING = {...}\n</code></pre>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#consequences","title":"Consequences","text":"<p>Positive:</p> <ul> <li>Single source of truth for yt-dlp configurations</li> <li>Consistent metadata collection across operations</li> <li>Unified return type simplifies function interfaces</li> <li>Easier configuration management through constants</li> <li>Better support for Dublin Core metadata standards</li> </ul> <p>Negative:</p> <ul> <li>Requires refactoring existing function calls</li> <li>Slightly more complex return type handling</li> <li>May need migration period for API changes</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#notes","title":"Notes","text":"<p>This change builds on the existing video_processing.py structure while standardizing the interface and reducing configuration duplication. It maintains the module's current functionality while making it more maintainable and consistent.</p>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#related-documents","title":"Related Documents","text":"<ul> <li>video_processing.py</li> <li>TextObject New Design.md (for Dublin Core metadata alignment)</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#adr-002-youtube-interface-redesign","title":"ADR 002: YouTube Interface Redesign","text":""},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#context_1","title":"Context","text":"<p>The current YouTube interface implementation in TNH Scholar, while functional, has become unwieldy and complex. The implementation mixes concerns across different tools and lacks a clear architectural boundary. As the system grows and evolves, we need a more maintainable and extensible approach to handling YouTube content acquisition.</p> <p>The current implementation has several issues:</p> <ul> <li>Mixed responsibilities between content acquisition and processing</li> <li>Duplicated code across tools (audio-transcribe and ytt-fetch)</li> <li>Complex error handling and configuration management</li> <li>Limited flexibility for future enhancements or backend changes</li> </ul> <p>This decision is being made in the context of our broader system architecture, which emphasizes modular design and the potential for continuous improvement through AI processing and training data generation.</p>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#decision_1","title":"Decision","text":"<p>We will implement a new YouTube interface based on a clear abstraction layer with separate interface definition and implementation. The core of this design will be an abstract YouTubeDownloader class with a concrete implementation using yt-dlp.</p> <p>The new architecture will consist of:</p> <pre><code>class YTDownloader:\n    \"\"\"Abstract base class for YouTube content retrieval.\"\"\"\n\n    def get_transcript(self, url: str, lang: str = \"en\") -&gt; VideoTranscript:\n        \"\"\"Retrieve video transcript with associated metadata.\"\"\"\n        pass\n\n    def get_audio(self, url: str) -&gt; VideoAudio:\n        \"\"\"Extract audio with associated metadata.\"\"\"\n        pass\n\n    def get_metadata(self, url: str) -&gt; VideoMetadata:\n        \"\"\"Retrieve video metadata only.\"\"\"\n        pass\n</code></pre> <p>With an initial implementation:</p> <pre><code>class DLPDownloader(YTDownloader):\n    \"\"\"yt-dlp based implementation of YouTube content retrieval.\"\"\"\n\n    def __init__(self, config: Optional[Dict] = None):\n        self.config = config or DEFAULT_CONFIG\n        self._setup_ytdlp()\n</code></pre>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#consequences_1","title":"Consequences","text":""},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#positive","title":"Positive","text":"<ul> <li>Clear separation of concerns improves maintainability</li> <li>Consistent interface across tools reduces code duplication</li> <li>Abstract base class enables future backend alternatives</li> <li>Standardized metadata handling across operations</li> <li>Simpler integration with existing tools</li> <li>Clearer path for future enhancements</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#negative","title":"Negative","text":"<ul> <li>Initial overhead of creating abstraction layer</li> <li>Need to refactor existing tools to use new interface</li> <li>Potential for increased complexity in configuration management</li> <li>May need to maintain backward compatibility during transition</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#neutral","title":"Neutral","text":"<ul> <li>Shifts complexity from implementation to interface design</li> <li>Changes how tools interact with YouTube functionality</li> <li>Requires updates to documentation and examples</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#implementation-approach","title":"Implementation Approach","text":"<p>The implementation will proceed in phases:</p> <ol> <li>Create core interface and yt-dlp implementation</li> <li>Refactor audio-transcribe to use new interface</li> <li>Update ytt-fetch to align with new design</li> <li>Enhance metadata handling and persistence</li> <li>Add support for additional features (playlists, caching)</li> </ol>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Enhance Existing Implementation</li> <li>Would require less immediate work</li> <li>Wouldn't address fundamental design issues</li> <li> <p>Would limit future flexibility</p> </li> <li> <p>Multiple Specialized Interfaces</p> </li> <li>Could optimize for specific use cases</li> <li>Would increase complexity and duplication</li> <li> <p>Would make maintenance more difficult</p> </li> <li> <p>Direct yt-dlp Integration</p> </li> <li>Simpler implementation</li> <li>Would limit flexibility for backend changes</li> <li>Would maintain current architectural issues</li> </ol>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#related-decisions","title":"Related Decisions","text":"<ul> <li>Overall TNH Scholar system architecture</li> <li>Pattern system design</li> <li>Metadata standardization</li> <li>Content processing pipeline design</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#additional-notes","title":"Additional Notes","text":"<p>This design aligns with our system's broader goals of modularity, extensibility, and support for AI-driven improvement. The abstract interface approach provides a foundation for future enhancements while maintaining the simplicity needed for rapid prototyping.</p>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#adr-003-resource-management-strategy-aligned-with-use-cases","title":"ADR 003: Resource Management Strategy Aligned with Use Cases","text":""},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#status_1","title":"Status","text":"<p>Proposed</p>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#context_2","title":"Context","text":"<p>The TNH Scholar project serves three distinct use cases with different requirements for resource management:</p> <ol> <li>CLI tools for technical users who manage their own data</li> <li>GUI interface for users who need structured defaults</li> <li>API access for programmatic integration</li> </ol> <p>The current implementation lacks clear separation between system configuration and data management. We need to establish principles that serve all use cases while maintaining flexibility.</p>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#decision_2","title":"Decision","text":"<p>We will implement a tiered approach to resource management that separates system configuration from data storage and adapts to each use case's needs.</p> <p>System Configuration</p> <p>All system configuration will reside in ~/.config/tnh-scholar/:</p> <ul> <li>Processing patterns</li> <li>Tool configurations</li> <li>Logging settings</li> <li>Default operational parameters</li> </ul> <p>Data Management Strategy</p> <p>For CLI Tools (Primary Use Case):</p> <ul> <li>Default to working directory for all operations</li> <li>Allow explicit path specification through command options</li> <li>Maintain pipeline-friendly interfaces</li> <li>Keep data management under user control</li> <li>Use system config only for operational settings</li> </ul> <p>For GUI Interface (Secondary Use Case):</p> <ul> <li>Provide structured defaults in ~/Library/Application Support/tnh-scholar/</li> <li>Maintain clear separation between user data and system configuration</li> <li>Allow advanced users to modify defaults through configuration</li> <li>Implement automated data management for ease of use</li> </ul> <p>For API Usage (Tertiary Use Case):</p> <ul> <li>Provide flexible interfaces for resource management</li> <li>Allow complete configuration of data locations</li> <li>Make no assumptions about storage locations</li> <li>Focus on functional interfaces over data management</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#consequences_2","title":"Consequences","text":"<p>Positive:</p> <ul> <li>Clear separation of configuration and data</li> <li>Flexibility for different user needs</li> <li>Simplified CLI tool development</li> <li>Structured defaults for GUI users</li> <li>Clean API interfaces</li> </ul> <p>Negative:</p> <ul> <li>More complex initial setup for GUI interface</li> <li>Need to maintain multiple data management approaches</li> <li>Potential for confusion about data locations</li> <li>Additional documentation requirements</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#implementation-notes","title":"Implementation Notes","text":"<p>The implementation will follow these principles:</p> <ul> <li>System configuration always uses ~/.config/tnh-scholar/</li> <li>CLI tools prioritize working directory operations</li> <li>GUI interface provides structured defaults</li> <li>API remains neutral on data location</li> <li>All tools support explicit path specification</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#related-decisions_1","title":"Related Decisions","text":"<ul> <li>ADR 001: Video Processing Return Types and Configuration</li> <li>ADR 002: YouTube Interface Redesign</li> </ul>"},{"location":"architecture/video-processing/adr/adr-vp01-video-processing/#additional-notes_1","title":"Additional Notes","text":"<p>This design supports the project's rapid prototyping phase by:</p> <ul> <li>Maintaining simplicity for CLI tools</li> <li>Providing clear extension points for GUI development</li> <li>Keeping API interfaces clean and flexible</li> <li>Allowing for future enhancement of data management</li> </ul> <p>The approach can evolve as the project matures, with potential for:</p> <ul> <li>Enhanced metadata management</li> <li>More sophisticated data organization</li> <li>Improved resource tracking</li> <li>Advanced configuration options</li> </ul>"},{"location":"architecture/ytt-fetch/","title":"Ytt Fetch","text":"<p>Table of Contents:</p> <p>Adr - Table of contents for architecture/ytt-fetch/adr</p> <p>Design - Table of contents for architecture/ytt-fetch/design</p> <p>This file auto-generated.</p>"},{"location":"architecture/ytt-fetch/adr/","title":"Adr","text":"<p>Table of Contents:</p> <p>ADR-YF00: Early yt-fetch Transcript Decisions (Historical) - Consolidates the original transcript ADR notes for yt-fetch before they were split into discrete records.</p> <p>ADR-YF01: YouTube Transcript Source Handling - Documents how yt-fetch prioritizes manual subtitles while accepting auto captions for maximum coverage.</p> <p>ADR-YF02: YouTube Transcript Format Selection - Locks yt-fetch to a single transcript format (initially VTT) for predictable downstream processing.</p> <p>This file auto-generated.</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/","title":"ADR-YF00: Early yt-fetch Transcript Decisions (Historical)","text":"<p>Preserves the original consolidated transcript decisions that led to ADR-YF01 and ADR-YF02.</p> <ul> <li>Status: Historical</li> <li>Date: 2024-01-15</li> </ul>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#adr-1-youtube-transcript-format-selection","title":"ADR-1: YouTube Transcript Format Selection","text":""},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#context","title":"Context","text":"<p>The yt-fetch CLI tool needs to download YouTube transcripts/captions. YouTube offers multiple formats: - VTT (Web Video Text Tracks) - TTML (Timed Text Markup Language)  - srv\u00bd/3 (YouTube internal formats) - json3 (YouTube JSON format)</p> <p>While yt-dlp offers format conversion capabilities, these are: - Poorly documented - Inconsistent in behavior - May change across versions</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#decision","title":"Decision","text":"<p>Standardize on VTT format output because: 1. It is a web standard format, likely to remain stable 2. Human readable and well-documented 3. Has wide library support if needed 4. Already the default format from yt-dlp 5. Available for both manual and auto-generated captions</p> <p>Implementation approach: - Use minimal yt-dlp options (writesubtitles, writeautomaticsub, subtitleslangs) - Accept VTT as default output without trying format conversion - Let downstream tools handle any needed format conversion</p> <pre><code># Example minimal implementation\nopts = {\n   \"writesubtitles\": True,\n   \"writeautomaticsub\": True,\n   \"subtitleslangs\": [\"en\"],\n   \"skip_download\": True\n}\n</code></pre>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#2024-01-15","title":"2024-01-15","text":""},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#adr-2-youtube-transcript-source-handling","title":"ADR-2: YouTube Transcript Source Handling","text":""},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#status","title":"Status","text":"<p>Proposed (supplements ADR: YouTube Transcript Format Selection)</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#context_1","title":"Context","text":"<p>When requesting transcripts from YouTube videos: - Videos may have manually uploaded subtitles - Videos may have auto-generated captions - Some videos may have both - Quality and accuracy can vary significantly between sources</p> <p>Currently yt-dlp options: <pre><code>opts = {\n    \"writesubtitles\": True,      # Get manual subtitles\n    \"writeautomaticsub\": True,   # Get auto-generated captions\n    \"subtitleslangs\": [\"en\"]     # Language selection\n}\n</code></pre></p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#decision_1","title":"Decision","text":"<ol> <li> <p>Initially accept both sources (manual and auto-generated) with preference given to manual subtitles when available (yt-dlp's default behavior)</p> </li> <li> <p>Flag this as a known limitation/consideration:</p> </li> <li>Source of transcript (manual vs auto) may affect quality</li> <li>No current mechanism to force selection of specific source</li> <li>Transcript source not clearly indicated in output</li> </ol>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#future-considerations","title":"Future Considerations","text":"<p>Future versions should consider: - Adding transcript source metadata - Option to specify preferred source - Quality indicators in output - Logging which source was used</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#consequences","title":"Consequences","text":""},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#positive","title":"Positive","text":"<ul> <li>Simple initial implementation</li> <li>Works with all video types</li> <li>Maximum transcript availability</li> </ul>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#negative","title":"Negative","text":"<ul> <li>Uncertain transcript source</li> <li>No quality indicators</li> <li>May get auto-generated when manual exists</li> <li>May get manual when auto-generated preferred</li> </ul>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#notes","title":"Notes","text":"<p>This limitation is acceptable for prototyping but should be revisited when: - Transcript quality becomes critical - Source attribution needed - Specific use cases require specific transcript types</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#2024-01-16","title":"2024-01-16","text":""},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#adr-3-update-youtube-transcript-format-selection-revision-2","title":"ADR-3 Update: YouTube Transcript Format Selection (Revision 2)","text":""},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#context_2","title":"Context","text":"<p>After testing various formats with real YouTube videos: - VTT shows duplicate line issues - TTML format provides cleaner output - TTML includes timing and styling information in structured XML</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#decision-update","title":"Decision Update","text":"<p>Changing preferred format from VTT to TTML based on: - Better transcript quality (fewer duplicates) - Clean, structured XML format - Includes all necessary timing data - Easy to parse with standard XML tools</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#learning-process-note","title":"Learning Process Note","text":"<p>Initial decision was based on theoretical advantages of VTT. Actual testing revealed TTML as superior choice.</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#adr-4-ttml-text-extraction-implementation","title":"ADR-4: TTML Text Extraction Implementation","text":""},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#status_1","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#context_3","title":"Context","text":"<p>Need to extract raw text content from YouTube TTML transcript files: - TTML files contain timing and styling metadata in XML format - Only need raw text content for downstream processing - Must handle potentially malformed XML from YouTube - Prefer minimal dependencies and simple implementation</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#decision_2","title":"Decision","text":"<p>Implement using Python's standard library xml.etree.ElementTree because: 1. No additional dependencies required (already part of Python standard library) 2. Simple, focused API sufficient for basic XML parsing 3. Built-in namespace handling 4. Lightweight and well-documented</p> <p>Implementation approach: - Strip XML namespaces to simplify parsing - Use ElementTree's findall() with XPath to locate text content - Extract and join raw text with newlines - Keep implementation minimal (under 20 lines)</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#consequences_1","title":"Consequences","text":""},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#positive_1","title":"Positive","text":"<ul> <li>Zero new dependencies</li> <li>Simple, maintainable code</li> <li>Handles basic TTML structure effectively</li> <li>Easy to test and debug</li> </ul>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#negative_1","title":"Negative","text":"<ul> <li>Limited validation of XML structure</li> <li>May need additional error handling for malformed XML</li> <li>No preservation of timing information (acceptable for current needs)</li> <li>Could break if YouTube significantly changes TTML format</li> </ul>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#notes_1","title":"Notes","text":"<p>While more robust XML parsing libraries exist (lxml, BeautifulSoup4), the standard library solution provides sufficient functionality for current requirements while maintaining simplicity. Error handling can be enhanced if needed based on production experience.</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf00-early-decisions/#future-considerations_1","title":"Future Considerations","text":"<ul> <li>Add error handling for malformed XML if needed</li> <li>Monitor YouTube TTML format changes</li> <li>Consider preserving timing data if needed for future features</li> </ul>"},{"location":"architecture/ytt-fetch/adr/adr-yf01-yt-transcript-source-handling/","title":"ADR-YF01: YouTube Transcript Source Handling","text":"<p>Keeps both manual and auto-generated transcripts available while acknowledging source ambiguity in early yt-fetch releases.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-01-15</li> </ul>"},{"location":"architecture/ytt-fetch/adr/adr-yf01-yt-transcript-source-handling/#context","title":"Context","text":"<p>When requesting transcripts from YouTube videos:</p> <ul> <li>Videos may have manually uploaded subtitles</li> <li>Videos may have auto-generated captions</li> <li>Some videos may have both</li> <li>Quality and accuracy can vary significantly between sources</li> </ul> <p>Currently yt-dlp options:</p> <p>```python opts = {     \"writesubtitles\": True,      # Get manual subtitles     \"writeautomaticsub\": True,   # Get auto-generated captions     \"subtitleslangs\": [\"en\"]     # Language selection }</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf01-yt-transcript-source-handling/#decision","title":"Decision","text":"<ol> <li> <p>Initially accept both sources (manual and auto-generated) with preference given to manual subtitles when available (yt-dlp's default behavior)</p> </li> <li> <p>Flag this as a known limitation/consideration:</p> </li> <li>Source of transcript (manual vs auto) may affect quality</li> <li>No current mechanism to force selection of specific source</li> <li>Transcript source not clearly indicated in output</li> </ol>"},{"location":"architecture/ytt-fetch/adr/adr-yf01-yt-transcript-source-handling/#future-considerations","title":"Future Considerations","text":"<p>Future versions should consider: - Adding transcript source metadata - Option to specify preferred source - Quality indicators in output - Logging which source was used</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf01-yt-transcript-source-handling/#consequences","title":"Consequences","text":""},{"location":"architecture/ytt-fetch/adr/adr-yf01-yt-transcript-source-handling/#positive","title":"Positive","text":"<ul> <li>Simple initial implementation</li> <li>Works with all video types</li> <li>Maximum transcript availability</li> </ul>"},{"location":"architecture/ytt-fetch/adr/adr-yf01-yt-transcript-source-handling/#negative","title":"Negative","text":"<ul> <li>Uncertain transcript source</li> <li>No quality indicators</li> <li>May get auto-generated when manual exists</li> <li>May get manual when auto-generated preferred</li> </ul>"},{"location":"architecture/ytt-fetch/adr/adr-yf01-yt-transcript-source-handling/#notes","title":"Notes","text":"<p>This limitation is acceptable for prototyping but should be revisited when: - Transcript quality becomes critical - Source attribution needed - Specific use cases require specific transcript types</p> <p>Would you like me to explore any specific aspect of this further, or shall we move on to implementation?</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf02-yt-transcript-format-selection/","title":"ADR-YF02: YouTube Transcript Format Selection","text":"<p>Selects a canonical transcript format for yt-fetch to keep downstream processing deterministic during early releases.</p> <ul> <li>Status: Proposed</li> <li>Date: 2025-01-15</li> </ul>"},{"location":"architecture/ytt-fetch/adr/adr-yf02-yt-transcript-format-selection/#context","title":"Context","text":"<p>The yt-fetch CLI tool needs to download YouTube transcripts/captions. YouTube offers multiple formats: - VTT (Web Video Text Tracks) - TTML (Timed Text Markup Language)  - srv\u00bd/3 (YouTube internal formats) - json3 (YouTube JSON format)</p> <p>While yt-dlp offers format conversion capabilities, these are: - Poorly documented - Inconsistent in behavior - May change across versions</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf02-yt-transcript-format-selection/#decision","title":"Decision","text":"<p>Standardize on VTT format output because: 1. It is a web standard format, likely to remain stable 2. Human readable and well-documented 3. Has wide library support if needed 4. Already the default format from yt-dlp 5. Available for both manual and auto-generated captions</p> <p>Implementation approach: - Use minimal yt-dlp options (writesubtitles, writeautomaticsub, subtitleslangs) - Accept VTT as default output without trying format conversion - Let downstream tools handle any needed format conversion</p> <p>```python</p>"},{"location":"architecture/ytt-fetch/adr/adr-yf02-yt-transcript-format-selection/#example-minimal-implementation","title":"Example minimal implementation","text":"<p>opts = {    \"writesubtitles\": True,    \"writeautomaticsub\": True,    \"subtitleslangs\": [\"en\"],    \"skip_download\": True }</p>"},{"location":"architecture/ytt-fetch/design/","title":"Design","text":"<p>Table of Contents:</p> <p>YouTube API vs yt-dlp Evaluation - Comparison of using the YouTube Data API versus yt-dlp for fetching Plum Village media assets.</p> <p>This file auto-generated.</p>"},{"location":"architecture/ytt-fetch/design/youtube-api-vs-yt-dlp-eval/","title":"YouTube API vs yt-dlp Evaluation","text":"<p>Comparison of using the YouTube Data API versus yt-dlp for fetching Plum Village media assets.</p>"},{"location":"architecture/ytt-fetch/design/youtube-api-vs-yt-dlp-eval/#youtube-data-api-google-cloud-approach","title":"YouTube Data API (Google Cloud) Approach","text":""},{"location":"architecture/ytt-fetch/design/youtube-api-vs-yt-dlp-eval/#advantages","title":"Advantages","text":"<ol> <li>Official solution with:</li> <li>Stable, documented API</li> <li>Clear rate limits and quotas</li> <li>Service level guarantees</li> <li> <p>Official support channels</p> </li> <li> <p>Additional capabilities:</p> </li> <li>Better metadata access</li> <li>Channel/playlist management</li> <li>Comment access</li> <li>Full YouTube ecosystem integration</li> </ol>"},{"location":"architecture/ytt-fetch/design/youtube-api-vs-yt-dlp-eval/#disadvantages","title":"Disadvantages","text":"<ol> <li>Setup overhead:</li> <li>Requires Google Cloud account</li> <li>API key/credentials management</li> <li>Project setup in Google Cloud Console</li> <li> <p>Quota management</p> </li> <li> <p>Cost considerations:</p> </li> <li>Free tier limits</li> <li>Usage-based pricing</li> <li> <p>Quota costs for transcript access</p> </li> <li> <p>Implementation complexity:</p> </li> <li>OAuth flow for some operations</li> <li>More complex credential management</li> <li>More code to maintain</li> </ol>"},{"location":"architecture/ytt-fetch/design/youtube-api-vs-yt-dlp-eval/#current-yt-dlp-approach","title":"Current yt-dlp Approach","text":""},{"location":"architecture/ytt-fetch/design/youtube-api-vs-yt-dlp-eval/#advantages_1","title":"Advantages","text":"<ol> <li>Simplicity:</li> <li>No authentication needed</li> <li>Minimal setup</li> <li> <p>Works immediately</p> </li> <li> <p>Cost:</p> </li> <li>Free to use</li> <li>No quota limits</li> <li> <p>No account needed</p> </li> <li> <p>Implementation:</p> </li> <li>Already working solution</li> <li>Minimal code</li> <li>Handles both manual and auto captions</li> </ol>"},{"location":"architecture/ytt-fetch/design/youtube-api-vs-yt-dlp-eval/#disadvantages_1","title":"Disadvantages","text":"<ol> <li>Unofficial:</li> <li>Could break with YouTube changes</li> <li>No guaranteed support</li> <li>Limited to public video access</li> </ol>"},{"location":"architecture/ytt-fetch/design/youtube-api-vs-yt-dlp-eval/#recommendation","title":"Recommendation","text":"<p>For this project's current needs (transcript downloading from public videos), yt-dlp remains the better choice because: 1. Matches current project scope 2. Zero setup overhead 3. No cost implications 4. Already working solution</p> <p>Consider YouTube API if project requirements expand to need: - Private video access - Channel management - Commercial deployment - Service level guarantees</p>"},{"location":"archive/","title":"Archive","text":"<p>Table of Contents:</p> <p>TNH Scholar - Comprehensive documentation for TNH Scholar, an AI-driven project exploring,</p> <p>This file auto-generated.</p>"},{"location":"cli-reference/","title":"CLI Reference","text":"<p>Table of Contents:</p> <p>audio-transcribe - Command-line tool for audio transcription tasks.</p> <p>json-to-srt - Convert JSONL transcription output (from audio-transcribe) into SRT subtitle files.</p> <p>nfmt - , a newline formatting utility, standardizes line endings and spacing in text files.</p> <p>Command Line Tools Overview - TNH Scholar provides a suite of command-line tools designed to work together for text processing. Each tool focuses on specific tasks while maintaining consistent interfaces and behavior. This overview introduces the available tools and their primary functions.</p> <p>sent-split - Split text into sentences using NLTK, with newline or space separators.</p> <p>srt-translate - Translate SRT subtitle files while preserving timecodes, using TNH Scholar translation patterns.</p> <p>tnh-fab - User-facing reference for the  CLI covering commands, options, and example workflows.</p> <p>tnh-setup - The  command configures the TNH Scholar environment, setting up necessary directories and downloading default patterns.</p> <p>token-count - The  command calculates the OpenAI API token count for text input. This is useful for ensuring that a text is within maximum token limits for the API model and also for estimating API costs.</p> <p>ytt-fetch - (Y)ou(T)ube (T)ranscript-(Fetch)ing utility.</p> <p>This file auto-generated.</p>"},{"location":"cli-reference/audio-transcribe/","title":"audio-transcribe","text":"<p>Command-line tool for audio transcription tasks.</p>"},{"location":"cli-reference/audio-transcribe/#usage","title":"Usage","text":"<pre><code>audio-transcribe [OPTIONS] [INPUT_FILE]\n</code></pre>"},{"location":"cli-reference/audio-transcribe/#options","title":"Options","text":"<pre><code>-s, --split              Split audio into chunks\n-t, --transcribe         Transcribe the audio chunks\n-y, --yt_url TEXT        Single YouTube URL to process\n-v, --yt_url_csv PATH    CSV file containing multiple YouTube URLs\n...\n</code></pre>"},{"location":"cli-reference/audio-transcribe/#examples","title":"Examples","text":""},{"location":"cli-reference/audio-transcribe/#download-and-transcribe-from-youtube","title":"Download and Transcribe from YouTube","text":"<pre><code>audio-transcribe --yt_url \"https://youtube.com/watch?v=example\" --split --transcribe\n</code></pre>"},{"location":"cli-reference/audio-transcribe/#process-local-audio-file","title":"Process Local Audio File","text":"<pre><code>audio-transcribe -f my_audio.mp3 --split --transcribe\n</code></pre>"},{"location":"cli-reference/json-to-srt/","title":"json-to-srt","text":"<p>Command-line utility that converts JSONL transcription files (from <code>audio-transcribe</code>) into standard SRT subtitle output. Reads from stdin by default and writes to stdout unless an output path is provided.</p>"},{"location":"cli-reference/json-to-srt/#usage","title":"Usage","text":"<pre><code>json-to-srt [OPTIONS] [INPUT_FILE]\n</code></pre> <p>If <code>INPUT_FILE</code> is omitted, the tool reads JSONL input from stdin. Use <code>-o/--output</code> to write the SRT result to a file.</p>"},{"location":"cli-reference/json-to-srt/#options","title":"Options","text":"<ul> <li><code>-o, --output PATH</code> \u2014 Optional output file path (default: stdout).</li> </ul>"},{"location":"cli-reference/json-to-srt/#examples","title":"Examples","text":"<pre><code># Convert a JSONL transcript to SRT and print to terminal\njson-to-srt transcript.jsonl\n\n# Convert and write to a file\njson-to-srt transcript.jsonl --output transcript.srt\n\n# Stream from stdin and write to stdout\ncat transcript.jsonl | json-to-srt\n</code></pre>"},{"location":"cli-reference/nfmt/","title":"nfmt","text":"<p><code>nfmt</code>, a newline formatting utility, standardizes line endings and spacing in text files.</p>"},{"location":"cli-reference/nfmt/#usage","title":"Usage","text":"<pre><code>nfmt [OPTIONS] [INPUT_FILE]\n</code></pre>"},{"location":"cli-reference/nfmt/#options","title":"Options","text":"<pre><code>-s, --spacing INTEGER  Number of newlines between paragraphs (default: 1)\n-o, --output PATH     Output file (default: stdout)\n</code></pre>"},{"location":"cli-reference/nfmt/#examples","title":"Examples","text":""},{"location":"cli-reference/nfmt/#basic-usage","title":"Basic Usage","text":"<pre><code># Format with single newline spacing\nnfmt input.txt &gt; formatted.txt\n\n# Format with double spacing\nnfmt -s 2 input.txt &gt; formatted.txt\n\n# Process from stdin\ncat input.txt | nfmt &gt; formatted.txt\n</code></pre>"},{"location":"cli-reference/nfmt/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Standardizing line endings before processing</li> <li>Preparing text for pattern-based processing</li> <li>Cleaning up transcribed text</li> </ul>"},{"location":"cli-reference/overview/","title":"Command Line Tools Overview","text":"<p>TNH Scholar provides a suite of command-line tools designed to work together for text processing. Each tool focuses on specific tasks while maintaining consistent interfaces and behavior. This overview introduces the available tools and their primary functions. </p>"},{"location":"cli-reference/overview/#available-tools","title":"Available tools","text":"<ul> <li>audio-transcribe \u2013 Transcribe audio files with diarization</li> <li>nfmt \u2013 Normalize and format text files</li> <li>tnh-fab \u2013 Pattern-driven text processing and translation</li> <li>tnh-setup \u2013 Environment setup helper</li> <li>token-count \u2013 Token estimation utility</li> <li>ytt-fetch \u2013 YouTube transcript fetcher</li> </ul>"},{"location":"cli-reference/overview/#tnh-fab","title":"TNH-FAB","text":"<p>The primary text processing tool, TNH-FAB ('fab' short for 'fabric'), provides core functionality for text manipulation and analysis. This versatile tool includes several subcommands:</p> <p>The punctuate command handles text punctuation and basic formatting. It can work with multiple languages and adapts to various writing styles.</p> <p>The section command analyzes text structure and identifies logical divisions. It helps organize content into meaningful segments for further processing. This is crucial for working with larger text where model context limits may be reached or where processing will be ineffective due to content size.</p> <p>The translate command performs line-by-line translation while maintaining document structure. It provides context-aware translation particularly suited for wisdom and mindfulness content.</p> <p>The process command applies custom pattern-based processing to text. It offers flexible text transformation capabilities through the pattern system.</p>"},{"location":"cli-reference/overview/#audio-transcribe","title":"Audio-Transcribe","text":"<p>The audio transcription tool handles conversion of audio content to text format. It provides several key capabilities:</p> <p>Audio downloading supports direct processing from YouTube URLs. The tool can handle both single videos and batch processing from CSV files.</p> <p>Audio splitting automatically divides long audio files into manageable chunks. It supports both silence-based and AI-assisted splitting methods.</p> <p>Transcription processes audio into text while maintaining timing information. The tool supports multiple languages and can include translations.</p>"},{"location":"cli-reference/overview/#ytt-fetch","title":"YTT-Fetch","text":"<p>The YouTube Transcript Fetch utility specializes in retrieving video transcripts. It offers streamlined functionality:</p> <p>Direct transcript downloading from YouTube videos eliminates the need for full video processing. This approach saves time and resources when only text is needed.</p> <p>Language selection allows retrieval of transcripts in specific languages when available. The tool automatically handles language code conversion.</p> <p>Output formatting provides options for saving or displaying retrieved transcripts. The tool integrates smoothly with other processing commands.</p>"},{"location":"cli-reference/overview/#token-count","title":"Token-Count","text":"<p>The token counting utility helps manage AI processing limits and costs. It provides essential information:</p> <p>Token calculation matches OpenAI's counting method exactly. This accuracy helps predict API usage and costs.</p> <p>Pipeline integration allows token counting at any processing stage. This capability helps optimize processing workflows.</p> <p>Standard input support enables flexible usage in command chains. The tool works seamlessly with other text processing commands.</p>"},{"location":"cli-reference/overview/#tnh-setup","title":"TNH-Setup","text":"<p>The setup tool handles initial configuration and maintenance of the TNH Scholar environment. It manages several aspects:</p> <p>Directory creation establishes the required folder structure for patterns and logs. The tool ensures proper permissions and organization.</p> <p>Pattern management handles downloading and installation of default patterns. It maintains the pattern repository structure.</p> <p>Environment verification checks for necessary API keys and configurations. The tool provides guidance for missing requirements.</p>"},{"location":"cli-reference/overview/#nfmt","title":"NFMT","text":"<p>The newline formatting utility standardizes text file formatting. It provides focused functionality:</p> <p>Line ending standardization ensures consistent text formatting. This standardization is particularly important for pattern processing.</p> <p>Spacing control allows customization of line spacing between paragraphs. The tool helps prepare text for various processing needs.</p> <p>Pipeline compatibility enables integration with other processing tools. This integration supports complex text processing workflows.</p>"},{"location":"cli-reference/overview/#workflow-integration","title":"Workflow Integration","text":"<p>The TNH Scholar command-line tools are designed to work together effectively:</p> <p>Pipeline Support: All tools support both file input/output and standard streams, enabling complex processing pipelines.</p> <p>Consistent Interfaces: Tools maintain consistent option patterns and behavior, making them easy to learn and use together.</p> <p>Error Handling: Tools are intended to provide consistent error reporting and status information, helping with workflow debugging. (In the prototyping phase not all error handling is implemented.)</p> <p>Resource Management: Tools coordinate resource usage and maintain efficient processing patterns.</p>"},{"location":"cli-reference/overview/#common-features","title":"Common Features","text":"<p>Several features are common across all TNH Scholar command-line tools:</p> <p>Standard Input/Output: All tools support both file-based and stream-based processing, enabling flexible usage.</p> <p>Verbose Modes: Tools provide detailed logging options for troubleshooting and monitoring.</p> <p>Documentation: Each tool includes help information and usage examples.</p> <p>Configuration: Tools respect both global and command-specific configuration options.</p>"},{"location":"cli-reference/overview/#getting-started","title":"Getting Started","text":"<p>To begin using the command-line tools:</p> <ol> <li>Install TNH Scholar using pip</li> <li>Run tnh-setup to configure your environment</li> <li>Review individual tool documentation for specific usage details</li> <li>Start with simple commands and gradually build more complex pipelines</li> </ol> <p>For detailed information about each tool, refer to their specific documentation sections.</p>"},{"location":"cli-reference/sent-split/","title":"sent-split","text":"<p>CLI tool for splitting text into sentences. Reads from a file (or stdin) and writes to stdout or a specified file. Uses NLTK sentence tokenization.</p>"},{"location":"cli-reference/sent-split/#usage","title":"Usage","text":"<pre><code>sent-split [OPTIONS] [INPUT_FILE]\n</code></pre> <p>If <code>INPUT_FILE</code> is omitted, the tool reads from stdin.</p>"},{"location":"cli-reference/sent-split/#options","title":"Options","text":"<ul> <li><code>-o, --output PATH</code> \u2014 Optional output file (default: stdout).</li> <li><code>-s, --space</code> \u2014 Separate sentences with spaces instead of newlines.</li> </ul>"},{"location":"cli-reference/sent-split/#examples","title":"Examples","text":"<pre><code># Split a text file into sentences (newline-separated)\nsent-split notes.txt --output sentences.txt\n\n# Split via stdin and print to stdout\ncat notes.txt | sent-split\n\n# Space-separated sentences\nsent-split notes.txt --space\n</code></pre>"},{"location":"cli-reference/srt-translate/","title":"srt-translate","text":"<p>Translate an SRT subtitle file from one language to another while keeping timecodes intact. Supports selecting a translation pattern and model, and writing the result to a new SRT file.</p>"},{"location":"cli-reference/srt-translate/#usage","title":"Usage","text":"<pre><code>srt-translate [OPTIONS] INPUT_FILE\n</code></pre>"},{"location":"cli-reference/srt-translate/#options","title":"Options","text":"<ul> <li><code>-o, --output PATH</code> \u2014 Output file path (default: <code>&lt;input&gt;-&lt;lang&gt;.srt</code>).</li> <li><code>-s, --source-language TEXT</code> \u2014 Source language code (auto-detected if omitted).</li> <li><code>-t, --target-language TEXT</code> \u2014 Target language code (default: <code>en</code>).</li> <li><code>-m, --model TEXT</code> \u2014 Optional model name to use for translation.</li> <li><code>-p, --pattern TEXT</code> \u2014 Optional translation pattern name.</li> <li><code>-g, --debug</code> \u2014 Enable debug logging.</li> <li><code>-d, --metadata PATH</code> \u2014 Path to YAML front matter providing translation context.</li> </ul>"},{"location":"cli-reference/srt-translate/#examples","title":"Examples","text":"<pre><code># Translate to English with defaults\nsrt-translate talk.srt\n\n# Translate to French with a specific pattern and model\nsrt-translate talk.srt --target-language fr --pattern line_translate --model gpt-4o\n\n# Write to a custom path and enable debug logging\nsrt-translate talk.srt --output translated.srt --debug\n</code></pre>"},{"location":"cli-reference/tnh-fab/","title":"tnh-fab","text":"<p>User-facing reference for the <code>tnh-fab</code> CLI covering commands, options, and example workflows.</p>"},{"location":"cli-reference/tnh-fab/#overview","title":"Overview","text":"<p><code>tnh-fab</code> is a specialized command-line tool, part of the Thich Nhat Hanh Scholar Project. <code>tnh-fab</code> can process multilingual texts using AI 'patterns'. It is originally developed to work with processing Dharma-based materials such as talks by Thich Nhat Hanh.</p> <p>It provides functionality for:</p> <ul> <li>Adding/correcting punctuation</li> <li>Identifying logical sections</li> <li>Performing line-based translation</li> <li>Applying custom text processing patterns</li> </ul>"},{"location":"cli-reference/tnh-fab/#installation","title":"Installation","text":"<pre><code>pip install tnh-scholar\n</code></pre>"},{"location":"cli-reference/tnh-fab/#basic-usage","title":"Basic Usage","text":"<pre><code>tnh-fab [COMMAND] [OPTIONS] [INPUT_FILE]\n</code></pre> <p>Input can be provided either as a file or through standard input (stdin).</p> <p>Global options:</p> <pre><code>-v, --verbose        Enable detailed logging\n--debug             Enable debug output\n--quiet             Suppress all non-error output\n</code></pre>"},{"location":"cli-reference/tnh-fab/#commands","title":"Commands","text":""},{"location":"cli-reference/tnh-fab/#1-punctuate","title":"1. Punctuate","text":"<p>Adds or corrects punctuation based on language-specific rules.</p> <p>Basic usage:</p> <pre><code>tnh-fab punctuate [OPTIONS] [INPUT_FILE]\n</code></pre> <p>Options:</p> <pre><code>-l, --language      Source language code (auto-detected if not specified)\n-y, --style         Punctuation style (default: 'APA')\n-c, --review-count  Number of review passes (default: 3)\n-p, --pattern       Pattern name (default: 'default_punctuate')\n</code></pre> <p>Examples:</p> <pre><code># Basic punctuation of a file\ntnh-fab punctuate input.txt\n\n# Punctuate Vietnamese text with specific style\ntnh-fab punctuate -l vi -y \"Modern\" vietnamese_text.txt\n\n# Process from stdin with extra review passes\ncat unpunctuated.txt | tnh-fab punctuate -c 5\n\n# Use custom pattern with specific language\ntnh-fab punctuate -p dharma_punctuation -l en text.txt\n</code></pre>"},{"location":"cli-reference/tnh-fab/#2-section","title":"2. Section","text":"<p>Analyzes text and divides it into logical sections.</p> <p>Basic usage:</p> <pre><code>tnh-fab section [OPTIONS] [INPUT_FILE]\n</code></pre> <p>Options:</p> <pre><code>-l, --language      Source language code (auto-detected if not specified)\n-n, --num-sections  Target number of sections (auto-calculated if not specified)\n-c, --review-count  Number of review passes (default: 3)\n-p, --pattern       Pattern name (default: 'default_section')\n</code></pre> <p>Examples:</p> <pre><code># Auto-detect sections\ntnh-fab section dharma_talk.txt\n\n# Create specific number of sections\ntnh-fab section -n 5 long_text.txt\n\n# Section Vietnamese text with custom pattern\ntnh-fab section -l vi -p vn_section_pattern text.txt\n\n# Process from stdin and save to file\ncat text.txt | tnh-fab section &gt; sections.json\n</code></pre>"},{"location":"cli-reference/tnh-fab/#3-translate","title":"3. Translate","text":"<p>Performs line-by-line translation while maintaining structure.</p> <p>Basic usage:</p> <pre><code>tnh-fab translate [OPTIONS] [INPUT_FILE]\n</code></pre> <p>Options:</p> <pre><code>-l, --language       Source language code (auto-detected if not specified)\n-r, --target         Target language code (default: 'en')\n-y, --style          Translation style\n--context-lines      Number of context lines (default: 3)\n--segment-size       Lines per translation segment (auto-calculated if not specified)\n-p, --pattern        Pattern name (default: 'default_line_translation')\n</code></pre> <p>Examples:</p> <pre><code># Basic Vietnamese to English translation\ntnh-fab translate -l vi vietnamese_text.txt\n\n# Translation with specific style\ntnh-fab translate -l vi -y \"American Dharma Teaching\" text.txt\n\n# French translation with increased context\ntnh-fab translate -l vi -r fr --context-lines 5 text.txt\n\n# Custom segment size and pattern\ntnh-fab translate --segment-size 20 -p custom_translation text.txt\n</code></pre>"},{"location":"cli-reference/tnh-fab/#4-process","title":"4. Process","text":"<p>Applies custom pattern-based processing with flexible structuring.</p> <p>Basic usage:</p> <pre><code>tnh-fab process -p PATTERN [OPTIONS] [INPUT_FILE]\n</code></pre> <p>Options:</p> <pre><code>-p, --pattern     Pattern name (required)\n-s, --section     Process using sections from JSON file\n-g, --paragraph   Process text by paragraphs\n-t, --template    YAML file with template values\n</code></pre> <p>Examples:</p> <pre><code># Basic processing with pattern\ntnh-fab process -p format_xml input.txt\n\n# Process using sections from file\ntnh-fab process -p format_xml -s sections.json input.txt\n\n# Process by paragraphs\ntnh-fab process -p format_xml -g long_text.txt\n\n# Process with template values\ntnh-fab process -p format_xml -t template.yaml input.txt\n</code></pre>"},{"location":"cli-reference/tnh-fab/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"cli-reference/tnh-fab/#pipeline-processing","title":"Pipeline Processing","text":""},{"location":"cli-reference/tnh-fab/#1-punctuate-section-and-process","title":"1. Punctuate, section, and process","text":"<pre><code>cat raw_text.txt | \\\ntnh-fab punctuate -l vi | \\\ntnh-fab section -n 5 | \\\ntnh-fab process -p format_xml &gt; final_output.xml\n</code></pre>"},{"location":"cli-reference/tnh-fab/#2-section-and-translate","title":"2. Section and translate","text":"<pre><code>tnh-fab section input.txt &gt; sections.json\ntnh-fab translate -l vi -s sections.json input.txt &gt; translated.txt\n</code></pre>"},{"location":"cli-reference/tnh-fab/#3-complete-processing-pipeline","title":"3. Complete processing pipeline","text":"<pre><code>cat vietnamese_text.txt | \\\ntnh-fab punctuate -l vi -y \"Modern\" | \\\ntnh-fab section --num-sections 10 | \\\ntnh-fab translate -r en --context-lines 5 | \\\ntnh-fab process -p format_xml &gt; final_output.xml\n</code></pre>"},{"location":"cli-reference/tnh-fab/#pattern-system","title":"Pattern System","text":"<p>Patterns are stored in the pattern directory (default or specified by TNH_PATTERN_DIR environment variable). Each command uses specific default patterns that can be overridden using the -p option.</p> <p>For information about creating custom patterns, please refer to the pattern documentation.</p>"},{"location":"cli-reference/tnh-setup/","title":"tnh-setup","text":"<p>The <code>tnh-setup</code> command configures the TNH Scholar environment, setting up necessary directories and downloading default patterns.</p>"},{"location":"cli-reference/tnh-setup/#usage","title":"Usage","text":"<pre><code>tnh-setup [OPTIONS]\n</code></pre>"},{"location":"cli-reference/tnh-setup/#options","title":"Options","text":"<pre><code>--skip-env       Skip API key setup check\n--skip-patterns  Skip pattern download\n--help          Show this message and exit\n</code></pre>"},{"location":"cli-reference/tnh-setup/#configuration-steps","title":"Configuration Steps","text":"<p>The setup process includes:</p> <ol> <li>Directory Creation</li> <li>Creates ~/.config/tnh_scholar/</li> <li>Creates patterns directory</li> <li> <p>Creates logs directory</p> </li> <li> <p>Pattern Download</p> </li> <li>Offers to download default patterns from GitHub</li> <li>Patterns are saved to ~/.config/tnh_scholar/patterns/</li> <li> <p>Maintains directory structure from repository</p> </li> <li> <p>Environment Check</p> </li> <li>Verifies OpenAI API key configuration</li> <li>Provides guidance if key is missing</li> </ol>"},{"location":"cli-reference/tnh-setup/#examples","title":"Examples","text":""},{"location":"cli-reference/tnh-setup/#complete-setup","title":"Complete Setup","text":"<pre><code># Run full setup\ntnh-setup\n</code></pre>"},{"location":"cli-reference/tnh-setup/#selective-setup","title":"Selective Setup","text":"<pre><code># Skip API key check\ntnh-setup --skip-env\n\n# Skip pattern download\ntnh-setup --skip-patterns\n\n# Skip both\ntnh-setup --skip-env --skip-patterns\n</code></pre>"},{"location":"cli-reference/tnh-setup/#default-patterns","title":"Default Patterns","text":"<p>When downloading patterns, the following are included:</p> <ul> <li>default_punctuate.md</li> <li>default_section.md</li> <li>default_line_translation.md</li> <li>default_xml_format.md</li> <li>default_xml_paragraph_format.md</li> </ul>"},{"location":"cli-reference/tnh-setup/#environment-variables","title":"Environment Variables","text":"<p>The setup process checks for and uses:</p> <ul> <li>OPENAI_API_KEY: Required for AI functionality</li> <li>TNH_PATTERN_DIR: Optional custom pattern directory</li> </ul>"},{"location":"cli-reference/tnh-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli-reference/tnh-setup/#missing-api-key","title":"Missing API Key","text":"<p>If the OpenAI API key is not found, the setup tool displays guidance:</p> <pre><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; OpenAI API key not found in environment. &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n\nFor AI processing with TNH-scholar:\n\n1. Get an API key from https://platform.openai.com/api-keys\n2. Set the OPENAI_API_KEY environment variable:\n\n   export OPENAI_API_KEY='your-api-key-here'  # Linux/Mac\n   set OPENAI_API_KEY=your-api-key-here       # Windows\n\nFor OpenAI API access help: https://platform.openai.com/\n</code></pre>"},{"location":"cli-reference/tnh-setup/#pattern-download-issues","title":"Pattern Download Issues","text":"<ul> <li>Check internet connection</li> <li>Verify GitHub access</li> <li>Ensure write permissions in config directory</li> <li>Check disk space</li> </ul>"},{"location":"cli-reference/tnh-setup/#post-setup-verification","title":"Post-Setup Verification","text":"<p>After running setup, verify:</p> <ol> <li>Directory Structure:</li> </ol> <pre><code>~/.config/tnh_scholar/\n\u251c\u2500\u2500 patterns/\n\u2514\u2500\u2500 logs/\n</code></pre> <ol> <li>Pattern Files:</li> <li>Check that pattern files are present</li> <li>Verify file permissions</li> <li> <p>Ensure proper formatting</p> </li> <li> <p>Environment:</p> </li> <li>Confirm API key is set</li> <li>Test basic AI functionality</li> </ol>"},{"location":"cli-reference/token-count/","title":"token-count","text":"<p>The <code>token-count</code> command calculates the OpenAI API token count for text input. This is useful for ensuring that a text is within maximum token limits for the API model and also for estimating API costs.</p>"},{"location":"cli-reference/token-count/#usage","title":"Usage","text":"<pre><code>token-count [INPUT_FILE]\n</code></pre> <p>If no input file is specified, reads from standard input (stdin).</p>"},{"location":"cli-reference/token-count/#examples","title":"Examples","text":""},{"location":"cli-reference/token-count/#count-tokens-in-file","title":"Count Tokens in File","text":"<pre><code>token-count input.txt\n</code></pre>"},{"location":"cli-reference/token-count/#count-tokens-from-stdin","title":"Count Tokens from Stdin","text":"<pre><code>echo \"Sample text\" | token-count\ncat input.txt | token-count\n</code></pre>"},{"location":"cli-reference/token-count/#pipeline-usage","title":"Pipeline Usage","text":"<pre><code># Count tokens after processing\ncat input.txt | tnh-fab punctuate | token-count\n\n# Count tokens at multiple stages\ncat input.txt | tee &gt;(token-count &gt;&amp;2) | \\\n  tnh-fab punctuate | tee &gt;(token-count &gt;&amp;2) | \\\n  tnh-fab process -p format_xml | token-count\n</code></pre>"},{"location":"cli-reference/token-count/#output","title":"Output","text":"<p>Returns a single integer representing the number of tokens in the input text, calculated using the OpenAI tokenizer.</p>"},{"location":"cli-reference/token-count/#notes","title":"Notes","text":"<ul> <li>Uses the same tokenizer as GPT-4</li> <li>Counts are exact matches to OpenAI API usage</li> <li>Useful for:</li> <li>Cost estimation</li> <li>Context window planning</li> <li>Processing pipeline optimization</li> <li>Model input validation</li> </ul>"},{"location":"cli-reference/token-count/#see-also","title":"See Also","text":"<ul> <li>OpenAI tokenizer documentation</li> <li>TNH Scholar API documentation for token counting</li> <li>tnh-fab documentation for text processing</li> </ul>"},{"location":"cli-reference/ytt-fetch/","title":"ytt-fetch","text":"<p>(Y)ou(T)ube (T)ranscript-(Fetch)ing utility.</p>"},{"location":"cli-reference/ytt-fetch/#usage","title":"Usage","text":"<pre><code>ytt-fetch [OPTIONS] URL\n</code></pre>"},{"location":"cli-reference/ytt-fetch/#options","title":"Options","text":"<pre><code>-l, --lang TEXT     Language code for transcript (default: en)\n-o, --output PATH   Save transcript text to file\n</code></pre>"},{"location":"cli-reference/ytt-fetch/#examples","title":"Examples","text":""},{"location":"cli-reference/ytt-fetch/#download-english-transcript","title":"Download English Transcript","text":"<pre><code>ytt-fetch \"https://youtube.com/watch?v=example\" -l en -o transcript.txt\n</code></pre>"},{"location":"cli-reference/ytt-fetch/#print-transcript-to-console","title":"Print Transcript to Console","text":"<pre><code>ytt-fetch \"https://youtube.com/watch?v=example\"\n</code></pre>"},{"location":"community/","title":"Community &amp; Outreach","text":"<p>This section contains documentation for community partners, stakeholders, and collaborators who support the TNH Scholar project within the Plum Village tradition.</p>"},{"location":"community/#contents","title":"Contents","text":"<ul> <li>Parallax Press &amp; Plum Village Editorial Overview - Project introduction for editorial and publishing stakeholders</li> </ul>"},{"location":"community/#purpose","title":"Purpose","text":"<p>These documents provide accessible, non-technical introductions to the TNH Scholar project for:</p> <ul> <li>Parallax Press and publishing partners</li> <li>Plum Village Editorial community members</li> <li>Monastic communities interested in the project's vision</li> <li>Practitioners exploring collaboration opportunities</li> <li>Potential funders and supporters</li> </ul>"},{"location":"community/#guiding-principles","title":"Guiding Principles","text":"<p>In the spirit of Th\u1ea7y's teachings, our community engagement emphasizes:</p> <ul> <li>Clarity and accessibility - Technical concepts explained with mindful language</li> <li>Transparency - Open sharing of goals, progress, and challenges</li> <li>Collaboration - Welcoming input and partnership from the community</li> <li>Respect for lineage - Honoring the Plum Village tradition and Th\u1ea7y's teachings</li> <li>Interbeing - Recognizing the interconnected nature of all contributions</li> </ul>"},{"location":"community/#contributing","title":"Contributing","text":"<p>If you're interested in supporting or collaborating with TNH Scholar, please:</p> <ul> <li>Review the Parallax Overview for an accessible project introduction</li> <li>Visit our Project Vision for the long-term aspirations</li> <li>Reach out via GitHub Discussions</li> <li>See Contributing to TNH Scholar (Prototype Phase) for how to participate as a tester, experimenter, or code contributor</li> </ul>"},{"location":"community/parallax-overview/","title":"TNH-Scholar: Project Overview for Parallax Press &amp; Plum Village Editorial Stakeholders","text":""},{"location":"community/parallax-overview/#1-purpose-of-this-document","title":"1. Purpose of This Document","text":"<p>This document provides a clear, non-technical introduction to the TNH\u2011Scholar project for members of Parallax Press and the Plum Village Editorial community. It outlines the project\u2019s goals, current capabilities, potential value to Parallax Press, and areas where collaboration or guidance would be mutually beneficial.</p>"},{"location":"community/parallax-overview/#2-short-project-description","title":"2. Short Project Description","text":"<p>TNH\u2011Scholar is a long-term digital humanities and AI\u2011assisted research project dedicated to:</p> <ul> <li>preserving, organizing, and making searchable the collected works of Th\u1ea7y.</li> <li>supporting translation, annotation, and scholarship across Vietnamese, English, French, and other languages,</li> <li>developing tools for practitioners, educators, editors, researchers, and archivists,</li> <li>ensuring high accuracy, transparency, and lineage\u2011faithfulness through provenance tracking and human\u2011guided AI workflows.</li> </ul> <p>Though it has been joked about, this project is not aimed at creating a chatbot to imitate Th\u1ea7y; chat based features certainly have the potential to be explored. It is aimed at creating a scholarly infrastructure, a set of tools that help people understand, translate, cross\u2011reference, and preserve the teachings with care.</p> <p>Core goals:</p> <ul> <li>A clean, well\u2011structured, multilingual digital corpus  </li> <li>Tools for translation, metadata extraction, and scholarly indexing  </li> <li>Research utilities for exploring teachings across decades  </li> <li>A foundation for future learning tools, training materials, and editorial workflows  </li> </ul> <p>The project intends deep respect for the lineage and for the careful stewardship of Th\u1ea7y\u2019s teachings.</p>"},{"location":"community/parallax-overview/#3-benefits-of-the-project-from-an-editorial-publishing-perspective","title":"3. Benefits of the Project from an Editorial &amp; Publishing Perspective","text":""},{"location":"community/parallax-overview/#31-support-for-translation-editing","title":"3.1 Support for Translation &amp; Editing","text":"<ul> <li>High\u2011accuracy draft translations with full provenance  </li> <li>Sentence\u2011level alignment tools  </li> <li>Automated glossaries for Buddhist and Plum Village terminology  </li> <li>Tools for comparing versions across languages</li> </ul>"},{"location":"community/parallax-overview/#32-supporting-publication-editorial-pipelines","title":"3.2 Supporting Publication &amp; Editorial Pipelines","text":"<ul> <li>A searchable corpus of all published (and historically significant unpublished) materials  </li> <li>Automatic metadata extraction (e.g., footnotes, headings, quotes)  </li> <li>Tools for preparing anthologies, themed collections, revised editions  </li> <li>Detection of duplicated text or passages reused across books, talks, or letters</li> </ul>"},{"location":"community/parallax-overview/#33-archival-preservation","title":"3.3 Archival &amp; Preservation","text":"<ul> <li>Clean digital text extracted from aging PDFs and scans  </li> <li>OCR correction tools  </li> <li>Structured digital formats for long-term storage  </li> <li>Cross\u2011referencing of early journals, handwritten pieces, letters, Dharma talks</li> </ul>"},{"location":"community/parallax-overview/#34-accessibility-community-benefit","title":"3.4 Accessibility &amp; Community Benefit","text":"<ul> <li>Resources for educators and sanghas  </li> <li>Study and research tools for practitioners  </li> <li>Future bilingual interactive readers or teaching companions  </li> <li>A foundation for future Plum Village digital learning platforms</li> </ul>"},{"location":"community/parallax-overview/#4-a-collaborative-vision-with-parallax-press","title":"4. A Collaborative Vision with Parallax Press","text":"<p>Parallax Press is uniquely positioned to act as:</p> <ul> <li>A Steward \u2014 ensuring the project remains aligned with Th\u1ea7y\u2019s values.</li> <li>A Beneficiary \u2014 gaining tools that support editing, translation, and publishing.</li> <li>A Contributor \u2014 with access to high-quality source materials.</li> <li>A Guide \u2014 offering feedback on editorial standards and lineage integrity.</li> </ul> <p>Possible roles:</p> <ul> <li>A liaison or advisory team</li> <li>Review of project direction documents  </li> <li>Guidance on terminology, lineage language, text selection  </li> <li>Access to digital source materials for specific books  </li> <li>Collaborative long-term vision for Plum Village\u2019s digital future  </li> </ul>"},{"location":"community/parallax-overview/#5-high-level-architecture-non-technical-overview","title":"5. High-Level Architecture (Non-Technical Overview)","text":"<p>TNH\u2011Scholar consists of:</p>"},{"location":"community/parallax-overview/#51-a-clean-structured-corpus","title":"5.1 A Clean, Structured Corpus","text":"<p>Digital text organized into:</p> <ul> <li>books, chapters, sections, paragraphs  </li> <li>talks, retreats, interviews, letters  </li> <li>early journals (e.g., Vietnamese Buddhist Review collections)</li> </ul>"},{"location":"community/parallax-overview/#52-processing-pipelines","title":"5.2 Processing Pipelines","text":"<p>Tools for:</p> <ul> <li>OCR \u2192 clean text  </li> <li>tagging metadata  </li> <li>extracting headings, quotes, footnotes  </li> <li>aligning translations  </li> <li>classifying themes and concepts</li> </ul>"},{"location":"community/parallax-overview/#53-a-genai-service","title":"5.3 A \u201cGenAI Service\u201d","text":"<p>A system that runs AI-powered tasks with:</p> <ul> <li>full transparency  </li> <li>reproducible inputs  </li> <li>strict provenance (never hidden)  </li> <li>human review loops  </li> </ul>"},{"location":"community/parallax-overview/#54-user-interfaces","title":"5.4 User Interfaces","text":"<ul> <li>command line tools for researchers  </li> <li>Jupyter notebooks for analysis  </li> <li>(future) VS Code tools for editors/translators  </li> <li>(future) interactive web portals for reading/study  </li> </ul>"},{"location":"community/parallax-overview/#6-tnhscholar-vision-potential-impact","title":"6. TNH\u2011Scholar Vision &amp; Potential Impact","text":""},{"location":"community/parallax-overview/#61-background","title":"6.1 Background","text":"<p>For decades, Th\u1ea7y produced a vast corpus of material across multiple languages and formats. Much of this exists in:</p> <ul> <li>scanned PDFs  </li> <li>partial digital manuscripts  </li> <li>older OCR with errors  </li> <li>translations with varying conventions  </li> <li>scattered archival sources  </li> </ul> <p>As scholarship grows, and as new generations seek to engage with these teachings, the need for a structured, unified, lineage\u2011transparent digital corpus manifests.</p>"},{"location":"community/parallax-overview/#62-goals-of-tnhscholar","title":"6.2 Goals of TNH\u2011Scholar","text":"<ul> <li>Provide a faithful, structured representation of Th\u1ea7y\u2019s teachings  </li> <li>Enable search, discovery, and thematic exploration </li> <li>Support translation workflows with accuracy and style fidelity  </li> <li>Offer scholarly infrastructure, not automated interpretation  </li> <li>Ensure ethical, transparent AI usage firmly grounded in human oversight  </li> <li>Preserve text for future generations in durable, well-defined digital formats  </li> </ul>"},{"location":"community/parallax-overview/#63-editorial-benefits","title":"6.3 Editorial Benefits","text":"<ol> <li>Faster first\u2011pass translation drafts with consistent terminology.  </li> <li>Tools for polishing translations using side-by-side alignment.  </li> <li>Reduction in manual OCR cleanup labor on old sources.  </li> <li>Structured metadata enabling easier anthology creation.  </li> <li>Support for future critical editions and chronology projects.  </li> </ol>"},{"location":"community/parallax-overview/#64-community-benefits","title":"6.4 Community Benefits","text":"<ul> <li>Practitioners gain indexed, searchable access to teachings.  </li> <li>Teachers gain bilingual tools for quoting or referencing materials.  </li> <li>Researchers gain deeper insight into historical development of themes.  </li> </ul>"},{"location":"community/parallax-overview/#65-long-term-vision","title":"6.5 Long-Term Vision","text":"<p>The TNH\u2011Scholar system becomes the base layer of a future:</p> <ul> <li>Plum Village Digital Archive  </li> <li>Multilingual Study Companion  </li> <li>Hermitage Research Tool for monastics  </li> <li>Teaching companion for Dharma instructors  </li> <li>Preservation system for rare or fragile materials  </li> </ul>"},{"location":"community/parallax-overview/#7-invitation-for-collaboration","title":"7. Invitation for Collaboration","text":"<p>Request for the Parallax Press Team:</p> <ul> <li>Review the project\u2019s principles and vision  </li> <li>Explore possible collaboration  </li> <li>Discuss mutual needs in publishing and digital preservation  </li> <li>Consider forming a small advisory/feedback group  </li> <li>Co-create a long-term roadmap for Plum Village digital infrastructure  </li> </ul> <p>With an aspiration for partnership, alignment, and mutual benefit.</p>"},{"location":"community/parallax-overview/#8-contact","title":"8. Contact","text":"<p>Project Lead: Aaron Solomon <code>aaron.kyle.solomon@gmail.com</code> (Deer Park Monastery)</p> <p>Project Repository (GitHub) Project Documentation</p> <p>This document may be shared within editorial leadership circles at Parallax Press, Plum Village, and related sangha communities.</p>"},{"location":"development/","title":"Development","text":"<p>Table of Contents:</p> <p>Contributing to TNH Scholar (Prototype Phase) - TNH Scholar is currently in rapid prototype phase, focusing on core functionality and basic usability. We welcome contributions that help validate and improve the prototype implementation.</p> <p>TNH Scholar Design Principles - Architectural patterns, design philosophy, and system organization principles for TNH Scholar development.</p> <p>Fine Tuning Strategy - Strategy outline and development plan for fine-tuning foundation models on Thich Nhat Hanh translations.</p> <p>Git Workflow &amp; Safety Guide - Safe git practices for TNH Scholar development to prevent data loss</p> <p>Human-AI Software Engineering Principles - This document presents the Human-AI Software Engineering Principles, a comprehensive framework that builds upon established software engineering, architecture, and design principles from human-only teams and extends them to optimize collaboration between humans and AI agents. Central to this framework is the clear distinction between the Design Phase and the Coding Phase, each with distinct goals, modes, and workflows. It also addresses challenges such as context window limitations and maintaining alignment despite session resets. In addition to general principles, this framework incorporates concrete documentation and planning strategies designed to support long-term, sustainable human-AI collaboration.</p> <p>Improvements / Initial structure - Initial high-level view of the TNH Scholar ecosystem.</p> <p>Development Documentation - Landing page for contributor guides, design principles, and engineering practices for TNH Scholar.</p> <p>Release Workflow - Automated release process for TNH Scholar with biweekly cadence during rapid prototyping.</p> <p>TNH Scholar Style Guide - Code formatting, naming conventions, and Python standards for TNH Scholar development.</p> <p>TNH Scholar System Design - High-level system design describing the cyclical AI processing architecture powering TNH Scholar.</p> <p>This file auto-generated.</p>"},{"location":"development/contributing-prototype-phase/","title":"Contributing to TNH Scholar (Prototype Phase)","text":"<p>TNH Scholar is currently in rapid prototype phase, focusing on core functionality and basic usability. We welcome contributions that help validate and improve the prototype implementation.</p>"},{"location":"development/contributing-prototype-phase/#we-need-testers-and-experimenters","title":"We Need Testers and Experimenters","text":"<p>You don't need coding experience to contribute meaningfully! The TNH Scholar project is actively seeking community members to:</p> <ul> <li>Explore the software - Try the CLI tools with real dharma talk content and see what works (and what doesn't)</li> <li>Report your experience - Share what you discover: pain points, confusing behavior, missing features, or delightful surprises</li> <li>Experiment with workflows - Test different command pipelines and patterns to process your materials</li> <li>Identify needs - Help us understand what practitioners and scholars actually need from these tools</li> </ul> <p>Your perspective as a practitioner, translator, or researcher using the tools is invaluable during this prototype phase.</p>"},{"location":"development/contributing-prototype-phase/#current-focus-areas","title":"Current Focus Areas","text":"<ol> <li> <p>TNH-FAB Command Line Tool</p> <ul> <li>Basic functionality testing</li> <li>Error case identification</li> <li>Command pipeline testing</li> <li>Pattern system integration</li> </ul> </li> <li> <p>Pattern System</p> <ul> <li>Pattern usage testing</li> <li>Pattern creation testing</li> <li>Version control functionality</li> <li>Concurrent access testing</li> </ul> </li> <li> <p>AUDIO-TRANSCRIBE Command Line Tool</p> <ul> <li>Basic functionality testing</li> <li>Error case identification</li> </ul> </li> </ol>"},{"location":"development/contributing-prototype-phase/#how-to-help","title":"How to Help","text":""},{"location":"development/contributing-prototype-phase/#getting-started-as-a-tester","title":"Getting Started as a Tester","text":"<ol> <li>Install TNH Scholar</li> </ol> <pre><code>pip install tnh-scholar\n</code></pre> <ol> <li>Try the Quick Start Guide</li> </ol> <p>Follow the Quick Start Guide to get familiar with basic operations</p> <ol> <li>Test with Your Own Materials</li> </ol> <p>Experiment with real dharma talk content using commands like:</p> <pre><code># Test basic commands\ntnh-fab punctuate input.txt\ntnh-fab section input.txt\ntnh-fab translate input.txt\ntnh-fab process -p pattern_name input.txt\n\n# Test pipeline operations\ncat input.txt | tnh-fab punctuate | tnh-fab section\n</code></pre> <ol> <li> <p>Explore the Pattern System</p> </li> <li> <p>Create test patterns in <code>~/.config/tnh-scholar/patterns/</code></p> </li> <li>Test pattern loading and application</li> <li> <p>Try custom workflow combinations</p> </li> <li> <p>Report What You Find</p> </li> </ol> <p>Share your discoveries via GitHub Issues:    - Clear description of the problem or observation    - Steps to reproduce (include the command used)    - Expected vs actual behavior    - Your environment (OS, Python version)    - Example files (if helpful)</p>"},{"location":"development/contributing-prototype-phase/#code-contributions","title":"Code Contributions","text":"<p>At this prototype stage:</p> <ul> <li>Start with bug fixes</li> <li>Keep changes focused</li> <li>Include tests for new functionality</li> <li>Follow existing code style</li> <li>See style guide and design principles for coding standards and architectural patterns.</li> </ul>"},{"location":"development/contributing-prototype-phase/#questions-and-discussion","title":"Questions and Discussion","text":"<ul> <li>Use GitHub Issues for feature discussions</li> <li>Tag issues with 'question' or 'discussion'</li> <li>Focus on prototype phase functionality</li> </ul> <p>This is a project in rapid prototype - we're looking for practical feedback on core functionality as well as possible new feature additions and new tools.</p>"},{"location":"development/design-principles/","title":"TNH Scholar Design Principles","text":"<p>Architectural patterns, design philosophy, and system organization principles for TNH Scholar development.</p>"},{"location":"development/design-principles/#overview","title":"Overview","text":"<p>This document establishes design principles for the TNH Scholar project. While the project is currently in a rapid prototyping phase, these guidelines aim to maintain architectural quality and consistency throughout development. The guide distinguishes between immediate prototyping requirements and standards for later production phases where appropriate.</p> <p>For code formatting and naming conventions, see Style Guide. For high-level project philosophy and vision, see Project Principles and Conceptual Architecture.</p>"},{"location":"development/design-principles/#core-design-philosophy","title":"Core Design Philosophy","text":"<p>The TNH Scholar system embraces several key philosophical principles:</p> <ul> <li>Evolutionary improvement through self-generated training data</li> <li>Modular design enabling flexible pipeline construction</li> <li>Balance of rapid prototyping with extensible architecture</li> <li>Focus on AI-enhanced content processing and transformation</li> </ul> <p>See Project Philosophy for deeper context.</p>"},{"location":"development/design-principles/#fundamental-principles","title":"Fundamental Principles","text":""},{"location":"development/design-principles/#single-responsibility","title":"Single Responsibility","text":"<p>Code should adhere to the principle of single responsibility, with functions and classes focused on one clear task:</p> <ul> <li>Functions perform one logical operation</li> <li>Classes manage related state or behavior</li> <li>Modules encapsulate coherent functionality</li> <li>Services handle one domain concern</li> </ul>"},{"location":"development/design-principles/#composition-over-inheritance","title":"Composition Over Inheritance","text":"<p>Favor composition over inheritance to build complex behavior from simple, reusable components:</p> <pre><code># \u2705 Preferred: Composition\nclass TextPipeline:\n    def __init__(\n        self,\n        punctuator: PunctuationService,\n        translator: TranslationService,\n        sectioner: SectioningService\n    ):\n        self.punctuator = punctuator\n        self.translator = translator\n        self.sectioner = sectioner\n\n# \ud83d\udeab Avoid: Deep inheritance hierarchies\nclass TranslatingPunctuatingSectioningProcessor(\n    PunctuationProcessor,\n    TranslationProcessor,\n    SectioningProcessor\n):\n    pass\n</code></pre>"},{"location":"development/design-principles/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Validation logic should be distinct from mutation or side effects to ensure maintainability:</p> <pre><code># \u2705 Preferred: Separate concerns\ndef validate_text_input(text: str) -&gt; None:\n    \"\"\"Validate input only.\"\"\"\n    if not text or not text.strip():\n        raise ValueError(\"Text cannot be empty\")\n\ndef process_text(text: str) -&gt; ProcessedText:\n    \"\"\"Process after validation.\"\"\"\n    validate_text_input(text)\n    return apply_processing(text)\n\n# \ud83d\udeab Avoid: Mixed concerns\ndef process_text(text: str) -&gt; ProcessedText:\n    \"\"\"Validation, mutation, and I/O all mixed.\"\"\"\n    if not text:\n        raise ValueError(\"Empty\")\n    result = apply_processing(text)\n    save_to_database(result)  # Side effect mixed with processing\n    return result\n</code></pre>"},{"location":"development/design-principles/#modularity-principles","title":"Modularity Principles","text":""},{"location":"development/design-principles/#design-for-modularity","title":"Design for Modularity","text":"<p>Each module, class, and function should have a single, well-defined responsibility.</p> <p>Guidelines:</p> <ul> <li>Encapsulate related functionality: Group related functions and classes into modules and packages to promote reuse and clarity</li> <li>Minimize coupling: Modules should interact through well-defined interfaces, minimizing dependencies and side effects</li> <li>Limit module size: Aim for modules that are small enough to be easily understood (generally &lt; 300 lines), but large enough to encapsulate a coherent set of functionality</li> <li>Explicit module exports: Use <code>__all__</code> to define public API of modules where appropriate</li> </ul>"},{"location":"development/design-principles/#helpers-should-be-small-and-composable","title":"Helpers Should Be Small and Composable","text":"<p>Helpers should be small (target 10 lines) and composable, enabling reuse and clarity:</p> <pre><code># \u2705 Preferred: Small, composable helpers\ndef extract_language_code(text_object: TextObject) -&gt; str:\n    \"\"\"Extract language code from text object.\"\"\"\n    return text_object.metadata.get(\"language\", \"en\")\n\ndef validate_language_code(code: str) -&gt; None:\n    \"\"\"Validate ISO 639-1 language code.\"\"\"\n    if len(code) != 2 or not code.isalpha():\n        raise ValueError(f\"Invalid language code: {code}\")\n\ndef get_validated_language(text_object: TextObject) -&gt; str:\n    \"\"\"Get and validate language code.\"\"\"\n    code = extract_language_code(text_object)\n    validate_language_code(code)\n    return code\n\n# \ud83d\udeab Avoid: Large, monolithic helpers\ndef process_and_validate_language_with_fallback_and_logging(\n    text_object: TextObject,\n    default: str = \"en\"\n) -&gt; str:\n    \"\"\"Do everything in one place (50+ lines).\"\"\"\n    # ... lots of mixed logic\n</code></pre>"},{"location":"development/design-principles/#interface-design","title":"Interface Design","text":""},{"location":"development/design-principles/#abstract-base-classes-and-protocols","title":"Abstract Base Classes and Protocols","text":"<p>All system interfaces must be defined via abstract base classes:</p> <ul> <li>Use <code>Protocol</code> for structural typing and interface contracts (no inheritance required)</li> <li>Use <code>ABC</code> only when enforcing init-time invariants or providing shared mixin behavior</li> </ul> <p>Example with Protocol:</p> <pre><code>from typing import Protocol\n\nclass PromptProvider(Protocol):\n    \"\"\"Protocol for prompt providers.\"\"\"\n\n    def get_prompt(self, name: str) -&gt; Prompt:\n        \"\"\"Retrieve prompt by name.\"\"\"\n        ...\n\n    def list_prompts(self) -&gt; list[str]:\n        \"\"\"List available prompt names.\"\"\"\n        ...\n</code></pre> <p>Example with ABC:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BaseProcessor(ABC):\n    \"\"\"Base processor with shared initialization.\"\"\"\n\n    def __init__(self, config: ProcessorConfig):\n        self.config = config\n        self._validate_config()\n\n    @abstractmethod\n    def _validate_config(self) -&gt; None:\n        \"\"\"Validate configuration at init time.\"\"\"\n        pass\n\n    @abstractmethod\n    def process(self, input_data: Any) -&gt; Any:\n        \"\"\"Process input data.\"\"\"\n        pass\n</code></pre>"},{"location":"development/design-principles/#dependency-injection","title":"Dependency Injection","text":"<p>Prefer dependency injection over global state:</p> <pre><code># \u2705 Preferred: Dependency injection\nclass TextProcessor:\n    def __init__(\n        self,\n        prompt_catalog: PromptCatalog,\n        ai_service: GenAIService\n    ):\n        self.prompt_catalog = prompt_catalog\n        self.ai_service = ai_service\n\n# \ud83d\udeab Avoid: Global singleton access\nclass TextProcessor:\n    def process(self, text: str) -&gt; str:\n        prompt = LocalPromptManager().get_prompt(\"default\")  # Global access\n        return global_ai_service.process(text, prompt)  # Global access\n</code></pre> <p>Note: During prototyping, singleton access (like <code>LocalPromptManager</code>) is acceptable for rapid development. Plan transition to dependency injection for production (see ADR-PT01).</p>"},{"location":"development/design-principles/#data-architecture","title":"Data Architecture","text":""},{"location":"development/design-principles/#immutability-by-default","title":"Immutability by Default","text":"<p>Keep data models immutable when possible for safer concurrent code:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass ProcessingResult:\n    \"\"\"Immutable processing result.\"\"\"\n    content: str\n    metadata: Dict[str, Any]\n    timestamp: datetime\n</code></pre>"},{"location":"development/design-principles/#strong-type-boundaries","title":"Strong Type Boundaries","text":"<p>Critical: No literals or untyped structures in application logic:</p> <ul> <li>Configuration values come from <code>Settings</code> (pydantic BaseSettings)</li> <li>Dicts are not used in app layers; prefer Pydantic models or dataclasses</li> <li>Enums replace string literals for identifiers</li> <li>Adapters handle dict conversions only at API transport boundaries</li> </ul> <p>See Style Guide: Strong Typing Standards for details.</p>"},{"location":"development/design-principles/#separation-of-data-and-logic","title":"Separation of Data and Logic","text":"<p>Separate data representation from business logic where appropriate:</p> <pre><code># \u2705 Preferred: Data models separate from services\n@dataclass\nclass PromptMetadata:\n    \"\"\"Data model.\"\"\"\n    name: str\n    version: str\n    variables: list[str]\n\nclass PromptService:\n    \"\"\"Business logic.\"\"\"\n    def render_prompt(self, metadata: PromptMetadata, context: Dict) -&gt; str:\n        ...\n\n# \ud83d\udeab Avoid: Mixed data and complex logic\nclass Prompt:\n    def __init__(self, name: str):\n        self.name = name\n        # ... lots of fields\n\n    def render_with_ai_fallback_and_caching(self, context: Dict) -&gt; str:\n        # ... 100 lines of business logic in data class\n</code></pre>"},{"location":"development/design-principles/#error-handling-philosophy","title":"Error Handling Philosophy","text":""},{"location":"development/design-principles/#explicit-over-implicit","title":"Explicit Over Implicit","text":"<p>Handle warnings and errors thoughtfully, distinguishing recoverable conditions from critical failures:</p> <ul> <li>Use specific exception types (not <code>Exception</code>)</li> <li>Let unknown exceptions propagate</li> <li>Document expected exceptions in docstrings</li> <li>Use custom exceptions for domain-specific errors</li> </ul>"},{"location":"development/design-principles/#fail-fast","title":"Fail Fast","text":"<p>During prototyping, fail fast to identify issues early:</p> <ul> <li>Don't mask exceptions with catch-all handlers</li> <li>Let stack traces propagate for debugging</li> <li>Add TODO comments for future error handling</li> </ul> <p>See Style Guide: Error Handling for implementation details.</p>"},{"location":"development/design-principles/#processing-architecture","title":"Processing Architecture","text":""},{"location":"development/design-principles/#pipeline-design","title":"Pipeline Design","text":"<p>Pipelines compose processors into workflows:</p> <ul> <li>Each processor has a single, well-defined transformation</li> <li>Processors are independent and reusable</li> <li>Pipeline orchestration is separate from processing logic</li> <li>Results flow through immutable data structures</li> </ul> <p>Example:</p> <pre><code>class ProcessingPipeline:\n    \"\"\"Orchestrates content processing workflow.\"\"\"\n\n    def __init__(\n        self,\n        processors: list[ContentProcessor],\n        training_collector: Optional[TrainingCollector] = None\n    ):\n        self.processors = processors\n        self.collector = training_collector\n\n    def execute(\n        self,\n        source: ContentSource\n    ) -&gt; tuple[ProcessedContent, Optional[TrainingData]]:\n        \"\"\"Execute pipeline with optional training data collection.\"\"\"\n        content = source.load()\n\n        for processor in self.processors:\n            content = processor.process(content)\n\n            if self.collector:\n                self.collector.collect(processor, content)\n\n        training_data = self.collector.finalize() if self.collector else None\n        return content, training_data\n</code></pre>"},{"location":"development/design-principles/#dispatch-patterns","title":"Dispatch Patterns","text":"<p>Employ dispatch patterns (prefer <code>match</code>/<code>case</code> when possible) to cleanly separate concerns and improve extensibility:</p> <pre><code># \u2705 Preferred: Pattern matching for dispatch\ndef route_processing(task: ProcessingTask) -&gt; ProcessedContent:\n    match task.type:\n        case TaskType.PUNCTUATE:\n            return punctuate_processor.process(task.content)\n        case TaskType.TRANSLATE:\n            return translation_processor.process(task.content)\n        case TaskType.SECTION:\n            return sectioning_processor.process(task.content)\n        case _:\n            raise ValueError(f\"Unknown task type: {task.type}\")\n</code></pre>"},{"location":"development/design-principles/#text-processing-principles","title":"Text Processing Principles","text":""},{"location":"development/design-principles/#explicit-text-handling","title":"Explicit Text Handling","text":"<p>Text handling should be explicit and consistent, favoring clarity in encoding and processing:</p> <ul> <li>Always specify encoding (UTF-8 default)</li> <li>Handle normalization explicitly (NFC, NFKC, etc.)</li> <li>Document text format assumptions</li> <li>Preserve provenance and metadata</li> </ul>"},{"location":"development/design-principles/#metadata-preservation","title":"Metadata Preservation","text":"<p>Throughout processing pipelines:</p> <ul> <li>Maintain document structure</li> <li>Preserve metadata across transformations</li> <li>Track provenance and processing history</li> <li>Support metadata enrichment</li> </ul>"},{"location":"development/design-principles/#testing-architecture","title":"Testing Architecture","text":""},{"location":"development/design-principles/#test-organization","title":"Test Organization","text":"<p>Tests follow this structure even during prototyping:</p> <pre><code>tests/\n\u251c\u2500\u2500 unit/              # Fast, isolated tests\n\u2502   \u251c\u2500\u2500 test_text_processing.py\n\u2502   \u2514\u2500\u2500 test_prompt_catalog.py\n\u251c\u2500\u2500 integration/       # Multi-component tests\n\u2502   \u2514\u2500\u2500 test_full_pipeline.py\n\u2514\u2500\u2500 conftest.py        # Shared fixtures\n</code></pre>"},{"location":"development/design-principles/#test-requirements-by-phase","title":"Test Requirements by Phase","text":"<p>Prototyping Phase:</p> <ul> <li>Basic unit tests for core functionality</li> <li>Critical path testing</li> <li>Basic integration tests</li> </ul> <p>Production Phase:</p> <ul> <li>Comprehensive unit test coverage (&gt;80%)</li> <li>Full integration test suite</li> <li>Performance testing</li> <li>Edge case handling</li> <li>Mock external services</li> </ul>"},{"location":"development/design-principles/#performance-principles","title":"Performance Principles","text":""},{"location":"development/design-principles/#resource-management","title":"Resource Management","text":"<p>Basic guidelines apply across phases:</p> <p>Memory Management:</p> <ul> <li>Stream large files (don't load entirely into memory)</li> <li>Clean up temporary files</li> <li>Monitor memory usage in processing pipelines</li> </ul> <p>Processing Optimization:</p> <ul> <li>Batch operations where possible</li> <li>Cache frequently used data (prompts, configurations)</li> <li>Monitor API usage and costs</li> <li>Use async/await for I/O-bound operations</li> </ul>"},{"location":"development/design-principles/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Defer expensive computations until needed:</p> <pre><code># \u2705 Preferred: Lazy evaluation\nclass PromptCatalog:\n    def __init__(self, directory: Path):\n        self.directory = directory\n        self._cache: Dict[str, Prompt] = {}\n\n    def get_prompt(self, name: str) -&gt; Prompt:\n        \"\"\"Load prompt on demand.\"\"\"\n        if name not in self._cache:\n            self._cache[name] = self._load_prompt(name)\n        return self._cache[name]\n\n# \ud83d\udeab Avoid: Eager loading of everything\nclass PromptCatalog:\n    def __init__(self, directory: Path):\n        self.prompts = {\n            p.name: p for p in self._load_all_prompts(directory)\n        }  # Loads hundreds of prompts at startup\n</code></pre>"},{"location":"development/design-principles/#refactoring-triggers","title":"Refactoring Triggers","text":"<p>Refactor when:</p> <ul> <li>Code becomes difficult to follow</li> <li>Duplication arises (DRY principle)</li> <li>New requirements suggest clearer abstractions</li> <li>Function/module exceeds complexity limits</li> <li>Tests become difficult to write or maintain</li> </ul>"},{"location":"development/design-principles/#development-phase-considerations","title":"Development Phase Considerations","text":""},{"location":"development/design-principles/#prototyping-phase-priorities","title":"Prototyping Phase Priorities","text":"<p>During prototyping, prioritize:</p> <ul> <li>Rapid iteration and experimentation</li> <li>Core functionality over comprehensive error handling</li> <li>Simple pipeline construction</li> <li>Clear component boundaries</li> <li>Basic testing and documentation</li> </ul>"},{"location":"development/design-principles/#production-phase-requirements","title":"Production Phase Requirements","text":"<p>For production, add:</p> <ul> <li>Comprehensive error handling</li> <li>Performance optimization</li> <li>Security hardening</li> <li>Full test coverage</li> <li>Complete documentation</li> <li>Monitoring and observability</li> </ul>"},{"location":"development/design-principles/#future-architecture-considerations","title":"Future Architecture Considerations","text":"<p>Areas marked for future development:</p> <ul> <li>Plugin system architecture</li> <li>Enhanced configuration management</li> <li>Rebuild of <code>ai_text_processing</code> suite with modern patterns</li> <li>Extended API integration (batch processing, alternate model services)</li> <li>Enhanced security features</li> <li>Performance optimization and async processing</li> <li>Extended prompt capabilities</li> <li>Model training and fine-tuning tools</li> </ul> <p>See Future Directions for long-term vision.</p>"},{"location":"development/design-principles/#related-documentation","title":"Related Documentation","text":"<ul> <li>Style Guide - Code formatting and naming conventions</li> <li>Object-Service Design Blueprint - Detailed architecture patterns</li> <li>System Design - High-level system architecture</li> <li>Project Principles - High-level project principles</li> <li>Conceptual Architecture - Conceptual system model</li> <li>Contributing Guide - Contribution workflow</li> </ul>"},{"location":"development/design-principles/#references","title":"References","text":"<ul> <li>Core Pattern Architecture - Legacy prompt/pattern design notes</li> <li>Object-Service Design Blueprint - Layer architecture and design patterns</li> <li>Clean Architecture - Robert C. Martin</li> <li>Domain-Driven Design - Eric Evans via Martin Fowler</li> </ul>"},{"location":"development/fine-tuning-strategy/","title":"Fine Tuning Strategy","text":"<p>Strategy outline and development plan for fine-tuning foundation models on Thich Nhat Hanh translations.</p>"},{"location":"development/fine-tuning-strategy/#initial-prompt","title":"Initial Prompt","text":"<p>hello claude,</p> <p>I'd like you to tell me how you would go about fine-tuning an openai gpt-4o model or an anthropic model to improve translations of thich nhat hanh's works between the languages he taught in: Vietnamese, English, French. This is a project initiated by the plum village community with full access to collected works and permission to develop this for internal use only (for translation purposes). It would be of secondary benefit if the model also learned something about thay's teachings and could reflect, answer questions and so on.</p> <p>give a high level design strategy and then a detailed plan, with initial skeleton development in python using stubs for functions and classes. be as thorough and complete as possible. Consider an arc of development from prototyping and testing phase through initial development to a final stage of full development.</p> <p>Outputs are</p> <p>1) design plan and overview with considerations. 2) process sequence from prototyping and testing through full implementation 3) initial skeleton structures: including file structure blueprint with all major modules, and all major classes and high level functions as stubs. For minor modules these can simply be listed in the file structure blueprint.</p>"},{"location":"development/fine-tuning-strategy/#translation-project-structure","title":"Translation Project Structure","text":"<pre><code># Project Structure\n\ntnh_translation/\n\u2502\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u2502\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 model_config.yaml\n\u2502   \u251c\u2500\u2500 training_config.yaml\n\u2502   \u2514\u2500\u2500 evaluation_config.yaml\n\u2502\n\u251c\u2500\u2500 notebooks/\n\u2502   \u251c\u2500\u2500 data_exploration.ipynb\n\u2502   \u251c\u2500\u2500 model_experimentation.ipynb\n\u2502   \u2514\u2500\u2500 evaluation_analysis.ipynb\n\u2502\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 corpus.py\n\u2502   \u2502   \u251c\u2500\u2500 preprocessing.py\n\u2502   \u2502   \u251c\u2500\u2500 alignment.py\n\u2502   \u2502   \u2514\u2500\u2500 validation.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 base_model.py\n\u2502   \u2502   \u251c\u2500\u2500 fine_tuning.py\n\u2502   \u2502   \u2514\u2500\u2500 serving.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 training/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 trainer.py\n\u2502   \u2502   \u251c\u2500\u2500 callbacks.py\n\u2502   \u2502   \u2514\u2500\u2500 optimization.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 evaluation/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 metrics.py\n\u2502   \u2502   \u251c\u2500\u2500 analysis.py\n\u2502   \u2502   \u2514\u2500\u2500 visualization.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 logging.py\n\u2502       \u251c\u2500\u2500 config.py\n\u2502       \u2514\u2500\u2500 helpers.py\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 test_data/\n\u2502   \u251c\u2500\u2500 test_models/\n\u2502   \u251c\u2500\u2500 test_training/\n\u2502   \u2514\u2500\u2500 test_evaluation/\n\u2502\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 prepare_data.py\n    \u251c\u2500\u2500 train_model.py\n    \u2514\u2500\u2500 evaluate_model.py\n</code></pre>"},{"location":"development/fine-tuning-strategy/#core-implementation-classes","title":"Core Implementation Classes","text":"<pre><code># src/data/corpus.py\n\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass TextUnit:\n    \"\"\"Represents a single unit of text (paragraph, sentence, etc.) in multiple languages\"\"\"\n    id: str\n    content: Dict[str, str]  # Language code -&gt; text\n    metadata: Dict[str, any]\n    source_file: Path\n    alignment_score: Optional[float] = None\n\nclass ParallelCorpus:\n    \"\"\"Manages multilingual parallel text corpus for training\"\"\"\n\n    def __init__(self, base_path: Path):\n        self.base_path = Path(base_path)\n        self.texts: List[TextUnit] = []\n        self.language_pairs: List[tuple] = []\n\n    def load_texts(self, source_paths: Dict[str, Path]):\n        \"\"\"Load texts from multiple sources with language mapping\"\"\"\n        pass\n\n    def align_texts(self, method: str = \"sentence\"):\n        \"\"\"Align texts across languages using specified method\"\"\"\n        pass\n\n    def validate_alignment(self, threshold: float = 0.8):\n        \"\"\"Validate alignment quality\"\"\"\n        pass\n\n    def export_training_data(self, output_path: Path):\n        \"\"\"Export aligned texts in training format\"\"\"\n        pass\n\n# src/data/preprocessing.py\n\nclass TextPreprocessor:\n    \"\"\"Handles text preprocessing for training data\"\"\"\n\n    def __init__(self, config: Dict):\n        self.config = config\n\n    def clean_text(self, text: str) -&gt; str:\n        \"\"\"Clean and normalize text\"\"\"\n        pass\n\n    def tokenize(self, text: str) -&gt; List[str]:\n        \"\"\"Tokenize text using appropriate method\"\"\"\n        pass\n\n    def handle_special_terms(self, text: str) -&gt; str:\n        \"\"\"Process domain-specific terminology\"\"\"\n        pass\n\n# src/models/base_model.py\n\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Dict, Any\n\nclass BaseModel(ABC):\n    \"\"\"Abstract base class for model implementations\"\"\"\n\n    def __init__(self, model_name: str, config: Dict[str, Any]):\n        self.model_name = model_name\n        self.config = config\n\n    @abstractmethod\n    def load_model(self):\n        \"\"\"Load pre-trained model\"\"\"\n        pass\n\n    @abstractmethod\n    def prepare_inputs(self, texts: List[str]) -&gt; Dict:\n        \"\"\"Prepare inputs for model\"\"\"\n        pass\n\n    @abstractmethod\n    def train_step(self, batch: Dict) -&gt; Dict:\n        \"\"\"Perform single training step\"\"\"\n        pass\n\nclass GPT4Model(BaseModel):\n    \"\"\"GPT-4 model implementation\"\"\"\n\n    def __init__(self, config: Dict[str, Any]):\n        super().__init__(\"gpt-4\", config)\n\n    def load_model(self):\n        \"\"\"Load GPT-4 model\"\"\"\n        pass\n\n    def prepare_inputs(self, texts: List[str]) -&gt; Dict:\n        \"\"\"Prepare inputs for GPT-4\"\"\"\n        pass\n\n# src/training/trainer.py\n\nclass ModelTrainer:\n    \"\"\"Handles model training and fine-tuning process\"\"\"\n\n    def __init__(\n        self,\n        model: BaseModel,\n        train_data: ParallelCorpus,\n        val_data: ParallelCorpus,\n        config: Dict[str, Any]\n    ):\n        self.model = model\n        self.train_data = train_data\n        self.val_data = val_data\n        self.config = config\n\n    def train(self):\n        \"\"\"Execute training loop\"\"\"\n        pass\n\n    def validate(self):\n        \"\"\"Run validation\"\"\"\n        pass\n\n    def save_checkpoint(self):\n        \"\"\"Save model checkpoint\"\"\"\n        pass\n\n# src/evaluation/metrics.py\n\nclass TranslationEvaluator:\n    \"\"\"Evaluates translation quality using multiple metrics\"\"\"\n\n    def __init__(self, metrics: List[str]):\n        self.metrics = metrics\n\n    def evaluate(\n        self,\n        predictions: List[str],\n        references: List[str]\n    ) -&gt; Dict[str, float]:\n        \"\"\"Calculate evaluation metrics\"\"\"\n        pass\n\n    def analyze_errors(\n        self,\n        predictions: List[str],\n        references: List[str]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Perform error analysis\"\"\"\n        pass\n\n# src/evaluation/analysis.py\n\nclass QualityAnalyzer:\n    \"\"\"Analyzes translation quality and provides detailed feedback\"\"\"\n\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n\n    def analyze_terminology(self, text: str) -&gt; Dict[str, Any]:\n        \"\"\"Analyze terminology usage\"\"\"\n        pass\n\n    def check_doctrinal_accuracy(self, text: str) -&gt; Dict[str, Any]:\n        \"\"\"Verify doctrinal accuracy\"\"\"\n        pass\n\n    def generate_report(self, analysis_results: Dict[str, Any]) -&gt; str:\n        \"\"\"Generate detailed analysis report\"\"\"\n        pass\n\n# Configuration and setup\n\ndef setup_logging():\n    \"\"\"Configure logging for the project\"\"\"\n    pass\n\ndef load_config(config_path: Path) -&gt; Dict[str, Any]:\n    \"\"\"Load configuration from YAML file\"\"\"\n    pass\n\ndef initialize_project(config: Dict[str, Any]):\n    \"\"\"Initialize project with configuration\"\"\"\n    pass\n</code></pre>"},{"location":"development/fine-tuning-strategy/#training-configuration","title":"Training Configuration","text":"<pre><code># Model Configuration\nmodel:\n  name: \"gpt-4\"\n  version: \"latest\"\n  max_sequence_length: 2048\n  batch_size: 16\n  learning_rate: 2e-5\n  warmup_steps: 1000\n  max_steps: 50000\n  save_steps: 1000\n  eval_steps: 500\n\n# Data Configuration\ndata:\n  languages:\n    - \"vi\"\n    - \"en\"\n    - \"fr\"\n  text_units:\n    - \"sentence\"\n    - \"paragraph\"\n  alignment:\n    method: \"hybrid\"  # sentence + semantic\n    min_score: 0.8\n  preprocessing:\n    lowercase: false\n    normalize_unicode: true\n    special_tokens:\n      - \"&lt;&lt;dharma&gt;&gt;\"\n      - \"&lt;&lt;sangha&gt;&gt;\"\n      - \"&lt;&lt;buddha&gt;&gt;\"\n\n# Training Configuration\ntraining:\n  objectives:\n    - name: \"translation\"\n      weight: 1.0\n    - name: \"understanding\"\n      weight: 0.5\n  optimization:\n    optimizer: \"adamw\"\n    weight_decay: 0.01\n    gradient_clip: 1.0\n  scheduling:\n    type: \"linear\"\n    num_warmup_steps: 1000\n  validation:\n    frequency: 1000\n    metrics:\n      - \"bleu\"\n      - \"ter\"\n      - \"comet\"\n      - \"dharma_accuracy\"\n\n# Evaluation Configuration\nevaluation:\n  metrics:\n    - name: \"bleu\"\n      weight: 0.3\n    - name: \"ter\"\n      weight: 0.2\n    - name: \"comet\"\n      weight: 0.3\n    - name: \"dharma_accuracy\"\n      weight: 0.2\n  human_evaluation:\n    sangha_review: true\n    expert_review: true\n    community_feedback: true\n  terminology:\n    check_consistency: true\n    verify_usage: true\n    maintain_register: true\n\n# Infrastructure Configuration\ninfrastructure:\n  compute:\n    device: \"cuda\"\n    precision: \"mixed\"\n    distributed: true\n  logging:\n    level: \"INFO\"\n    save_path: \"logs/\"\n  monitoring:\n    tensorboard: true\n    wandb: true\n  checkpointing:\n    save_best: true\n    save_last: true\n    max_checkpoints: 5\n</code></pre>"},{"location":"development/fine-tuning-strategy/#high-level-implementation-plan-for-fine-tuning","title":"High Level Implementation Plan for Fine-Tuning","text":""},{"location":"development/fine-tuning-strategy/#1-foundation-phase-2-3-weeks","title":"1. Foundation Phase (2-3 weeks)","text":"<ul> <li>Core data structures &amp; basic pipeline</li> <li>Minimal preprocessing</li> <li>Simple model integration</li> <li>Basic evaluation</li> </ul>"},{"location":"development/fine-tuning-strategy/#2-rapid-prototype-phase-4-6-weeks","title":"2. Rapid Prototype Phase (4-6 weeks)","text":"<ul> <li>End-to-end workflow</li> <li>Basic fine-tuning capability</li> <li>Simple translation testing</li> <li>Initial quality metrics</li> </ul>"},{"location":"development/fine-tuning-strategy/#3-enhancement-phase-ongoing","title":"3. Enhancement Phase (Ongoing)","text":"<ul> <li>Advanced features integration</li> <li>Quality improvements</li> <li>Specialized capabilities</li> <li>Production readiness</li> </ul>"},{"location":"development/fine-tuning-strategy/#intermediate-level-plan","title":"Intermediate Level Plan","text":""},{"location":"development/fine-tuning-strategy/#1-foundation-phase","title":"1. Foundation Phase","text":""},{"location":"development/fine-tuning-strategy/#a-data-pipeline-week-1","title":"a. Data Pipeline (Week 1)","text":"<ul> <li>Basic corpus loader</li> <li>Simple text alignment</li> <li>Minimal preprocessing</li> <li>Validation scaffolding</li> </ul>"},{"location":"development/fine-tuning-strategy/#b-model-integration-week-2","title":"b. Model Integration (Week 2)","text":"<ul> <li>OpenAI API integration</li> <li>Basic prompt engineering</li> <li>Simple fine-tuning setup</li> <li>Test harness creation</li> </ul>"},{"location":"development/fine-tuning-strategy/#c-basic-infrastructure-week-3","title":"c. Basic Infrastructure (Week 3)","text":"<ul> <li>Configuration management</li> <li>Logging setup</li> <li>Experiment tracking</li> <li>Initial testing framework</li> </ul>"},{"location":"development/fine-tuning-strategy/#2-rapid-prototype-phase","title":"2. Rapid Prototype Phase","text":""},{"location":"development/fine-tuning-strategy/#a-core-workflow-weeks-4-5","title":"a. Core Workflow (Weeks 4-5)","text":"<ul> <li>Data loading pipeline</li> <li>Training loop implementation</li> <li>Basic evaluation metrics</li> <li>Result logging</li> </ul>"},{"location":"development/fine-tuning-strategy/#b-initial-testing-weeks-6-7","title":"b. Initial Testing (Weeks 6-7)","text":"<ul> <li>Small-scale fine-tuning</li> <li>Translation testing</li> <li>Quality assessment</li> <li>Performance analysis</li> </ul>"},{"location":"development/fine-tuning-strategy/#c-pipeline-integration-weeks-8-9","title":"c. Pipeline Integration (Weeks 8-9)","text":"<ul> <li>Workflow automation</li> <li>Error handling</li> <li>Basic monitoring</li> <li>Documentation</li> </ul>"},{"location":"development/fine-tuning-strategy/#3-enhancement-phase","title":"3. Enhancement Phase","text":""},{"location":"development/fine-tuning-strategy/#a-quality-improvements","title":"a. Quality Improvements","text":"<ul> <li>Advanced preprocessing</li> <li>Better alignment methods</li> <li>Enhanced validation</li> <li>Extended metrics</li> </ul>"},{"location":"development/fine-tuning-strategy/#b-feature-integration","title":"b. Feature Integration","text":"<ul> <li>Terminology management</li> <li>Review system</li> <li>Quality analysis</li> <li>Advanced monitoring</li> </ul>"},{"location":"development/fine-tuning-strategy/#detailed-implementation-plan","title":"Detailed Implementation Plan","text":""},{"location":"development/fine-tuning-strategy/#1-foundation-phase_1","title":"1. Foundation Phase","text":""},{"location":"development/fine-tuning-strategy/#week-1-data-pipeline","title":"Week 1: Data Pipeline","text":"<pre><code>Day 1-2: Corpus Management\n- Implement basic ParallelCorpus class\n- Create TextUnit data structure\n- Build simple file loading\n- Set up basic validation\n\nDay 3-4: Text Alignment\n- Implement sentence splitting\n- Basic alignment matching\n- Simple alignment scoring\n- Basic quality checks\n\nDay 5: Preprocessing\n- Text cleaning functions\n- Basic tokenization\n- Language detection\n- Format standardization\n</code></pre>"},{"location":"development/fine-tuning-strategy/#week-2-model-integration","title":"Week 2: Model Integration","text":"<pre><code>Day 1-2: API Setup\n- OpenAI client setup\n- Authentication handling\n- Basic API wrapper\n- Error handling\n\nDay 3-4: Fine-tuning Basics\n- Data formatting\n- Basic training loop\n- Simple inference\n- Result handling\n\nDay 5: Test Framework\n- Unit test setup\n- Integration tests\n- Performance metrics\n- Basic logging\n</code></pre>"},{"location":"development/fine-tuning-strategy/#week-3-infrastructure","title":"Week 3: Infrastructure","text":"<pre><code>Day 1-2: Configuration\n- YAML config setup\n- Environment management\n- Parameter handling\n- Version control\n\nDay 3-4: Logging &amp; Monitoring\n- Logging implementation\n- Basic metrics tracking\n- Error reporting\n- Status monitoring\n\nDay 5: Integration\n- Pipeline assembly\n- Basic workflow\n- Documentation\n- Initial testing\n</code></pre>"},{"location":"development/fine-tuning-strategy/#2-rapid-prototype-phase_1","title":"2. Rapid Prototype Phase","text":""},{"location":"development/fine-tuning-strategy/#weeks-4-5-core-workflow","title":"Weeks 4-5: Core Workflow","text":"<pre><code>Days 1-3: Pipeline Integration\n- Data loading workflow\n- Preprocessing pipeline\n- Model integration\n- Basic evaluation\n\nDays 4-7: Training Implementation\n- Fine-tuning loop\n- Batch processing\n- Checkpointing\n- Initial validation\n\nDays 8-10: Quality Control\n- Basic metrics\n- Result validation\n- Error analysis\n- Performance tracking\n</code></pre>"},{"location":"development/fine-tuning-strategy/#weeks-6-7-testing-refinement","title":"Weeks 6-7: Testing &amp; Refinement","text":"<pre><code>Days 1-5: Initial Testing\n- Small dataset testing\n- Performance evaluation\n- Error analysis\n- Pipeline validation\n\nDays 6-10: Refinement\n- Workflow optimization\n- Error handling\n- Performance tuning\n- Documentation\n</code></pre>"},{"location":"development/fine-tuning-strategy/#critical-implementation-priorities","title":"Critical Implementation Priorities","text":""},{"location":"development/fine-tuning-strategy/#1-data-management","title":"1. Data Management","text":"<pre><code># Minimal ParallelCorpus implementation\nclass ParallelCorpus:\n    def load_texts(self):\n        # Priority 1: Basic file loading\n        # Priority 2: Error handling\n        # Priority 3: Metadata handling\n        pass\n\n    def align_texts(self):\n        # Priority 1: Simple matching\n        # Priority 2: Quality scoring\n        # Priority 3: Advanced alignment\n        pass\n</code></pre>"},{"location":"development/fine-tuning-strategy/#2-model-integration","title":"2. Model Integration","text":"<pre><code># Basic model wrapper\nclass ModelWrapper:\n    def prepare_fine_tuning(self):\n        # Priority 1: Data formatting\n        # Priority 2: Parameter setup\n        # Priority 3: Advanced options\n        pass\n\n    def train(self):\n        # Priority 1: Basic training loop\n        # Priority 2: Monitoring\n        # Priority 3: Optimization\n        pass\n</code></pre>"},{"location":"development/fine-tuning-strategy/#3-evaluation-framework","title":"3. Evaluation Framework","text":"<pre><code># Core evaluation\nclass Evaluator:\n    def evaluate_translation(self):\n        # Priority 1: Basic metrics\n        # Priority 2: Quality analysis\n        # Priority 3: Advanced metrics\n        pass\n\n    def generate_report(self):\n        # Priority 1: Basic statistics\n        # Priority 2: Detailed analysis\n        # Priority 3: Advanced insights\n        pass\n</code></pre>"},{"location":"development/fine-tuning-strategy/#extension-points","title":"Extension Points","text":""},{"location":"development/fine-tuning-strategy/#1-data-pipeline","title":"1. Data Pipeline","text":"<ul> <li>Advanced text alignment</li> <li>Enhanced preprocessing</li> <li>Quality validation</li> <li>Metadata handling</li> </ul>"},{"location":"development/fine-tuning-strategy/#2-model-development","title":"2. Model Development","text":"<ul> <li>Advanced fine-tuning</li> <li>Multiple model support</li> <li>Hyperparameter optimization</li> <li>Model ensembling</li> </ul>"},{"location":"development/fine-tuning-strategy/#3-evaluation","title":"3. Evaluation","text":"<ul> <li>Custom metrics</li> <li>Automated analysis</li> <li>Review integration</li> <li>Quality assurance</li> </ul> <p>Note: This implementation plan is intended to provide a path to a working prototype while maintaining extensibility for future enhancements.</p>"},{"location":"development/git-workflow/","title":"Git Workflow &amp; Safety Guide","text":"<p>This guide establishes safe git practices to prevent accidental data loss.</p>"},{"location":"development/git-workflow/#critical-safety-rules","title":"Critical Safety Rules","text":""},{"location":"development/git-workflow/#never-execute-without-approval","title":"NEVER Execute Without Approval","text":"<p>These commands are destructive and require explicit human approval:</p> <pre><code># DANGEROUS - Never run without approval\ngit reset --hard &lt;ref&gt;\ngit push --force\ngit branch -D &lt;branch&gt;\ngit rebase -i\ngit filter-branch\n</code></pre>"},{"location":"development/git-workflow/#always-check-before-switching-branches","title":"ALWAYS Check Before Switching Branches","text":"<p>Before <code>git checkout &lt;branch&gt;</code>:</p> <pre><code># Check for unpushed commits\ngit log --branches --not --remotes --oneline\n\n# Check branch tracking status\ngit branch -vv\n\n# If unpushed commits exist, push first!\ngit push -u origin &lt;current-branch&gt;\n</code></pre>"},{"location":"development/git-workflow/#safe-workflow","title":"Safe Workflow","text":""},{"location":"development/git-workflow/#recommended-approach-simple-feature-branches","title":"Recommended Approach: Simple Feature Branches","text":"<p>Best practice: Merge feature branches directly to main, avoiding complex multi-tier merge chains.</p> <pre><code># Create feature branch from main\ngit checkout main\ngit pull origin main\ngit checkout -b feature/my-feature\n\n# Push immediately - safety first!\ngit push -u origin feature/my-feature\n\n# Work and commit\ngit add .\ngit commit -m \"feat: implement feature\"\n\n# Push frequently\ngit push\n\n# When ready: Create PR to main\ngh pr create --base main --head feature/my-feature\n\n# After PR merges: Delete branch\ngit checkout main\ngit pull\ngit branch -d feature/my-feature\ngit push origin --delete feature/my-feature\n</code></pre> <p>Why this is safer:</p> <ul> <li>Simpler workflow = fewer opportunities for staleness errors</li> <li>No intermediate version branches to keep in sync</li> <li>Clear linear path: feature \u2192 main</li> <li>Tags (not branches) preserve release points</li> </ul>"},{"location":"development/git-workflow/#alternative-multi-tier-merges-more-complex","title":"Alternative: Multi-Tier Merges (More Complex)","text":"<p>If you must use version branches (e.g., <code>version-0.2.0</code>), follow these critical steps:</p> <pre><code># 1. Create feature branch\ngit checkout -b feature/my-feature\ngit push -u origin feature/my-feature\n\n# 2. Create PR #1: feature \u2192 version-branch\ngh pr create --base version-0.2.0 --head feature/my-feature\n\n# 3. CRITICAL: After PR merges, UPDATE LOCAL REFERENCE\ngit fetch origin version-0.2.0\ngit checkout version-0.2.0\ngit pull origin version-0.2.0\n\n# 4. VERIFY work is present\ngit log --oneline -10\nls -la path/to/expected/new/files/\n\n# 5. Check staleness before any destructive operations\n./scripts/git-check-staleness.sh version-0.2.0\n\n# 6. Now safe to create next PR\ngh pr create --base main --head version-0.2.0\n\n# 7. After final merge to main, update main\ngit fetch origin main\ngit checkout main\ngit pull origin main\n\n# 8. Verify and delete temporary branches\ngit branch -d version-0.2.0 feature/my-feature\n</code></pre> <p>Warning: This workflow is more error-prone. The December 7 incident occurred because step #3 (fetch after PR merge) was skipped.</p>"},{"location":"development/git-workflow/#switching-branches","title":"Switching Branches","text":"<pre><code># ALWAYS check unpushed work first\ngit check-unpushed\n\n# If unpushed commits exist:\ngit push\n\n# Then switch\ngit checkout main\n</code></pre>"},{"location":"development/git-workflow/#recovering-from-mistakes","title":"Recovering from Mistakes","text":"<p>If you accidentally lose commits:</p> <pre><code># Check reflog for lost commits\ngit reflog\n\n# Find the commit SHA (e.g., c6532f5)\ngit log &lt;sha&gt; --stat\n\n# Recover the commit\ngit checkout -b recovery-branch &lt;sha&gt;\n\n# Or cherry-pick it\ngit cherry-pick &lt;sha&gt;\n</code></pre>"},{"location":"development/git-workflow/#recovery-examples","title":"Recovery Examples","text":""},{"location":"development/git-workflow/#lost-branch-after-reset","title":"Lost Branch After Reset","text":"<p>Scenario: Ran <code>git reset --hard</code> and lost work</p> <pre><code># Find lost commit in reflog\ngit reflog | grep \"commit:\"\n\n# Recreate branch at lost commit\ngit checkout -b recovered-work &lt;lost-commit-sha&gt;\n\n# Push immediately!\ngit push -u origin recovered-work\n</code></pre>"},{"location":"development/git-workflow/#unpushed-branch-switched-away","title":"Unpushed Branch Switched Away","text":"<p>Scenario: Switched away from branch without pushing</p> <pre><code># Find the branch in reflog\ngit reflog show &lt;old-branch-name&gt;\n\n# Checkout the commit\ngit checkout &lt;commit-sha&gt;\n\n# Recreate branch\ngit checkout -b &lt;old-branch-name&gt;\n\n# Push it!\ngit push -u origin &lt;old-branch-name&gt;\n</code></pre>"},{"location":"development/git-workflow/#branch-staleness-detection","title":"Branch Staleness Detection","text":""},{"location":"development/git-workflow/#using-git-check-stalenesssh","title":"Using git-check-staleness.sh","text":"<p>Purpose: Detects if a local branch reference is stale (out of sync) compared to its remote tracking branch.</p> <p>Location: <code>scripts/git-check-staleness.sh</code></p> <p>When to use:</p> <ul> <li>Before any <code>git reset --hard &lt;branch&gt;</code> operation</li> <li>After a PR merges on GitHub (before using that branch locally)</li> <li>Before complex git operations involving branch references</li> <li>When you're unsure if local branch matches remote</li> </ul> <p>Usage:</p> <pre><code># Check specific branch\n./scripts/git-check-staleness.sh version-0.2.0\n\n# Check current branch\n./scripts/git-check-staleness.sh\n\n# Check a branch against a specific remote/branch (non-origin setups)\n./scripts/git-check-staleness.sh --remote upstream --branch main feature/123-new-flow\n</code></pre> <p>The script resolves the branch's configured upstream tracking ref by default (e.g., <code>feature/foo@{u}</code>) so it works with custom tracking branches. Use <code>--remote</code>/<code>--branch</code> only when you need to override that default.</p> <p>Example output (up-to-date):</p> <pre><code>Fetching remote state...\n\u2713 Branch 'main' is up-to-date\n  SHA: a6523a9...\n</code></pre> <p>Example output (stale):</p> <pre><code>Fetching remote state...\n\u2717 Branch 'version-0.2.0' is STALE!\n\n  Local:  d51fc87...\n  Remote: 5bf012d...\n\n\u26a0  Local is 5 commit(s) BEHIND remote\n\nTo update local branch:\n  git checkout version-0.2.0 &amp;&amp; git pull\n</code></pre> <p>Exit codes:</p> <ul> <li><code>0</code> - Branch is up-to-date or has no remote tracking branch</li> <li><code>1</code> - Branch is stale (local differs from remote)</li> <li><code>2</code> - Invalid usage or branch doesn't exist</li> </ul> <p>Critical use case: The December 7 incident would have been prevented if this script was run before <code>git reset --hard version-0.2.0</code>.</p>"},{"location":"development/git-workflow/#post-remote-merge-protocol","title":"Post-Remote-Merge Protocol","text":"<p>ALWAYS after a GitHub PR merge:</p> <pre><code># 1. Fetch the merged branch\ngit fetch origin &lt;branch-name&gt;\n\n# 2. Check staleness\n./scripts/git-check-staleness.sh &lt;branch-name&gt;\n\n# 3. Update local if stale\ngit checkout &lt;branch-name&gt;\ngit pull\n\n# 4. Verify expected content\nls -la path/to/new/files/\ngit log --oneline -5\n\n# NOW safe to use branch in local operations\n</code></pre> <p>Why this matters: Branch names in git commands refer to LOCAL references, not remote state. After a PR merge on GitHub, the remote branch updates but your local reference stays stale until explicitly fetched and updated.</p>"},{"location":"development/git-workflow/#git-aliases-already-configured","title":"Git Aliases (Already Configured)","text":"<p>These safe aliases are configured globally:</p> <pre><code># Check unpushed commits\ngit check-unpushed\n\n# Reset with confirmation prompt\ngit safe-reset &lt;ref&gt;\n\n# Push with unpushed summary\ngit safe-push\n</code></pre>"},{"location":"development/git-workflow/#branch-protection-rules","title":"Branch Protection Rules","text":""},{"location":"development/git-workflow/#main-branch","title":"Main Branch","text":"<ul> <li>Never <code>git reset</code> on main without approval</li> <li>Never force push to main</li> <li>Always create feature branches for new work</li> </ul>"},{"location":"development/git-workflow/#feature-branches","title":"Feature Branches","text":"<ul> <li>Push to origin immediately after creation</li> <li>Push after every significant commit</li> <li>Never delete until merged and pushed</li> </ul>"},{"location":"development/git-workflow/#automation-hooks","title":"Automation &amp; Hooks","text":""},{"location":"development/git-workflow/#pre-checkout-hook","title":"Pre-checkout Hook","text":"<p>Warns when switching branches with unpushed commits. Located at:</p> <ul> <li><code>.git/hooks/pre-checkout</code></li> </ul> <p>To bypass (not recommended):</p> <pre><code>git checkout --no-verify &lt;branch&gt;\n</code></pre>"},{"location":"development/git-workflow/#emergency-steps","title":"Emergency Steps","text":"<p>If you lose work and can't recover:</p> <ol> <li>DO NOT RUN ANY MORE GIT COMMANDS (you might overwrite reflog)</li> <li>Check <code>.git/logs/</code> manually for historical refs</li> <li>Run <code>git fsck --lost-found</code> to find dangling commits</li> <li>Contact repository maintainer with reflog output</li> </ol>"},{"location":"development/git-workflow/#incident-december-7-2025","title":"Incident: December 7, 2025","text":"<p>What Happened: <code>git reset --hard version-0.2.0</code> on main orphaned 39 files (4,508 lines) of prompt system work.</p> <p>Root Cause: Reset used stale local branch reference. Local <code>version-0.2.0</code> was not fetched after remote PR #12 merge on GitHub. Local branch pointed to <code>d51fc87</code> (version bump), while remote had merged work at <code>5bf012d</code>.</p> <p>Key Learning: Branch names in git commands refer to LOCAL references, not remote state. After a GitHub PR merge, the remote branch updates but the local reference stays stale until explicitly fetched.</p> <p>Recovery: All work recovered from git reflog (commit <code>c6532f5</code>).</p> <p>Prevention Measures Implemented:</p> <ul> <li>Added post-remote-merge fetch protocol to <code>.claude/CLAUDE.md</code></li> <li>Added pre-reset content verification protocol</li> <li>Created <code>scripts/git-check-staleness.sh</code> to detect stale branches</li> <li>Enhanced this workflow guide with simplified workflow recommendations</li> <li>Installed pre-checkout hook</li> <li>Configured git safety aliases</li> </ul> <p>Full Details: See Incident Report: Git Recovery 2025-12-07</p>"},{"location":"development/git-workflow/#references","title":"References","text":"<ul> <li>Git Best Practices</li> <li>Git Reflog Documentation</li> <li>Contributing Guide</li> </ul>"},{"location":"development/human-ai-software-engineering-principles/","title":"Human-AI Software Engineering Principles","text":"<p>This document presents the Human-AI Software Engineering Principles, a comprehensive framework that builds upon established software engineering, architecture, and design principles from human-only teams and extends them to optimize collaboration between humans and AI agents. Central to this framework is the clear distinction between the Design Phase and the Coding Phase, each with distinct goals, modes, and workflows. It also addresses challenges such as context window limitations and maintaining alignment despite session resets. In addition to general principles, this framework incorporates concrete documentation and planning strategies designed to support long-term, sustainable human-AI collaboration.</p>"},{"location":"development/human-ai-software-engineering-principles/#1-core-philosophy-dialogical-alignment","title":"1. Core Philosophy: Dialogical Alignment","text":"<p>At the heart of this framework lies the principle of dialogical alignment\u2014an ongoing, interactive process of maintaining shared understanding and coordinated goals between human and AI collaborators. This alignment is one layer within a broader system of practices designed to optimize collaboration efficiency, reliability, and adaptability across both design and coding phases.</p>"},{"location":"development/human-ai-software-engineering-principles/#2-phases-of-engineering-design-vs-coding","title":"2. Phases of Engineering: Design vs Coding","text":"<p>Engineering work is divided into two primary phases, each with distinct objectives and collaboration modes:</p> <ul> <li> <p>Design Phase (Exploratory Mode): This phase emphasizes creativity, curiosity, and broad ideation. It involves open-ended discussion, exploration of solution spaces, and conceptual modeling. The goal is to generate ideas, define architecture, and plan approaches without premature convergence or repetitive loops.</p> </li> <li> <p>Coding Phase (Convergent Mode): This phase focuses on decision-making, refinement, and alignment. It involves translating designs into working code, iterative improvement, validation, and ensuring correctness. The goal is clarity, consensus, and delivering actionable outcomes.</p> </li> </ul> <p>Both phases integrate essential engineering activities such as testing, debugging, refinement, and iteration, which are treated as integral loops within the workflow rather than isolated steps.</p>"},{"location":"development/human-ai-software-engineering-principles/#3-roles-and-responsibilities","title":"3. Roles and Responsibilities","text":"<ul> <li>Human Agent(s): Provide domain expertise, set requirements, validate outputs, and guide the AI through iterative feedback across both design and coding phases.</li> <li>AI Agent(s): Generate, transform, and organize code and documentation artifacts based on human input and learned patterns.</li> <li>Shared Responsibility: Both agents engage in continuous communication to ensure mutual understanding and alignment throughout the engineering lifecycle.</li> </ul>"},{"location":"development/human-ai-software-engineering-principles/#4-artifacts","title":"4. Artifacts","text":"<ul> <li>Codebase: The evolving source code, organized into modules and components.</li> <li>Documentation: Includes design documents, ADRs (Architecture Decision Records), and inline comments.</li> <li>ADRs and Snapshots: Captured decisions and system states that serve as reference points for future sessions.</li> <li>Task Lists and Backlogs: Structured to track progress and plan next steps.</li> <li>Capsules: Bundled collections of relevant artifacts and context representing a session\u2019s core content.</li> <li>Deltas: Incremental changes or updates made during or between sessions.</li> <li>Session Summaries: High-level overviews capturing session intent, key decisions, and outcomes.</li> <li>Conceptual Map / Index of Abstracts: A navigational tool organizing abstracts and top-of-document summaries to provide a structured overview of the knowledge base.</li> </ul>"},{"location":"development/human-ai-software-engineering-principles/#5-loops-and-interactions-mapping-to-phases","title":"5. Loops and Interactions: Mapping to Phases","text":"<p>The framework organizes iterative workflows into loops explicitly mapped to engineering phases:</p> <ul> <li>Exploration Loop (Design Phase): Early-stage ideation and prototyping to explore solution spaces and generate design concepts.</li> <li>Refinement Loop (Coding Phase): Iterative improvement cycles incorporating feedback and testing to enhance code quality and alignment.</li> <li>Alignment Loop (Coding Phase): Continuous dialogical checks to confirm shared understanding and consensus during implementation.</li> <li>Error Recovery Loop (Testing/Debugging/Correction): Mechanisms to detect, communicate, and resolve misunderstandings, bugs, or mistakes throughout both phases.</li> </ul>"},{"location":"development/human-ai-software-engineering-principles/#6-macros-and-pattern-usage","title":"6. Macros and Pattern Usage","text":"<ul> <li>Reusable Macros: Predefined prompts, templates, and code snippets to streamline common tasks.</li> <li>Best Practice Patterns: Established coding and design patterns adapted for human-AI workflows.</li> <li>Automation Scripts: Tools to automate repetitive or error-prone steps.</li> </ul>"},{"location":"development/human-ai-software-engineering-principles/#7-context-memory-strategy","title":"7. Context &amp; Memory Strategy","text":"<p>Effective collaboration requires managing context and memory across sessions.</p>"},{"location":"development/human-ai-software-engineering-principles/#ai-specific-considerations","title":"AI-specific considerations","text":"<ul> <li>Context Window Limitations: AI models have finite input sizes; prioritize and compress information to fit within these constraints.</li> <li>Session Continuity: Use snapshots, ADRs, structured artifacts, capsules, and deltas to maintain continuity across resets or new sessions.</li> <li>Persistence Strategy: Capture critical decisions and system states regularly to minimize loss of context and facilitate resumption.</li> <li>Capsule vs Library Layers: Capsules contain session-specific context and deltas, while the Library layer holds stable, abstracted knowledge such as abstracts and top-of-document summaries.</li> <li>Session Boot Sequence: A structured process to initialize sessions by loading relevant capsules, summaries, and context from the library to establish shared understanding before work begins.</li> </ul>"},{"location":"development/human-ai-software-engineering-principles/#8-communication-and-feedback-phase-specific-approaches","title":"8. Communication and Feedback: Phase-Specific Approaches","text":"<p>Communication strategies differ between phases to support their unique goals:</p> <ul> <li>Design Phase (Exploratory Mode):</li> <li>Emphasizes open-ended, creative dialogue.</li> <li>Encourages questions, brainstorming, and conceptual clarifications.</li> <li> <p>Uses flexible, unstructured formats to foster ideation and avoid premature convergence.</p> </li> <li> <p>Coding Phase (Convergent Mode):</p> </li> <li>Prioritizes explicit, clear, and unambiguous communication.</li> <li>Employs structured formats, checkpoints, and validation steps to ensure correctness.</li> <li>Includes clarification protocols to prevent drift and maintain alignment.</li> </ul> <p>Across both phases:</p> <ul> <li>Mode Declaration: Explicitly state the current mode of collaboration\u2014Exploratory (design) or Convergent (coding)\u2014to align communication and expectations.</li> </ul>"},{"location":"development/human-ai-software-engineering-principles/#9-quality-assurance","title":"9. Quality Assurance","text":"<ul> <li>Automated Testing: Integrate tests early and often during coding and refinement.</li> <li>Code Reviews: Human oversight to catch subtle issues.</li> <li>Continuous Integration: Regular builds and deployments to detect integration problems promptly.</li> </ul>"},{"location":"development/human-ai-software-engineering-principles/#10-adaptability-and-learning","title":"10. Adaptability and Learning","text":"<ul> <li>Continuous Improvement: Incorporate lessons learned into macros, prompts, and workflows.</li> <li>Knowledge Sharing: Maintain shared repositories of best practices and artifacts.</li> <li>Scalability: Design processes to accommodate growing complexity and team size.</li> </ul>"},{"location":"development/human-ai-software-engineering-principles/#11-meta-level-design-documentation-framework","title":"11. Meta-Level Design &amp; Documentation Framework","text":"<p>This framework introduces a meta-level strategy to organize long-term human-AI collaboration around a Design-OS metaphor, structured documentation, and flexible collaboration modes explicitly connected to the phases of engineering:</p> <ul> <li>Design-OS Metaphor: The collaboration environment is conceptualized as an operating system with a kernel (core shared knowledge and processes), a periphery (dynamic, session-specific artifacts), and rituals (established workflows and communication protocols).</li> <li>Capsule + Deltas + Session Intent: Each session is encapsulated in a Capsule containing relevant artifacts and context, with Deltas capturing incremental changes. The Session Intent frames the goals and focus for the session, guiding interactions.</li> <li>Outcomes as Historical Design Memory: Sessions produce structured outcomes including Decisions, Open Questions, and Findings &amp; Ideas that build a living historical record of the design evolution.</li> <li>Library Layer with Abstracts: Instead of relying on YAML or other rigid formats, the Library layer organizes knowledge through Abstracts and Top-of-Document Summaries, providing concise, human- and AI-readable overviews.</li> <li>Conceptual Map / Index of Abstracts: A navigational tool that maps relationships between abstracts, enabling efficient exploration and retrieval of knowledge.</li> <li>Two Modes of Collaboration Linked to Phases:</li> <li>Exploratory Mode = Design Phase: Prioritizes creativity, curiosity, and broad ideation; encourages open-ended discussion and avoids premature convergence or repetitive loops.</li> <li>Convergent Mode = Coding Phase: Focuses on decision-making, refinement, and alignment; emphasizes clarity, consensus, and actionable outcomes.</li> <li>Emphasis on Creativity and Curiosity: The framework encourages maintaining a balance between exploration and convergence, avoiding unproductive cycles by clearly signaling mode shifts and leveraging structured artifacts.</li> </ul> <p>By integrating these meta-level strategies with the core principles, this framework supports sustainable, scalable, and effective human-AI software engineering collaboration that adapts to evolving project needs and complexity.</p> <p>By combining these principles, the Human-AI Software Engineering Principles framework aims to foster robust, efficient, and aligned collaboration between human developers and AI agents, leveraging the strengths of both to create high-quality software.</p>"},{"location":"development/improvements-initial-structure/","title":"Improvements / Initial structure","text":"<p>Initial high-level view of the TNH Scholar ecosystem.</p> <p>Core Processing Pipelines:</p> <ol> <li>Media Acquisition &amp; Transformation</li> </ol> <pre><code>Sources -&gt; Raw Content -&gt; Processed Content -&gt; Formatted Output\n\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500     \u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\n     \u2502              \u2502             \u2502              \u2502\n   Video          Audio        Sections       XML/Web\n   Audio          Text         Translation    Publication\n   PDFs           Transcript   Formatting     Training Data\n   Journals       OCR         \n   Books\n</code></pre> <ol> <li>AI Processing Lifecycle</li> </ol> <pre><code>Source Content -&gt; Training Data -&gt; Model Training -&gt; Enhanced Processing\n      \u2502              \u2502                \u2502                \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n                                                   \u2502   \u2502\n                                                   v   v\n                                             Improved Content\n</code></pre> <ol> <li>Tool Categories:</li> </ol> <pre><code>Acquisition Tools     Processing Tools    AI Integration       Publication Tools\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500     \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500       \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nytt-fetch            tnh-fab             OpenAI Interface    XML Formatting\naudio-transcribe     OCR Processing      Pattern System      Web Publishing\nPDF Processing       Text Processing     Model Training      Search Indexing\n</code></pre> <p>This high-level view suggests some key improvements needed:</p> <ol> <li> <p>Standard Interfaces</p> </li> <li> <p>Common base classes for content types</p> </li> <li>Shared metadata structures</li> <li> <p>Consistent processing patterns</p> </li> <li> <p>Pipeline Management</p> </li> <li> <p>Better workflow definition</p> </li> <li>Progress tracking</li> <li> <p>Error recovery</p> </li> <li> <p>Tool Integration</p> </li> <li> <p>Clearer boundaries between tools</p> </li> <li>Standard communication formats</li> <li>Simplified composition</li> </ol>"},{"location":"development/overview/","title":"Development Documentation","text":"<p>This directory contains documentation for developers contributing to TNH Scholar.</p>"},{"location":"development/overview/#contents","title":"Contents","text":"<ul> <li>contributing-prototype-phase.md - Contributing guidelines for the prototype phase</li> <li>design-principles.md - Core design principles and patterns</li> <li>fine-tuning-strategy.md - Strategy for fine-tuning AI models</li> <li>human-ai-software-engineering-principles.md - Principles for human-AI collaborative development</li> <li>improvements-initial-structure.md - Initial structural improvements and refactoring notes</li> <li>style-guide.md - Code style guide and conventions</li> <li>system-design.md - Overall system design and architecture</li> </ul>"},{"location":"development/overview/#getting-started-as-a-contributor","title":"Getting Started as a Contributor","text":"<ol> <li>Read contributing-prototype-phase.md for contribution guidelines</li> <li>Review design-principles.md to understand our approach</li> <li>Follow the style-guide.md for code consistency</li> </ol>"},{"location":"development/overview/#development-philosophy","title":"Development Philosophy","text":"<p>TNH Scholar embraces human-AI collaborative development. See human-ai-software-engineering-principles.md for our approach to working with AI assistants.</p>"},{"location":"development/overview/#related-documentation","title":"Related Documentation","text":"<ul> <li>Project Vision &amp; Philosophy - Project goals and philosophy</li> <li>Architecture - Technical architecture documentation</li> <li>Docs Operations - Documentation standards and templates</li> </ul>"},{"location":"development/release-workflow/","title":"Release Workflow","text":"<p>This document describes the automated release process for TNH Scholar, designed to support biweekly (or faster) releases during rapid prototyping with minimal manual effort.</p>"},{"location":"development/release-workflow/#quick-reference","title":"Quick Reference","text":"<p>Standard patch release workflow (e.g., x.x.y \u2192 0.x.y+1):</p> <pre><code>make release-patch       # Bump version + update TODO.md\nmake changelog-draft     # Generate CHANGELOG entry\n# Edit CHANGELOG.md with generated content\nmake release-commit      # Commit version changes\nmake release-tag         # Tag and push to remote\nmake release-publish     # Strip frontmatter, build, publish to PyPI, restore README (automated)\n</code></pre> <p>Dry-run mode: Add <code>DRY_RUN=1</code> to any command to preview without making changes:</p> <pre><code>make release-patch DRY_RUN=1    # Preview version bump\nmake release-commit DRY_RUN=1   # Preview commit\nmake release-tag DRY_RUN=1      # Preview tag and push\nmake release-publish DRY_RUN=1  # Preview PyPI publish\n</code></pre>"},{"location":"development/release-workflow/#prerequisites","title":"Prerequisites","text":""},{"location":"development/release-workflow/#one-time-setup","title":"One-Time Setup","text":"<p>Before first release, complete these configuration steps:</p> <ol> <li>PyPI API Token: Configure Poetry with your PyPI credentials</li> </ol> <pre><code>poetry config pypi-token.pypi &lt;your-api-token&gt;\n</code></pre> <p>Get a token from: https://pypi.org/manage/account/token/</p> <ol> <li>Git Configuration: Ensure git is configured with your name and email</li> </ol> <pre><code>git config user.name \"Your Name\"\ngit config user.email \"your.email@example.com\"\n</code></pre> <ol> <li>Quality Checks Pass: Verify the project builds cleanly</li> </ol> <pre><code>make release-check  # Runs: test, type-check, lint, docs-verify\n</code></pre>"},{"location":"development/release-workflow/#pre-release-verification","title":"Pre-Release Verification","text":"<p>Before starting any release, ensure all quality checks pass:</p> <pre><code>make release-check\n</code></pre> <p>This runs:</p> <ul> <li><code>make test</code> - Full test suite with pytest</li> <li><code>make type-check</code> - MyPy type checking</li> <li><code>make lint</code> - Ruff linting</li> <li><code>make docs-verify</code> - Documentation build, link check, and spell check</li> </ul> <p>Expected output: <code>\u2705 All quality checks passed - ready to release</code></p> <p>If checks fail, fix the issues before proceeding with the release.</p>"},{"location":"development/release-workflow/#release-types","title":"Release Types","text":"<p>Choose the appropriate version bump based on the changes:</p>"},{"location":"development/release-workflow/#patch-release-0xy-0xy1","title":"Patch Release (0.x.Y \u2192 0.x.Y+1)","text":"<pre><code>make release-patch\n</code></pre> <p>Use for:</p> <ul> <li>Bug fixes</li> <li>Documentation improvements</li> <li>Minor refactoring without API changes</li> <li>Dependency updates</li> <li>Code cleanup and formatting</li> </ul>"},{"location":"development/release-workflow/#minor-release-0xy-0x10","title":"Minor Release (0.X.y \u2192 0.X+1.0)","text":"<pre><code>make release-minor\n</code></pre> <p>Use for:</p> <ul> <li>New features that maintain backward compatibility</li> <li>Significant documentation reorganization</li> <li>New CLI commands or subcommands</li> <li>Major improvements to existing features</li> </ul>"},{"location":"development/release-workflow/#major-release-xyz-x100","title":"Major Release (X.y.z \u2192 X+1.0.0)","text":"<pre><code>make release-major\n</code></pre> <p>Use for:</p> <ul> <li>Breaking API changes</li> <li>Incompatible CLI changes</li> <li>Major architectural shifts</li> </ul> <p>Note: Major releases are rare during alpha/beta phases.</p>"},{"location":"development/release-workflow/#step-by-step-workflow","title":"Step-by-Step Workflow","text":""},{"location":"development/release-workflow/#step-1-bump-version","title":"Step 1: Bump Version","text":"<p>Choose the appropriate version bump:</p> <pre><code>make release-patch   # For 0.1.4 \u2192 0.1.5\n# OR\nmake release-minor   # For 0.1.4 \u2192 0.2.0\n# OR\nmake release-major   # For 0.1.4 \u2192 1.0.0\n</code></pre> <p>What happens:</p> <ul> <li>Updates <code>pyproject.toml</code> version field</li> <li>Updates <code>TODO.md</code> version header to match</li> <li>Displays next steps</li> </ul> <p>Example output:</p> <pre><code>\ud83d\ude80 Bumping patch version (0.x.Y -&gt; 0.x.Y+1)...\nBumping version from 0.1.4 to 0.1.5\n\ud83d\udcdd Updating version to 0.1.5 in TODO.md...\n\u2705 Version updated to 0.1.5\n\nNext steps:\n  1. Run 'make changelog-draft' to generate CHANGELOG entry\n  2. Edit CHANGELOG.md with the generated content\n  3. Run 'make release-commit' to commit changes\n  4. Run 'make release-tag' to tag and push\n  5. Run 'make release-publish' to publish to PyPI\n</code></pre>"},{"location":"development/release-workflow/#step-2-generate-changelog-entry","title":"Step 2: Generate CHANGELOG Entry","text":"<pre><code>make changelog-draft\n</code></pre> <p>What happens:</p> <ul> <li>Analyzes git commits since the last tag</li> <li>Categorizes commits by type (Added, Changed, Fixed, Documentation, etc.)</li> <li>Generates a formatted CHANGELOG entry</li> </ul> <p>Example output:</p> <pre><code>\ud83d\udcdd Generating CHANGELOG entry from git history...\n\n## [0.1.5] - 2025-12-06\n\n### Added\n\n- Version sync pre-commit hook to prevent version mismatches\n- Python-based link checker (md-dead-link-check) for documentation\n\n### Changed\n\n- Replaced lychee with Python-native link checking tool\n- Improved developer onboarding with fewer external dependencies\n\n### Documentation\n\n- Created comprehensive release workflow documentation\n\n============================================================\n\ud83d\udcdd Draft CHANGELOG entry for v0.1.5\nBased on 8 commits since v0.1.4\n============================================================\n\n\ud83d\udc46 Copy this to CHANGELOG.md and edit as needed\n</code></pre>"},{"location":"development/release-workflow/#step-3-edit-changelogmd","title":"Step 3: Edit CHANGELOG.md","text":"<ol> <li>Open <code>CHANGELOG.md</code> (in project root) in your editor</li> <li>Copy the generated entry from your terminal</li> <li>Paste at the top of the file (after the header, before previous versions)</li> <li>Edit the entry for clarity and completeness:</li> <li>Remove noise commits (merge commits, trivial fixes)</li> <li>Rewrite commit messages for user-facing clarity</li> <li>Group related changes together</li> <li>Add context or references where needed</li> <li>Add \"Notes\" or \"Breaking Changes\" sections if applicable</li> <li>Save the file</li> </ol> <p>Example polished entry:</p> <pre><code># Changelog\n\nAll notable changes to TNH Scholar will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html)\n\n## [0.1.5] - 2025-12-06\n\n### Added\n\n- **Version Sync Pre-commit Hook**: Automatically validates that pyproject.toml and TODO.md versions match before allowing commits\n- **Python-based Link Checker**: Replaced lychee (Rust tool) with md-dead-link-check for pure Python toolchain\n\n### Changed\n\n- Simplified developer setup by removing non-Python tooling requirements\n- Restored full documentation verification pipeline with Python-native tools\n\n### Documentation\n\n- Created comprehensive release workflow documentation in docs/development/\n\n### Notes\n\n- This release focuses on release automation improvements (Phase 2)\n- No code functionality changes - purely tooling and documentation infrastructure\n- External link checking now uses pure Python stack for better Poetry integration\n\n## [0.1.4] - 2025-12-05\n...\n</code></pre>"},{"location":"development/release-workflow/#step-4-commit-version-changes","title":"Step 4: Commit Version Changes","text":"<pre><code>make release-commit\n</code></pre> <p>What happens:</p> <ul> <li>Stages <code>pyproject.toml</code>, <code>TODO.md</code>, <code>CHANGELOG.md</code>, and <code>poetry.lock</code></li> <li>Creates a formatted commit with a standard message</li> <li>Shows next steps</li> </ul> <p>Commit message format:</p> <pre><code>chore: Bump version to 0.1.5\n\n- Update version in pyproject.toml\n- Update TODO.md version header\n- Add 0.1.5 release notes to CHANGELOG.md\n\n\ud83e\udd16 Generated with Claude Code\n\nCo-Authored-By: Claude &lt;noreply@anthropic.com&gt;\n</code></pre>"},{"location":"development/release-workflow/#step-5-tag-and-push","title":"Step 5: Tag and Push","text":"<pre><code>make release-tag\n</code></pre> <p>What happens:</p> <ul> <li>Creates an annotated git tag (e.g., <code>v0.1.5</code>)</li> <li>Pushes the current branch to the remote</li> <li>Pushes the tag to the remote</li> </ul> <p>Tag message format:</p> <pre><code>Release v0.1.5\n\nSee CHANGELOG.md for full details.\n\n\ud83e\udd16 Generated with Claude Code\n\nCo-Authored-By: Claude &lt;noreply@anthropic.com&gt;\n</code></pre> <p>Example output:</p> <pre><code>\ud83c\udff7\ufe0f  Tagging version v0.1.5...\n\ud83d\udce4 Pushing branch and tag...\n\u2705 Tagged and pushed v0.1.5\n\nNext: Run 'make release-publish' to publish to PyPI\n</code></pre>"},{"location":"development/release-workflow/#step-6-publish-to-pypi","title":"Step 6: Publish to PyPI","text":"<pre><code>make release-publish\n</code></pre> <p>What happens (all automated):</p> <ol> <li>Prepares README for PyPI: Strips YAML frontmatter to ensure clean rendering</li> <li>Creates backup at <code>README.md.bak</code></li> <li> <p>Removes frontmatter that would display as plain text on PyPI</p> </li> <li> <p>Builds distributions: Creates wheel and source distribution using Poetry</p> </li> <li> <p>Publishes to PyPI: Uploads both distributions</p> </li> <li> <p>Restores README: Automatically restores original README with frontmatter</p> </li> </ol> <p>Why frontmatter stripping is needed: PyPI doesn't process YAML frontmatter and will display it as plain text, making your project description look unprofessional. The automation handles this for you.</p> <p>Example output:</p> <pre><code>\ud83d\udcdd Preparing README for PyPI (stripping YAML frontmatter)...\n\u2713 Backed up README.md to /path/to/README.md.bak\n\u2713 Stripped 443 bytes of frontmatter from README.md\n\n\ud83d\udce6 README.md is ready for PyPI build\n\ud83d\udca1 Run 'python scripts/prepare_pypi_readme.py --restore' to restore original\n\n\ud83d\udce6 Building package...\nBuilding tnh-scholar (0.1.5)\n  - Building sdist\n  - Built tnh_scholar-0.1.5.tar.gz\n  - Building wheel\n  - Built tnh_scholar-0.1.5-py3-none-any.whl\n\n\ud83d\udce4 Publishing to PyPI...\nPublishing tnh-scholar (0.1.5) to PyPI\n - Uploading tnh_scholar-0.1.5-py3-none-any.whl 100%\n - Uploading tnh_scholar-0.1.5.tar.gz 100%\n\n\ud83d\udcdd Restoring original README...\n\u2713 Restored README.md from backup\n\n\u2705 Published v0.1.5 to PyPI\n\n\ud83c\udf89 Release complete! Check https://pypi.org/project/tnh-scholar/\n</code></pre>"},{"location":"development/release-workflow/#step-7-verify-release","title":"Step 7: Verify Release","text":"<p>After publishing, verify the release was successful:</p> <ol> <li>Check PyPI: Visit https://pypi.org/project/tnh-scholar/</li> <li>Verify the new version is listed</li> <li>Important: Check that README renders correctly without YAML frontmatter</li> <li> <p>Confirm the project description starts with \"# TNH Scholar README\" and not with \"---\"</p> </li> <li> <p>Test Installation:</p> </li> </ol> <pre><code>pip install --upgrade tnh-scholar==0.1.5\npython -c \"import tnh_scholar; print(tnh_scholar.__version__)\"\n</code></pre> <p>Expected output: <code>0.1.5</code></p> <ol> <li>Check GitHub:</li> <li>Tag appears in releases: https://github.com/aaronksolomon/tnh-scholar/tags</li> <li>Verify the commit and tag messages are correct</li> </ol>"},{"location":"development/release-workflow/#advanced-usage","title":"Advanced Usage","text":""},{"location":"development/release-workflow/#all-in-one-release","title":"All-in-One Release","text":"<p>If you've already edited CHANGELOG.md and are confident in your changes:</p> <pre><code>make release-full\n</code></pre> <p>This runs: <code>release-commit</code> \u2192 <code>release-tag</code> \u2192 <code>release-publish</code> in sequence.</p> <p>\u26a0\ufe0f Use with caution: This skips intermediate verification steps. Only use this after you've verified CHANGELOG.md is correct.</p>"},{"location":"development/release-workflow/#dry-run-mode","title":"Dry Run Mode","text":"<p>Preview any release command without making changes by adding <code>DRY_RUN=1</code>:</p> <pre><code># Preview version bump\nmake release-patch DRY_RUN=1\n\n# Preview commit\nmake release-commit DRY_RUN=1\n\n# Preview tag and push\nmake release-tag DRY_RUN=1\n\n# Preview PyPI publish\nmake release-publish DRY_RUN=1\n</code></pre> <p>Example dry-run output for version bump:</p> <pre><code>\ud83d\udd0d DRY RUN MODE - No changes will be made\n\n\ud83d\ude80 Would bump patch version: 0.1.4 \u2192 0.1.5\n\nCommands that would run:\n  poetry version patch\n  sed -i.bak 's/&gt; \\*\\*Version\\*\\*:.*/&gt; **Version**: 0.1.5 (Alpha)/' TODO.md\n\nTo execute: make release-patch\n</code></pre> <p>Benefits:</p> <ul> <li>See exact commands that will run</li> <li>Verify commit messages and tag messages before creating them</li> <li>Catch errors early (wrong version bump, missing files, etc.)</li> <li>Safe way to learn the release workflow</li> <li>Useful for documenting the process</li> </ul> <p>When to use dry-run:</p> <ul> <li>First time using a release target</li> <li>Testing a new release workflow</li> <li>Verifying what will be committed/tagged/published</li> <li>Training new contributors</li> <li>Before major releases</li> </ul> <p>Test version bumps without dry-run:</p> <pre><code># Check current version\npoetry version -s\n\n# Test version bump calculation (doesn't modify files)\npoetry version --dry-run patch   # Shows: 0.1.4 \u2192 0.1.5\npoetry version --dry-run minor   # Shows: 0.1.4 \u2192 0.2.0\n\n# Actually bump (modifies pyproject.toml)\nmake release-patch\n\n# Undo if needed (before committing)\ngit checkout pyproject.toml TODO.md poetry.lock\n</code></pre>"},{"location":"development/release-workflow/#skip-steps","title":"Skip Steps","text":"<p>You can run individual targets if you need to customize the workflow:</p> <pre><code># Just generate changelog (no version bump)\nmake changelog-draft\n\n# Just commit (if you bumped version manually)\nmake release-commit\n\n# Just tag (if you committed manually)\nmake release-tag\n\n# Just publish (if you tagged manually)\nmake release-publish\n</code></pre>"},{"location":"development/release-workflow/#automation-features","title":"Automation Features","text":""},{"location":"development/release-workflow/#version-sync-pre-commit-hook","title":"Version Sync Pre-commit Hook","text":"<p>A pre-commit hook validates that <code>pyproject.toml</code> and <code>TODO.md</code> versions match before allowing commits. This prevents version drift bugs.</p> <p>What it checks:</p> <ul> <li>Reads version from <code>pyproject.toml</code> via <code>poetry version -s</code></li> <li>Extracts version from <code>TODO.md</code> using pattern matching</li> <li>Fails commit if versions don't match</li> </ul> <p>How to fix a mismatch:</p> <pre><code># Use one of the release targets to sync versions\nmake release-patch   # Bump patch version\nmake release-minor   # Bump minor version\nmake release-major   # Bump major version\n</code></pre> <p>The hook runs automatically on every commit. You can also run it manually:</p> <pre><code>poetry run pre-commit run version-sync --all-files\n</code></pre>"},{"location":"development/release-workflow/#documentation-verification","title":"Documentation Verification","text":"<p>The <code>make docs-verify</code> target runs a comprehensive documentation quality check:</p> <ol> <li>Drift Check: Ensures README.md and docs/index.md are in sync</li> <li>Build Check: Runs MkDocs in strict mode (fails on warnings)</li> <li>Link Check: Validates external links using md-dead-link-check (Python-native)</li> <li>Spell Check: Runs codespell on README and docs</li> </ol> <p>All documentation tools are Python-based and managed by Poetry for consistent developer experience.</p>"},{"location":"development/release-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/release-workflow/#no-previous-tag-found","title":"\"No previous tag found\"","text":"<p>Problem: First release, no git tags exist yet.</p> <p>Solution: Write the CHANGELOG entry manually, skip <code>make changelog-draft</code>.</p>"},{"location":"development/release-workflow/#version-mismatch-in-todomd","title":"\"Version mismatch in TODO.md\"","text":"<p>Problem: TODO.md version marker not found or malformed.</p> <p>Solution: Ensure TODO.md contains a line matching the pattern:</p> <pre><code>&gt; **Version**: 0.1.4 (Alpha)\n</code></pre> <p>The version sync hook validates this format.</p>"},{"location":"development/release-workflow/#pypi-authentication-failed","title":"\"PyPI authentication failed\"","text":"<p>Problem: No PyPI API token configured.</p> <p>Solution:</p> <pre><code>poetry config pypi-token.pypi &lt;your-token&gt;\n</code></pre> <p>Get a token from: https://pypi.org/manage/account/token/</p>"},{"location":"development/release-workflow/#tests-failing-in-release-check","title":"\"Tests failing in release-check\"","text":"<p>Problem: Code has failing tests, type errors, or lint violations.</p> <p>Solution: Fix issues before releasing:</p> <pre><code>make test           # Run pytest test suite\nmake type-check     # Run mypy type checks\nmake lint           # Run ruff linting\nmake docs-verify    # Build and validate docs\n</code></pre> <p>Address all failures before proceeding with the release.</p>"},{"location":"development/release-workflow/#git-push-rejected","title":"\"Git push rejected\"","text":"<p>Problem: Remote has changes you don't have locally.</p> <p>Solution:</p> <pre><code>git pull --rebase origin $(git branch --show-current)\n# Resolve conflicts if any\nmake release-tag  # Try again\n</code></pre>"},{"location":"development/release-workflow/#link-check-failing","title":"\"Link check failing\"","text":"<p>Problem: External links are broken or timing out.</p> <p>Solution:</p> <ol> <li>Review the failing links in the output</li> <li>Update or remove broken links</li> <li>For transient failures, retry after a few minutes</li> <li>For sites that block automated requests, add to the exclude list in <code>pyproject.toml</code>:</li> </ol> <pre><code>[tool.md_dead_link_check]\nexclude_links = [\n    \"https://example.com/*\",  # Add problematic domains\n]\n</code></pre>"},{"location":"development/release-workflow/#tips-for-efficient-releases","title":"Tips for Efficient Releases","text":""},{"location":"development/release-workflow/#use-conventional-commits","title":"Use Conventional Commits","text":"<p>Format commits to make CHANGELOG generation more accurate:</p> <pre><code>git commit -m \"feat: add new feature X\"\ngit commit -m \"fix: resolve bug in Y\"\ngit commit -m \"docs: update getting started guide\"\ngit commit -m \"chore: bump dependencies\"\n</code></pre> <p>Commit prefixes:</p> <ul> <li><code>feat:</code> \u2192 Added section</li> <li><code>fix:</code> \u2192 Fixed section</li> <li><code>docs:</code> \u2192 Documentation section</li> <li><code>chore:</code>, <code>build:</code>, <code>ci:</code> \u2192 Infrastructure section</li> <li><code>refactor:</code>, <code>perf:</code> \u2192 Changed section</li> <li><code>test:</code> \u2192 Testing section</li> </ul>"},{"location":"development/release-workflow/#batch-related-changes","title":"Batch Related Changes","text":"<p>For rapid iteration, you can release after any logical batch of commits:</p> <pre><code># Week 1: Development\ngit commit -m \"feat: implement feature A\"\ngit commit -m \"feat: implement feature B\"\ngit commit -m \"fix: resolve edge case in feature A\"\ngit commit -m \"docs: document features A and B\"\n\n# Week 2: Release day\nmake release-patch\nmake changelog-draft\n# Edit CHANGELOG.md\nmake release-commit\nmake release-tag\nmake release-publish\n\n# Total time: 10-15 minutes\n</code></pre>"},{"location":"development/release-workflow/#schedule-regular-releases","title":"Schedule Regular Releases","text":"<p>Consider a biweekly release cadence during active development:</p> <ul> <li>Every other Friday: Release day</li> <li>Morning: Run <code>make release-check</code> to catch issues early</li> <li>Afternoon: Execute release workflow</li> <li>Result: Consistent, predictable releases that build user trust</li> </ul>"},{"location":"development/release-workflow/#parallel-development","title":"Parallel Development","text":"<p>You can work on multiple features in parallel using branches:</p> <pre><code># Create feature branches from stable release\ngit checkout -b feature-a v0.1.4\n# ... work on feature A ...\n\ngit checkout -b feature-b v0.1.4\n# ... work on feature B ...\n\n# When ready, merge to main and release\ngit checkout main\ngit merge feature-a\nmake release-patch  # Release 0.1.5\n</code></pre> <p>This allows rapid iteration without blocking work on new features.</p>"},{"location":"development/release-workflow/#release-checklist","title":"Release Checklist","text":"<p>Use this checklist to ensure complete releases:</p> <ul> <li> All quality checks pass (<code>make release-check</code>)</li> <li> Version bumped with appropriate type (<code>make release-patch/minor/major</code>)</li> <li> CHANGELOG.md updated with clear, user-friendly descriptions</li> <li> Version changes committed (<code>make release-commit</code>)</li> <li> Tag created and pushed (<code>make release-tag</code>)</li> <li> Package published to PyPI (<code>make release-publish</code> - automatically strips/restores README frontmatter)</li> <li> PyPI listing verified at https://pypi.org/project/tnh-scholar/</li> <li> Installation test from PyPI successful (<code>pip install tnh-scholar==X.Y.Z</code>)</li> <li> GitHub tag appears in repository tags</li> <li> GitHub Release created with release notes</li> </ul>"},{"location":"development/release-workflow/#related-documentation","title":"Related Documentation","text":"<ul> <li><code>Makefile</code> (project root) - Release automation implementation</li> <li><code>CHANGELOG.md</code> (project root) - Version history</li> <li>Contributing Guide - Contribution guidelines</li> <li>Markdown Standards - Documentation standards</li> </ul>"},{"location":"development/style-guide/","title":"TNH Scholar Style Guide","text":"<p>Code formatting, naming conventions, and Python standards for TNH Scholar development.</p>"},{"location":"development/style-guide/#overview","title":"Overview","text":"<p>This style guide establishes coding standards for the TNH Scholar project. These guidelines ensure code quality, consistency, and maintainability across all development phases. For architectural design principles, see Design Principles.</p>"},{"location":"development/style-guide/#python-standards","title":"Python Standards","text":""},{"location":"development/style-guide/#version-requirement","title":"Version Requirement","text":"<p>The project uses Python 3.12.4 exclusively, taking advantage of modern Python features including strict typing, pattern matching, and improved error messages. This version requirement ensures consistency across all components and enables use of the latest language features.</p>"},{"location":"development/style-guide/#pep-8-compliance","title":"PEP 8 Compliance","text":"<p>All Python code follows PEP 8 with project-specific adaptations detailed below.</p>"},{"location":"development/style-guide/#code-organization","title":"Code Organization","text":""},{"location":"development/style-guide/#import-conventions","title":"Import Conventions","text":"<p>Import organization follows this pattern:</p> <ol> <li>Standard library imports</li> <li>External package imports</li> <li>Internal package imports</li> <li>Relative imports (use sparingly)</li> </ol> <p>Example:</p> <pre><code>from pathlib import Path\nfrom typing import Optional, Dict\n\nimport click\nfrom pydantic import BaseModel\n\nfrom tnh_scholar.utils import ensure_directory_exists\nfrom .environment import check_env\n</code></pre>"},{"location":"development/style-guide/#absolute-vs-relative-imports","title":"Absolute vs Relative Imports","text":"<p>Preferred: Use absolute imports from the top-level package (<code>tnh_scholar.</code>) for all intra-project references.</p> <p>Rationale: Maintains explicit architectural boundaries, avoids ambiguity in layered modules, and ensures IDE/refactor tooling compatibility.</p> <p>Example:</p> <pre><code># \u2705 Preferred\nfrom tnh_scholar.gen_ai_service.models.domain import Message\n\n# \ud83d\udeab Avoid\nfrom ..models.domain import Message\n</code></pre> <p>Exception: Relative imports may be used only for very local module groups (e.g., sibling adapters or mappers within the same provider directory) when the reference is clearly confined to that module cluster and no cross-layer boundary is crossed.</p>"},{"location":"development/style-guide/#module-structure","title":"Module Structure","text":"<p>Each module should maintain this general structure:</p> <pre><code>\"\"\"Module docstring providing overview and purpose.\"\"\"\n\n# Standard imports\n# External imports\n# Internal imports\n\n# Module-level constants\nDEFAULT_CHUNK_SIZE = 1024\n\n# Classes\nclass ExampleClass:\n    \"\"\"Class docstring.\"\"\"\n\n# Functions\ndef example_function():\n    \"\"\"Function docstring.\"\"\"\n</code></pre>"},{"location":"development/style-guide/#naming-conventions","title":"Naming Conventions","text":""},{"location":"development/style-guide/#file-and-directory-naming","title":"File and Directory Naming","text":"<p>Python files: Use lowercase with underscores</p> <pre><code>audio_processing.py\ntext_processor.py\n</code></pre> <p>Directory names: Use lowercase with underscores</p> <pre><code>text_processing/\ngen_ai_service/\n</code></pre> <p>Exception cases (traditional conventions):</p> <ul> <li><code>README.md</code></li> <li><code>LICENSE</code></li> <li><code>CONTRIBUTING.md</code></li> <li>Requirements files (<code>requirements.txt</code>, <code>dev-requirements.txt</code>)</li> </ul>"},{"location":"development/style-guide/#function-and-method-names","title":"Function and Method Names","text":"<ul> <li>Use lowercase with underscores: <code>process_text()</code>, <code>get_pattern()</code></li> <li>Names must be descriptive and scoped appropriately, reflecting their purpose without ambiguity</li> <li>Document side effects in name if not obvious: <code>update_and_save()</code>, <code>fetch_and_cache()</code></li> </ul>"},{"location":"development/style-guide/#class-names","title":"Class Names","text":"<ul> <li>Use PascalCase: <code>TextProcessor</code>, <code>PromptCatalog</code></li> <li>Keep cohesive and avoid unnecessary complexity</li> </ul>"},{"location":"development/style-guide/#variable-names","title":"Variable Names","text":"<ul> <li>Use lowercase with underscores: <code>text_content</code>, <code>max_tokens</code></li> <li>Make names self-explanatory and accessible</li> <li>Avoid single-letter names except for loop counters in short scopes</li> </ul>"},{"location":"development/style-guide/#type-annotations","title":"Type Annotations","text":""},{"location":"development/style-guide/#required-type-hints","title":"Required Type Hints","text":"<p>Type annotations are required for all function signatures, even during prototyping:</p> <pre><code>def process_text(\n    text: str,\n    language: Optional[str] = None,\n    max_tokens: int = 0\n) -&gt; str:\n    \"\"\"Process text with optional language specification.\"\"\"\n</code></pre>"},{"location":"development/style-guide/#custom-types","title":"Custom Types","text":"<p>Define custom types for complex structures:</p> <pre><code>from typing import NewType\n\nMarkdownStr = NewType('MarkdownStr', str)\nLanguageCode = NewType('LanguageCode', str)\n</code></pre>"},{"location":"development/style-guide/#type-handling-best-practices","title":"Type Handling Best Practices","text":"<p>Always prefer structured classes over plain dictionaries for data with consistent fields:</p> <ul> <li>Use <code>.</code> attribute access instead of <code>['key']</code> dictionary lookups</li> <li>Leverage type hints to catch errors at development time</li> <li>Encapsulate related logic within the class that owns the data</li> </ul>"},{"location":"development/style-guide/#data-models","title":"Data Models","text":""},{"location":"development/style-guide/#pydantic-vs-dataclasses","title":"Pydantic vs Dataclasses","text":"<p>Use Pydantic V2 when:</p> <ul> <li>Data validation is important (especially for external inputs)</li> <li>Working with API interfaces where data needs parsing and validation</li> <li>Serialization features are needed</li> </ul> <p>Use dataclasses when:</p> <ul> <li>Creating simple internal data structures with minimal validation needs</li> <li>Serialization features aren't needed</li> <li>Improved performance is required</li> </ul>"},{"location":"development/style-guide/#pydantic-best-practices","title":"Pydantic Best Practices","text":"<pre><code>from pydantic import BaseModel, Field, computed_field\n\nclass TextObject(BaseModel):\n    \"\"\"Represents processed text with metadata.\"\"\"\n\n    language: str = Field(..., description=\"ISO 639-1 language code\")\n    sections: List[LogicalSection]\n    metadata: Optional[Dict[str, Any]] = None\n\n    @computed_field\n    @property\n    def word_count(self) -&gt; int:\n        \"\"\"Compute total word count across all sections.\"\"\"\n        return sum(len(s.content.split()) for s in self.sections)\n</code></pre> <p>Best practices:</p> <ul> <li>Use <code>@computed_field</code> for derived properties included in serialization</li> <li>Leverage field validation with standard validators or custom methods</li> <li>Use <code>model_config</code> for class-level configuration</li> <li>Take advantage of automatic type coercion for cleaner interfaces</li> <li>Create factory methods (<code>from_dict</code>, <code>from_legacy_format</code>) for special parsing needs</li> </ul>"},{"location":"development/style-guide/#strong-typing-standards","title":"Strong Typing Standards","text":"<p>Critical project requirements:</p> <ul> <li>Always use typed classes, enums, and dataclasses</li> <li>Avoid literal strings and numbers in app logic</li> <li>Configuration values come from <code>Settings</code> (pydantic BaseSettings), policies, or prompt metadata \u2014 never hardcoded</li> <li>Dicts are not used in app layers; prefer Pydantic models or dataclasses</li> <li>Enums replace string literals for identifiers (e.g., provider names, roles, intent types)</li> <li>Adapters may handle dict conversions only at API transport boundaries</li> </ul> <p>Abstract interfaces:</p> <ul> <li>Use <code>Protocol</code> for structural typing and interface contracts (no inheritance required)</li> <li>Use <code>ABC</code> only when enforcing init-time invariants or providing shared mixin behavior</li> <li>All system interfaces must be defined via abstract base classes</li> </ul> <p>Goal: Zero literals, zero dicts, clear typing, explicit configuration \u2014 ensuring predictable behavior and strong IDE/type support.</p>"},{"location":"development/style-guide/#function-and-method-complexity","title":"Function and Method Complexity","text":""},{"location":"development/style-guide/#size-limits","title":"Size Limits","text":"<ul> <li>Target length: 15-20 lines of code (excluding docstring)</li> <li>Cyclomatic complexity: 7 or less</li> <li>If a function grows beyond limits, refactor into smaller helpers</li> </ul>"},{"location":"development/style-guide/#single-responsibility","title":"Single Responsibility","text":"<p>Each function or method should perform one logical task. Avoid mixing concerns (e.g., validation, mutation, and I/O in a single function).</p>"},{"location":"development/style-guide/#control-flow","title":"Control Flow","text":"<p>Use early returns to reduce nesting:</p> <pre><code># \u2705 Preferred\ndef process_text(text: str) -&gt; str:\n    if not text:\n        return \"\"\n\n    if len(text) &gt; MAX_LENGTH:\n        raise ValueError(\"Text too long\")\n\n    return text.upper()\n\n# \ud83d\udeab Avoid deep nesting\ndef process_text(text: str) -&gt; str:\n    if text:\n        if len(text) &lt;= MAX_LENGTH:\n            return text.upper()\n        else:\n            raise ValueError(\"Text too long\")\n    else:\n        return \"\"\n</code></pre> <p>Use <code>match</code>/<code>case</code> for multi-condition branching (3+ conditions):</p> <pre><code># \u2705 Preferred for 3+ conditions\nmatch processing_mode:\n    case \"punctuate\":\n        return punctuate_text(text)\n    case \"translate\":\n        return translate_text(text, target_lang)\n    case \"section\":\n        return section_text(text)\n    case _:\n        raise ValueError(f\"Unknown mode: {processing_mode}\")\n\n# \ud83d\udeab Avoid long if/elif chains\nif processing_mode == \"punctuate\":\n    return punctuate_text(text)\nelif processing_mode == \"translate\":\n    return translate_text(text, target_lang)\n# ... many more elifs\n</code></pre>"},{"location":"development/style-guide/#code-documentation","title":"Code Documentation","text":""},{"location":"development/style-guide/#docstring-style","title":"Docstring Style","text":"<p>The project follows Google's Python documentation style for all docstrings.</p> <p>Classes:</p> <pre><code>class TextProcessor:\n    \"\"\"A class that processes text using configurable prompts.\n\n    Implements prompt-based text processing with configurable token limits\n    and language support. Designed for extensibility through the prompt system.\n\n    Attributes:\n        prompt: A Prompt instance defining processing instructions.\n        max_tokens: An integer specifying maximum tokens for processing.\n\n    Note:\n        Prompt instances should be initialized with proper template validation.\n    \"\"\"\n</code></pre> <p>Functions:</p> <pre><code>def process_text(text: str, language: Optional[str] = None) -&gt; str:\n    \"\"\"Processes text according to prompt instructions.\n\n    Applies the configured prompt to input text, handling language-specific\n    requirements and token limitations.\n\n    Args:\n        text: Input text to process.\n        language: Optional ISO 639-1 language code. Defaults to None for\n            auto-detection.\n\n    Returns:\n        A string containing the processed text.\n\n    Raises:\n        ValueError: If text is empty or invalid.\n        PromptError: If prompt application fails.\n\n    Examples:\n        &gt;&gt;&gt; processor = TextProcessor(prompt)\n        &gt;&gt;&gt; result = process_text(\"Input text\", language=\"en\")\n        &gt;&gt;&gt; print(result)\n        Processed text output\n    \"\"\"\n</code></pre>"},{"location":"development/style-guide/#documentation-requirements-by-phase","title":"Documentation Requirements by Phase","text":"<p>Prototyping Phase:</p> <ul> <li>Basic function/class documentation</li> <li>Essential usage examples</li> <li>Known limitations noted</li> </ul> <p>Production Phase:</p> <ul> <li>Comprehensive API documentation</li> <li>Multiple usage examples</li> <li>Error handling documentation</li> <li>Performance considerations</li> <li>Security implications</li> </ul>"},{"location":"development/style-guide/#error-handling","title":"Error Handling","text":""},{"location":"development/style-guide/#prototyping-phase","title":"Prototyping Phase","text":"<p>During prototyping, error handling should prioritize visibility of failure cases over comprehensive handling.</p> <p>Preferred approach \u2014 allow exceptions to propagate:</p> <pre><code># TODO: Add error handling for ValueError and PromptError\nresult = process_text(input_text)\n</code></pre> <p>When try blocks are needed, use minimal handling:</p> <pre><code>try:\n    # TODO: Handle specific exceptions in production\n    result = process_text(input_text)\nexcept:\n    # Maintain stack trace while documenting intent\n    raise\n</code></pre> <p>This approach maintains clear visibility of failure modes and preserves full stack traces for debugging.</p>"},{"location":"development/style-guide/#production-phase","title":"Production Phase","text":"<p>Production code requires comprehensive error handling:</p> <pre><code>try:\n    result = process_text(input_text)\nexcept ValueError as e:\n    logger.error(f\"Invalid input format: {e}\")\n    raise InvalidInputError(str(e)) from e\nexcept APIError as e:\n    logger.error(f\"API processing failed: {e}\")\n    raise ProcessingError(str(e)) from e\n</code></pre> <p>Do NOT write catch-all exception handling:</p> <pre><code># \ud83d\udeab Avoid\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {e}\")\n    raise SystemError(f\"unexpected error: {e}\") from e\n</code></pre> <p>Prefer letting unknown exceptions propagate.</p>"},{"location":"development/style-guide/#logging","title":"Logging","text":""},{"location":"development/style-guide/#prototyping-phase_1","title":"Prototyping Phase","text":"<p>Basic logging configuration is acceptable:</p> <pre><code>logger = get_logger(__name__)\nlogger.info(\"Processing started\")\nlogger.debug(\"Processing details: %s\", details)  # DEBUG level especially important\nlogger.error(\"Processing failed\")\n</code></pre>"},{"location":"development/style-guide/#production-phase_1","title":"Production Phase","text":"<p>Production logging should include:</p> <ul> <li>Log levels properly used (DEBUG, INFO, WARNING, ERROR, CRITICAL)</li> <li>Structured logging where appropriate</li> <li>Contextual information</li> <li>Error tracebacks</li> <li>Provenance and fingerprinting if required</li> </ul>"},{"location":"development/style-guide/#development-tooling","title":"Development Tooling","text":""},{"location":"development/style-guide/#required-tools","title":"Required Tools","text":"<ul> <li>Code formatting: <code>black</code> for automatic code formatting</li> <li>Linting: <code>ruff</code> to enforce style and complexity limits</li> <li>Type checking: <code>mypy</code> to enforce type annotations</li> <li>Complexity analysis: Sourcery to monitor function complexity</li> <li>Pre-commit hooks: Automate code quality checks</li> </ul>"},{"location":"development/style-guide/#optional-tools","title":"Optional Tools","text":"<ul> <li><code>radon</code> or <code>flake8-cognitive-complexity</code> for stricter cyclomatic complexity enforcement</li> </ul>"},{"location":"development/style-guide/#sourcery-standards","title":"Sourcery Standards","text":"<p>Prototyping Phase:</p> <ul> <li>Basic Sourcery review</li> </ul> <p>Production Phase:</p> <ul> <li>All files must pass Sourcery review with no unresolved issues</li> <li>All functions should have a quality score of 60% or better</li> <li>Functions with lower scores must be clearly documented with rationale (legacy code, necessary complexity for algorithmic or performance reasons)</li> </ul>"},{"location":"development/style-guide/#security-standards","title":"Security Standards","text":""},{"location":"development/style-guide/#api-key-management","title":"API Key Management","text":"<p>Consistent across all phases:</p> <ul> <li>No keys in code (ever)</li> <li>Use environment variables</li> <li>Secure configuration loading</li> <li>Support key rotation</li> </ul>"},{"location":"development/style-guide/#input-validation","title":"Input Validation","text":"<p>Prototyping Phase:</p> <ul> <li>Basic input validation</li> <li>Type checking</li> <li>Simple sanitization</li> </ul> <p>Production Phase:</p> <ul> <li>Comprehensive validation</li> <li>Security scanning</li> <li>Input sanitization</li> <li>Output escaping</li> </ul>"},{"location":"development/style-guide/#version-control","title":"Version Control","text":""},{"location":"development/style-guide/#git-workflow","title":"Git Workflow","text":"<p>Standards apply across all phases:</p> <ul> <li>Feature branches for development</li> <li>Clear, descriptive commit messages</li> <li>Regular main branch updates</li> <li>Version tags for releases</li> </ul>"},{"location":"development/style-guide/#commit-message-format","title":"Commit Message Format","text":"<pre><code>Brief summary (50 chars or less)\n\nDetailed explanation if needed (wrap at 72 chars):\n- What changed\n- Why it changed\n- References to issues/ADRs\n\n\ud83e\udd16 Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude &lt;noreply@anthropic.com&gt;\n</code></pre>"},{"location":"development/style-guide/#related-documentation","title":"Related Documentation","text":"<ul> <li>Design Principles - Architectural patterns and design philosophy</li> <li>Contributing Guide - Contribution workflow and standards</li> <li>Project Principles - High-level project principles</li> <li>System Design - Overall system architecture</li> </ul>"},{"location":"development/style-guide/#references","title":"References","text":"<ul> <li>PEP 8 \u2013 Style Guide for Python Code</li> <li>Google Python Style Guide</li> <li>Pydantic V2 Documentation</li> </ul>"},{"location":"development/system-design/","title":"TNH Scholar System Design","text":"<p>High-level system design describing the cyclical AI processing architecture powering TNH Scholar.</p> <p>Terminology Note: This document uses historical \"Pattern\" terminology. In current TNH Scholar documentation, \"Pattern\" has been replaced with \"Prompt\" to align with industry standards. See ADR-DD03 for details.</p> <p>When reading this document: \"Pattern-based processing\" \u2192 \"Prompt-based processing\"</p>"},{"location":"development/system-design/#1-overview-and-vision","title":"1. Overview and Vision","text":"<p>TNH Scholar is an AI-enhanced content processing and transformation system designed to work with dharma materials. The system is distinguished by its cyclical learning architecture, where processing outputs can be used to generate training data for model improvement, creating a self-enhancing feedback loop.</p>"},{"location":"development/system-design/#11-core-design-philosophy","title":"1.1 Core Design Philosophy","text":"<p>The system embraces several key philosophical principles:</p> <ul> <li>Evolutionary improvement through self-generated training data</li> <li>Modular design enabling flexible pipeline construction</li> <li>Balance of rapid prototyping with extensible architecture</li> <li>Focus on AI-enhanced content processing and transformation</li> </ul>"},{"location":"development/system-design/#12-system-objectives","title":"1.2 System Objectives","text":"<p>Primary objectives of the system include:</p> <ul> <li>Transform multilingual content across various formats and mediums</li> <li>Generate high-quality training data from processing operations</li> <li>Enable continuous improvement of AI processing capabilities</li> <li>Support flexible pipeline construction for varied content workflows</li> </ul>"},{"location":"development/system-design/#2-system-architecture","title":"2. System Architecture","text":""},{"location":"development/system-design/#21-core-components","title":"2.1 Core Components","text":"<p>The system is built around four primary subsystems that form a cyclical processing chain:</p>"},{"location":"development/system-design/#1-content-acquisition-system","title":"1. Content Acquisition System","text":"<ul> <li>Source content ingestion from various platforms</li> <li>Metadata extraction and standardization</li> <li>Initial content preparation and validation</li> </ul>"},{"location":"development/system-design/#2-processing-system","title":"2. Processing System","text":"<ul> <li>AI-enhanced content transformation</li> <li>Pattern-based processing workflows</li> <li>Metadata enrichment and validation</li> </ul>"},{"location":"development/system-design/#3-training-data-generation","title":"3. Training Data Generation","text":"<ul> <li>Automated extraction of training pairs</li> <li>Quality assessment of generated data</li> <li>Training data storage and management</li> </ul>"},{"location":"development/system-design/#4-model-enhancement","title":"4. Model Enhancement","text":"<ul> <li>Training data integration</li> <li>Model fine-tuning and validation</li> <li>Processing capability improvement</li> </ul>"},{"location":"development/system-design/#22-processing-flow","title":"2.2 Processing Flow","text":"<p>The system (can) operate in a cyclical pattern. This diagram shows the flow of content through the system:</p> <pre><code>Content \u2192 AI Processing \u2192 Processed Content \u2192 Training Data \u2192 Model Training\n    \u2191                                                                  \u2193\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Enhanced Processing \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>flowchart TD\n    AP((AI Processing)) --&gt; PC[Processed Content]\n    PC --&gt; TD[Training Data]\n    TD --&gt; MT[Model Training]\n    MT --&gt; AP\n    RC[Raw Content] --&gt; AP\n\n    style AP fill:#bbf,stroke:#333,stroke-width:4px\n    style PC fill:#bfb,stroke:#333,stroke-width:2px\n    style TD fill:#fbf,stroke:#333,stroke-width:2px\n    style MT fill:#fbb,stroke:#333,stroke-width:2px\n    style RC fill:#f9f,stroke:#333,stroke-width:2px</code></pre> <p>This cycle enables:</p> <ul> <li>Continuous system improvement</li> <li>Accumulation of specialized training data</li> <li>Evolution of processing capabilities</li> </ul>"},{"location":"development/system-design/#3-core-principles","title":"3. Core Principles","text":""},{"location":"development/system-design/#31-modularity","title":"3.1 Modularity","text":"<p>Every system component should be:</p> <ul> <li>Independently deployable</li> <li>Clearly scoped in functionality</li> <li>Accessible through well-defined interfaces</li> <li>Composable into larger processing chains</li> </ul>"},{"location":"development/system-design/#32-extensibility","title":"3.2 Extensibility","text":"<p>The system supports extension through:</p> <ul> <li>Abstract interfaces for core components</li> <li>Plugin architecture for new processors</li> <li>Configurable processing pipelines</li> <li>Standardized metadata handling</li> </ul>"},{"location":"development/system-design/#33-simplicity","title":"3.3 Simplicity","text":"<p>Development prioritizes:</p> <ul> <li>Clear, focused component responsibilities</li> <li>Minimal dependencies between modules</li> <li>Straightforward pipeline construction</li> <li>Rapid prototyping capabilities</li> </ul>"},{"location":"development/system-design/#4-component-framework","title":"4. Component Framework","text":""},{"location":"development/system-design/#41-base-interfaces","title":"4.1 Base Interfaces","text":"<p>Core system interfaces define standard operations:</p> <pre><code>class ContentProcessor(ABC):\n    \"\"\"Base interface for content processing operations.\"\"\"\n\n    @abstractmethod\n    def process(self, content: RawContent) -&gt; ProcessedContent:\n        \"\"\"Transform input content using current processing capability.\"\"\"\n        pass\n\n    @abstractmethod\n    def generate_training_data(\n        self, \n        raw: RawContent, \n        processed: ProcessedContent\n    ) -&gt; TrainingData:\n        \"\"\"Extract training data from processing results.\"\"\"\n        pass\n</code></pre>"},{"location":"development/system-design/#42-pipeline-architecture","title":"4.2 Pipeline Architecture","text":"<p>Pipelines compose processors into workflows:</p> <pre><code>class ProcessingPipeline:\n    \"\"\"Orchestrates content processing workflow.\"\"\"\n\n    def __init__(\n        self,\n        processors: List[ContentProcessor],\n        training_collector: Optional[TrainingCollector] = None\n    ):\n        self.processors = processors\n        self.collector = training_collector\n\n    def execute(\n        self, \n        source: ContentSource\n    ) -&gt; Tuple[ProcessedContent, Optional[TrainingData]]:\n        \"\"\"Execute pipeline with optional training data collection.\"\"\"\n        pass\n</code></pre>"},{"location":"development/system-design/#5-implementation-guidelines","title":"5. Implementation Guidelines","text":""},{"location":"development/system-design/#51-development-phases","title":"5.1 Development Phases","text":"<p>Development follows a staged approach:</p> <p>Phase 1: Rapid Prototyping</p> <ul> <li>Focus on core functionality</li> <li>Minimal error handling</li> <li>Basic logging</li> <li>Proof of concept implementations</li> </ul> <p>Phase 2: Stabilization</p> <ul> <li>Enhanced error handling</li> <li>Comprehensive logging</li> <li>Performance optimization</li> <li>Testing infrastructure</li> </ul> <p>Phase 3: Production Hardening</p> <ul> <li>Full error recovery</li> <li>Performance monitoring</li> <li>Production logging</li> <li>Deployment automation</li> </ul>"},{"location":"development/system-design/#52-code-standards","title":"5.2 Code Standards","text":"<p>Code development prioritizes:</p> <ul> <li>Strong typing</li> <li>Clear documentation</li> <li>Consistent naming</li> <li>Modular design</li> </ul>"},{"location":"development/system-design/#53-testing-strategy","title":"5.3 Testing Strategy","text":"<p>Testing emphasis varies by phase:</p> <ul> <li>Prototyping: Basic functionality validation</li> <li>Stabilization: Comprehensive unit testing</li> <li>Production: Full test coverage and integration testing</li> </ul>"},{"location":"development/system-design/#6-current-state-and-future-directions","title":"6. Current State and Future Directions","text":""},{"location":"development/system-design/#61-implemented-components","title":"6.1 Implemented Components","text":"<p>Current system includes:</p> <ul> <li>YouTube content acquisition</li> <li>Audio transcription processing</li> <li>Pattern-based text processing</li> <li>Basic pipeline orchestration</li> </ul>"},{"location":"development/system-design/#62-planned-enhancements","title":"6.2 Planned Enhancements","text":"<p>Near-term priorities:</p> <ul> <li>Enhanced metadata handling</li> <li>Improved pipeline composition</li> <li>Training data collection framework</li> <li>Model fine-tuning infrastructure</li> </ul>"},{"location":"development/system-design/#63-future-capabilities","title":"6.3 Future Capabilities","text":"<p>Long-term objectives:</p> <ul> <li>Automated quality assessment</li> <li>Advanced pipeline orchestration</li> <li>Distributed processing support</li> <li>Enhanced training data generation</li> </ul>"},{"location":"development/system-design/#7-design-decisions-and-tradeoffs","title":"7. Design Decisions and Tradeoffs","text":""},{"location":"development/system-design/#71-current-design-choices","title":"7.1 Current Design Choices","text":"<p>The system currently prioritizes:</p> <ul> <li>Rapid prototype development</li> <li>Simple pipeline construction</li> <li>Clear component boundaries</li> <li>Flexible processing flows</li> </ul>"},{"location":"development/system-design/#72-design-tradeoffs","title":"7.2 Design Tradeoffs","text":"<p>Key tradeoffs include:</p> <ul> <li>Simplicity over comprehensive error handling</li> <li>Flexibility over optimization</li> <li>Rapid development over complete documentation</li> <li>Basic logging over detailed instrumentation</li> </ul>"},{"location":"development/system-design/#8-conclusion","title":"8. Conclusion","text":"<p>The TNH Scholar system design provides a framework for AI-enhanced content processing with self-improvement capabilities. Its modular architecture and cyclical processing model enable continuous enhancement of processing capabilities while maintaining flexibility for varied content workflows.</p> <p>Success metrics for this design include:</p> <ul> <li>Ease of new pipeline creation</li> <li>Quality of generated training data</li> <li>Rate of processing improvement</li> <li>System extensibility</li> </ul>"},{"location":"development/incident-reports/","title":"Incident Reports","text":"<p>Table of Contents:</p> <p>Incident Report: Git Recovery - December 7, 2025 - Post-mortem analysis of orphaned commits and successful recovery of prompt system implementation (ADR-PT04)</p> <p>This file auto-generated.</p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/","title":"Incident Report: Git Recovery - December 7, 2025","text":"<p>Incident ID: IR-2025-12-07-001 Date: December 7-8, 2025 Severity: High (Data loss risk) Status: \u2705 Resolved Affected: v0.2.0 release, prompt_system implementation</p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#executive-summary","title":"Executive Summary","text":"<p>On December 7-8, 2025, approximately 39 files (4,508 lines of code) containing the prompt system implementation (ADR-PT04) were orphaned due to a <code>git reset --hard</code> operation that used a stale local branch reference. All work was successfully recovered from git reflog on December 7, 2025, and the v0.2.0 tag was corrected to point to the complete release.</p> <p>Impact: None - All work recovered, no data lost Root Cause: Reset to stale local branch reference - local <code>version-0.2.0</code> not fetched after remote PR merge Recovery Time: ~4 hours</p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#timeline","title":"Timeline","text":"<p>All times in PST (UTC-8):</p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#december-6-2025","title":"December 6, 2025","text":"<ul> <li>04:56: Created <code>version-0.1.4</code> branch on GitHub</li> <li>16:30: Created <code>prompt-system-refactor</code> branch on GitHub</li> <li>20:13: Created <code>version-0.2.0</code> branch on GitHub</li> <li>21:01: Codex snapshot commit (WIP) <code>0b019ed</code></li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#december-7-2025","title":"December 7, 2025","text":"<ul> <li>19:03: Completed prompt system implementation on <code>prompt-system-refactor</code> (commit <code>c6532f5</code>)</li> <li>19:30: PR #11 merged <code>prompt-system-refactor</code> \u2192 <code>version-0.1.4</code> (merge commit <code>87b6603</code>)</li> <li>19:33: PR #12 merged <code>version-0.1.4</code> \u2192 <code>version-0.2.0</code> (merge commit <code>5bf012d</code>)</li> <li>19:37: INCIDENT: Reset main to stale branch reference</li> <li>Command: <code>git reset --hard version-0.2.0</code> (session log line 396)</li> <li>Problem: Local <code>version-0.2.0</code> not fetched after remote PR #12 merge</li> <li>Local branch pointed to <code>d51fc87</code> (version bump), not <code>5bf012d</code> (merged state)</li> <li>Missing step: <code>git fetch origin version-0.2.0</code> before reset</li> <li>This orphaned all prompt system work (commit <code>c6532f5</code> and descendants)</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#december-8-2025","title":"December 8, 2025","text":"<ul> <li>03:30: PR #11 branch <code>prompt-system-refactor</code> deleted from GitHub (automatic cleanup)</li> <li>03:33: PR #12 branch <code>version-0.1.4</code> deleted from GitHub (automatic cleanup)</li> <li>03:38: Branch <code>version-0.2.0</code> deleted from GitHub (automatic cleanup)</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#december-7-2025-recovery","title":"December 7, 2025 (Recovery)","text":"<ul> <li>~20:00: Discovered orphaned work</li> <li>~20:30: Located commit <code>c6532f5</code> in git reflog</li> <li>~21:00: Recovered all 39 files from reflog</li> <li>~21:15: Created PR #13 with recovered work</li> <li>~21:20: Merged PR #13 to main</li> <li>~21:22: Corrected v0.2.0 tag to point to proper release</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#root-cause-analysis","title":"Root Cause Analysis","text":""},{"location":"development/incident-reports/2025-12-07-git-recovery/#what-happened","title":"What Happened","text":"<p>The incident occurred when prompt system work (commit <code>c6532f5</code> and descendants) became orphaned after a git operation on the main branch:</p> <ol> <li>Tag created: <code>v0.2.0</code> tag was created at commit <code>0d7d459</code> (version bump commit)</li> <li>Work done on branch: Prompt system work was merged into <code>version-0.2.0</code> branch via PRs #11 and #12 (merge commit <code>5bf012d</code>)</li> <li>Destructive operation on main: A git command was executed that caused main to not include the prompt system work</li> <li>Branches deleted: GitHub auto-deleted branches <code>prompt-system-refactor</code>, <code>version-0.1.4</code>, and <code>version-0.2.0</code> after PR merges (standard behavior)</li> <li>Work orphaned: With branches deleted and main not including the work, commit <code>c6532f5</code> and all descendants became unreachable from any branch</li> </ol>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#why-it-happened-root-cause-stale-branch-reference","title":"Why It Happened (Root Cause: Stale Branch Reference)","text":"<p>Root Cause Identified (via session log forensics):</p> <p>The incident was caused by resetting <code>main</code> to a stale local branch reference that had not been updated after a remote PR merge.</p> <p>The Fatal Sequence (from recovered session log):</p> <ol> <li>PR #12 created: Merging <code>version-0.1.4</code> \u2192 <code>version-0.2.0</code> (session log line 316)</li> <li>PR #12 merged on GitHub: User confirms merge completion (session log line 321)</li> <li>Remote <code>origin/version-0.2.0</code> now points to <code>5bf012d</code> (includes prompt system)</li> <li>Local <code>version-0.2.0</code> still points to <code>d51fc87</code> (version bump only)</li> <li>Reset executed: <code>git reset --hard version-0.2.0</code> (session log line 396)</li> <li>Used local stale reference <code>d51fc87</code></li> <li>Should have used remote updated reference <code>5bf012d</code></li> <li>Result: Reset to pre-merge state, orphaning all new work</li> </ol> <p>The Missing Step:</p> <p>Between the PR merge and the reset, this was required but omitted:</p> <pre><code># REQUIRED: Update local branch after remote merge\ngit fetch origin version-0.2.0\ngit checkout version-0.2.0\ngit pull\n\n# Verify prompt system work is present\ngit log --oneline -10\nls -la src/tnh_scholar/prompt_system/\n\n# THEN safe to reset\ngit checkout main\ngit reset --hard version-0.2.0\n</code></pre> <p>Why This Mistake Happened:</p> <ul> <li>Git semantics gap: Branch names in git commands refer to local references, not remote state</li> <li>Human error: User merged PR on GitHub without checking out/updating local branch</li> <li>Agent assumption: Agent assumed <code>version-0.2.0</code> would automatically point to merged state</li> <li>No verification: No check that local branch matched remote branch before reset</li> <li>Workflow complexity: Multi-tier merge chain (feature \u2192 0.1.4 \u2192 0.2.0 \u2192 main) increased opportunity for staleness</li> </ul> <p>Evidence:</p> <p>Session log confirms reset moved to commit <code>d51fc87</code>:</p> <pre><code>git reset --hard version-0.2.0\nHEAD is now at d51fc87 feat: Automate PyPI README frontmatter stripping\n</code></pre> <p>This is the version bump commit before PR #12 merge. The prompt system work was in merge commit <code>5bf012d</code>, which existed on remote but not in the local branch reference.</p> <p>Contributing Factors:</p> <ul> <li>No branch protection on main: Destructive operations were possible without review</li> <li>No post-merge fetch protocol: Safety rules didn't require fetch after remote operations</li> <li>No content verification: Didn't verify expected files present in reset target</li> <li>No pre-reset safety checks: No verification of unpushed commits before destructive operations</li> <li>Complex workflow: Using version branches as development location instead of feature branches merged to main</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#what-was-at-risk","title":"What Was At Risk","text":""},{"location":"development/incident-reports/2025-12-07-git-recovery/#code-changes-39-files-4508-insertions-130-deletions","title":"Code Changes (39 files, 4,508 insertions, 130 deletions)","text":"<p>Prompt System Package (28 files):</p> <ul> <li><code>src/tnh_scholar/prompt_system/</code> - Complete implementation (22 files)</li> <li><code>tests/prompt_system/</code> - Test suite (6 files)</li> <li>Architecture: config, domain, transport, adapters, mappers, service layers</li> </ul> <p>Documentation (7 files):</p> <ul> <li><code>docs/architecture/prompt-system/adr/adr-pt04-prompt-system-refactor.md</code> (1,198 lines)</li> <li><code>docs/architecture/metadata/adr/adr-md02-metadata-object-service-integration.md</code> (465 lines)</li> <li><code>docs/architecture/object-service/object-service-design-gaps.md</code> (566 lines)</li> <li><code>docs/architecture/project-policies/adr/adr-pp01-rapid-prototype-versioning.md</code> (299 lines)</li> <li><code>docs/architecture/project-policies/index.md</code></li> <li><code>docs/architecture/project-policies/versioning-policy-implementation-summary.md</code></li> <li><code>VERSIONING.md</code> (176 lines)</li> </ul> <p>Modified Files (4 files):</p> <ul> <li><code>AGENTLOG.md</code> - Session documentation</li> <li><code>src/tnh_scholar/gen_ai_service/pattern_catalog/adapters/prompts_adapter.py</code> - Integration changes</li> <li><code>src/tnh_scholar/metadata/metadata.py</code> - Infrastructure updates</li> <li>Various documentation updates</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#recovery-process","title":"Recovery Process","text":""},{"location":"development/incident-reports/2025-12-07-git-recovery/#discovery","title":"Discovery","text":"<ol> <li>User noticed ADR-PT04 was missing from repository</li> <li>Checked recent commits - prompt system not present</li> <li>Checked git reflog - found orphaned commit <code>c6532f5</code></li> </ol>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#forensic-analysis","title":"Forensic Analysis","text":"<p>Comprehensive analysis performed:</p> <pre><code># Found orphaned commits\ngit fsck --lost-found --no-reflogs\n# Result: 173 orphaned commits total\n\n# Located prompt system work\ngit reflog | grep -E \"prompt-system|version-0\"\n# Found: c6532f5 (Dec 7, 19:03)\n\n# Verified PR merge commits still in reflog\ngit show 87b6603  # PR #11 merge\ngit show 5bf012d  # PR #12 merge\n\n# Confirmed prompt_system code identical\ngit diff --stat 5bf012d c6532f5 -- src/tnh_scholar/prompt_system/\n# Result: No differences\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#recovery-steps","title":"Recovery Steps","text":"<ol> <li>Created recovery branch:</li> </ol> <pre><code>git checkout -b recovery/prompt-system-refactor\n</code></pre> <ol> <li>Recovered files from orphaned commit:</li> </ol> <pre><code>git checkout c6532f5 -- src/tnh_scholar/prompt_system/\ngit checkout c6532f5 -- tests/prompt_system/\ngit checkout c6532f5 -- docs/architecture/metadata/\ngit checkout c6532f5 -- docs/architecture/prompt-system/\ngit checkout c6532f5 -- docs/architecture/project-policies/\ngit checkout c6532f5 -- docs/architecture/object-service/object-service-design-gaps.md\ngit checkout c6532f5 -- VERSIONING.md\ngit checkout c6532f5 -- AGENTLOG.md\ngit checkout c6532f5 -- src/tnh_scholar/gen_ai_service/pattern_catalog/adapters/prompts_adapter.py\ngit checkout c6532f5 -- src/tnh_scholar/metadata/metadata.py\n</code></pre> <ol> <li>Verified recovery:</li> </ol> <pre><code>poetry run pytest tests/prompt_system/ -v\n# Result: All 14 tests passing\n</code></pre> <ol> <li>Committed and pushed immediately:</li> </ol> <pre><code>git commit -m \"feat: Recover prompt system implementation (ADR-PT04)...\"\ngit push -u origin recovery/prompt-system-refactor\n</code></pre> <ol> <li> <p>Created PR #13 with comprehensive recovery documentation</p> </li> <li> <p>Merged to main (user performed merge)</p> </li> <li> <p>Corrected v0.2.0 tag:</p> </li> </ol> <pre><code># Delete incorrect tag\ngit tag -d v0.2.0\ngit push origin :refs/tags/v0.2.0\n\n# Create corrected tag with documentation\ngit tag -a v0.2.0 -m \"Release v0.2.0 (Corrected)...\"\ngit push origin v0.2.0\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#forensic-findings","title":"Forensic Findings","text":""},{"location":"development/incident-reports/2025-12-07-git-recovery/#orphaned-commits-analysis","title":"Orphaned Commits Analysis","text":"<p>Total orphaned commits: 173</p> <p>Categories:</p> <ol> <li>Codex snapshots (most common): WIP commits from various dates</li> <li>Example: <code>0b019ed</code> (Dec 6, 21:01) - Earlier version of prompt system</li> <li>Development branches: Old feature branches never merged</li> <li>Experimental work: Test commits, prototypes</li> </ol> <p>Most Recent Orphaned Work:</p> <ul> <li><code>0b019ed</code> - Codex snapshot with partial prompt system (Dec 6)</li> <li>Superseded by <code>c6532f5</code> (more complete implementation)</li> <li>No unique work lost</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#github-event-log","title":"GitHub Event Log","text":"<p>Retrieved from GitHub API (<code>/repos/.../events</code>):</p> <pre><code>{\n  \"type\": \"DeleteEvent\",\n  \"ref\": \"version-0.2.0\",\n  \"created_at\": \"2025-12-08T03:39:43Z\"\n}\n{\n  \"type\": \"DeleteEvent\",\n  \"ref\": \"prompt-system-refactor\",\n  \"created_at\": \"2025-12-08T03:38:36Z\"\n}\n{\n  \"type\": \"DeleteEvent\",\n  \"ref\": \"version-0.1.4\",\n  \"created_at\": \"2025-12-08T03:38:19Z\"\n}\n</code></pre> <p>All branch deletions occurred automatically after PR merges (standard GitHub behavior).</p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#comparison-original-vs-recovered","title":"Comparison: Original vs Recovered","text":"<p>Differences between PR #12 merge (<code>5bf012d</code>) and recovered main (<code>a6523a9</code>):</p> <ul> <li>Added: <code>docs/development/git-workflow.md</code> (safety documentation - created during recovery)</li> <li>Modified: Minor doc updates (README.md, TODO.md) - unrelated to incident</li> <li>prompt_system code: Identical (0 differences)</li> </ul> <p>Conclusion: 100% of prompt system implementation recovered with no data loss.</p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#preventative-measures-implemented","title":"Preventative Measures Implemented","text":""},{"location":"development/incident-reports/2025-12-07-git-recovery/#1-git-safety-rules-claudeclaudemd","title":"1. Git Safety Rules (<code>.claude/CLAUDE.md</code>)","text":"<p>Added comprehensive safety rules for AI agents:</p> <pre><code>**CRITICAL - NEVER VIOLATE THESE RULES:**\n\n1. **NEVER run `git reset --hard` without explicit user approval**\n2. **NEVER run `git reset --soft/--mixed` on main branch without explicit approval**\n3. **NEVER delete branches without explicit approval**\n4. **NEVER force push (`git push --force`) without explicit approval**\n5. **ALWAYS push feature branches to origin before switching away**\n6. **ALWAYS verify unpushed commits before any reset operation**\n7. **ASK FIRST** before any destructive git operation**\n\n**Required checks before destructive operations:**\n- Run `git status` and `git branch -vv` to check unpushed work\n- Run `git log --branches --not --remotes` to see unpushed commits\n- Explicitly confirm with user what will be lost\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#2-post-remote-merge-safety-protocol-claudeclaudemd","title":"2. Post-Remote-Merge Safety Protocol (<code>.claude/CLAUDE.md</code>)","text":"<p>Added critical protocol for handling remote PR merges:</p> <pre><code>### Post-Merge Safety Protocol\n\n**CRITICAL: After ANY GitHub PR merge, ALWAYS:**\n\n1. **Fetch the branch**: `git fetch origin &lt;branch&gt;`\n2. **Verify merge commit present**: `git log origin/&lt;branch&gt; --oneline -5`\n3. **Check local vs remote**: Compare `git rev-parse &lt;branch&gt;` vs `git rev-parse origin/&lt;branch&gt;`\n4. **Update local if stale**: `git checkout &lt;branch&gt; &amp;&amp; git pull`\n5. **Verify expected content**: List key files/dirs that should be present\n\n### Pre-Reset Content Verification\n\n**BEFORE `git reset --hard &lt;target&gt;`, ALWAYS:**\n\n1. **Fetch target**: `git fetch origin &lt;target&gt;` (if branch exists on remote)\n2. **Preview changes**: `git diff --stat HEAD..&lt;target&gt;`\n3. **Verify expected work present**:\n   - If expecting new directories/files: `git ls-tree -r &lt;target&gt; --name-only | grep &lt;expected-path&gt;`\n4. **Check unpushed work**: `git log --branches --not --remotes --oneline`\n5. **Confirm with user**: Show exactly what will be lost/gained\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#3-branch-staleness-detection-script","title":"3. Branch Staleness Detection Script","text":"<p>Created <code>scripts/git-check-staleness.sh</code>:</p> <pre><code>#!/bin/bash\n# Detect if local branch reference is stale compared to remote\n\nbranch=\"${1:-HEAD}\"\ngit fetch origin \"$branch\" 2&gt;/dev/null\n\nlocal_sha=$(git rev-parse \"$branch\" 2&gt;/dev/null)\nremote_sha=$(git rev-parse \"origin/$branch\" 2&gt;/dev/null)\n\nif [ -n \"$remote_sha\" ] &amp;&amp; [ \"$local_sha\" != \"$remote_sha\" ]; then\n    echo \"\u26a0\ufe0f  WARNING: Local branch '$branch' is STALE!\"\n    echo \"   Local:  $local_sha\"\n    echo \"   Remote: $remote_sha\"\n    echo \"\"\n    echo \"   Update first: git checkout $branch &amp;&amp; git pull\"\n    exit 1\nfi\n\necho \"\u2713 Branch '$branch' is up-to-date\"\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#4-git-workflow-documentation","title":"4. Git Workflow Documentation","text":"<p>Created <code>docs/development/git-workflow.md</code>:</p> <ul> <li>Safe workflow patterns</li> <li>Recovery procedures</li> <li>Incident documentation</li> <li>Pre-checkout hooks</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#5-git-safety-aliases","title":"5. Git Safety Aliases","text":"<p>Configured globally:</p> <pre><code>git config --global alias.check-unpushed 'log --branches --not --remotes --oneline'\ngit config --global alias.safe-reset '!f() { echo \"\u26a0\ufe0f DANGER...\"; read -p \"Type YES: \" confirm; ...; }; f'\ngit config --global alias.safe-push '!f() { git log --branches --not --remotes &amp;&amp; git push \"$@\"; }; f'\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#6-pre-checkout-hook","title":"6. Pre-Checkout Hook","text":"<p>Installed at <code>.git/hooks/pre-checkout</code>:</p> <pre><code>#!/bin/bash\n# Warn when switching branches with unpushed commits\ncurrent_branch=$(git symbolic-ref --short HEAD)\nunpushed=$(git log --branches --not --remotes --oneline)\n\nif [ -n \"$unpushed\" ]; then\n    echo \"\u26a0\ufe0f WARNING: Unpushed commits on '$current_branch'\"\n    read -p \"Continue? (y/N) \" -n 1 -r\n    [[ ! $REPLY =~ ^[Yy]$ ]] &amp;&amp; exit 1\nfi\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#7-workflow-improvements","title":"7. Workflow Improvements","text":"<p>Recommendations for preventing similar incidents:</p> <ul> <li>Always fetch after remote PR merges before using branch references in local operations</li> <li>Always push branches immediately after creation with <code>git push -u origin &lt;branch&gt;</code></li> <li>Use feature branches merged to main instead of version branches as development locations</li> <li>Prefer merge workflows over reset workflows for integrating changes to main</li> <li>Verify expected content present before destructive operations (check key files/directories)</li> <li>Check branch staleness using <code>scripts/git-check-staleness.sh &lt;branch&gt;</code> before reset operations</li> <li>Enable branch protection on main to prevent direct pushes and force pushes</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#lessons-learned","title":"Lessons Learned","text":""},{"location":"development/incident-reports/2025-12-07-git-recovery/#what-went-well","title":"What Went Well","text":"<p>\u2705 Git reflog preserved work: All commits still accessible locally \u2705 Quick identification: Issue discovered within hours \u2705 Complete recovery: 100% of work recovered with no data loss \u2705 Comprehensive forensics: Thorough analysis ensured no missed work \u2705 Documentation: Full incident documentation for future reference</p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#what-could-be-improved","title":"What Could Be Improved","text":"<p>\u274c No branch protection: Main branch had no safeguards \u274c No post-merge fetch protocol: Local branches not updated after remote PR merges \u274c No content verification: Did not verify expected files present before reset \u274c Git semantics gap: Agent/user didn't understand branch names use local refs, not remote state \u274c Confusing workflow: Using version branches as primary development location \u274c No pre-push verification: Branches not pushed during development \u274c No safety checks: No verification before destructive git operations</p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#action-items","title":"Action Items","text":"<p>Immediate (Completed):</p> <ul> <li> Add git safety rules to <code>.claude/CLAUDE.md</code></li> <li> Create git workflow documentation</li> <li> Install pre-checkout hooks</li> <li> Configure git safety aliases</li> <li> Correct v0.2.0 tag</li> <li> Document incident</li> </ul> <p>Short-term (Recommended):</p> <ul> <li> Enable branch protection on main (require PR reviews)</li> <li> Standardize release workflow (avoid version branches)</li> <li> Add pre-commit hook to check for unpushed work</li> <li> Document tag vs branch naming conventions</li> </ul> <p>Long-term (Future consideration):</p> <ul> <li> Automated backups of git reflog</li> <li> CI checks for orphaned commits</li> <li> Team training on git recovery procedures</li> <li> Feature request to Claude Code team: Session audit logging (see Appendix B)</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#appendix-a-claude-code-session-audit-logging-feature-request","title":"Appendix A: Claude Code Session Audit Logging (Feature Request)","text":""},{"location":"development/incident-reports/2025-12-07-git-recovery/#problem","title":"Problem","text":"<p>This incident revealed a critical gap: no audit trail of commands executed by Claude Code sessions. Without shell command history, we cannot definitively determine: - What exact git command caused the incident - The sequence of operations leading to data loss - Whether the issue was human error, agent error, or tool misconfiguration</p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#proposed-solution","title":"Proposed Solution","text":"<p>Add opt-in session audit logging to Claude Code with two simple log streams:</p> <p>1. File Operation Log (<code>~/.claude/audit/file-operations.jsonl</code>): <pre><code>{\"timestamp\": \"2025-12-07T19:37:42-08:00\", \"session_id\": \"abc123\", \"operation\": \"write\", \"path\": \"/path/to/file.ts\", \"tool\": \"Write\"}\n{\"timestamp\": \"2025-12-07T19:37:45-08:00\", \"session_id\": \"abc123\", \"operation\": \"edit\", \"path\": \"/path/to/file.ts\", \"tool\": \"Edit\"}\n</code></pre></p> <p>2. Shell Command Log (<code>~/.claude/audit/shell-commands.jsonl</code>): <pre><code>{\"timestamp\": \"2025-12-07T19:37:50-08:00\", \"session_id\": \"abc123\", \"command\": \"git status\", \"cwd\": \"/path/to/repo\", \"exit_code\": 0}\n{\"timestamp\": \"2025-12-07T19:37:51-08:00\", \"session_id\": \"abc123\", \"command\": \"git reset --hard version-0.2.0\", \"cwd\": \"/path/to/repo\", \"exit_code\": 0}\n</code></pre></p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#configuration","title":"Configuration","text":"<p>Add to <code>claude-code-settings.json</code>: <pre><code>{\n  \"auditLog\": {\n    \"enabled\": true,\n    \"path\": \"~/.claude/audit\",\n    \"logFileOperations\": true,\n    \"logShellCommands\": true,\n    \"maxSizeMB\": 100,\n    \"retentionDays\": 30\n  }\n}\n</code></pre></p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#benefits","title":"Benefits","text":"<ul> <li>Incident forensics: Reconstruct exact sequence of operations</li> <li>Security auditing: Track what AI agents are doing</li> <li>Debugging: Understand why operations failed</li> <li>Compliance: Meet audit requirements for regulated environments</li> <li>Privacy-preserving: No file contents logged, only metadata</li> <li>Opt-in: Users control whether logging is enabled</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Use newline-delimited JSON (JSONL) for easy parsing and streaming</li> <li>Log only metadata (timestamps, paths, commands) - no sensitive data</li> <li>Implement log rotation to prevent unbounded growth</li> <li>No performance impact (async logging)</li> <li>Compatible with existing <code>cleanupPeriodDays</code> setting</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#why-this-matters","title":"Why This Matters","text":"<p>This incident would have been immediately diagnosable with command logging. Instead, we spent hours on forensics and still cannot determine root cause. Simple audit logging would prevent this ambiguity for all future incidents.</p>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#technical-details","title":"Technical Details","text":""},{"location":"development/incident-reports/2025-12-07-git-recovery/#commands-used-in-recovery","title":"Commands Used in Recovery","text":"<pre><code># Discovery\ngit reflog --all | head -n 50\ngit fsck --lost-found --no-reflogs\ngit log --all --oneline --grep=\"prompt\"\n\n# Analysis\ngit show c6532f5 --stat\ngit diff --stat HEAD c6532f5\ngit ls-tree -r c6532f5 --name-only | grep prompt_system\n\n# Recovery\ngit checkout -b recovery/prompt-system-refactor\ngit checkout c6532f5 -- [files...]\ngit add .\ngit commit -m \"feat: Recover prompt system...\"\ngit push -u origin recovery/prompt-system-refactor\n\n# Tag correction\ngit tag -d v0.2.0\ngit push origin :refs/tags/v0.2.0\ngit tag -a v0.2.0 -m \"Release v0.2.0 (Corrected)...\"\ngit push origin v0.2.0\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#files-recovered","title":"Files Recovered","text":"<p>Complete list in PR #13: https://github.com/aaronksolomon/tnh-scholar/pull/13</p> <p>Summary:</p> <ul> <li>35 new files</li> <li>4 modified files</li> <li>4,508 insertions</li> <li>130 deletions</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#references","title":"References","text":""},{"location":"development/incident-reports/2025-12-07-git-recovery/#related-documents","title":"Related Documents","text":"<ul> <li>Git Workflow Guide - Safe git practices</li> <li>PR #13 - Recovery PR</li> <li>ADR-PT04 - Recovered work</li> <li>Tag Correction Plan (v0.2.0 tag fix, documentation pending)</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#git-commits","title":"Git Commits","text":"<ul> <li><code>c6532f5</code> - Original prompt system implementation (orphaned)</li> <li><code>87b6603</code> - PR #11 merge commit (version-0.1.4 branch)</li> <li><code>5bf012d</code> - PR #12 merge commit (version-0.2.0 branch)</li> <li><code>e65ac7d</code> - Recovery commit</li> <li><code>a6523a9</code> - PR #13 merge to main</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#github-events","title":"GitHub Events","text":"<ul> <li>PR #11: https://github.com/aaronksolomon/tnh-scholar/pull/11</li> <li>PR #12: https://github.com/aaronksolomon/tnh-scholar/pull/12</li> <li>PR #13: https://github.com/aaronksolomon/tnh-scholar/pull/13</li> </ul>"},{"location":"development/incident-reports/2025-12-07-git-recovery/#appendix-orphaned-commits-sample","title":"Appendix: Orphaned Commits Sample","text":"<p>First 10 orphaned commits from <code>git fsck</code>:</p> <pre><code>0b019ed - codex snapshot (2025-12-06 21:01) - Partial prompt system\n318192e - codex snapshot (2025-11-28 16:10)\n8001092 - codex snapshot (2025-11-23 21:02)\n5c0283d - codex snapshot (2025-11-20 15:07)\n1c8498b - codex snapshot (2025-11-22 07:37)\n72840c1 - [old development work]\n8104125 - [old development work]\n65050e9 - [old development work]\n878664d - [old development work]\n8886ed2 - [old development work]\n</code></pre> <p>Total: 173 orphaned commits (mostly historical, no unique work lost)</p> <p>Report prepared by: Claude Sonnet 4.5 Reviewed by: Aaron Solomon (phapman) Date: December 7, 2025 Status: Final</p>"},{"location":"development/incident-reports/2025-12-07-reference/","title":"2025 12 07 Reference","text":"<p>Table of Contents:</p> <p>forensic-analysis</p> <p>Implementation Summary: Git Safety Improvements - Summary of remediation work completed after the 2025-12-07 git recovery incident.</p> <p>incident-report-updates</p> <p>This file auto-generated.</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/","title":"Forensic Analysis: December 7, 2025 Git Data Loss Incident","text":"<p>Analyst: Claude Sonnet 4.5 Date: December 8, 2025 Evidence Sources:</p> <ul> <li>Session log: <code>convo-with-hard-reset.txt</code></li> <li>Incident report: <code>docs/development/incident-reports/2025-12-07-git-recovery.md</code></li> <li>Git reflog analysis</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#executive-summary","title":"Executive Summary","text":"<p>Root Cause Identified: The data loss was caused by resetting <code>main</code> to a branch reference (<code>version-0.2.0</code>) that did NOT yet contain the prompt system work, instead of waiting for the work to be merged to that branch first.</p> <p>Critical Timeline Error:</p> <ul> <li>The <code>git reset --hard version-0.2.0</code> occurred at line 396 of the session</li> <li>At that moment, PR #12 (which would merge the prompt system work into <code>version-0.2.0</code>) had only just been created at line 316</li> <li>PR #12 was not merged until line 321 (\"done!\")</li> <li>This means main was reset to version-0.2.0 BEFORE version-0.2.0 received the prompt system work</li> </ul> <p>Severity: High - 39 files (4,508 lines) orphaned Impact: No permanent data loss (recovered via reflog) Prevention Status: Partially addressed; additional safeguards needed</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#detailed-timeline-reconstruction","title":"Detailed Timeline Reconstruction","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#phase-1-pr-creation-and-merge-sequence-lines-251-321","title":"Phase 1: PR Creation and Merge Sequence (Lines 251-321)","text":"Line Event Status 251 User merges PR #11 (<code>prompt-system-refactor</code> \u2192 <code>version-0.1.4</code>) \u2705 Complete 252 Claude recognizes need to merge 0.1.4 \u2192 0.2.0 \u2705 Correct 268-316 Claude creates PR #12 (<code>version-0.1.4</code> \u2192 <code>version-0.2.0</code>) \u2705 PR Created 321 User: \"done! next is the complex main work.\" \u26a0\ufe0f PR #12 merged <p>Key Point: At line 321, PR #12 is merged, so <code>version-0.2.0</code> branch NOW contains the prompt system work.</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#phase-2-main-branch-analysis-lines-322-378","title":"Phase 2: Main Branch Analysis (Lines 322-378)","text":"Line Event Analysis 322-378 Claude analyzes divergence between <code>main</code> and <code>version-0.2.0</code> Correct process 368-378 Finds common ancestor <code>8e82804</code>, counts divergent commits Good forensics 379-391 Proposes reset workflow \u26a0\ufe0f Workflow correct IF executed properly <p>Key Point: The analysis at this stage is correct - <code>version-0.2.0</code> has all the work after PR #12 merged.</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#phase-3-the-fatal-error-lines-392-398","title":"Phase 3: THE FATAL ERROR (Lines 392-398)","text":"<pre><code>Line 392: User: \"agreed! proceed.\"\nLine 393: Claude: \"Perfect! Let me proceed with resetting main to version-0.2.0:\"\nLine 394-396:\n    Bash Reset main to version-0.2.0\n    IN: git reset --hard version-0.2.0\nLine 397-398:\n    OUT: HEAD is now at d51fc87 feat: Automate PyPI README frontmatter stripping\n</code></pre> <p>CRITICAL FINDING:</p> <p>The reset moved main to commit <code>d51fc87</code> - but this commit is the version bump commit from line 363:</p> <pre><code>Line 363: d51fc87 feat: Automate PyPI README frontmatter stripping in release workflow\nLine 364: 0d7d459 chore: Bump version to 0.2.0\n</code></pre> <p>This commit <code>d51fc87</code> is BEFORE the PR #12 merge that added prompt system work!</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#root-cause-analysis","title":"Root Cause Analysis","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#the-critical-mistake","title":"The Critical Mistake","text":"<p>What the agent THOUGHT it was doing:</p> <ul> <li>Resetting <code>main</code> to the branch <code>version-0.2.0</code> which (after PR #12 merge at line 321) should contain all the prompt system work</li> </ul> <p>What ACTUALLY happened:</p> <ul> <li>The local git reference <code>version-0.2.0</code> had NOT been updated after the remote PR #12 merge</li> <li>The local branch still pointed to commit <code>d51fc87</code> (the version bump, before the merge)</li> <li>Therefore <code>git reset --hard version-0.2.0</code> moved main to the pre-merge state</li> <li>This orphaned all the work that was ONLY accessible via the now-deleted branches</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#the-missing-step","title":"The Missing Step","text":"<p>What should have been done between line 391 and line 396:</p> <pre><code># REQUIRED: Update local branch reference after remote merge\ngit fetch origin version-0.2.0\ngit checkout version-0.2.0\ngit pull origin version-0.2.0\n\n# THEN verify the prompt system work is present\ngit log --oneline -10\nls -la src/tnh_scholar/prompt_system/  # Verify directory exists\n\n# ONLY THEN proceed with reset\ngit checkout main\ngit reset --hard version-0.2.0\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#why-this-happened-conceptual-confusion","title":"Why This Happened: Conceptual Confusion","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#understanding-branch-references","title":"Understanding Branch References","text":"<p>The agent made a fundamental error about git branch references:</p> <ol> <li>After a PR merge on GitHub: The remote branch <code>origin/version-0.2.0</code> updates</li> <li>Local branch reference unchanged: The local <code>version-0.2.0</code> still points to old commit</li> <li>Reset uses local reference: <code>git reset --hard version-0.2.0</code> uses the local, outdated reference</li> <li>Result: Reset to wrong commit, orphaning all new work</li> </ol>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#the-tag-vs-branch-confusion-red-herring","title":"The Tag vs Branch Confusion (Red Herring)","text":"<p>The incident report focuses heavily on \"tag vs branch confusion\" but the session log proves this was NOT the issue:</p> <ul> <li>The agent correctly used <code>version-0.2.0</code> (branch name, no <code>v</code> prefix)</li> <li>The agent correctly distinguished from tag <code>v0.2.0</code></li> <li>The command <code>git reset --hard version-0.2.0</code> was syntactically correct</li> </ul> <p>The real issue: Using a stale local branch reference without fetching after remote merge.</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#contributing-factors","title":"Contributing Factors","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#1-workflow-complexity","title":"1. Workflow Complexity","text":"<p>The multi-branch merge sequence created multiple opportunities for error:</p> <ul> <li>PR #11: <code>prompt-system-refactor</code> \u2192 <code>version-0.1.4</code></li> <li>PR #12: <code>version-0.1.4</code> \u2192 <code>version-0.2.0</code></li> <li>Reset: <code>main</code> to <code>version-0.2.0</code></li> </ul> <p>Each step assumed the previous step's remote changes were reflected locally.</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#2-implicit-assumptions","title":"2. Implicit Assumptions","text":"<p>The agent assumed that:</p> <ul> <li>After user said \"done!\" (line 321), the branch reference was up-to-date</li> <li>The branch name <code>version-0.2.0</code> would automatically point to the merged state</li> <li>No fetch/pull was needed between remote merge and local reset</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#3-lack-of-verification","title":"3. Lack of Verification","text":"<p>No verification step between the merge and reset:</p> <ul> <li>No <code>git fetch</code> before reset</li> <li>No <code>git log</code> to verify prompt_system commits present</li> <li>No file system check (<code>ls src/tnh_scholar/prompt_system/</code>)</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#4-agent-safety-rules-incomplete","title":"4. Agent Safety Rules Incomplete","text":"<p>The existing git safety rules (from incident report, lines 284-299) don't cover:</p> <ul> <li>Fetch requirement: Always fetch before using branch references after remote operations</li> <li>Post-merge verification: Verify work is present before destructive operations</li> <li>State validation: Check that local branch matches expected remote state</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#what-saved-us","title":"What Saved Us","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#recovery-factors","title":"Recovery Factors","text":"<ol> <li>Git Reflog: All commits preserved locally (30-90 day retention)</li> <li>Quick Discovery: Issue found within hours</li> <li>User Expertise: User recognized orphaned commits and knew reflog recovery</li> <li>No Local Cleanup: Local repo still had commit <code>c6532f5</code> in reflog</li> </ol>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#lucky-breaks","title":"Lucky Breaks","text":"<ol> <li>Branches not yet deleted from remote: PR branches auto-delete after 3 hours, incident happened within window</li> <li>Backup-main existed: Though not needed, provided psychological safety</li> <li>Work was pushed: PRs #11 and #12 pushed work to remote, providing alternative recovery path</li> </ol>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#critical-gaps-in-safeguards","title":"Critical Gaps in Safeguards","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#gap-1-no-post-remote-merge-fetch-requirement","title":"Gap 1: No Post-Remote-Merge Fetch Requirement","text":"<p>Current git safety rules DO NOT require:</p> <ul> <li>Fetching before using branch references</li> <li>Verifying local branch matches remote after remote operations</li> </ul> <p>Proposed addition:</p> <pre><code>**ALWAYS fetch and verify after remote PR merges:**\n- Run `git fetch origin &lt;branch&gt;` before using branch in local operations\n- Run `git log &lt;branch&gt; --oneline -5` to verify expected commits present\n- Verify key directories/files exist if doing destructive operations\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#gap-2-no-pre-reset-content-verification","title":"Gap 2: No Pre-Reset Content Verification","text":"<p>Current rules check for unpushed work but NOT for expected content:</p> <p>Proposed addition:</p> <pre><code>**Before git reset --hard, verify target contains expected work:**\n- List key files/directories that should be present after reset\n- Use `git diff --name-only HEAD..&lt;target&gt;` to preview what will change\n- If adding work (like prompt_system/), verify target has those files:\n  `git ls-tree -r &lt;target&gt; --name-only | grep &lt;expected-path&gt;`\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#gap-3-no-branch-staleness-detection","title":"Gap 3: No Branch Staleness Detection","text":"<p>No way to detect if local branch reference is stale:</p> <p>Proposed addition:</p> <pre><code>**Check local branch staleness before destructive operations:**\n```bash\n# Compare local vs remote branch\ngit fetch origin\nLOCAL=$(git rev-parse &lt;branch&gt;)\nREMOTE=$(git rev-parse origin/&lt;branch&gt;)\nif [ \"$LOCAL\" != \"$REMOTE\" ]; then\n    echo \"\u26a0\ufe0f WARNING: Local branch is stale!\"\n    echo \"Local:  $LOCAL\"\n    echo \"Remote: $REMOTE\"\n    read -p \"Update local branch first? (Y/n) \" confirm\nfi\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#recommended-safeguards","title":"Recommended Safeguards","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#immediate-high-priority","title":"Immediate (High Priority)","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#1-enhanced-git-safety-rules","title":"1. Enhanced Git Safety Rules","text":"<p>Add to <code>.claude/CLAUDE.md</code>:</p> <pre><code>### Post-Merge Safety Protocol\n\n**CRITICAL: After ANY GitHub PR merge, ALWAYS:**\n1. Fetch the branch: `git fetch origin &lt;branch&gt;`\n2. Verify the merge commit is present: `git log origin/&lt;branch&gt; --oneline -5`\n3. If using branch for reset/merge, check local vs remote: `git rev-parse &lt;branch&gt;` vs `git rev-parse origin/&lt;branch&gt;`\n4. Update local branch if stale: `git checkout &lt;branch&gt; &amp;&amp; git pull`\n\n### Pre-Reset Verification Protocol\n\n**BEFORE `git reset --hard &lt;target&gt;`, ALWAYS:**\n1. Verify target exists and is up-to-date: `git fetch &amp;&amp; git rev-parse &lt;target&gt;`\n2. Preview changes: `git diff --stat HEAD..&lt;target&gt;`\n3. If expecting new work, verify it's in target: `git ls-tree -r &lt;target&gt; --name-only | grep &lt;expected-path&gt;`\n4. Check for unpushed work: `git log --branches --not --remotes`\n5. Confirm with user showing exactly what will be lost/gained\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#2-pre-reset-hook-enhancement","title":"2. Pre-Reset Hook Enhancement","text":"<p>Update <code>.git/hooks/pre-reset</code> (if feasible) or add to automation:</p> <pre><code>#!/bin/bash\n# Check if target branch reference is stale\n\ntarget=\"$1\"\nif git show-ref --verify --quiet \"refs/heads/$target\"; then\n    git fetch origin \"$target\" 2&gt;/dev/null\n    local_sha=$(git rev-parse \"$target\" 2&gt;/dev/null)\n    remote_sha=$(git rev-parse \"origin/$target\" 2&gt;/dev/null)\n\n    if [ -n \"$remote_sha\" ] &amp;&amp; [ \"$local_sha\" != \"$remote_sha\" ]; then\n        echo \"\u26a0\ufe0f  ERROR: Local branch '$target' is stale!\"\n        echo \"   Local:  $local_sha\"\n        echo \"   Remote: $remote_sha\"\n        echo \"\"\n        echo \"   Update first: git checkout $target &amp;&amp; git pull\"\n        exit 1\n    fi\nfi\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#short-term-medium-priority","title":"Short-term (Medium Priority)","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#3-workflow-simplification","title":"3. Workflow Simplification","text":"<p>Avoid complex multi-branch merge chains:</p> <ul> <li>Prefer: Feature branch \u2192 main (direct)</li> <li>Avoid: Feature \u2192 version-0.1.4 \u2192 version-0.2.0 \u2192 main (complex)</li> </ul> <p>Recommendation: Use GitHub's merge queue or single-step merges to main</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#4-agent-prompt-enhancement","title":"4. Agent Prompt Enhancement","text":"<p>Add to Claude Code system prompt for git operations:</p> <pre><code>**Git Branch Reference Safety:**\n- Branch names in git commands refer to LOCAL references, not remote state\n- After any GitHub PR merge, local branches are STALE until fetched\n- ALWAYS run `git fetch origin &lt;branch&gt;` before using branch in reset/merge\n- NEVER assume local branch = remote branch without explicit fetch\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#long-term-lower-priority","title":"Long-term (Lower Priority)","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#5-automated-pre-flight-checks","title":"5. Automated Pre-Flight Checks","text":"<p>Create <code>scripts/safe-reset.sh</code>:</p> <pre><code>#!/bin/bash\n# Safe git reset with verification\ntarget=\"$1\"\ngit fetch origin\n# ... verification logic ...\ngit reset --hard \"$target\"\n</code></pre> <p>Configure agent to use this instead of direct <code>git reset --hard</code></p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#6-session-state-persistence","title":"6. Session State Persistence","text":"<p>Track session state across commands:</p> <ul> <li>Record when PRs are merged (remote state change)</li> <li>Mark local branches as \"needs fetch\" after remote changes</li> <li>Require fetch before using marked branches</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#incident-report-updates-needed","title":"Incident Report Updates Needed","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#section-why-it-happened-root-cause-unknown","title":"Section: \"Why It Happened (Root Cause: Unknown)\"","text":"<p>Current text (line 80):</p> <p>We cannot definitively determine the exact git command that caused the incident.</p> <p>Should be updated to:</p> <p>Root Cause Identified: The incident was caused by resetting <code>main</code> to a stale local branch reference. The command <code>git reset --hard version-0.2.0</code> used the local branch reference that pointed to commit <code>d51fc87</code> (version bump commit), while the remote <code>origin/version-0.2.0</code> had been updated with PR #12 merge to include prompt system work at commit <code>5bf012d</code>. The agent failed to fetch the branch after the remote PR merge, causing the reset to use outdated state.</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#section-contributing-factors","title":"Section: \"Contributing Factors\"","text":"<p>Add:</p> <ul> <li>Stale branch reference: Local <code>version-0.2.0</code> not updated after remote PR #12 merge</li> <li>No post-merge fetch: Agent proceeded with reset immediately after user confirmed merge without fetching</li> <li>No content verification: Agent did not verify prompt_system work was present in reset target</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#section-tag-vs-branch-confusion","title":"Section: Tag vs Branch Confusion","text":"<p>Revise (lines 92-98):</p> <p>This section can be removed or significantly reduced. The session log proves the agent correctly used the branch name <code>version-0.2.0</code> (not the tag <code>v0.2.0</code>). The issue was NOT tag vs branch confusion, but rather using a stale local branch reference.</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#lessons-learned-additional","title":"Lessons Learned (Additional)","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#what-went-wrong-not-previously-documented","title":"What Went Wrong (Not Previously Documented)","text":"<p>\u274c Stale branch reference: Reset used local reference without fetching after remote merge \u274c No post-merge verification: Did not verify prompt_system work was in target before reset \u274c Implicit assumptions: Assumed branch name would point to merged state without explicit fetch \u274c Incomplete safety rules: Existing rules did not cover post-remote-merge fetch requirements</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#new-understanding","title":"New Understanding","text":"<p>\ud83d\udd0d Git branch references are local: Branch names in git commands use local refs, not remote state \ud83d\udd0d Remote merges don't update local: GitHub PR merges update <code>origin/branch</code>, not local <code>branch</code> \ud83d\udd0d Fetch is not optional: After remote changes, fetch is REQUIRED before using branch references \ud83d\udd0d Content verification crucial: Before destructive operations, verify expected files/commits present</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#testing-the-fix","title":"Testing the Fix","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#simulation-test","title":"Simulation Test","text":"<p>To verify safeguards prevent recurrence:</p> <pre><code># Simulate the incident scenario\ngit checkout -b test-main\ngit reset --hard &lt;some-old-commit&gt;\n\n# Simulate remote PR merge (don't fetch)\n# Now try reset WITHOUT fetch - should be blocked by new safeguards\n\ngit reset --hard &lt;branch-name&gt;  # Should fail or warn about staleness\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#expected-behavior-with-safeguards","title":"Expected Behavior with Safeguards","text":"<ol> <li>Pre-reset hook: Detects local branch is behind remote, blocks operation</li> <li>Agent prompt: Requires fetch before reset, verifies content present</li> <li>User confirmation: Shows diff of what will change, requires explicit approval</li> </ol>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#conclusion","title":"Conclusion","text":""},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#the-smoking-gun","title":"The Smoking Gun","text":"<p>Line 396: <code>git reset --hard version-0.2.0</code> used a stale local branch reference that pointed to commit <code>d51fc87</code>, orphaning all work that had been merged to the remote branch via PR #12.</p>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#the-fix","title":"The Fix","text":"<p>Required action between line 321 (PR merge) and line 396 (reset):</p> <pre><code>git fetch origin version-0.2.0\ngit log origin/version-0.2.0 --oneline -10  # Verify merge commit present\ngit checkout version-0.2.0\ngit pull  # Update local reference\n# NOW safe to reset main to version-0.2.0\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/forensic-analysis/#safeguard-priority","title":"Safeguard Priority","text":"<p>Critical (implement immediately):</p> <ul> <li>Post-merge fetch requirement in git safety rules</li> <li>Pre-reset content verification protocol</li> <li>Branch staleness detection</li> </ul> <p>Important (implement soon):</p> <ul> <li>Workflow simplification to reduce merge chain complexity</li> <li>Agent prompt enhancement about branch reference semantics</li> </ul> <p>Nice-to-have (future):</p> <ul> <li>Automated safe-reset script</li> <li>Session state tracking for branch staleness</li> </ul> <p>Analysis complete. All evidence points to stale branch reference as root cause.</p>"},{"location":"development/incident-reports/2025-12-07-reference/implementation-summary/","title":"Implementation Summary: Git Safety Improvements","text":"<p>Summary of the work completed after the 2025-12-07 Git Recovery incident, including documentation updates and automation to prevent recurrence.</p>"},{"location":"development/incident-reports/2025-12-07-reference/implementation-summary/#what-was-done","title":"What Was Done","text":""},{"location":"development/incident-reports/2025-12-07-reference/implementation-summary/#1-updated-incident-report","title":"1. Updated Incident Report \u2705","text":"<p>File: /development/incident-reports/2025-12-07-git-recovery.md</p> <p>Key updates: - Revised executive summary and timeline with verified commands. - Rewrote root cause analysis using forensic evidence. - Added Post-Remote-Merge Safety Protocol and Branch Staleness Detection Script to preventative measures. - Expanded workflow improvements with concrete safeguards.</p> <p>Key Finding Documented:</p> <p>The incident was caused by resetting <code>main</code> to a stale local branch reference that had not been updated after a remote PR merge.</p>"},{"location":"development/incident-reports/2025-12-07-reference/implementation-summary/#2-enhanced-git-safety-rules","title":"2. Enhanced Git Safety Rules \u2705","text":"<p>File: <code>~/.claude/CLAUDE.md</code></p> <ul> <li>Added Post-Remote-Merge Safety Protocol (fetch, verify merge commit, update local, verify content).</li> <li>Added Pre-Reset Content Verification (fetch, diff, verify, check for unpushed work, confirm).</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/implementation-summary/#3-created-branch-staleness-detection-script","title":"3. Created Branch Staleness Detection Script \u2705","text":"<p>File: scripts/git-check-staleness.sh</p> <p>Features: - Detects if a local branch reference is stale compared to remote. - Shows ahead/behind commit counts with color-coded output. - Provides update instructions and returns exit code 0 (fresh) or 1 (stale).</p> <p>Example usage:</p> <pre><code>./scripts/git-check-staleness.sh &lt;branch-name&gt;\n./scripts/git-check-staleness.sh  # checks current branch\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/implementation-summary/#testing-performed","title":"Testing Performed","text":"<ul> <li>Markdown linting for updated docs.</li> <li>Manual validation of CLAUDE.md instructions.</li> <li>Verified staleness script on up-to-date and stale branches; exit codes correct.</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/implementation-summary/#how-these-changes-prevent-recurrence","title":"How These Changes Prevent Recurrence","text":""},{"location":"development/incident-reports/2025-12-07-reference/implementation-summary/#the-original-mistake-simplified","title":"The Original Mistake (Simplified)","text":"<ol> <li>Merge PR on GitHub (remote updates).</li> <li>Run <code>git reset --hard &lt;branch&gt;</code> using stale local ref.</li> <li>Work lost because local ref lacked merged commits.</li> </ol>"},{"location":"development/incident-reports/2025-12-07-reference/implementation-summary/#with-new-safeguards","title":"With New Safeguards","text":"<ol> <li>Fetch remote before using branch references.</li> <li>Verify merge presence and compare SHAs.</li> <li>Run staleness check before destructive commands.</li> <li>Proceed only after showing user a preview of the target state.</li> </ol> <p>Multiple layers now prevent loss: process (protocols), verification (diffs), automation (staleness script), and clear documentation.</p>"},{"location":"development/incident-reports/2025-12-07-reference/implementation-summary/#related-artifacts","title":"Related Artifacts","text":"<ul> <li>Forensic Analysis</li> <li>Incident Report Updates Guide</li> <li>Session Log Compaction Summary</li> <li>Session Conversation Log</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/","title":"Proposed Updates to Incident Report","text":"<p>Based on forensic analysis of session log <code>convo-with-hard-reset.txt</code></p>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#1-update-executive-summary-line-22","title":"1. Update Executive Summary (Line 22)","text":"<p>Current:</p> <p>Root Cause: Reset to TAG v0.2.0 instead of merging BRANCH version-0.2.0</p> <p>Proposed:</p> <p>Root Cause: Reset to stale local branch reference - local <code>version-0.2.0</code> not fetched after remote PR merge</p>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#2-update-timeline-line-46-49","title":"2. Update Timeline (Line 46-49)","text":"<p>Current: <pre><code>- **19:37**: **INCIDENT**: Destructive git operation executed on main (exact command unknown)\n  - Git reflog shows: \"reset: moving to version-0.2.0\"\n  - Main branch did not include prompt system work after operation\n  - This orphaned all prompt system work (commit `c6532f5` and descendants)\n</code></pre></p> <p>Proposed: <pre><code>- **19:37**: **INCIDENT**: Reset main to stale branch reference\n  - Command: `git reset --hard version-0.2.0` (session log line 396)\n  - Problem: Local `version-0.2.0` not fetched after remote PR #12 merge\n  - Local branch pointed to `d51fc87` (version bump), not `5bf012d` (merged state)\n  - Missing step: `git fetch origin version-0.2.0` before reset\n  - This orphaned all prompt system work (commit `c6532f5` and descendants)\n</code></pre></p>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#3-rewrite-root-cause-section-lines-80-106","title":"3. Rewrite Root Cause Section (Lines 80-106)","text":"<p>Replace entire section \"Why It Happened (Root Cause: Unknown)\" with:</p>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#why-it-happened-root-cause-stale-branch-reference","title":"Why It Happened (Root Cause: Stale Branch Reference)","text":"<p>Root Cause Identified: The incident was caused by resetting <code>main</code> to a stale local branch reference that had not been updated after a remote PR merge.</p> <p>The Fatal Sequence (from session log):</p> <ol> <li>Line 316: PR #12 created (<code>version-0.1.4</code> \u2192 <code>version-0.2.0</code>)</li> <li>Line 321: User confirms \"done!\" - PR #12 merged on GitHub</li> <li>Remote <code>origin/version-0.2.0</code> now points to <code>5bf012d</code> (includes prompt system)</li> <li>Local <code>version-0.2.0</code> still points to <code>d51fc87</code> (version bump only)</li> <li>Line 396: Agent executes <code>git reset --hard version-0.2.0</code></li> <li>Uses local reference <code>d51fc87</code> (stale)</li> <li>Should have used remote reference <code>5bf012d</code> (current)</li> <li>Result: Reset to pre-merge state, orphaning all new work</li> </ol> <p>The Missing Step:</p> <p>Between the PR merge (line 321) and the reset (line 396), this was required but omitted:</p> <pre><code># REQUIRED: Update local branch after remote merge\ngit fetch origin version-0.2.0\ngit checkout version-0.2.0\ngit pull\n\n# Verify prompt system work is present\ngit log --oneline -10\nls -la src/tnh_scholar/prompt_system/\n\n# THEN safe to reset\ngit checkout main\ngit reset --hard version-0.2.0\n</code></pre> <p>Why This Mistake Happened:</p> <ul> <li>Git semantics misunderstanding: Branch names in git commands refer to local references, not remote state</li> <li>Implicit assumption: Agent assumed <code>version-0.2.0</code> would automatically point to merged state after user said \"done\"</li> <li>No verification: No check that local branch matched remote branch before reset</li> <li>Workflow complexity: Three-tier merge chain (feature \u2192 0.1.4 \u2192 0.2.0 \u2192 main) created multiple opportunities for staleness</li> </ul> <p>Evidence:</p> <p>Session log line 397-398: <pre><code>git reset --hard version-0.2.0\nHEAD is now at d51fc87 feat: Automate PyPI README frontmatter stripping\n</code></pre></p> <p>Commit <code>d51fc87</code> is the version bump commit before PR #12 merge. The prompt system work was in merge commit <code>5bf012d</code>, which was on remote but not in local branch reference.</p> <p>Contributing Factors:</p> <ul> <li>No branch protection on main: Allowed direct reset without review</li> <li>No post-merge fetch protocol: Safety rules didn't require fetch after remote operations</li> <li>No content verification: Didn't verify expected files present in reset target</li> <li>Complex workflow: Multi-branch merge chain increased surface area for errors</li> <li>Missing session logging: Unable to see exact git state at time of operation (though we recovered it from log)</li> </ul>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#4-update-preventative-measures-add-new-section","title":"4. Update Preventative Measures (Add New Section)","text":"<p>Add after line 299 (existing git safety rules):</p>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#2-post-remote-merge-safety-protocol","title":"2. Post-Remote-Merge Safety Protocol","text":"<p>Added to <code>.claude/CLAUDE.md</code>:</p> <pre><code>### Post-Merge Safety Protocol\n\n**CRITICAL: After ANY GitHub PR merge, ALWAYS:**\n\n1. **Fetch the branch**: `git fetch origin &lt;branch&gt;`\n2. **Verify merge commit present**: `git log origin/&lt;branch&gt; --oneline -5`\n3. **Check local vs remote**: Compare `git rev-parse &lt;branch&gt;` vs `git rev-parse origin/&lt;branch&gt;`\n4. **Update local if stale**: `git checkout &lt;branch&gt; &amp;&amp; git pull`\n5. **Verify expected content**: List key files/dirs that should be present\n\n**Example**:\n```bash\n# After PR merges feature \u2192 target-branch on GitHub\ngit fetch origin target-branch\ngit log origin/target-branch --oneline -5  # See merge commit\ngit checkout target-branch\ngit pull\nls -la expected/new/directory/  # Verify content present\n# NOW safe to use target-branch in local operations\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#pre-reset-content-verification","title":"Pre-Reset Content Verification","text":"<p>BEFORE <code>git reset --hard &lt;target&gt;</code>, ALWAYS:</p> <ol> <li>Fetch target: <code>git fetch origin &lt;target&gt;</code> (if branch exists on remote)</li> <li>Preview changes: <code>git diff --stat HEAD..&lt;target&gt;</code></li> <li>Verify expected work present:    <pre><code># If expecting new directories/files\ngit ls-tree -r &lt;target&gt; --name-only | grep &lt;expected-path&gt;\n</code></pre></li> <li>Check unpushed work: <code>git log --branches --not --remotes --oneline</code></li> <li>Confirm with user: Show exactly what will be lost/gained</li> </ol> <p>Example: <pre><code># Before: git reset --hard version-0.2.0\ngit fetch origin version-0.2.0\ngit diff --stat HEAD..version-0.2.0  # Preview changes\ngit ls-tree -r version-0.2.0 --name-only | grep prompt_system  # Verify content\ngit log --branches --not --remotes --oneline  # Check unpushed work\n# Show user the diff and get explicit confirmation\n</code></pre> <pre><code>---\n\n## 5. Update Lessons Learned (Line 359-365)\n\n**Add to \"What Could Be Improved\" section:**\n\n```markdown\n\u274c **No post-merge fetch protocol**: Local branches not updated after remote PR merges\n\u274c **No content verification**: Did not verify expected files present before reset\n\u274c **Git semantics gap**: Agent didn't understand branch names use local refs, not remote state\n\u274c **Insufficient verification**: No check that local branch matched remote before destructive operation\n</code></pre></p>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#6-update-action-items-line-367-390","title":"6. Update Action Items (Line 367-390)","text":"<p>Add to \"Immediate (Completed)\" section:</p> <pre><code>- [x] Add post-remote-merge fetch protocol to `.claude/CLAUDE.md`\n- [x] Add pre-reset content verification protocol\n- [x] Document git branch reference semantics in safety rules\n</code></pre> <p>Add to \"Short-term (Recommended)\" section:</p> <pre><code>- [ ] Create `scripts/safe-reset.sh` wrapper with automatic staleness detection\n- [ ] Add pre-reset hook to detect stale branch references\n- [ ] Simplify workflow: prefer feature \u2192 main (avoid multi-tier merge chains)\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#7-removerevise-tag-vs-branch-confusion-discussion","title":"7. Remove/Revise Tag vs Branch Confusion Discussion","text":"<p>Lines 92-98 (in old \"Root Cause\" section) should be removed or revised:</p> <p>The session log proves the agent correctly used <code>version-0.2.0</code> (branch name) and not <code>v0.2.0</code> (tag name). The issue was NOT tag/branch confusion but rather using a stale local branch reference.</p> <p>If keeping any discussion of naming:</p> <pre><code>**Note on naming**: While tag `v0.2.0` and branch `version-0.2.0` have similar names, the session log confirms the agent correctly used the branch name. The issue was that the local branch reference was stale, not that the wrong ref type was used.\n</code></pre>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#8-update-appendix-b-feature-request-lines-394-455","title":"8. Update Appendix B (Feature Request) - Lines 394-455","text":"<p>Add new section at line 409 (after shell command log example):</p> <pre><code>**3. Branch Staleness Log** (`~/.claude/audit/branch-state.jsonl`):\n```jsonl\n{\"timestamp\": \"2025-12-07T19:37:30-08:00\", \"session_id\": \"abc123\", \"operation\": \"pr_merge_detected\", \"branch\": \"version-0.2.0\", \"local_sha\": \"d51fc87\", \"remote_sha\": \"5bf012d\", \"stale\": true}\n{\"timestamp\": \"2025-12-07T19:37:51-08:00\", \"session_id\": \"abc123\", \"operation\": \"git_reset\", \"target\": \"version-0.2.0\", \"target_sha\": \"d51fc87\", \"warning\": \"target_may_be_stale\"}\n</code></pre> <p>Why This Matters:</p> <p>This incident would have been prevented with branch staleness tracking. The audit log would have shown: 1. Local branch <code>version-0.2.0</code> = <code>d51fc87</code> 2. Remote branch <code>origin/version-0.2.0</code> = <code>5bf012d</code> (after PR #12) 3. WARNING: Using stale local reference in reset operation ```</p>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#summary-of-key-changes","title":"Summary of Key Changes","text":"<ol> <li>Root cause identified: Stale local branch reference (was \"unknown\")</li> <li>Exact command sequence: Documented from session log (was missing)</li> <li>Missing step documented: Need to fetch after remote merge (new finding)</li> <li>Tag confusion removed: Not the actual issue (was red herring)</li> <li>New safeguards added: Post-merge fetch protocol, content verification</li> <li>Agent understanding gap: Document git branch reference semantics</li> </ol>"},{"location":"development/incident-reports/2025-12-07-reference/incident-report-updates/#files-to-update","title":"Files to Update","text":"<ol> <li><code>/development/incident-reports/2025-12-07-git-recovery.md</code> - Main incident report</li> <li><code>.claude/CLAUDE.md</code> - Add post-merge fetch protocol (if not already done)</li> <li><code>/development/git-workflow.md</code> - Add branch staleness section (if exists)</li> </ol> <p>Next Steps: Review forensic analysis and apply updates to incident report as appropriate.</p>"},{"location":"docs-ops/","title":"Docs Ops","text":"<p>Table of Contents:</p> <p>ADR Template - Reusable template for TNH Scholar architecture decision records.</p> <p>Markdown Standards - House style, linting, and structure requirements for TNH Scholar documentation.</p> <p>MkDocs Strict Warning Backlog - Checklist to drive MkDocs builds to zero warnings in strict mode.</p> <p>Preview TNH Scholar Theme - Quick guide to previewing the custom zen theme locally</p> <p>TNH Scholar Theme Design - Zen-inspired documentation theme blending mindfulness aesthetics with modern AI tooling</p> <p>This file auto-generated.</p>"},{"location":"docs-ops/adr-template/","title":"ADR-XXX: Concise Decision Title","text":"<p>One-sentence or short paragraph summarizing the decision.</p> <ul> <li>Filename: <code>adr-&lt;modulecode&gt;&lt;number&gt;-&lt;descriptor&gt;.md</code> (e.g., <code>adr-dd01-docs-reorg.md</code>). Append <code>-strategy</code> for strategy ADRs.</li> <li>Heading: <code># ADR-&lt;MODULECODE&gt;&lt;NUMBER&gt;: Title</code> (uppercase module code for readability).</li> <li>Status: Proposed</li> <li>Date: YYYY-MM-DD</li> <li>Authors: Initial creator of ADR (typically an AI agent or system, plus human initiator/reviewer)</li> <li>Owner: Person or group responsible (typically aaronksolomon (git name), current repo maintainer/builder)</li> </ul>"},{"location":"docs-ops/adr-template/#adr-editing-policy","title":"ADR Editing Policy","text":"<p>IMPORTANT: How you edit this ADR depends on its status (note that the adr status is not the same as the markdown frontmatter status -&gt; see markdown standards for frontmatter status codes.)</p> <ul> <li><code>proposed</code> status: ADR is in the design loop. We may rewrite or edit the document as needed to refine the design.</li> <li><code>accepted</code>, <code>wip</code>,  status: Coding has begun. NEVER edit the original Context/Decision/Consequences sections. Only append addendums (see below).</li> <li>Status transitions: If we need to make significant changes to an ADR in <code>accepted</code> or <code>wip</code> status, and an addendum is insufficient, we should supersede the ADR with a new one. General rule: no edits accept addendums after moving out of proposed.</li> </ul> <p>Rationale: Once implementation begins, the original decision must be preserved for historical context. Changes during/after implementation are tracked as addendums to show the evolution of thinking.</p>"},{"location":"docs-ops/adr-template/#context","title":"Context","text":"<p>Describe the background, forces, and problems that make the decision necessary. Provide enough detail for future readers to understand the environment without reading code.</p>"},{"location":"docs-ops/adr-template/#decision","title":"Decision","text":"<p>State the decision clearly. Use bullet points or sub-headings if the decision has multiple parts (e.g., architecture, tooling, processes).</p>"},{"location":"docs-ops/adr-template/#consequences","title":"Consequences","text":"<ul> <li>Positive: List the benefits or opportunities created by this decision.</li> <li>Negative: Call out trade-offs, risks, or work created.</li> </ul>"},{"location":"docs-ops/adr-template/#alternatives-considered","title":"Alternatives Considered","text":"<p>Summarize other options that were evaluated and why they were rejected (optional but encouraged).</p>"},{"location":"docs-ops/adr-template/#open-questions","title":"Open Questions","text":"<p>Document any follow-up work, unresolved issues, or validation steps to revisit later.</p>"},{"location":"docs-ops/adr-template/#as-built-notes-addendums","title":"As-Built Notes &amp; Addendums","text":"<p>Optional section for post-decision updates. Never edit the original Context/Decision/Consequences sections - always append addendums here to preserve historical decision-making context.</p>"},{"location":"docs-ops/adr-template/#addendum-yyyy-mm-dd-brief-title","title":"Addendum YYYY-MM-DD: Brief Title","text":"<p>Context: Describe what changed or was discovered during implementation.</p> <p>Decision: Document the actual implementation decision or deviation from the original plan.</p> <p>Rationale: Explain why the change was necessary.</p> <p>Implementation Changes: List specific code/config changes.</p> <p>References: Link to related TODOs, issues, or ADRs.</p>"},{"location":"docs-ops/markdown-standards/","title":"Markdown Standards","text":"<p>A consistent Markdown style keeps the documentation easy to navigate, parse, and automate. This guide defines the contract that every <code>.md</code> file in TNH Scholar must follow.</p>"},{"location":"docs-ops/markdown-standards/#file-directory-naming","title":"File &amp; Directory Naming","text":"<ul> <li>Use lowercase kebab-case (<code>example-file-name.md</code>) for all filenames and directories. Avoid spaces and CamelCase.</li> <li>Prefer descriptive names that communicate the document scope (e.g., <code>architecture-system-overview.md</code>, not <code>overview.md</code>).</li> <li>Historical files that still use legacy names may remain until the archive migration, but new or renamed files must conform.</li> <li>Subdirectory landings: use <code>index.md</code> as the folder entry page; use <code>overview.md</code> for curated summaries within that folder. Avoid <code>README.md</code> in subdirectories (MkDocs doesn\u2019t treat it as an entry page).</li> </ul>"},{"location":"docs-ops/markdown-standards/#required-front-matter","title":"Required Front Matter","text":"<p>All Markdown files must start with YAML front matter so the doc tooling can build the global index (initial state example):</p> <pre><code>---\ntitle: \"Human-readable Title\"\ndescription: \"One-sentence summary\"\nowner: \"\"\nauthor: \"\"\nstatus: draft\ncreated: \"YYYY-MM-DD\"\n---\n</code></pre> <ul> <li><code>title</code> and <code>description</code> should match the real content (not just repeat headings).</li> <li><code>owner</code> may remain blank while we have a single maintainer but should be populated when an area gains ownership.</li> <li><code>author</code> records provenance (person, tool, or AI agent responsible for the initial version of the document). Use a short identifier or comma-separated list when multiple authors collaborate.</li> <li><code>status</code> tracks document lifecycle. See status values below.</li> <li><code>created</code> reflects the original commit date when possible (auto-filled by tooling).</li> <li><code>auto_generated</code> (boolean) indicates whether the file is machine-generated. When <code>true</code>, only <code>current</code>, <code>archived</code>, <code>deprecated</code>, or <code>superseded</code> statuses are allowed.</li> <li><code>updated</code> date of change, added if file is updated.</li> <li>Prompt Template Exception: files in <code>patterns/</code> (soon living under <code>docs/prompt-templates/</code>) use a prompt-specific front matter schema that includes runtime variables and other metadata. Continue using that specialized format until the Prompt Template standard is finalized (TBD); do not remodel those files to the generic doc front matter without an explicit migration plan.</li> </ul>"},{"location":"docs-ops/markdown-standards/#document-status-values","title":"Document Status Values","text":"<ul> <li><code>proposed</code> = early RFC/discussion stage</li> <li><code>draft</code> = initial iteration pending approval</li> <li><code>wip</code> = actively being revised (expected to change)</li> <li><code>current</code> = approved baseline</li> <li><code>deprecated</code> = still valid but being phased out</li> <li><code>superseded</code> = replaced by newer version (see link in doc)</li> <li><code>archived</code> = historical reference only</li> </ul> <p>Auto-generated file constraint: Files with <code>auto_generated: true</code> may only use <code>current</code>, <code>archived</code>, <code>deprecated</code>, or <code>superseded</code> status. Manual lifecycle states (<code>proposed</code>, <code>draft</code>, <code>wip</code>) are invalid for generated content.</p>"},{"location":"docs-ops/markdown-standards/#heading-summary-rules","title":"Heading &amp; Summary Rules","text":"<ul> <li>Immediately after front matter, include a <code># Title</code> heading that exactly matches the YAML <code>title</code> field (character-for-character, including punctuation and capitalization).</li> <li>Follow the heading with a one-sentence or single-paragraph description that orients the reader. This description should expand on or clarify the <code>description</code> field from front matter. ADRs and reference docs must comply even if they also list metadata (Status, Date, Owner) right after the summary.</li> <li>Use hierarchical headings (<code>##</code>, <code>###</code>, etc.) without skipping levels. Avoid deep nesting beyond <code>####</code>.</li> <li>Validation: A linting script (<code>scripts/validate_titles.py</code>) will verify YAML <code>title</code> matches <code># Heading</code> exactly. CI will warn (but not fail) on mismatches until all legacy docs are updated.</li> </ul>"},{"location":"docs-ops/markdown-standards/#adr-format","title":"ADR Format","text":"<ul> <li>Store ADRs under module directories such as <code>docs/architecture/&lt;module&gt;/adr/</code> (legacy ADRs move there during the restructure).</li> <li>File naming convention: <code>adr-&lt;modulecode&gt;&lt;number&gt;-&lt;descriptor&gt;.md</code>, all lowercase, hyphen-separated (e.g., <code>adr-dd01-docs-reorg-strat.md``adr-kb02-knowledge-store.md</code>). Strategy ADRs append <code>-strat</code> to the descriptor (<code>adr-dd01-docs-reorg-strat.md</code>) so higher-level directional docs are easy to spot.</li> <li>The visible title in the Markdown file must use uppercase module codes, e.g., <code># ADR-DD01: Documentation System Reorganization Strategy</code>. A simple lowercase-to-uppercase transform keeps filenames and titles aligned.</li> <li>Each ADR uses the same front matter and heading rules as any other doc.</li> <li>After the introductory paragraph, list metadata bullets:</li> </ul> <pre><code>- **Status**: Proposed\n- **Date**: 2025-02-27\n- **Owner**: Documentation Working Group\n- **Author**: Codex (GPT-5)\n</code></pre> <ul> <li> <p>Organize ADRs under module-specific folders (<code>docs/architecture/&lt;module&gt;/adr/ADR-&lt;code&gt;.md</code>). Module codes can be one to four uppercase characters (e.g., <code>ADR-A01</code>, <code>ADR-KB01</code>, <code>ADR-DD01</code>) to keep numbering human-readable while signaling the owning subsystem.</p> </li> <li> <p>Standard sections: <code>## Context</code>, <code>## Decision</code>, <code>## Consequences</code>, <code>## Open Questions</code>. Add <code>## Alternatives Considered</code> if helpful.</p> </li> <li>A reusable template lives in <code>docs/docs-ops/adr-template.md</code>.</li> </ul>"},{"location":"docs-ops/markdown-standards/#content-guidelines","title":"Content Guidelines","text":"<ul> <li>Use fenced code blocks with language hints (```bash, ```python, ```yaml, etc.).</li> <li>Link formatting:</li> <li>Use absolute links only for internal documentation references. Absolute links are relative to the MkDocs root (<code>/docs</code> directory).</li> <li>Example: <code>/architecture/overview.md</code> (resolves to <code>docs/architecture/overview.md</code> in the repository).</li> <li>Example: <code>/architecture/docs-system/adr/adr-dd01-docs-reorg-strat.md</code> (resolves to <code>docs/architecture/docs-system/adr/adr-dd01-docs-reorg-strat.md</code>).</li> <li>Never use relative links like <code>../architecture/overview.md</code> or paths that include <code>/docs/</code> in the link itself.</li> <li>Always use absolute links starting with <code>/</code> not relative links to avoid broken references when files are moved or reorganized.</li> <li>For external URLs, use full absolute URLs (e.g., <code>https://example.com</code>).</li> <li>Repository root file references:</li> <li>When drafting documentation that links to tracked repository root files (e.g., <code>README.md</code>, <code>CONTRIBUTING.md</code>, <code>VERSIONING.md</code>), always link to the generated copies under <code>/project/repo-root/&lt;name&gt;.md</code>.</li> <li>Example: Link to <code>/project/repo-root/versioning.md</code> instead of <code>/VERSIONING.md</code> to match MkDocs target paths and avoid later rewrites.</li> <li>These generated files are created by <code>scripts/sync_root_docs.py</code> from the repository root originals.</li> <li>Adding new repository root files:</li> <li>If adding a new file to the repository root that should be surfaced in the documentation (e.g., <code>VERSIONING.md</code>), you must update <code>scripts/sync_root_docs.py</code>:<ol> <li>Add the filename to the <code>ROOT_DOCS</code> tuple (lines 64-72).</li> <li>Add a mapping entry to <code>ROOT_DOC_DEST_MAP</code> (lines 74-82) that specifies the kebab-case destination filename.</li> </ol> </li> <li>This ensures the file is automatically synced to <code>/docs/project/repo-root/</code> during the MkDocs build process.</li> <li>Tables should include header separators (<code>| --- |</code>) so markdownlint can validate alignment.</li> <li>When embedding lists, keep them short and use parallel grammar. Use numbered lists only when order matters.</li> <li>Reference other documents via their kebab-case paths (matching the naming rule).</li> <li>Directory links: MkDocs resolves pages, not bare folders. Do not link to a directory path (<code>[Architecture](/architecture/index.md)</code>). Instead:</li> <li>Link to a specific page (e.g., <code>[Architecture Overview](/architecture/overview.md)</code>), or</li> <li>Add an <code>index.md</code> in that directory and link to it (<code>/architecture/index.md</code>).</li> <li><code>README.md</code> inside a folder is not treated as a landing page by MkDocs; prefer <code>index.md</code> for folder entry points.</li> </ul>"},{"location":"docs-ops/markdown-standards/#linting-automation","title":"Linting &amp; Automation","text":"<ul> <li>We standardize on <code>markdownlint</code>. Add a <code>.markdownlint.json</code> configuration (future task) and run <code>npx markdownlint '**/*.md'</code> (or the Make target) locally.</li> <li>CI must run markdownlint alongside MkDocs builds; documentation PRs fail if lint errors remain.</li> <li>Documentation tooling (e.g., <code>documentation_index.md</code>) assumes compliant front matter and headings\u2014if a file deviates, the generator will flag it. Regenerate the index with <code>scripts/generate_doc_index.py</code>; the script writes <code>auto_generated: true</code> into the front matter so downstream tooling knows not to edit it manually.</li> <li>The repo-wide <code>.markdownlint.json</code> disables MD025 (\"multiple top-level headings\") and MD013 (line length) because front matter + title duplication and long tables are intentional.</li> </ul>"},{"location":"docs-ops/markdown-standards/#exceptions-legacy-content","title":"Exceptions &amp; Legacy Content","text":"<ul> <li>During the archive migration, some historical files may temporarily violate the standards. Tag them with <code>status: historical</code> and move them under <code>docs/archive/</code>.</li> <li>When editing legacy docs, clean up the formatting to meet the standard where practical instead of propagating exceptions.</li> </ul> <p>Following these standards keeps the documentation approachable for humans and structured for the automation we depend on.</p>"},{"location":"docs-ops/mkdocs-strict-warning-backlog/","title":"MkDocs Strict Warning Backlog","text":"<p>Tracking all current <code>mkdocs build --strict</code> warnings so we can clear them systematically.</p>"},{"location":"docs-ops/mkdocs-strict-warning-backlog/#navigation-link-breaks","title":"Navigation / Link Breaks","text":"<ul> <li> Remove or replace stale nav references generated into <code>docs-nav.md</code> (e.g., <code>docs-design/planning/roadmap.md</code>, <code>docs-design/planning/maintenance.md</code>, translation experiment files).</li> <li> Fix broken relative links in ADRs/design docs (e.g., <code>architecture/gen-ai-service/design/migration-plan.md</code> pointing to <code>genai-service-strategy.md</code>, ADR links with doubled <code>architecture/</code> segments).</li> <li> Audit <code>docs/documentation_index.md</code> for links that do not exist post-reorg; prune or update paths so they resolve within <code>docs/</code>.</li> <li> Normalize absolute links in <code>docs/index.md</code> and mirrored README (<code>docs/project/repo-root/repo-readme.md</code>) to correct site-relative paths.</li> </ul>"},{"location":"docs-ops/mkdocs-strict-warning-backlog/#autorefs-mirrored-root-docs","title":"Autorefs / Mirrored Root Docs","text":"<ul> <li> Regenerate <code>docs/project/repo-root/TODO.md</code> after converting root TODO links to GitHub URLs so mkdocs-autorefs can resolve them.</li> </ul>"},{"location":"docs-ops/mkdocs-strict-warning-backlog/#mkdocstrings-griffe-signature-docstring-alignment","title":"Mkdocstrings (Griffe) \u2014 Signature / Docstring Alignment","text":"<ul> <li> <code>src/tnh_scholar/ai_text_processing/*</code>: add missing type hints and align docstrings/signatures (ai_text_processing.py, text_object.py, line_translator.py, prompts.py yield docs).</li> <li> <code>src/tnh_scholar/audio_processing/*</code>: fix parameters documented but not in signatures (audio_legacy.py, timed_text.py, transcription/* including assemblyai_service.py, srt_processor.py, vtt_processor.py, transcription_service.py, transcription_pipeline.py).</li> <li> <code>src/tnh_scholar/journal_processing/journal_process.py</code>: ensure docstrings match signatures/return types for batch/process helpers.</li> <li> <code>src/tnh_scholar/cli_tools/audio_transcribe/audio_transcribe.py</code>: align CLI option docs with function signatures.</li> <li> <code>src/tnh_scholar/logging_config.py</code>: clean docstring field lists and add missing annotations.</li> <li> <code>src/tnh_scholar/ocr_processing/*</code>: add missing params/return annotations in ocr_processing.py and ocr_editor.py.</li> <li> <code>src/tnh_scholar/text_processing/numbered_text.py</code>: fix docstring list formatting (<code>name: description</code> pair).</li> <li> <code>src/tnh_scholar/utils/tnh_audio_segment.py</code>: add annotations for **kwargs overloads.</li> <li> <code>src/tnh_scholar/video_processing/video_processing_old1.py</code>: align docstrings with signatures or mark as deprecated appropriately.</li> <li> <code>src/tnh_scholar/xml_processing/extract_tags.py</code>: add return type annotation for set.</li> </ul>"},{"location":"docs-ops/mkdocs-strict-warning-backlog/#verification","title":"Verification","text":"<ul> <li> Run <code>poetry run mkdocs build --strict</code> and confirm zero warnings.</li> </ul>"},{"location":"docs-ops/preview-theme/","title":"Preview TNH Scholar Theme","text":"<p>Quick guide to preview the custom zen-inspired documentation theme.</p>"},{"location":"docs-ops/preview-theme/#quick-preview","title":"Quick Preview","text":"<p>Build and serve the documentation locally:</p> <pre><code># Build the documentation\npoetry run mkdocs build\n\n# Serve locally with live reload\npoetry run mkdocs serve\n</code></pre> <p>Then open http://127.0.0.1:8000 in your browser.</p>"},{"location":"docs-ops/preview-theme/#theme-features-to-test","title":"Theme Features to Test","text":""},{"location":"docs-ops/preview-theme/#visual-elements","title":"Visual Elements","text":"<ol> <li>Color Palette</li> <li>Dark sumi ink background (#1a1a1a)</li> <li>Warm rice paper text (#e8e4d8)</li> <li>Bamboo green accents (#7a9b76)</li> <li> <p>Check contrast and readability</p> </li> <li> <p>Typography</p> </li> <li>Source Serif 4 headings (elegant, lightweight)</li> <li>Inter body text (clean, professional)</li> <li> <p>JetBrains Mono code (developer-friendly)</p> </li> <li> <p>Spacing</p> </li> <li>Generous breathing room between sections</li> <li>Comfortable reading width (48rem max)</li> <li>Organic padding and margins</li> </ol>"},{"location":"docs-ops/preview-theme/#interactive-elements","title":"Interactive Elements","text":"<ol> <li>Navigation</li> <li>Hover effects on links (bamboo green underline)</li> <li>Active page highlighting</li> <li> <p>Smooth page transitions</p> </li> <li> <p>Code Blocks</p> </li> <li>Copy button functionality</li> <li>Syntax highlighting with zen gold</li> <li> <p>Left border in bamboo green</p> </li> <li> <p>Search</p> </li> <li>Dark theme integration</li> <li>Suggestion popups</li> <li>Result highlighting</li> </ol>"},{"location":"docs-ops/preview-theme/#content-types","title":"Content Types","text":"<p>Test the theme with different content:</p> <ol> <li>Headers (h1-h6)</li> <li>Lists (bulleted, numbered, task lists)</li> <li>Tables (check hover effects)</li> <li>Blockquotes (lotus pink border)</li> <li>Admonitions (note, tip, warning, danger)</li> <li>Code (inline and blocks)</li> <li>Links (internal and external)</li> </ol>"},{"location":"docs-ops/preview-theme/#sample-content","title":"Sample Content","text":"<p>Visit these pages to see theme in action:</p> <ul> <li>Index - Hero content and overview</li> <li>Architecture Overview - Technical content with code</li> <li>Quick Start Guide - User-facing docs</li> <li>Theme Design - Color palette showcase</li> <li>ADR Template - Structured document format</li> </ul>"},{"location":"docs-ops/preview-theme/#customization","title":"Customization","text":""},{"location":"docs-ops/preview-theme/#adjust-colors","title":"Adjust Colors","text":"<p>Edit <code>docs/stylesheets/tnh-zen.css</code>:</p> <pre><code>:root {\n  /* Change any color variable */\n  --tnh-bamboo-green: #7a9b76;  /* Primary accent */\n  --tnh-meditation-blue: #6b8e9e; /* Links */\n  --tnh-zen-gold: #c9a961;      /* Code */\n}\n</code></pre>"},{"location":"docs-ops/preview-theme/#modify-spacing","title":"Modify Spacing","text":"<pre><code>:root {\n  --tnh-space-breath: 2rem;   /* Section spacing */\n  --tnh-space-pause: 1.5rem;  /* Element spacing */\n  --tnh-space-moment: 1rem;   /* Internal padding */\n}\n</code></pre>"},{"location":"docs-ops/preview-theme/#change-fonts","title":"Change Fonts","text":"<p>Edit <code>mkdocs.yaml</code>:</p> <pre><code>theme:\n  font:\n    text: Inter              # Body font\n    code: JetBrains Mono     # Code font\n</code></pre> <p>For headings, edit <code>docs/overrides/main.html</code> Google Fonts import.</p>"},{"location":"docs-ops/preview-theme/#browser-testing","title":"Browser Testing","text":"<p>Test across browsers for consistency:</p> <ul> <li>Chrome/Edge (Chromium)</li> <li>Firefox</li> <li>Safari</li> <li>Mobile browsers (responsive design)</li> </ul>"},{"location":"docs-ops/preview-theme/#dark-mode","title":"Dark Mode","text":"<p>The theme is designed for dark mode by default. To add light mode support, you would need to create alternate color schemes in <code>mkdocs.yaml</code>:</p> <pre><code>theme:\n  palette:\n    # Dark mode (default)\n    - scheme: slate\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n    # Light mode (inverted palette)\n    - scheme: default\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n</code></pre>"},{"location":"docs-ops/preview-theme/#performance","title":"Performance","text":"<p>Check the theme performs well:</p> <ol> <li>Page Load: Should be fast with minimal CSS</li> <li>Smooth Scrolling: Anchor links scroll smoothly</li> <li>Transitions: Hover effects are smooth (0.3s)</li> <li>Font Loading: Web fonts load progressively</li> </ol>"},{"location":"docs-ops/preview-theme/#accessibility","title":"Accessibility","text":"<p>Verify accessibility features:</p> <ol> <li>Contrast Ratios: All text meets WCAG AA standards</li> <li>Focus Indicators: Bamboo green outline on focus</li> <li>Keyboard Navigation: Tab through all interactive elements</li> <li>Screen Readers: Semantic HTML structure</li> </ol>"},{"location":"docs-ops/preview-theme/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs-ops/preview-theme/#css-not-loading","title":"CSS Not Loading","text":"<pre><code># Clear the site directory and rebuild\nrm -rf site/\npoetry run mkdocs build\n</code></pre>"},{"location":"docs-ops/preview-theme/#fonts-not-showing","title":"Fonts Not Showing","text":"<p>Check Google Fonts loaded in browser DevTools: 1. Open Network tab 2. Filter by \"fonts.googleapis.com\" 3. Verify Inter, Source Serif 4, and JetBrains Mono loaded</p>"},{"location":"docs-ops/preview-theme/#colors-look-wrong","title":"Colors Look Wrong","text":"<p>Verify Material theme version: <pre><code>poetry show mkdocs-material\n</code></pre></p> <p>Should be compatible with Material 9.x+</p>"},{"location":"docs-ops/preview-theme/#production-deployment","title":"Production Deployment","text":"<p>Before deploying:</p> <ol> <li>Build with strict mode: <code>poetry run mkdocs build --strict</code></li> <li>Test all links: Run lychee link checker</li> <li>Verify responsive: Test mobile layouts</li> <li>Check performance: Use Lighthouse audit</li> </ol>"},{"location":"docs-ops/preview-theme/#feedback","title":"Feedback","text":"<p>The theme is designed to be minimal, peaceful, and functional. Adjust as needed while maintaining:</p> <ul> <li>Dark backgrounds for reduced eye strain</li> <li>Warm, organic color palette</li> <li>Generous spacing for breathing room</li> <li>Clear visual hierarchy</li> <li>Professional yet approachable aesthetic</li> </ul> <p>Walk mindfully through the documentation.</p>"},{"location":"docs-ops/theme-design/","title":"TNH Scholar Theme Design","text":"<p>The TNH Scholar documentation theme embodies Thich Nhat Hanh's aesthetic philosophy: minimalist zen calligraphy meets modern AI, with organic simplicity and mindful design.</p>"},{"location":"docs-ops/theme-design/#design-philosophy","title":"Design Philosophy","text":""},{"location":"docs-ops/theme-design/#inspiration-sources","title":"Inspiration Sources","text":"<p>Thich Nhat Hanh's Calligraphy - Brush strokes on rice paper - Minimal, intentional marks - Breathing space between elements - Organic, flowing forms</p> <p>Plum Village Tradition - Earth tones and natural colors - Peaceful, contemplative atmosphere - Accessible clarity - Grounded presence</p> <p>Modern AI Context - Technical precision with warmth - Clear information architecture - Professional yet approachable - Dark mode for reduced eye strain</p>"},{"location":"docs-ops/theme-design/#color-palette","title":"Color Palette","text":""},{"location":"docs-ops/theme-design/#primary-colors-sumi-ink-rice-paper-inverted","title":"Primary Colors (Sumi Ink &amp; Rice Paper - Inverted)","text":"<pre><code>--tnh-ink-black: #1a1a1a;      /* Deep sumi ink background */\n--tnh-charcoal: #2d2d2d;       /* Brush charcoal surfaces */\n--tnh-slate: #3d3d3d;          /* Stone slate elevated */\n--tnh-rice-paper: #f5f1e8;     /* Warm cream text */\n--tnh-soft-white: #e8e4d8;     /* Aged paper primary text */\n</code></pre>"},{"location":"docs-ops/theme-design/#accent-colors-buddhist-tradition","title":"Accent Colors (Buddhist Tradition)","text":"<pre><code>--tnh-lotus-pink: #d4a5a5;     /* Lotus flower - wisdom */\n--tnh-bamboo-green: #7a9b76;   /* Bamboo - resilience */\n--tnh-zen-gold: #c9a961;       /* Temple gold - code */\n--tnh-plum-purple: #9d8ba6;    /* Plum blossom - beauty */\n--tnh-meditation-blue: #6b8e9e; /* Still water - links */\n</code></pre>"},{"location":"docs-ops/theme-design/#color-meanings","title":"Color Meanings","text":"<ul> <li>Bamboo Green (<code>#7a9b76</code>): Primary accent, navigation, active states</li> <li>Represents flexibility and resilience</li> <li> <p>Used for interactive elements and highlights</p> </li> <li> <p>Meditation Blue (<code>#6b8e9e</code>): Links and navigation</p> </li> <li>Represents calm presence and clarity</li> <li> <p>Gentle on the eyes for extended reading</p> </li> <li> <p>Zen Gold (<code>#c9a961</code>): Code and technical content</p> </li> <li>Represents illumination and understanding</li> <li> <p>Warm contrast against dark backgrounds</p> </li> <li> <p>Lotus Pink (<code>#d4a5a5</code>): Warnings and special notices</p> </li> <li>Represents wisdom emerging from complexity</li> <li>Soft, non-alarming attention-getter</li> </ul>"},{"location":"docs-ops/theme-design/#typography","title":"Typography","text":""},{"location":"docs-ops/theme-design/#font-stack","title":"Font Stack","text":"<p>Headings: Source Serif 4 (200-500 weight) - Elegant, classical serif with modern clarity - Light weights (200-300) for main headings - Reminiscent of brush calligraphy fluidity</p> <p>Body Text: Inter (300-600 weight) - Highly legible sans-serif - Excellent screen rendering - Professional and approachable</p> <p>Code: JetBrains Mono - Designed for developers - Clear distinction of characters - Comfortable for extended reading</p>"},{"location":"docs-ops/theme-design/#typographic-scale","title":"Typographic Scale","text":"<ul> <li>h1: 2.5rem, weight 200, bottom border</li> <li>h2: 2rem, weight 300</li> <li>h3: 1.5rem, bamboo green color</li> <li>Body: 1rem, line-height 1.8 (spacious)</li> </ul>"},{"location":"docs-ops/theme-design/#spacing-philosophy","title":"Spacing Philosophy","text":""},{"location":"docs-ops/theme-design/#mindful-breathing-room","title":"Mindful Breathing Room","text":"<pre><code>--tnh-space-breath: 2rem;    /* Major section breaks */\n--tnh-space-pause: 1.5rem;   /* Between elements */\n--tnh-space-moment: 1rem;    /* Internal padding */\n</code></pre> <p>Spacing follows the concept of \"breathing room\" - enough space for ideas to settle, similar to pauses in meditation.</p>"},{"location":"docs-ops/theme-design/#design-elements","title":"Design Elements","text":""},{"location":"docs-ops/theme-design/#headings","title":"Headings","text":"<ul> <li>Lightweight, elegant serif font</li> <li>Generous spacing above and below</li> <li>H1 with subtle underline in bamboo green</li> <li>H3 colored to create visual hierarchy</li> </ul>"},{"location":"docs-ops/theme-design/#links","title":"Links","text":"<ul> <li>Meditation blue default color</li> <li>Subtle underline on hover (bamboo green)</li> <li>Smooth color transitions (0.3s)</li> </ul>"},{"location":"docs-ops/theme-design/#code-blocks","title":"Code Blocks","text":"<ul> <li>Dark background (#252525) for reduced eye strain</li> <li>Bamboo green left border for visual anchor</li> <li>Zen gold text for syntax elements</li> <li>Comfortable padding and border radius</li> </ul>"},{"location":"docs-ops/theme-design/#blockquotes","title":"Blockquotes","text":"<ul> <li>Lotus pink left border</li> <li>Subtle background tint (5% opacity)</li> <li>Italic text style</li> <li>Used for teachings, insights, important notes</li> </ul>"},{"location":"docs-ops/theme-design/#tables","title":"Tables","text":"<ul> <li>Bamboo green header underline</li> <li>Subtle row hover (5% green tint)</li> <li>Clear borders in slate</li> <li>Spacious padding (0.75rem)</li> </ul>"},{"location":"docs-ops/theme-design/#admonitions","title":"Admonitions","text":"<ul> <li>Color-coded by type (note, tip, warning, danger)</li> <li>Subtle background tint</li> <li>Left border for visual weight</li> <li>Breathing space around content</li> </ul>"},{"location":"docs-ops/theme-design/#interactive-elements","title":"Interactive Elements","text":""},{"location":"docs-ops/theme-design/#navigation","title":"Navigation","text":"<ul> <li>Active links in bamboo green</li> <li>Smooth hover transitions</li> <li>Clear visual hierarchy</li> <li>Instant page transitions</li> </ul>"},{"location":"docs-ops/theme-design/#buttons","title":"Buttons","text":"<ul> <li>Bamboo green background</li> <li>Subtle lift on hover</li> <li>Rounded corners (4px)</li> <li>Organic shadow</li> </ul>"},{"location":"docs-ops/theme-design/#search","title":"Search","text":"<ul> <li>Integrated into dark theme</li> <li>Suggestion support</li> <li>Highlight matching terms</li> <li>Smooth dropdown</li> </ul>"},{"location":"docs-ops/theme-design/#material-theme-integration","title":"Material Theme Integration","text":"<p>The theme extends Material for MkDocs with custom CSS overrides.</p>"},{"location":"docs-ops/theme-design/#features-enabled","title":"Features Enabled","text":"<ul> <li><code>navigation.instant</code> - Smooth page transitions</li> <li><code>navigation.tracking</code> - URL tracking</li> <li><code>navigation.tabs</code> - Top-level organization</li> <li><code>navigation.sections</code> - Hierarchical structure</li> <li><code>navigation.top</code> - Back-to-top button</li> <li><code>search.suggest</code> - Search suggestions</li> <li><code>search.highlight</code> - Result highlighting</li> <li><code>content.code.copy</code> - Copy code blocks</li> <li><code>content.tabs.link</code> - Linked content tabs</li> </ul>"},{"location":"docs-ops/theme-design/#custom-overrides","title":"Custom Overrides","text":"<p>Location: <code>docs/overrides/main.html</code> - Google Fonts integration (Inter, Source Serif 4, JetBrains Mono) - Material color scheme overrides - Custom meta tags - Mindful interaction scripts</p>"},{"location":"docs-ops/theme-design/#files","title":"Files","text":""},{"location":"docs-ops/theme-design/#theme-components","title":"Theme Components","text":"<pre><code>docs/\n\u251c\u2500\u2500 stylesheets/\n\u2502   \u2514\u2500\u2500 tnh-zen.css          # Main theme stylesheet\n\u2514\u2500\u2500 overrides/\n    \u2514\u2500\u2500 main.html             # Template overrides\n</code></pre>"},{"location":"docs-ops/theme-design/#configuration","title":"Configuration","text":"<pre><code># mkdocs.yaml\ntheme:\n  name: material\n  palette:\n    scheme: slate              # Dark mode base\n    primary: custom            # Custom primary color\n    accent: custom             # Custom accent color\n  font:\n    text: Inter\n    code: JetBrains Mono\n  custom_dir: docs/overrides\n\nextra_css:\n  - stylesheets/tnh-zen.css\n</code></pre>"},{"location":"docs-ops/theme-design/#design-principles","title":"Design Principles","text":""},{"location":"docs-ops/theme-design/#zen-aesthetics","title":"Zen Aesthetics","text":"<ol> <li>Simplicity: Remove unnecessary elements</li> <li>Space: Generous breathing room</li> <li>Intention: Every element serves a purpose</li> <li>Organic: Natural, flowing forms</li> <li>Presence: Clear visual hierarchy</li> </ol>"},{"location":"docs-ops/theme-design/#accessibility","title":"Accessibility","text":"<ol> <li>Contrast: WCAG AA compliant color ratios</li> <li>Typography: Highly legible fonts, generous sizing</li> <li>Focus: Clear focus indicators (bamboo green)</li> <li>Smooth: Gentle transitions and animations</li> <li>Semantic: Proper HTML structure</li> </ol>"},{"location":"docs-ops/theme-design/#technical-excellence","title":"Technical Excellence","text":"<ol> <li>Performance: Minimal CSS, efficient selectors</li> <li>Responsive: Mobile-first design</li> <li>Modern: CSS custom properties</li> <li>Maintainable: Well-documented, organized</li> <li>Extensible: Easy to customize</li> </ol>"},{"location":"docs-ops/theme-design/#future-enhancements","title":"Future Enhancements","text":""},{"location":"docs-ops/theme-design/#potential-additions","title":"Potential Additions","text":"<ul> <li> Custom favicon with lotus or bamboo icon</li> <li> Animated brush stroke dividers</li> <li> Calligraphy-style logo</li> <li> Optional light mode with inverted palette</li> <li> Seasonal color variations (spring/summer/autumn/winter)</li> <li> Mindfulness timer integration for reading breaks</li> <li> Poetry excerpts in sidebar</li> </ul>"},{"location":"docs-ops/theme-design/#typography-refinements","title":"Typography Refinements","text":"<ul> <li> Consider Noto Serif CJK for Vietnamese/Chinese characters</li> <li> Explore variable fonts for smoother scaling</li> <li> Test readability across different screen sizes</li> </ul>"},{"location":"docs-ops/theme-design/#color-adjustments","title":"Color Adjustments","text":"<ul> <li> A11y audit for all color combinations</li> <li> Optional high-contrast mode</li> <li> Color-blind friendly palette verification</li> </ul>"},{"location":"docs-ops/theme-design/#usage-guidelines","title":"Usage Guidelines","text":""},{"location":"docs-ops/theme-design/#for-documentation-authors","title":"For Documentation Authors","text":"<p>Use bamboo green for: - Active navigation elements - Completed tasks - Success messages - Primary CTAs</p> <p>Use meditation blue for: - Links to other pages - Reference materials - External resources</p> <p>Use zen gold for: - Code and technical terms - Configuration values - File paths and commands</p> <p>Use lotus pink for: - Important notices - Wisdom quotes - Special callouts</p>"},{"location":"docs-ops/theme-design/#writing-style","title":"Writing Style","text":"<p>Match the visual aesthetic with content: - Clear, concise language - Breathing space between ideas - Intentional word choice - Accessible technical explanations</p>"},{"location":"docs-ops/theme-design/#inspiration-references","title":"Inspiration &amp; References","text":"<ul> <li>Thich Nhat Hanh Calligraphy</li> <li>Material Design Color System</li> <li>Zen and the Art of Web Design</li> <li>Japanese Aesthetics</li> <li>Buddhist Symbolism</li> </ul> <p>May this theme support clarity, peace, and understanding in documentation.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Table of Contents:</p> <p>Configuration - TNH Scholar requires some initial configuration to function properly. This guide covers the essential configuration steps and options.</p> <p>Installation - Install instructions for TNH Scholar, a Python package for text processing and analysis, using .</p> <p>Quick Start Guide - TNH Scholar provides powerful text processing capabilities through several command-line tools. This guide will help you get started with the basic workflows.</p> <p>This file auto-generated.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>TNH Scholar requires some initial configuration to function properly. This guide covers the essential configuration steps and options.</p>"},{"location":"getting-started/configuration/#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/configuration/#openai-api-key","title":"OpenAI API Key","text":"<p>TNH Scholar's AI functionality requires an OpenAI API key. To configure this:</p> <ol> <li>Obtain an API key from OpenAI's platform</li> <li>Set the environment variable:</li> </ol> <pre><code># Linux/Mac\nexport OPENAI_API_KEY='your-api-key-here'\n\n# Windows\nset OPENAI_API_KEY=your-api-key-here\n</code></pre> <p>In development configuration (where you have downloaded the tnh-scholar repository) you can also use a <code>.env</code> file in your project directory:</p> <pre><code>OPENAI_API_KEY=your-api-key-here\n</code></pre>"},{"location":"getting-started/configuration/#directory-structure","title":"Directory Structure","text":"<p>TNH Scholar creates and uses the following directory structure:</p> <pre><code>~/.config/tnh_scholar/\n\u251c\u2500\u2500 patterns/         # Prompt storage\n\u2514\u2500\u2500 logs/            # Log files\n</code></pre>"},{"location":"getting-started/configuration/#prompt-configuration","title":"Prompt Configuration","text":""},{"location":"getting-started/configuration/#prompt-directory","title":"Prompt Directory","text":"<p>Prompts can be stored in:</p> <ol> <li>Default location: <code>~/.config/tnh_scholar/patterns/</code></li> <li>Custom location specified by <code>TNH_PATTERN_DIR</code> environment variable (note: uses legacy \"PATTERN\" name for backwards compatibility)</li> </ol> <p>To use a custom prompt directory:</p> <pre><code>export TNH_PATTERN_DIR=/path/to/prompts  # Variable name retained for compatibility\n</code></pre>"},{"location":"getting-started/configuration/#default-prompts","title":"Default Prompts","text":"<p>The system includes several default prompts:</p> <ul> <li>default_punctuate.md</li> <li>default_section.md</li> <li>default_line_translation.md</li> <li>default_xml_format.md</li> <li>default_xml_paragraph_format.md</li> </ul> <p>These can be downloaded during setup or manually added later.</p>"},{"location":"getting-started/configuration/#configuration-file","title":"Configuration File","text":"<p>The system looks for configuration in this order:</p> <ol> <li>Command line arguments</li> <li>Environment variables</li> <li>Project-level config: <code>./.tnh-fab.yaml</code></li> <li>User-level config: <code>~/.config/tnh_scholar/tnh-fab/config.yaml</code></li> </ol> <p>Example configuration file:</p> <pre><code>defaults:\n  language: auto\n  output_format: txt\n\npunctuate:\n  pattern: default_punctuate  # Config key name retained for compatibility\n  style: APA\n  review_count: 3\n\nsection:\n  pattern: default_section  # Config key name retained for compatibility\n  review_count: 3\n\ntranslate:\n  pattern: default_line_translation  # Config key name retained for compatibility\n  target_language: en\n  style: \"American Dharma Teaching\"\n  context_lines: 3\n  review_count: 3\n\nprocess:\n  wrap_document: true\n\npatterns:  # Config section name retained for compatibility\n  path: ~/.config/tnh_scholar/patterns\n\nlogging:\n  level: INFO\n  file: ~/.tnh-fab.log\n</code></pre>"},{"location":"getting-started/configuration/#initial-setup","title":"Initial Setup","text":"<p>The <code>tnh-setup</code> command automates configuration:</p> <pre><code># Full setup\ntnh-setup\n\n# Skip specific steps\ntnh-setup --skip-env        # Skip API key check\ntnh-setup --skip-patterns   # Skip prompt download\n</code></pre> <p>This will:</p> <ol> <li>Create necessary directories</li> <li>Offer to download default prompts</li> <li>Check for OpenAI API key</li> <li>Set up basic configuration</li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Install instructions for TNH Scholar, a Python package for text processing and analysis, using <code>pip</code>.</p> <p>Simple install:</p> <pre><code>pip install tnh-scholar\n</code></pre>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.12.4</li> <li>OpenAI API key for AI-powered features</li> <li>Git (for prompt version control)</li> </ul>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>For basic usage:</p> <pre><code>pip install tnh-scholar\ntnh-setup  # Configure default prompts and directories\n</code></pre>"},{"location":"getting-started/installation/#feature-specific-installation","title":"Feature-Specific Installation","text":"<p>Install optional components based on your needs:</p> <ul> <li>OCR capabilities: <code>pip install \"tnh-scholar[ocr]\"</code></li> <li>GUI tools: <code>pip install \"tnh-scholar[gui]\"</code></li> <li>Query features: <code>pip install \"tnh-scholar[query]\"</code></li> <li>Development tools: <code>pip install \"tnh-scholar[dev]\"</code></li> </ul>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<ol> <li>Set your OpenAI API key:</li> </ol> <pre><code>export OPENAI_API_KEY='your-api-key'\n</code></pre> <p>Or add it to your .env file.</p> <ol> <li>Run the setup tool:</li> </ol> <pre><code>tnh-setup\n</code></pre> <p>This will:    - Create the configuration directory (~/.config/tnh-scholar)    - Download default prompts    - Set up initial configuration</p>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code>tnh-fab --help\n</code></pre>"},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Missing API Key    If you see authentication errors, ensure OPENAI_API_KEY is set correctly.</p> </li> <li> <p>Python Version Mismatch    TNH Scholar requires Python 3.12.4 exactly. Check your version:</p> </li> </ol> <pre><code>python --version\n</code></pre> <p>For troubleshooting, see our GitHub Issues.</p>"},{"location":"getting-started/quick-start-guide/","title":"Quick Start Guide","text":"<p>TNH Scholar provides powerful text processing capabilities through several command-line tools. This guide will help you get started with the basic workflows.</p>"},{"location":"getting-started/quick-start-guide/#initial-setup","title":"Initial Setup","text":"<p>After installation, run the setup tool:</p> <pre><code>tnh-setup\n</code></pre> <p>This creates necessary directories and downloads default prompts.</p>"},{"location":"getting-started/quick-start-guide/#core-tools","title":"Core Tools","text":"<p>TNH Scholar includes several specialized tools:</p>"},{"location":"getting-started/quick-start-guide/#tnh-fab","title":"tnh-fab","text":"<p>The main text processing tool, providing functions for:</p> <ul> <li>Text punctuation and formatting</li> <li>Section analysis</li> <li>Translation</li> <li>Prompt-based processing</li> </ul> <p>Example usage:</p> <pre><code># Add punctuation to text\ntnh-fab punctuate input.txt &gt; punctuated.txt\n\n# Translate Vietnamese text to English\ntnh-fab translate -l vi input.txt &gt; translated.txt\n</code></pre>"},{"location":"getting-started/quick-start-guide/#audio-transcribe","title":"audio-transcribe","text":"<p>Process and transcribe audio content:</p> <pre><code># Transcribe from YouTube\naudio-transcribe --yt_url \"https://youtube.com/watch?v=example\" --split --transcribe\n\n# Process local audio\naudio-transcribe -f recording.mp3 --split --transcribe\n</code></pre>"},{"location":"getting-started/quick-start-guide/#ytt-fetch","title":"ytt-fetch","text":"<p>Download YouTube transcripts:</p> <pre><code># Get English transcript\nytt-fetch \"https://youtube.com/watch?v=example\" -l en -o transcript.txt\n</code></pre>"},{"location":"getting-started/quick-start-guide/#nfmt","title":"nfmt","text":"<p>Format text file newlines:</p> <pre><code># Normalize newlines in a file\nnfmt input.txt &gt; formatted.txt\n</code></pre>"},{"location":"getting-started/quick-start-guide/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quick-start-guide/#text-processing-pipeline","title":"Text Processing Pipeline","text":"<pre><code># Complete processing pipeline\ncat input.txt | \\\ntnh-fab punctuate | \\\ntnh-fab section | \\\ntnh-fab translate -l vi | \\\ntnh-fab process -p format_xml &gt; output.xml\n</code></pre>"},{"location":"getting-started/quick-start-guide/#audio-processing","title":"Audio Processing","text":"<pre><code># Download and transcribe\naudio-transcribe --yt_url \"https://example.com/video\" --split --transcribe\n\n# Post-process transcription\ntnh-fab punctuate transcript.txt | \\\ntnh-fab section &gt; processed.txt\n</code></pre>"},{"location":"getting-started/quick-start-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Review the Prompt System documentation</li> <li>Explore detailed CLI documentation for all available tools</li> <li>Check out example notebooks in the repository</li> </ul>"},{"location":"project/","title":"Project","text":"<p>Table of Contents:</p> <p>conceptual-architecture</p> <p>future-directions</p> <p>philosophy</p> <p>principles</p> <p>vision</p> <p>This file auto-generated.</p>"},{"location":"project/conceptual-architecture/","title":"Conceptual Architecture of TNH-Scholar","text":"<p>This document presents a high-level, implementation-agnostic conceptual architecture of TNH-Scholar \u2014 the system\u2019s core abstractions and how they relate.</p>"},{"location":"project/conceptual-architecture/#1-conceptual-layers","title":"1. Conceptual Layers","text":"<p>At a high level, TNH-Scholar can be seen as four interacting layers:</p> <ol> <li>Corpus Layer</li> <li>Raw scans, OCR output, EPUBs, text files.</li> <li>Cleaned and structured texts with metadata.</li> <li> <p>Versioned, annotated, multi-lingual corpora.</p> </li> <li> <p>Processing &amp; Enrichment Layer</p> </li> <li>Text parsing, cleaning, sectioning.</li> <li>Metadata extraction and tagging.</li> <li> <p>Alignment (cross-language, cross-version, cross-corpus).</p> </li> <li> <p>GenAI &amp; Automation Layer</p> </li> <li>GenAIService:<ul> <li>Prompt/Pattern catalog,</li> <li>Model routing and configuration,</li> <li>Provenance and fingerprinting.</li> </ul> </li> <li> <p>Pipelines that:</p> <ul> <li>Translate,</li> <li>Summarize,</li> <li>Tag and classify,</li> <li>Evaluate and compare outputs.</li> </ul> </li> <li> <p>UX &amp; Integration Layer</p> </li> <li>Command line tools and batch jobs.</li> <li>VS Code extensions and developer tooling.</li> <li>Web-based viewers (e.g., JVB viewer).</li> <li>Future interactive agents and dashboards.</li> </ol> <p>Conceptually:</p> <p>Corpus \u2192 Process &amp; Enrich \u2192 GenAI &amp; Automation \u2192 UX &amp; Tools \u2192 Back to Corpus (via new annotations, translations, and metadata).</p>"},{"location":"project/conceptual-architecture/#2-core-conceptual-entities","title":"2. Core Conceptual Entities","text":"<p>Some central conceptual entities:</p> <ul> <li>TextObject / Document Unit</li> <li>A structured representation of text plus metadata.</li> <li> <p>May correspond to:</p> <ul> <li>A page,</li> <li>A section,</li> <li>A chapter,</li> <li>A sutra, poem, or exercise.</li> </ul> </li> <li> <p>Metadata Record</p> </li> <li> <p>Information about:</p> <ul> <li>Source document,</li> <li>Page and line references,</li> <li>Language,</li> <li>Section type (heading, paragraph, quote, exercise, etc.),</li> <li>Historical and bibliographic context.</li> </ul> </li> <li> <p>Prompt / Pattern</p> </li> <li>A structured instruction template for GenAI systems.</li> <li> <p>Lives in a Prompt/Pattern Catalog with:</p> <ul> <li>Keys, labels, descriptions,</li> <li>Versioning and provenance.</li> </ul> </li> <li> <p>GenAIService Request &amp; Result</p> </li> <li>Request:<ul> <li>Prompt key,</li> <li>Input text and context,</li> <li>Model configuration.</li> </ul> </li> <li> <p>Result:</p> <ul> <li>Output text,</li> <li>Usage statistics,</li> <li>Provenance and fingerprint metadata.</li> </ul> </li> <li> <p>Provenance &amp; Fingerprint</p> </li> <li>Provenance: \u201cWhat happened, when, using which inputs and models?\u201d</li> <li>Fingerprint: A compact identifier for (request, config, model, input) tuples used in transformations.</li> </ul>"},{"location":"project/conceptual-architecture/#3-conceptual-data-flow","title":"3. Conceptual Data Flow","text":"<p>A typical high-level flow might be:</p> <ol> <li>Ingest</li> <li>OCR or parse an input source (PDF, EPUB, images).</li> <li> <p>Create an initial TextObject or document representation.</p> </li> <li> <p>Clean &amp; Structure</p> </li> <li>Apply rules, regex, and structural heuristics.</li> <li>Tag headings, paragraphs, quotes, footnotes, exercises.</li> <li> <p>Attach metadata (page numbers, section types, document IDs).</p> </li> <li> <p>Enrich via GenAIService</p> </li> <li>Use patterns to:<ul> <li>Refine section boundaries,</li> <li>Suggest headings,</li> <li>Propose translations,</li> <li>Generate queries and test pairs,</li> <li>Identify entities or concepts.</li> </ul> </li> <li> <p>All GenAI calls run through GenAIService with provenance.</p> </li> <li> <p>Store &amp; Index</p> </li> <li>Persist structured text and metadata.</li> <li> <p>Build indices for:</p> <ul> <li>Search and retrieval,</li> <li>Cross-language alignment,</li> <li>Topic and concept exploration.</li> </ul> </li> <li> <p>Expose via UX &amp; Tools</p> </li> <li>JVB viewer for bilingual page-level exploration.</li> <li>VS Code tools for developers and text engineers.</li> <li> <p>CLIs and batch jobs for large-scale processing.</p> </li> <li> <p>Evaluate &amp; Iterate</p> </li> <li>Use patterns and tools to:<ul> <li>Evaluate translation quality,</li> <li>Assess sectioning and metadata quality,</li> <li>Identify gaps or errors.</li> </ul> </li> <li>Feed these insights back into:<ul> <li>Data cleaning rules,</li> <li>Prompt/pattern design,</li> <li>Future ADRs and design refinements.</li> </ul> </li> </ol>"},{"location":"project/conceptual-architecture/#4-conceptual-integration-points","title":"4. Conceptual Integration Points","text":"<p>Some key conceptual seams:</p> <ul> <li>GenAIService CLI / API</li> <li> <p>A boundary between:</p> <ul> <li>The core project,</li> <li>External tooling (VS Code, scripts, other agents).</li> </ul> </li> <li> <p>Prompt/Pattern Catalog</p> </li> <li> <p>A boundary between:</p> <ul> <li>Stable, named operations (\u201ctranslate this page\u201d),</li> <li>The evolving internals of prompts and model selection.</li> </ul> </li> <li> <p>Corpus Store</p> </li> <li>A boundary between:<ul> <li>Data and metadata,</li> <li>The tools that operate on them.</li> </ul> </li> </ul> <p>These seams are critical for:</p> <ul> <li>Testing,</li> <li>Refactoring,</li> <li>Building agentic workflows,</li> <li>Integrating with external systems without coupling everything together.</li> </ul>"},{"location":"project/conceptual-architecture/#5-relationship-to-architecture-docs-and-adrs","title":"5. Relationship to Architecture Docs and ADRs","text":"<p>This conceptual architecture is intentionally:</p> <ul> <li>Higher-level than any single ADR.</li> <li>More stable than implementation details.</li> <li>A reference point for:</li> <li>Evaluating new features,</li> <li>Deciding where new components \u201clive\u201d,</li> <li>Keeping the system mentally manageable.</li> </ul> <p>Detailed architecture docs (under <code>docs/architecture</code>) and ADRs should:</p> <ul> <li>Be consistent with this conceptual view,</li> <li>Refine and specialize parts of it,</li> <li>Update it when fundamental assumptions or seams change.</li> </ul> <p>This document should be updated when:</p> <ul> <li>New layers are added (e.g., a new data store or major subsystem),</li> <li>The conceptual flow between layers significantly changes,</li> <li>New categories of tools (e.g., agent orchestrators) become central, rather than experimental.</li> </ul>"},{"location":"project/future-directions/","title":"Future Directions of TNH-Scholar","text":"<p>This document explores long-term, blue-sky horizons of the TNH-Scholar project and potential descendant systems. These are not commitments, but informed possibilities that arise naturally from the project's philosophy, architecture, and trajectory.</p> <p>TNH-Scholar is intentionally designed as a foundational system \u2014 a clean corpus, structured text models, provenance-rich transformations, agent-ready pipelines, and a pattern-driven GenAI interface. This foundation enables not only the current scholarly workflows, but also a number of long-horizon possibilities.</p>"},{"location":"project/future-directions/#the-sections-below-outline-potential-future-evolutions","title":"The sections below outline potential future evolutions.","text":"<pre><code>flowchart TD\n\n    A[Foundational System&lt;br/&gt;Clean Corpus \u2022 Structured Text \u2022 Provenance \u2022 Patterns] --&gt; B[Semi-Autonomous Agent Loops]\n    A --&gt; C[Autonomous Corpus Pipelines]\n    A --&gt; D[Intelligent Scholarly Assistants]\n    A --&gt; E[Corpus-Aware Model Training / Evaluation]\n    A --&gt; F[Agentic Application Framework]\n    A --&gt; G[Ecosystem-Level Integrations]\n\n    B --&gt; B1[Code Maintenance Agents&lt;br/&gt;Refactor \u2022 Test \u2022 Evaluate \u2022 Plan]\n    B --&gt; B2[Research / Data Agents&lt;br/&gt;Cleaning \u2022 Sectioning \u2022 Alignment]\n\n    C --&gt; C1[Continuous Ingest + Cleanup]\n    C --&gt; C2[Metadata + Sectioning Pipelines]\n    C --&gt; C3[Translation + Evaluation Loops]\n\n    D --&gt; D1[Semantic Research Companion]\n    D --&gt; D2[Interactive Dharma Exploration]\n\n    E --&gt; E1[Domain-Specific Models]\n    E --&gt; E2[Corpus-Aligned Evaluation Loops]\n\n    F --&gt; F1[General Codebase Agents]\n    F --&gt; F2[Document / Data Transformation Systems]\n\n    G --&gt; G1[Advanced UX Layers&lt;br/&gt;VS Code \u2022 Web \u2022 Jupyter]\n    G --&gt; G2[Distributed Scholarly Tools&lt;br/&gt;APIs \u2022 Collaborators \u2022 Multi-modal]\n\n    style A fill:#fdf6e3,stroke:#b58900,stroke-width:2px\n    style B fill:#eee8d5,stroke:#b58900\n    style C fill:#eee8d5,stroke:#b58900\n    style D fill:#eee8d5,stroke:#b58900\n    style E fill:#eee8d5,stroke:#b58900\n    style F fill:#eee8d5,stroke:#b58900\n    style G fill:#eee8d5,stroke:#b58900</code></pre>"},{"location":"project/future-directions/#1-semi-autonomous-long-running-agent-loops","title":"1. Semi-Autonomous, Long-Running Agent Loops","text":"<p>A natural evolution of the GenAIService + PromptCatalog + provenance system is the creation of long-running, semi-autonomous agents that execute sequences of tasks with human oversight.</p> <p>These loops could support:</p>"},{"location":"project/future-directions/#11-code-oriented-agents-descendant-projects","title":"1.1 Code-oriented agents (descendant projects)","text":"<p>Agents that can:</p> <ul> <li>Parse ADRs and design docs  </li> <li>Generate or refactor code patches  </li> <li>Evaluate quality using pattern-based evaluation prompts  </li> <li>Run tests  </li> <li>Detect architectural drift  </li> <li>Open pull requests  </li> <li>Summarize changes for humans  </li> <li>Make plans like:</li> <li>\u201cnew-problem-encountered\u201d</li> <li>\u201cdesign revision required\u201d</li> <li>\u201crefactor recommended\u201d</li> <li>\u201cevaluation failure\u2014request human review\u201d</li> </ul> <p>This forms the basis of:</p> <p>An AI-augmented software engineering assistant capable of maintaining complex codebases using structured, documented intent.</p> <p>A direct descendant of TNH-Scholar could be a general-purpose agentic software engineering platform using these same abstractions.</p>"},{"location":"project/future-directions/#2-autonomous-corpus-processing-pipelines","title":"2. Autonomous Corpus Processing Pipelines","text":"<p>TNH-Scholar\u2019s data layer (OCR \u2192 structured text \u2192 metadata \u2192 alignment) can be expanded into autonomous pipelines that continuously refine the corpus.</p>"},{"location":"project/future-directions/#these-pipelines-could","title":"These pipelines could","text":"<ul> <li>Automatically detect new scans or materials  </li> <li>Run cleanup/normalization stages  </li> <li>Apply sectioning &amp; metadata tagging patterns  </li> <li>Align bilingual or trilingual segments  </li> <li>Evaluate translation quality  </li> <li>Surface anomalies or inconsistencies for human review  </li> <li>Trigger model updates or fine-tuning rounds  </li> </ul> <p>This becomes:</p> <p>A living, evolving scholarly corpus with transparent, traceable transformations and continuous improvement.</p>"},{"location":"project/future-directions/#3-intelligent-scholarly-assistants","title":"3. Intelligent Scholarly Assistants","text":"<p>Once the corpus is structured and richly annotated, future systems could support:</p>"},{"location":"project/future-directions/#31-semantic-research-companions","title":"3.1 Semantic research companions","text":"<p>Agents that:</p> <ul> <li>Trace a concept (e.g., interbeing, emptiness, mindfulness) across decades of talks  </li> <li>Construct cross-lingual concept graphs  </li> <li>Surface related sutras, commentaries, and historical contexts  </li> <li>Link concepts across Vietnamese, English, Chinese, Pali, Sanskrit, Tibetan sources  </li> <li>Generate reading paths, study plans, or commentary maps  </li> </ul>"},{"location":"project/future-directions/#32-interactive-dharma-exploration","title":"3.2 Interactive Dharma exploration","text":"<p>Higher-level interfaces could enable:</p> <ul> <li>Interactive Q&amp;A grounded in verifiable citations  </li> <li>Multilingual guided meditation or sutra explanations  </li> <li>Diachronic examination of teachings over time  </li> <li>Timeline exploration of Th\u00edch Nh\u1ea5t H\u1ea1nh\u2019s writings and talks  </li> </ul> <p>This brings the tradition into rich conversation with practitioners and scholars, with accuracy and transparency.</p>"},{"location":"project/future-directions/#4-model-training-corpus-aware-ai-systems","title":"4. Model Training &amp; Corpus-Aware AI Systems","text":"<p>TNH-Scholar could become the foundation for:</p>"},{"location":"project/future-directions/#41-domain-specific-models","title":"4.1 Domain-specific models","text":"<ul> <li>Multilingual Buddhist embedding models</li> <li>Custom translation models fine-tuned on Plum Village sources</li> <li>Topic-specific summarizers</li> <li>Dialogue systems grounded in verifiable citations</li> </ul>"},{"location":"project/future-directions/#42-corpus-aligned-model-evaluation-loops","title":"4.2 Corpus-aligned model evaluation loops","text":"<p>With provenance and pattern-driven evaluation, you could build:</p> <ul> <li>Continuous training pipelines</li> <li>Regression tests for translation or summarization accuracy</li> <li>Style- and lineage-aware evaluation criteria</li> <li>Model quality dashboards</li> </ul> <p>These models would not replace human teachers but enhance research, translation, and accessibility.</p>"},{"location":"project/future-directions/#43-training-pipeline-research-direction","title":"4.3 Training Pipeline Research Direction","text":"<p>Status: Research spike planned (see GitHub Issue #6)</p> <p>The processed content generated by TNH Scholar's AI workflows could serve as training data for model fine-tuning:</p> <p>Research Questions:</p> <ul> <li>How to effectively extract training pairs from processed content?</li> <li>What fine-tuning approaches are most suitable (OpenAI fine-tuning, open source alternatives)?</li> <li>What are resource requirements for training?</li> <li>How to evaluate training effectiveness?</li> <li>What infrastructure is needed?</li> </ul> <p>Potential Approaches:</p> <ul> <li>Extract human-reviewed translation pairs for fine-tuning</li> <li>Use sectioning outputs as examples for structure-aware models</li> <li>Create domain-specific evaluation datasets from validated outputs</li> <li>Develop feedback loops between model performance and corpus quality</li> </ul> <p>Considerations:</p> <ul> <li>Balance between prototype phase priorities and long-term research</li> <li>Resource constraints (compute, storage, API costs)</li> <li>Quality assurance for training data</li> <li>Community involvement in evaluation and validation</li> </ul> <p>This research direction aligns with the long-term vision of corpus-aware AI systems while remaining grounded in current prototype capabilities.</p>"},{"location":"project/future-directions/#5-agentic-application-development-framework","title":"5. Agentic Application Development Framework","text":"<p>TNH-Scholar\u2019s architecture (patterns \u2192 GenAIService \u2192 provenance \u2192 structured data) could generalize to:</p> <p>A modular agentic automation framework for any domain.</p> <p>Possible future descendant projects:</p> <ul> <li>A codebase-maintaining agent system  </li> <li>A domain-specific document-processing AI  </li> <li>A pattern-driven data transformation engine  </li> <li>A provenance-preserving automation fabric</li> </ul> <p>The philosophical and architectural foundations of TNH-Scholar (structured data, documented intent, provenance-first, pattern-based prompting) make it an ideal parent project for a broader agentic ecosystem.</p>"},{"location":"project/future-directions/#6-ecosystem-level-integrations","title":"6. Ecosystem-Level Integrations","text":"<p>Future possibilities include:</p>"},{"location":"project/future-directions/#61-advanced-ux-layers","title":"6.1 Advanced UX layers","text":"<ul> <li>VS Code development agent integration  </li> <li>In-browser corpus exploration environments  </li> <li>Interactive bilingual study interfaces  </li> <li>Multi-panel JVB + text + translation + metadata views  </li> <li>Notebook-based agent workflows (e.g., Jupyter, VS Code notebooks)</li> </ul>"},{"location":"project/future-directions/#62-distributed-scholarly-tools","title":"6.2 Distributed scholarly tools","text":"<ul> <li>APIs for universities or monasteries  </li> <li>Collaborative annotation environments  </li> <li>Integrations with digital humanities platforms  </li> <li>Cross-repository semantic search  </li> <li>Multi-modal study tools for audio/video/text composites  </li> </ul>"},{"location":"project/future-directions/#7-long-term-vision","title":"7. Long-Term Vision","text":"<p>Many of these horizons converge into a singular possibility:</p> <p>A living, evolving, transparent, agent-assisted repository of Plum Village teachings and related Buddhist sources \u2014 continually cleaned, translated, aligned, evaluated, and enriched, with humans guiding the meaning and quality.</p> <p>This is the highest vision of TNH-Scholar:</p> <ul> <li>A bridge between ancient wisdom and modern AI practice.  </li> <li>A platform that supports, rather than automates, interpretation.  </li> <li>A system that grows with care, clarity, and purpose.  </li> </ul> <p>This document is intentionally speculative. As the project matures, some directions will solidify into real designs; others may remain guiding inspirations. It should be updated when major new horizons emerge or when certain horizons become active workstreams.</p>"},{"location":"project/philosophy/","title":"TNH-Scholar Project Philosophy","text":"<p>This document captures the project\u2019s foundational philosophy \u2014 the conceptual, ethical, and methodological lens through which TNH-Scholar is designed and understood.</p>"},{"location":"project/philosophy/#1-humanai-collaboration-not-replacement","title":"1. Human\u2013AI Collaboration, Not Replacement","text":"<p>TNH-Scholar assumes that:</p> <ul> <li>Human practitioners, monastics, and scholars are the primary interpreters of the teachings.</li> <li>AI systems are tools that:</li> <li>Surface patterns,</li> <li>Accelerate mechanical tasks,</li> <li>Support navigation and cross-referencing,</li> <li>Help maintain a complex technical and textual ecosystem.</li> </ul> <p>Philosophically:</p> <p>AI should amplify human care, attention, and discernment \u2014 not bypass them.</p>"},{"location":"project/philosophy/#2-fidelity-and-clarity-over-novelty","title":"2. Fidelity and Clarity Over Novelty","text":"<p>The project values:</p> <ul> <li>Faithful representation of source texts over creative reinterpretation.</li> <li>Clarity of structure and provenance over opaque \u201csmart\u201d behavior.</li> <li>Plain explanations and transparent pipelines over magic.</li> </ul> <p>This applies equally to:</p> <ul> <li>Corpus cleaning and transformation,</li> <li>Translation,</li> <li>Search and retrieval,</li> <li>UX design.</li> </ul>"},{"location":"project/philosophy/#3-one-cohesive-system-many-facets","title":"3. One Cohesive System, Many Facets","text":"<p>TNH-Scholar keeps related work in a single repository because:</p> <ul> <li>The same conceptual and technical foundations (TextObject models, metadata, GenAIService, provenance) underlie:</li> <li>Corpus preparation,</li> <li>Translation pipelines,</li> <li>Knowledge base construction,</li> <li>UX layers (JVB viewer, VS Code integrations),</li> <li>Agentic workflows.</li> <li>Having one cohesive system:</li> <li>Preserves cross-cutting consistency,</li> <li>Makes it easier to reason about long-term evolution,</li> <li>Reduces fragmentation of design philosophy.</li> </ul> <p>The repo is large by necessity, but its coherence is a feature, not a bug.</p>"},{"location":"project/philosophy/#4-narrative-and-structure-as-teaching-supports","title":"4. Narrative and Structure as Teaching Supports","text":"<p>The project treats:</p> <ul> <li>Structure (chapters, sections, exercises, poems, plays, sutras),</li> <li>Metadata (time, place, audience, language),</li> <li>Narrative flow (how a teaching unfolds in context),</li> </ul> <p>as part of the teaching itself, not incidental.  </p> <p>Therefore:</p> <ul> <li>Tools are designed to preserve and highlight structure and narrative, not flatten them.</li> <li>Visualizations (e.g., JVB viewer) should respect page layout and historical context while making them navigable and searchable.</li> </ul>"},{"location":"project/philosophy/#5-agents-as-careful-assistants","title":"5. Agents as Careful Assistants","text":"<p>In future, more agentic patterns may emerge:</p> <ul> <li>Agents that:</li> <li>Read design docs and ADRs,</li> <li>Propose changes,</li> <li>Refactor code,</li> <li>Run evaluation patterns,</li> <li>Suggest next steps.</li> </ul> <p>The philosophical stance is:</p> <ul> <li>Agents are careful assistants, operating within:</li> <li>Clear scopes,</li> <li>Strong provenance,</li> <li>Human-specified constraints,</li> <li>Oversight and review.</li> </ul> <p>They should help maintain alignment with the project\u2019s purpose, not drift away from it.</p>"},{"location":"project/philosophy/#6-embracing-iteration-and-walking-skeletons","title":"6. Embracing Iteration and Walking Skeletons","text":"<p>TNH-Scholar accepts that:</p> <ul> <li>The system will grow in fits and starts.</li> <li>Walking skeletons (minimal, end-to-end prototypes) are favored:</li> <li>Over big, untested designs.</li> <li>Over premature generalization.</li> </ul> <p>The philosophy is:</p> <p>Make something small, real, and testable. Learn from it. Then refine.</p> <p>This aligns with both mindful practice (step-by-step, attentive) and good engineering.</p>"},{"location":"project/philosophy/#7-documentation-as-shared-mind","title":"7. Documentation as Shared Mind","text":"<p>The project treats documentation as:</p> <ul> <li>A shared \u201cmind\u201d for:</li> <li>Human contributors,</li> <li>AI coding agents,</li> <li>Future maintainers.</li> <li>A medium to:</li> <li>Capture philosophy and intent,</li> <li>Record decisions,</li> <li>Guide automated tools.</li> </ul> <p>This is why project philosophy and vision are documented explicitly, not just held in one person\u2019s head or scattered across conversations.</p> <p>This document should evolve as the project\u2019s understanding deepens. When the philosophy shifts, that shift should be visible and explainable here.</p>"},{"location":"project/principles/","title":"TNH-Scholar Project Principles","text":"<p>This document defines the guiding principles, values, and constraints that shape all design and engineering decisions in the TNH-Scholar project.</p>"},{"location":"project/principles/#1-dharma-respecting-design","title":"1. Dharma-Respecting Design","text":"<ul> <li>Faithfulness before cleverness: When in doubt, preserve and accurately represent the source text rather than \u201cimproving\u201d it.</li> <li>Lineage awareness: Tools should make it easy to find and verify original sources and context, not hide them.</li> <li>Transparency about AI: Any AI-generated or AI-transformed content must be clearly identifiable as such.</li> </ul>"},{"location":"project/principles/#2-provenance-and-traceability","title":"2. Provenance and Traceability","text":"<ul> <li>Provenance is first-class: Every non-trivial transformation (cleaning, translation, tagging, summarization) should have:</li> <li>Source reference(s),</li> <li>Transformation description,</li> <li>Model/config snapshot,</li> <li>Timestamp and unique fingerprint where practical.</li> <li>No silent mutations: Avoid transformations that overwrite data without leaving a trace (log, provenance marker, or version history).</li> <li>Inspectable by design: It should be possible for a motivated user to trace \u201cwhere did this output come from?\u201d at a reasonable level of detail.</li> </ul>"},{"location":"project/principles/#3-structured-text-as-canonical","title":"3. Structured Text as Canonical","text":"<ul> <li>Text + metadata &gt; opaque blobs:</li> <li>Use structured representations (XML/HTML-like, JSON, TextObject models) instead of free-floating strings whenever feasible.</li> <li>Preserve structure, then enhance it:</li> <li>Start from accurate transcription and basic structure,</li> <li>Then add layers: headings, sections, footnotes, semantic tags.</li> <li>Metadata is not an afterthought:</li> <li>Section types, page numbers, document IDs, language tags, etc. are essential; treat them as core data, not optional extras.</li> </ul>"},{"location":"project/principles/#4-human-in-the-loop-automation","title":"4. Human-in-the-Loop Automation","text":"<ul> <li>Automation serves humans, not the other way around:</li> <li>Automated pipelines should be reviewable, interruptible, and correctable by humans.</li> <li>Gradual autonomy:</li> <li>Start with simple, inspectable, human-supervised loops,</li> <li>Only move toward more autonomous agents when evaluation and guardrails are strong.</li> <li>Explainable workflows:</li> <li>Pipelines should be describable in plain language (\u201cwhat happens to the text at each step?\u201d).</li> </ul>"},{"location":"project/principles/#5-simplicity-composability-and-testability","title":"5. Simplicity, Composability, and Testability","text":"<ul> <li>Walking skeletons first:</li> <li>Prefer minimal, end-to-end vertical slices over large, half-finished subsystems.</li> <li>Small, composable tools:</li> <li>Favor small utilities that can be combined in flexible ways (CLIs, services, pattern-based prompts) over monolithic pipelines.</li> <li>Type safety and explicit interfaces:</li> <li>Use strong typing and clearly defined models to reduce ambiguity.</li> <li>Documented seams:</li> <li>Integration points (e.g., GenAIService CLI, PromptCatalog APIs, data processing interfaces) should be documented and tested.</li> </ul>"},{"location":"project/principles/#6-documentation-as-architecture","title":"6. Documentation as Architecture","text":"<ul> <li>Docs are part of the system:</li> <li>ADRs, design docs, and philosophy documents are not optional; they are a key part of the project\u2019s architecture.</li> <li>Decision-first thinking:</li> <li>Significant design changes should be accompanied by ADRs or design notes.</li> <li>AI-readable documentation:</li> <li>Documents should be written so that both humans and AI coding agents can use them to generate or maintain code.</li> </ul>"},{"location":"project/principles/#7-ethical-ai-use","title":"7. Ethical AI Use","text":"<ul> <li>No misleading authority:</li> <li>AI outputs should not be presented as if they were canonically \u201cTh\u00edch Nh\u1ea5t H\u1ea1nh speaking,\u201d even if styled after him.</li> <li>Context, not replacement:</li> <li>AI is used to point toward teachings, summarize, and support understanding \u2014 not to replace original texts or human teachers.</li> <li>Respect for privacy and copyrights:</li> <li>Handle sensitive data, unpublished materials, and copyright constraints with care, explicit agreements, and clear boundaries.</li> </ul>"},{"location":"project/principles/#8-engineering-discipline-for-a-long-lived-system","title":"8. Engineering Discipline for a Long-Lived System","text":"<ul> <li>Refactor as you go:</li> <li>Regularly clean interfaces and internal structure as the system evolves.</li> <li>Keep the repo integrable:</li> <li>Maintain a coherent structure that can be understood by new contributors after a reasonable onboarding period.</li> <li>Prefer boring infrastructure:</li> <li>Choose stable, well-understood technologies when possible (e.g., Postgres, standard Python tooling), especially for core components.</li> </ul> <p>These principles are meant to be referenced when making choices. If a change conflicts with them, that conflict should be explicit and justified.</p>"},{"location":"project/vision/","title":"TNH-Scholar Project Vision","text":"<p>This document defines the long-term north star of the TNH-Scholar project \u2014 its purpose, scope, aspirations, and the future directions it aims toward.</p>"},{"location":"project/vision/#1-purpose","title":"1. Purpose","text":"<p>TNH-Scholar exists to serve the living Plum Village tradition by:</p> <ul> <li>Making Th\u00edch Nh\u1ea5t H\u1ea1nh\u2019s teachings more accessible, discoverable, and navigable.</li> <li>Supporting deep, careful study by monastics, lay students, researchers, and practitioners.</li> <li>Providing trustworthy, transparent AI-assisted tools that respect the Dharma, the lineage, and the humans who work with them.</li> </ul> <p>The project is not \u201cjust\u201d a software system. It is a long-term scholarly and technical infrastructure for interacting with a body of teachings.</p>"},{"location":"project/vision/#2-core-vision","title":"2. Core Vision","text":"<p>At a high level, TNH-Scholar aims to become:</p> <ul> <li>A canonical, clean, multi-lingual corpus of Th\u00edch Nh\u1ea5t H\u1ea1nh\u2019s work and related sources, with:</li> <li>High-fidelity text,</li> <li>Rich metadata,</li> <li>Sentence-level alignment across languages,</li> <li>Footnotes, references, and contextual markers.</li> <li>A flexible AI-assisted research environment that:</li> <li>Helps users search, explore, compare, and understand teachings.</li> <li>Provides transparent reasoning and provenance for all AI outputs.</li> <li>A foundation for future interactive tools:</li> <li>Conversational exploration agents,</li> <li>Guided study companions,</li> <li>Practice-support tools,</li> <li>Visual and audio interfaces (e.g., JVB viewer, audio transcription pipelines).</li> </ul>"},{"location":"project/vision/#3-long-horizon-aspirations","title":"3. Long-Horizon Aspirations","text":"<p>Over the long term, TNH-Scholar is envisioned to support:</p> <ul> <li>Cross-lingual, cross-corpus research:</li> <li>Aligning Vietnamese, English, French, Chinese, P\u0101li, Sanskrit, Tibetan sources.</li> <li>Surfacing conceptual parallels across canons and modern commentaries.</li> <li>Rich interactive experiences:</li> <li>Bilingual reading environments,</li> <li>Side-by-side views of scanned pages, cleaned text, translations, and annotations,</li> <li>Audio + transcript + translation aligned at sentence/segment level.</li> <li>Agentic workflows (human-supervised automations):</li> <li>Corpus cleaning and enrichment loops,</li> <li>Translation and evaluation pipelines,</li> <li>Semi-automated test and data-generation loops for models,</li> <li>Eventually: code-aware agents that help maintain the TNH-Scholar system itself.</li> </ul> <p>The long-term vision is not to replace human scholars or practitioners, but to:</p> <p>Extend human capacity for understanding, cross-referencing, and preserving the teachings \u2014 while always keeping human judgment and responsibility at the center.</p>"},{"location":"project/vision/#4-scope-and-non-scope","title":"4. Scope and Non-Scope","text":"<p>In scope:</p> <ul> <li>Tools to structure, clean, annotate, translate, and search the corpus.</li> <li>Infrastructure to support reliable AI-assisted workflows (GenAIService, patterns, provenance).</li> <li>Developer and research tooling (CLI, VS Code integration, batch jobs, evaluation tools).</li> </ul> <p>Out of scope (for TNH-Scholar itself):</p> <ul> <li>Becoming a generic LLM platform unrelated to Plum Village or Buddhist studies.</li> <li>Building closed or opaque systems where the source texts, transformations, and models cannot be inspected or critiqued.</li> <li>Automation that removes humans from the loop in any way that would obscure responsibility, ethical judgment, or interpretive nuance.</li> </ul>"},{"location":"project/vision/#5-relationship-to-spin-offs-and-descendants","title":"5. Relationship to Spin-Offs and Descendants","text":"<p>TNH-Scholar may eventually give rise to:</p> <ul> <li>Separate tools focused on:</li> <li>Software engineering assistance,</li> <li>General AI infrastructure,</li> <li>Generic prompt/pattern frameworks.</li> <li>Specialized research platforms built on its data and abstractions.</li> </ul> <p>The guiding principle is:</p> <p>TNH-Scholar remains the reference home for Dharma-focused, Plum Village\u2013centric work. Spin-offs may reuse its design patterns and components, but do not dilute this core identity.</p>"},{"location":"project/vision/#6-time-scale","title":"6. Time Scale","text":"<p>This project is intended as a multi-year, possibly multi-decade effort. Design choices should:</p> <ul> <li>Favor composability and clarity over short-term hacks.</li> <li>Respect that future maintainers (human and AI) will need to understand the intent behind the system.</li> <li>Support gradual refinement rather than one-off prototypes that cannot grow.</li> </ul> <p>This vision document is living. It should be revisited when major architectural shifts occur or when the project\u2019s role in the broader ecosystem meaningfully changes.</p>"},{"location":"project/repo-root/","title":"Repo Root","text":"<p>Repository root documents are mirrored here for discoverability.  Edit the originals in the repository root; these copies are generated during the docs build.</p>"},{"location":"project/repo-root/#files","title":"Files","text":"<ul> <li>TNH Scholar README</li> <li>TNH Scholar TODO List</li> <li>TNH Scholar CHANGELOG</li> <li>TNH Scholar CONTRIBUTING</li> <li>TNH-Scholar DEV_SETUP</li> <li>TNH Scholar Versioning Policy</li> <li>TNH Scholar Release Checklist</li> </ul>"},{"location":"project/repo-root/changelog/","title":"TNH Scholar CHANGELOG","text":""},{"location":"project/repo-root/changelog/#tnh-scholar-changelog","title":"TNH Scholar CHANGELOG","text":"<p>All notable changes to TNH Scholar will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"project/repo-root/changelog/#020-2025-12-06","title":"[0.2.0] - 2025-12-06","text":""},{"location":"project/repo-root/changelog/#major-infrastructure-improvements","title":"Major Infrastructure Improvements","text":"<p>This release represents a significant maturation of project infrastructure, building on the documentation reorganization from v0.1.4. Combined, these releases deliver comprehensive documentation tooling and streamlined release automation.</p> <p>Versioning Note: The v0.1.3 \u2192 v0.1.4 transition should have been v0.1.3 \u2192 v0.2.0 given the scope of documentation system changes. This release (v0.2.0) acknowledges both the documentation infrastructure (v0.1.4) and release automation improvements as a cohesive minor release milestone.</p>"},{"location":"project/repo-root/changelog/#added","title":"Added","text":"<ul> <li>Comprehensive Dry-Run Mode for Release Workflow</li> <li>Preview all release commands before execution (<code>DRY_RUN=1</code> parameter)</li> <li>Shows exact commands, commit messages, and tag messages before creating them</li> <li>Prevents costly mistakes (git tags, PyPI publishes)</li> <li>Supports all release targets: version bump, commit, tag, publish</li> <li> <p>Clear visual feedback with \"\ud83d\udd0d DRY RUN MODE\" indicator</p> </li> <li> <p>Version Sync Pre-commit Hook</p> </li> <li>Automatically validates <code>pyproject.toml</code> and <code>TODO.md</code> versions match</li> <li>Prevents version drift bugs before commit</li> <li>Clear error messages with fix instructions</li> <li> <p>Runs on every commit via pre-commit framework</p> </li> <li> <p>Python-Native Link Checker</p> </li> <li>Replaced lychee (Rust tool) with md-dead-link-check (Python package)</li> <li>Pure Python toolchain, no external dependencies required</li> <li>Configured to check external links only (MkDocs validates internal links)</li> <li> <p>Better Poetry integration and developer onboarding</p> </li> <li> <p>Comprehensive Release Workflow Documentation</p> </li> <li>Production-ready documentation at <code>docs/development/release-workflow.md</code></li> <li>Complete guide covering prerequisites, step-by-step workflow, troubleshooting</li> <li>Conforms to ADR-DD01 markdown standards</li> <li>Documents dry-run mode, automation features, and best practices</li> <li> <p>Examples, tips for efficient releases, and conventional commit guidance</p> </li> <li> <p>Markdown Link Standard Enforcement</p> </li> <li>Added validation to detect relative links (<code>../</code>) in documentation</li> <li>Enforces absolute path links from <code>/docs/</code> root</li> <li>Updated markdown standards with clear examples and guidelines</li> <li>Prevents MkDocs strict mode issues</li> </ul>"},{"location":"project/repo-root/changelog/#changed","title":"Changed","text":"<ul> <li>Simplified Developer Setup</li> <li>Removed non-Python tooling hard requirements (lychee now optional)</li> <li>All documentation tools managed by Poetry</li> <li> <p>Improved new contributor onboarding experience</p> </li> <li> <p>Code Quality Improvements</p> </li> <li>Applied ruff auto-fixes to 21 files (28 automatic style fixes)</li> <li>Import organization and sorting</li> <li>Removed unused imports and trailing whitespace</li> <li>All tests passing after cleanup (94/94)</li> </ul>"},{"location":"project/repo-root/changelog/#infrastructure","title":"Infrastructure","text":"<ul> <li>Release Automation Phase 2</li> <li>Restored full <code>docs-verify</code> pipeline with Python-native tools</li> <li>Enhanced Makefile with dry-run support for all release targets</li> <li>Version sync validation integrated into git workflow</li> <li>Reduced external tool dependencies</li> </ul>"},{"location":"project/repo-root/changelog/#notes","title":"Notes","text":"<ul> <li>No code functionality changes - purely tooling, automation, and documentation infrastructure</li> <li>This release focuses on making the development and release process \"feel very smooth\"</li> <li>External link checking now uses pure Python stack</li> <li>All 94 tests passing with 1 deprecation warning (audioop in pydub, Python 3.13)</li> </ul>"},{"location":"project/repo-root/changelog/#014-2025-12-05","title":"[0.1.4] - 2025-12-05","text":""},{"location":"project/repo-root/changelog/#added_1","title":"Added","text":"<ul> <li>Documentation System Reorganization (Phase 1) - Major infrastructure overhaul:</li> <li>New hierarchical directory structure with 13 architecture modules</li> <li>Automated documentation tooling (8 build scripts for generation and validation)</li> <li>Comprehensive markdown standards with quality enforcement (markdownlint, codespell, lychee)</li> <li>Auto-generated documentation index and navigation map</li> <li>MkDocs configuration rewrite with filesystem-driven navigation</li> <li>Custom zen-inspired theme with collapsible navigation</li> <li>GitHub Pages deployment with CI/CD verification</li> <li> <p>Drift monitoring between README.md and docs/index.md</p> </li> <li> <p>Pattern \u2192 Prompt Terminology Standardization (ADR-DD03 Phase 1):</p> </li> <li>Renamed user-facing documentation from \"Pattern\" to \"Prompt\" terminology</li> <li>Updated README.md, docs/index.md, getting-started/, user-guide/</li> <li> <p>Retained legacy compatibility (TNH_PATTERN_DIR, --pattern flags)</p> </li> <li> <p>Release Automation (Phase 1):</p> </li> <li>Makefile release targets for streamlined release workflow</li> <li>CHANGELOG generator script (auto-generate from git commits)</li> <li>Automated version bump, commit, tag, and publish workflow</li> <li> <p>Reduces release time from 2 hours to 10 minutes (83% reduction)</p> </li> <li> <p>Architecture Documentation:</p> </li> <li>3 new ADRs: ADR-DD01 (docs reorganization), ADR-DD02 (navigation strategy), ADR-DD03 (terminology migration)</li> <li>Reorganized and standardized existing ADRs across 13 architecture modules</li> <li>Created comprehensive architecture overview and index pages</li> <li> <p>30+ design documents reorganized and renamed to match standards</p> </li> <li> <p>Enhanced Root Documentation:</p> </li> <li>AGENTLOG.md: Detailed session logs for all work sessions</li> <li>CHANGELOG.md: Maintained and updated changelog</li> <li>Expanded README.md with refined vision and architecture snapshot</li> <li>Restructured TODO.md with priority roadmap (Priority 1-3)</li> <li> <p>Enhanced CONTRIBUTING.md and DEV_SETUP.md</p> </li> <li> <p>Build &amp; Quality Infrastructure:</p> </li> <li>Pre-commit hooks configuration for markdown linting and spell checking</li> <li>Makefile targets: <code>docs</code>, <code>docs-verify</code>, <code>docs-quickcheck</code>, <code>check-drift</code>, <code>release-*</code></li> <li>.lychee.toml for link checking configuration</li> <li>.codespell-ignore.txt for technical/dharma terms</li> <li> <p>.markdownlint.json for markdown linting rules</p> </li> <li> <p>Research Documentation:</p> </li> <li>New: RAG research directions document</li> <li>Reorganized: Existing research files renamed to match markdown standards</li> </ul>"},{"location":"project/repo-root/changelog/#changed_1","title":"Changed","text":"<ul> <li>Directory Structure: Reorganized from flat to hierarchical architecture</li> <li>docs/cli/ \u2192 docs/cli-reference/ (consolidated CLI documentation)</li> <li>docs/design/ \u2192 docs/architecture// (module-specific organization) <li>docs/user-guide/patterns.md \u2192 docs/user-guide/prompt-system.md</li> <li>Split design-guide.md into style-guide.md and design-principles.md (Python standards)</li> <li> <p>Moved object-service architecture from development/ to architecture/</p> </li> <li> <p>CI/CD Workflows:</p> </li> <li>Enhanced .github/workflows/ci.yml with markdownlint, codespell, link checking</li> <li> <p>Added .github/workflows/docs.yml for documentation build and GitHub Pages deployment</p> </li> <li> <p>Navigation: MkDocs configuration rewrite with absolute link validation enabled</p> </li>"},{"location":"project/repo-root/changelog/#documentation","title":"Documentation","text":"<ul> <li>Established comprehensive markdown standards in docs/docs-ops/markdown-standards.md</li> <li>ADR naming convention: <code>adr-&lt;modulecode&gt;&lt;number&gt;-&lt;descriptor&gt;.md</code></li> <li>Module-specific storage: <code>docs/architecture/&lt;module&gt;/adr/</code> organization</li> <li>Archive structure: <code>docs/archive/</code> (top-level) + <code>docs/architecture/&lt;module&gt;/archive/</code> (module-specific)</li> </ul>"},{"location":"project/repo-root/changelog/#notes_1","title":"Notes","text":"<ul> <li>No breaking changes: Purely documentation infrastructure improvements</li> <li>Navigation changes: Users should update bookmarks from docs/cli/ to docs/cli-reference/</li> <li>Phase 2 work: Outstanding items tracked in TODO #9 (archive expansion, gap filling, additional testing)</li> <li>Merge PR: Merged docs-reorg branch with 96 commits, 429 files changed (67,885 additions, 7,616 deletions)</li> </ul>"},{"location":"project/repo-root/changelog/#references","title":"References","text":"<ul> <li>ADR-DD01: Documentation System Reorganization Strategy</li> <li>ADR-DD02: Main Content and Navigation Strategy</li> <li>ADR-DD03: Pattern\u2192Prompt Terminology Migration</li> <li>TODO #9: Documentation Reorganization</li> </ul>"},{"location":"project/repo-root/changelog/#unreleased","title":"Unreleased","text":""},{"location":"project/repo-root/changelog/#documentation_1","title":"Documentation","text":"<ul> <li>Auto-generated Documentation Index System (2025-12-05):</li> <li>Implemented dual-format auto-generated documentation indexing (ADR-DD01 Addendum 3)</li> <li>Created <code>scripts/generate_doc_index.py</code> to generate both <code>documentation_index.md</code> (comprehensive searchable table) and <code>documentation_map.md</code> (hierarchical navigation)</li> <li>Created <code>scripts/append_doc_map_to_index.py</code> to inject documentation map into index.md at build time</li> <li>Documentation Map now auto-generated from filesystem and frontmatter metadata, eliminating manual maintenance</li> <li> <p>Both formats always in sync with actual documentation structure</p> </li> <li> <p>Phase 2 Documentation Reorganization (ADR-DD01/ADR-DD02 completion):</p> </li> <li>Completed comprehensive file reorganization: renamed 75+ architecture documents for clarity and consistency</li> <li>Established canonical naming patterns: <code>adr-XX-descriptive-name.md</code> for ADRs, <code>system-design.md</code> for design docs</li> <li>Created README.md files for major sections (architecture/, cli/, development/, getting-started/)</li> <li>Removed obsolete CLI reference stubs (pending auto-generation)</li> <li>Archived historical research artifacts and experiment files</li> <li>Reorganized reference materials (yt-dlp docs, GPT-4 experiments) into categorized subdirectories</li> <li>Updated all cross-references and internal links for reorganized structure</li> <li> <p>Achieved zero mkdocs build warnings after reorganization</p> </li> <li> <p>Standardized Markdown front matter, titles, and summary paragraphs across the docs tree (prompt-pattern files excluded pending dedicated schema).</p> </li> <li>Updated <code>docs/docs-ops/markdown-standards.md</code> to spell out the Prompt Template front matter exception.</li> <li>Regenerated <code>documentation_index.md</code> after metadata fixes.</li> <li>Filesystem-driven navigation: Removed hardcoded <code>mkdocs.yaml</code> nav section and adopted <code>mkdocs-literate-nav</code> + <code>mkdocs-gen-files</code>.</li> <li>Added <code>docs/nav.md</code> as the source of truth for navigation hierarchy.</li> <li>MkDocs now automatically syncs nav with filesystem structure.</li> <li>CLI docs and prompt template catalog are auto-generated from codebase artifacts.</li> <li>Fixed GitHub Actions workflows: YAML parsing errors in frontmatter, package installation, and GitHub Pages deployment permissions.</li> <li>Cleaned docstrings and type hints in the AI/text/audio/journal/ocr modules so MkDocs + Griffe stop emitting annotation warnings.</li> <li>Added project philosophy and vision documentation in <code>docs/project/</code> (philosophy.md, vision.md, principles.md, conceptual-architecture.md, future-directions.md).</li> <li>Added Parallax Press stakeholder overview document at <code>docs/tnh_scholar_parallax_overview.md</code>.</li> <li>Updated README.md with refined vision statement and getting started section.</li> <li>Updated docs/index.md with expanded vision and goals.</li> <li>Updated TODO.md with Part 4g documentation testing workflow.</li> <li>Pattern\u2192Prompt terminology standardization (ADR-DD03 Phase 1): Updated all user-facing documentation to use \"Prompt\" instead of \"Pattern\" to align with industry standards and gen-ai-service refactoring.</li> <li>Added historical terminology note to docs/index.md</li> <li>Updated README, getting-started/, user-guide/ documentation</li> <li>Renamed docs/user-guide/patterns.md \u2192 prompts.md</li> <li>Renamed docs/architecture/pattern-system/ \u2192 prompt-system/</li> <li>Updated ADR-DD01 and ADR-DD02 references</li> <li>Documentation structure reorganization (Python community standards):</li> <li>Split design-guide.md into style-guide.md (code formatting, PEP 8) and design-principles.md (architectural patterns)</li> <li>Moved object-service architecture to canonical location (development/architecture/ \u2192 architecture/object-service/)</li> <li>Converted object-service-design-blueprint-v2 to ADR-OS01 (adopted V3, deleted V1)</li> <li>Created design-overview.md and updated implementation-status.md with resolved items</li> <li>Created forward-looking prompt-architecture.md documenting current V1 and planned V2 (PromptCatalog, fingerprinting, VS Code integration)</li> <li>Moved pattern-core-design.md to archive/ with historical terminology note</li> <li>Fixed all 35 mkdocs build --strict warnings from reorganization (link updates, regenerated index)</li> <li>Navigation cleanup: removed the mirrored \u201cProject Docs\u201d (repo-root copies) from MkDocs navigation to avoid confusing duplication with <code>docs/project</code>.</li> </ul>"},{"location":"project/repo-root/changelog/#developer-experience","title":"Developer Experience","text":"<ul> <li>Added pre-commit hooks configuration with codespell, trailing whitespace removal, and basic file checks.</li> <li>Added lychee link checker with <code>.lychee.toml</code> configuration for documentation quality assurance.</li> <li>Added Makefile targets for link checking (<code>make check-links</code>, <code>make check-links-verbose</code>).</li> <li>Added <code>scripts/sync_root_docs.py</code> to sync root-level docs into MkDocs structure and wired into build system.</li> <li>MkDocs strict mode cleanup: Fixed all 136 warnings to achieve zero-warning builds.</li> <li>Fixed autorefs warnings in TODO.md and regenerated mirrored root docs.</li> <li>Aligned docstrings/signatures and type annotations across AI/text/audio/journal/OCR/utils modules to satisfy griffe.</li> <li>Restored full mkdocstrings options in API documentation.</li> <li>Created <code>docs/docs-ops/mkdocs-warning-backlog.md</code> to track progress and future doc additions.</li> </ul>"},{"location":"project/repo-root/contributing-root/","title":"TNH Scholar CONTRIBUTING","text":""},{"location":"project/repo-root/contributing-root/#tnh-scholar-contributing","title":"TNH Scholar CONTRIBUTING","text":"<p>Guidance on contributing to the TNH Scholar Project. TNH Scholar is rapidly evolving, but we strive for a predictable, reproducible development workflow.  This document summarizes how to get set up, what tools we use, and what we expect in pull requests.</p>"},{"location":"project/repo-root/contributing-root/#1-development-environment","title":"1. Development environment","text":"<p>TNH Scholar uses pyenv for Python version pinning and Poetry for dependency management. Follow DEV_SETUP.md for detailed instructions. The essentials are:</p> <ol> <li>Install <code>pyenv</code> and <code>Python 3.12.4</code> (<code>pyenv install 3.12.4 --skip-existing</code>).</li> <li>Install Poetry and enable in-project environments: <code>poetry config virtualenvs.in-project true</code>.</li> <li>Clone the repo and run one of the Make targets:</li> </ol> <pre><code>make setup        # runtime dependencies only\nmake setup-dev    # runtime + dev dependencies (recommended for contributors)\n</code></pre> <p>The Makefile mirrors our CI configuration, so using it locally guarantees parity.</p>"},{"location":"project/repo-root/contributing-root/#2-day-to-day-workflow","title":"2. Day-to-day workflow","text":"<ul> <li>Create a feature branch from <code>main</code> (or the branch requested in your issue).</li> <li>Keep changes focused; open separate PRs for unrelated fixes.</li> <li>Use Poetry to run tools so the correct virtualenv is used:</li> </ul> <pre><code>make lint         # poetry run ruff check .\nmake format       # poetry run ruff format .\npoetry run mypy src/\nmake test         # poetry run pytest\n</code></pre> <ul> <li>Install git hooks so notebook prep and other checks run automatically:</li> </ul> <pre><code>pre-commit install\n</code></pre> <p>When you commit notebook changes, the hook creates/upgrades <code>*_local.ipynb</code> copies and strips outputs from the originals.</p> <ul> <li>Update documentation (README, DEV_SETUP, etc.) whenever behavior or commands change.</li> </ul>"},{"location":"project/repo-root/contributing-root/#3-pull-request-checklist","title":"3. Pull request checklist","text":"<ul> <li> All tests pass locally (<code>make test</code>).</li> <li> Linting and formatting pass (<code>make lint</code>, <code>make format</code>, <code>poetry run mypy src/</code>).</li> <li> New functionality includes tests and, when relevant, documentation updates.</li> <li> Commit messages and PR descriptions explain the motivation and approach.</li> <li> CI is green \u2014 it runs the same Poetry-based workflow described above.</li> </ul>"},{"location":"project/repo-root/contributing-root/#4-reporting-issues-proposing-ideas","title":"4. Reporting issues &amp; proposing ideas","text":"<p>Use GitHub Issues to report bugs or request features. Please include:</p> <ul> <li>Steps to reproduce (commands, inputs, expected vs. actual behavior).</li> <li>Environment details (OS, Python version, whether you're using the Poetry env).</li> <li>Relevant logs or stack traces.</li> </ul> <p>For design discussions, open an issue and tag it with <code>discussion</code> so we can keep the history public.</p>"},{"location":"project/repo-root/contributing-root/#5-need-help","title":"5. Need help?","text":"<p>If you get stuck during setup, consult DEV_SETUP.md first \u2014 it is the canonical source for environment instructions. If that doesn\u2019t answer your question, open an issue describing the problem and what you have tried.</p> <p>Thanks for helping make TNH Scholar better!</p>"},{"location":"project/repo-root/dev-setup-guide/","title":"TNH-Scholar DEV_SETUP","text":""},{"location":"project/repo-root/dev-setup-guide/#tnh-scholar-dev_setup","title":"TNH-Scholar DEV_SETUP","text":"<p>This document outlines the standard development environment for TNH\u2011Scholar. The goals are: clarity, reproducibility, stability, and low onboarding friction.</p> <p>The project uses pyenv to manage Python versions and Poetry to manage dependencies and virtual environments.</p>"},{"location":"project/repo-root/dev-setup-guide/#1-install-pyenv-python-version-manager","title":"1. Install pyenv (Python version manager)","text":"<p>Follow official instructions for your platform: https://github.com/pyenv/pyenv</p> <p>Recommended macOS setup (Homebrew):</p> <pre><code>brew install pyenv\n</code></pre> <p>After installation, ensure your shell is configured:</p> <pre><code>echo 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.zshrc\n</code></pre> <p>Reload your shell:</p> <pre><code>source ~/.zshrc\n</code></pre>"},{"location":"project/repo-root/dev-setup-guide/#2-install-the-required-python-version","title":"2. Install the required Python version","text":"<p>TNH\u2011Scholar currently targets: <code>Python 3.12.4</code></p> <p>Install it with pyenv:</p> <pre><code>pyenv install 3.12.4 --skip-existing\n</code></pre> <p>Then set the local version inside the project root:</p> <pre><code>cd tnh-scholar\npyenv local 3.12.4\n</code></pre> <p>This ensures all commands inside this directory use that exact Python interpreter.</p>"},{"location":"project/repo-root/dev-setup-guide/#3-install-poetry-dependency-project-manager","title":"3. Install Poetry (dependency + project manager)","text":"<p>Installation (official method):</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Ensure Poetry is added to your PATH (the installer will tell you where to place this):</p> <pre><code>export PATH=\"$HOME/.local/bin:$PATH\"\n</code></pre> <p>Verify installation:</p> <pre><code>poetry --version\n</code></pre>"},{"location":"project/repo-root/dev-setup-guide/#4-configure-poetry-for-inproject-virtual-environments","title":"4. Configure Poetry for in\u2011project virtual environments","text":"<p>This keeps the <code>.venv/</code> folder inside the repository, making environment resolution easier for IDEs, Jupyter, and tooling.</p> <pre><code>poetry config virtualenvs.in-project true\n</code></pre>"},{"location":"project/repo-root/dev-setup-guide/#5-create-the-virtual-environment-install-project-dependencies","title":"5. Create the virtual environment &amp; install project dependencies","text":"<p>Inside the project root, the recommended way to create the environment is via the <code>Makefile</code> (see below):</p> <pre><code>make setup\n</code></pre> <p>This will:</p> <ul> <li>Ensure the correct Python version via pyenv.</li> <li>Create or reuse the Poetry-managed virtualenv.</li> <li>Install all core (non-dev) dependencies from <code>pyproject.toml</code>.</li> </ul> <p>For development work (tests, linting, docs, Jupyter, etc.), use:</p> <pre><code>make setup-dev\n</code></pre> <p>This will:</p> <ul> <li>Do everything <code>make setup</code> does plus:</li> <li>Install all dev dependencies (pytest, ruff, ipykernel, etc.).</li> <li>Register the <code>tnh-scholar</code> Jupyter kernel.</li> </ul> <p>If you prefer to do it manually without <code>make</code>:</p> <pre><code>poetry env use python\npoetry install\npoetry install --with dev\npoetry run python -m ipykernel install --user --name tnh-scholar --display-name \"Python (tnh-scholar)\"\n</code></pre>"},{"location":"project/repo-root/dev-setup-guide/#6-common-development-commands","title":"6. Common development commands","text":""},{"location":"project/repo-root/dev-setup-guide/#run-the-test-suite","title":"Run the test suite","text":"<pre><code>poetry run pytest\n</code></pre>"},{"location":"project/repo-root/dev-setup-guide/#lint-format","title":"Lint &amp; format","text":"<pre><code>poetry run ruff check .\npoetry run ruff format .\n</code></pre>"},{"location":"project/repo-root/dev-setup-guide/#run-any-script-or-tool","title":"Run any script or tool","text":"<pre><code>poetry run &lt;command&gt;\n</code></pre> <p>Example:</p> <pre><code>poetry run python scripts/some_tool.py\n</code></pre>"},{"location":"project/repo-root/dev-setup-guide/#7-makefile","title":"7. Makefile","text":"<p>The repository includes a <code>Makefile</code> at the project root:</p> <pre><code>PYTHON_VERSION = 3.12.4\nPOETRY        = poetry\n\n.PHONY: setup setup-dev test lint format kernel\n\nsetup:\n pyenv install -s $(PYTHON_VERSION)\n pyenv local $(PYTHON_VERSION)\n $(POETRY) env use python\n $(POETRY) install\n\nsetup-dev:\n pyenv install -s $(PYTHON_VERSION)\n pyenv local $(PYTHON_VERSION)\n $(POETRY) env use python\n $(POETRY) install --with dev\n $(POETRY) run python -m ipykernel install --user --name tnh-scholar --display-name \"Python (tnh-scholar)\"\n\ntest:\n $(POETRY) run pytest\n\nlint:\n $(POETRY) run ruff check .\n\nformat:\n $(POETRY) run ruff format .\n\nkernel:\n $(POETRY) run python -m ipykernel install --user --name tnh-scholar --display-name \"Python (tnh-scholar)\"\n</code></pre> <p>Standard onboarding:</p> <pre><code># minimal (no dev tools)\nmake setup\n\n# full development environment (recommended)\nmake setup-dev\n</code></pre>"},{"location":"project/repo-root/dev-setup-guide/#8-optional-jupyter-kernel-registration","title":"8. Optional: Jupyter kernel registration","text":"<p>The recommended way to register the Jupyter kernel is via:</p> <pre><code>make setup-dev\n</code></pre> <p>which automatically installs dev dependencies and registers the kernel.</p> <p>If you need to re-register the kernel manually:</p> <pre><code>make kernel\n</code></pre> <p>After that, VS Code or Jupyter will show a kernel named <code>Python (tnh-scholar)</code>.</p>"},{"location":"project/repo-root/dev-setup-guide/#9-ci-and-production-recommendations","title":"9. CI and production recommendations","text":"<ul> <li>Commit both <code>pyproject.toml</code> and <code>poetry.lock</code> to version control.</li> <li>In CI, always install using the lockfile:</li> </ul> <pre><code>poetry install --no-root --no-interaction --no-ansi\n</code></pre> <ul> <li>When updating dependencies, update intentionally:</li> </ul> <pre><code>poetry add &lt;package&gt;\npoetry update &lt;package&gt;\n</code></pre> <ul> <li>Avoid global packages. Use Poetry environments or <code>pipx</code> for global tools.</li> </ul>"},{"location":"project/repo-root/dev-setup-guide/#10-summary","title":"10. Summary","text":"<p>Your workflow should look like this:</p> <ol> <li>pyenv selects the Python version  </li> <li>Poetry manages the environment + dependencies  </li> <li><code>.venv/</code> stays local to the project  </li> <li>All tools run through <code>poetry run</code> </li> <li>Lockfile ensures reproducible builds  </li> </ol> <p>This combination is stable, clean, and well\u2011understood\u2014ideal for TNH\u2011Scholar\u2019s long\u2011term architecture and multi\u2011year development cycle.</p>"},{"location":"project/repo-root/release_checklist/","title":"TNH Scholar Release Checklist","text":""},{"location":"project/repo-root/release_checklist/#tnh-scholar-release-checklist","title":"TNH Scholar Release Checklist","text":"<p>Checklist of tasks required before publishing a TNH Scholar release.</p> <ul> <li> Update version in pyproject.toml</li> <li> Update CHANGELOG.md</li> <li> Run full test suite: <code>pytest</code></li> <li> Run type checks: <code>mypy</code></li> <li> Run linting: <code>ruff check .</code></li> <li> Build test</li> <li> TestPyPI upload &amp; install test</li> <li> PyPI upload</li> <li> Tag release in git</li> </ul>"},{"location":"project/repo-root/repo-readme/","title":"TNH Scholar README","text":""},{"location":"project/repo-root/repo-readme/#tnh-scholar-readme","title":"TNH Scholar README","text":"<p>TNH Scholar is an AI-driven project designed to explore, query, process and translate the teachings of Thich Nhat Hanh and the Plum Village community. The project provides tools for practitioners and scholars to engage with mindfulness and spiritual wisdom through natural language processing and machine learning models.</p>"},{"location":"project/repo-root/repo-readme/#vision-goals","title":"Vision &amp; Goals","text":"<p>TNH Scholar aims to make the teachings of Thich Nhat Hanh and the Plum Village tradition more accessible and discoverable through modern AI techniques. By combining natural language processing, machine learning, and careful curation, we create pathways for practitioners and scholars to translate, search, organize, process and otherwise find meaningful connections among the body of teachings.</p>"},{"location":"project/repo-root/repo-readme/#features","title":"Features","text":"<p>TNH Scholar is currently in active prototyping. Key capabilities:</p> <ul> <li>Audio and transcript processing: <code>audio-transcribe</code> with diarization and YouTube support</li> <li>Text formatting and translation: <code>tnh-gen</code> CLI (rename in flight; see ADR-VSC01 and ADR-VSC02) for punctuation, translation, sectioning, and prompt-driven processing</li> <li>Acquisition utilities: <code>ytt-fetch</code> for transcripts; <code>token-count</code> and <code>nfmt</code> for prep and planning</li> <li>Setup and configuration: <code>tnh-setup</code> plus guided config in Getting Started</li> <li>Prompt system: See ADRs under docs/architecture/prompt-system/index.md for decisions and roadmap</li> </ul>"},{"location":"project/repo-root/repo-readme/#quick-start","title":"Quick Start","text":""},{"location":"project/repo-root/repo-readme/#installation-pypi","title":"Installation (PyPI)","text":"<pre><code>pip install tnh-scholar\ntnh-setup\n</code></pre> <p>Prerequisites: Python 3.12.4+, OpenAI API key (CLI tools), Google Vision (optional OCR), pip or Poetry.</p>"},{"location":"project/repo-root/repo-readme/#development-setup-from-source","title":"Development setup (from source)","text":"<p>Follow DEV_SETUP.md for the full workflow. Short version:</p> <pre><code>pyenv install 3.12.4\npoetry config virtualenvs.in-project true\nmake setup        # runtime deps\nmake setup-dev    # runtime + dev deps (recommended)\n</code></pre>"},{"location":"project/repo-root/repo-readme/#set-openai-credentials","title":"Set OpenAI credentials","text":"<pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"project/repo-root/repo-readme/#example-usage","title":"Example usage","text":"<p>Transcribe Audio from YouTube:</p> <pre><code>audio-transcribe --yt_url \"https://youtube.com/watch?v=example\" --split --transcribe\n</code></pre> <p>Process Text with TNH-GEN (was previously tnh-fab: CLI refactor and rename in progress; see ADR-VSC01/VSC02):</p> <p>Download Video Transcripts:</p> <pre><code>ytt-fetch \"https://youtube.com/watch?v=example\" -l en -o transcript.txt\n</code></pre>"},{"location":"project/repo-root/repo-readme/#getting-started","title":"Getting Started","text":"<ul> <li>Practitioners: Install, configure credentials, and follow the Quick Start Guide; workflows live in the User Guide.</li> <li>Developers: Set up via DEV_SETUP.md and Contributing; review System Design and the CLI docs; run <code>make docs</code> to view locally.</li> <li>Project Philosophy &amp; Vision: Developers and researchers should review the conceptual foundations in <code>docs/project/vision.md</code>, <code>docs/project/philosophy.md</code>, <code>docs/project/principles.md</code>, and <code>docs/project/conceptual-architecture.md</code> to understand the system\u2019s long-term direction and design intent.</li> <li>Researchers: Explore Research for experiments and direction; see Architecture for pipelines/ADRs (e.g., ADR-K01).</li> </ul>"},{"location":"project/repo-root/repo-readme/#documentation-overview","title":"Documentation Overview","text":"<p>Comprehensive documentation is available in multiple formats:</p> <ul> <li>Online Documentation: aaronksolomon.github.io/tnh-scholar/</li> <li>GitHub Repository: github.com/aaronksolomon/tnh-scholar</li> </ul>"},{"location":"project/repo-root/repo-readme/#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started \u2013 Installation, setup, and first steps</li> <li>CLI Docs \u2013 Command-line tool documentation</li> <li>User Guide \u2013 Detailed usage guides, prompts, and workflows</li> <li>API Reference \u2013 Python API documentation for programmatic use</li> <li>Architecture \u2013 Design decisions, ADRs, and system overview</li> <li>Development \u2013 Contributing guidelines and development setup</li> <li>Research \u2013 Research notes, experiments, and background</li> <li>Documentation Operations \u2013 Documentation roadmap and maintenance</li> </ul>"},{"location":"project/repo-root/repo-readme/#architecture-overview","title":"Architecture Overview","text":"<ul> <li>Documentation strategy: ADR-DD01 and ADR-DD02</li> <li>GenAI, transcription, and prompt system ADRs live under Architecture (see ADR-A*, ADR-TR*, ADR-PT*).</li> <li>System design references: Object\u2013Service Design and System Design.</li> </ul>"},{"location":"project/repo-root/repo-readme/#development","title":"Development","text":"<ul> <li>Common commands: <code>make test</code>, <code>make lint</code>, <code>make format</code>, <code>make docs</code>, <code>poetry run mypy src/</code></li> <li>Optional dependency groups (development only): <code>tnh-scholar[ocr]</code>, <code>tnh-scholar[gui]</code>, <code>tnh-scholar[query]</code>, <code>tnh-scholar[dev]</code></li> <li>Troubleshooting and workflows: DEV_SETUP.md</li> </ul>"},{"location":"project/repo-root/repo-readme/#contributing","title":"Contributing","text":"<p>See CONTRIBUTING.md for coding standards, testing expectations, and PR workflow. We welcome contributions from practitioners, developers, and scholars.</p>"},{"location":"project/repo-root/repo-readme/#project-status","title":"Project Status","text":"<p>TNH Scholar is currently in alpha stage (v0.1.3). Expect ongoing API and workflow changes during active development.</p>"},{"location":"project/repo-root/repo-readme/#support-community","title":"Support &amp; Community","text":"<ul> <li>Bug reports &amp; feature requests: GitHub Issues</li> <li>Questions &amp; discussions: GitHub Discussions</li> </ul>"},{"location":"project/repo-root/repo-readme/#documentation-map","title":"Documentation Map","text":"<p>For an auto-generated list of every document (titles and metadata), see the Documentation Index.</p>"},{"location":"project/repo-root/repo-readme/#license","title":"License","text":"<p>This project is licensed under the GPL-3.0 License.</p> <p>For more information, visit the full documentation or explore the source code.</p>"},{"location":"project/repo-root/todo-list/","title":"TNH Scholar TODO List","text":""},{"location":"project/repo-root/todo-list/#tnh-scholar-todo-list","title":"TNH Scholar TODO List","text":"<p>Roadmap tracking the highest-priority TNH Scholar tasks and release blockers.</p> <p>Last Updated: 2025-12-09 Version: 0.2.0 (Alpha) Status: Active Development - Documentation Reorganization Phase</p>"},{"location":"project/repo-root/todo-list/#priority-roadmap","title":"Priority Roadmap","text":"<p>This section organizes work into three priority levels based on criticality for production readiness.</p>"},{"location":"project/repo-root/todo-list/#priority-1-critical-path-to-beta","title":"Priority 1: Critical Path to Beta","text":"<p>Goal: Remove blockers to production readiness. These items must be completed before beta release.</p> <p>Status: \u2158 Complete \u2705</p>"},{"location":"project/repo-root/todo-list/#1-add-pytest-to-ci","title":"1. \u2705 Add pytest to CI","text":"<ul> <li>Status: COMPLETED</li> <li>Location: .github/workflows/ci.yml</li> <li>What: Tests now run in CI with coverage reporting</li> <li>Command: <code>pytest --maxfail=1 --cov=tnh_scholar --cov-report=term-missing</code></li> </ul>"},{"location":"project/repo-root/todo-list/#2-fix-packaging-issues","title":"2. \u2705 Fix Packaging Issues","text":"<ul> <li>Status: COMPLETED</li> <li>Location: pyproject.toml</li> <li>What:</li> <li>\u2705 Runtime dependencies declared (pydantic-settings, python-json-logger, tenacity)</li> <li>\u2705 Python version pinned to 3.12.4</li> <li>\u26a0\ufe0f Pattern directory import issue still pending (see Configuration &amp; Data Layout below)</li> </ul>"},{"location":"project/repo-root/todo-list/#3-remove-library-sysexit-calls","title":"3. \u2705 Remove Library sys.exit() Calls","text":"<ul> <li>Status: COMPLETED</li> <li>Location: gen_ai_service/infra/issue_handler.py</li> <li>What: Library code now raises ConfigurationError by default</li> <li>Test: tests/gen_ai_service/test_service.py::test_missing_api_key_raises_configuration_error</li> </ul>"},{"location":"project/repo-root/todo-list/#4-implement-core-stubs","title":"4. \ud83d\udea7 Implement Core Stubs","text":"<ul> <li>Status: PRELIMINARY IMPLEMENTATION COMPLETE \u2705 - Needs Polish &amp; Registry Integration</li> <li>Priority: HIGH</li> <li>Review: Code review completed 2025-12-10 - Grade: A- (92/100) \u2b50\u2b50\u2b50\u2b50\u2b50</li> <li>Core Implementation:</li> <li> params_policy.py \u2014 Policy precedence implemented \u2705<ul> <li>\u2705 Policy precedence: call hint \u2192 prompt metadata \u2192 defaults</li> <li>\u2705 Settings cached via <code>@lru_cache</code> (excellent optimization)</li> <li>\u2705 Strong typing with <code>ResolvedParams</code> Pydantic model</li> <li>\u2705 Routing diagnostics in <code>routing_reason</code> field</li> <li>Score: 95/100 - Excellent implementation</li> </ul> </li> <li> model_router.py \u2014 Capability-based routing implemented \u2705<ul> <li>\u2705 Declarative routing table with <code>_MODEL_CAPABILITIES</code></li> <li>\u2705 Structured output fallback (JSON mode capability switching)</li> <li>\u2705 Intent-aware architecture foundation</li> <li>\u26a0\ufe0f Intent routing currently placeholder (line 98-101)</li> <li>Score: 92/100 - Strong implementation</li> </ul> </li> <li> safety_gate.py \u2014 Three-layer safety checks implemented \u2705<ul> <li>\u2705 Character limit, context window, budget estimation</li> <li>\u2705 Typed exceptions (<code>SafetyBlocked</code>)</li> <li>\u2705 Structured <code>SafetyReport</code> with actionable diagnostics</li> <li>\u2705 Content type handling (string/list with warnings)</li> <li>\u2705 Prompt metadata integration (<code>safety_level</code>)</li> <li>\u26a0\ufe0f Price constant hardcoded (line 30: <code>_PRICE_PER_1K_TOKENS = 0.005</code>)</li> <li>\u26a0\ufe0f Post-check currently stubbed</li> <li>Score: 94/100 - Excellent implementation</li> </ul> </li> <li> <p> completion_mapper.py \u2014 Bi-directional mapping implemented \u2705</p> <ul> <li>\u2705 Clean transport \u2192 domain transformation</li> <li>\u2705 Error details surfaced in <code>policy_applied</code></li> <li>\u2705 Status handling (OK/FAILED/INCOMPLETE)</li> <li>\u2705 Pure mapper functions (no side effects)</li> <li>\u26a0\ufe0f <code>policy_applied</code> uses <code>Dict[str, object]</code> (should be more specific)</li> <li>Score: 91/100 - Strong implementation</li> </ul> </li> <li> <p>High Priority (Before Merging):</p> </li> <li> Add Google-style docstrings to public functions (style-guide.md:315-341)<ul> <li><code>apply_policy()</code>, <code>select_provider_and_model()</code>, <code>pre_check()</code>, <code>post_check()</code>, <code>provider_to_completion()</code></li> </ul> </li> <li> Move <code>_PRICE_PER_1K_TOKENS</code> constant to Settings or registry (blocks ADR-A14)<ul> <li>Moved to <code>Settings.price_per_1k_tokens</code>; safety gate now consumes setting.</li> </ul> </li> <li> <p> Type tightening in completion_mapper</p> <ul> <li>Added <code>PolicyApplied</code> alias (<code>dict[str, str | int | float]</code>).</li> </ul> </li> <li> <p>Medium Priority (V1 Completion):</p> </li> <li> <p> Promote <code>policy_applied</code> typing to a shared domain type (CompletionEnvelope) to avoid loose <code>dict</code> usage across the service.</p> </li> <li> <p> Capability registry extraction (\u2192 ADR-A14)</p> <ul> <li>Create <code>runtime_assets/registries/providers/openai.jsonc</code></li> <li>Implement <code>RegistryLoader</code> with JSONC support</li> <li>Refactor <code>model_router.py</code> to use registry</li> <li>Refactor <code>safety_gate.py</code> to use registry pricing</li> <li>See: ADR-A14: File-Based Registry System</li> </ul> </li> <li> Intent routing implementation<ul> <li>Document planned approach or create follow-up issue</li> <li>Current: placeholder at model_router.py:98-101</li> </ul> </li> <li> <p> Post-check safety implementation</p> <ul> <li>Add content validation logic to <code>safety_gate.post_check()</code></li> <li>Current: stubbed at safety_gate.py:124-133</li> </ul> </li> <li> <p>Low Priority (Future Work):</p> </li> <li> Warning enum system<ul> <li>Create typed warning codes instead of strings</li> <li>Affects: safety_gate, completion_mapper, model_router</li> </ul> </li> <li> Enhanced diagnostics<ul> <li>More granular routing reasons</li> <li>Detailed safety check diagnostics</li> </ul> </li> <li> <p> Message.content Type Architecture Investigation (design quality, non-blocking)</p> <ul> <li>Location: gen_ai_service/models/domain.py:92-96</li> <li>Issue: Sourcery identifies <code>Union[str, List[ChatCompletionContentPartParam]]</code> as source of complexity</li> <li>Context: Current design intentionally supports OpenAI's flexible content API (plain text OR structured parts with images/etc)</li> <li>Investigation Areas:</li> <li>Document current usage patterns across codebase</li> <li>Assess downstream complexity: where are type checks needed?</li> <li>Evaluate normalization strategies (always list? separate fields? utility methods?)</li> <li>Consider provider compatibility (Anthropic, etc)</li> <li>Draft ADR or addendum to existing GenAI ADRs if design change warranted</li> <li>Impact: Affects message representation throughout GenAIService</li> </ul> </li> <li> <p>Review Summary:</p> </li> <li>Strengths: Excellent architectural alignment, strong typing, proper separation of concerns, clean integration</li> <li>Minor Issues: Missing function docstrings, hardcoded price constant, one dict type needing refinement</li> <li>Overall: Production-ready with minor polish (estimated 1 hour total)</li> <li>Detailed Review: See code review session 2025-12-10</li> </ul>"},{"location":"project/repo-root/todo-list/#5-unify-openai-clients","title":"5. \u2705 Unify OpenAI Clients","text":"<ul> <li>Status: COMPLETED \u2705</li> <li>Priority: HIGH</li> <li>ADR: ADR-A13: Legacy Client Migration</li> <li>Plan: Migration Plan</li> <li>What: Unified OpenAI client implementations by migrating from legacy to modern architecture</li> <li>Modern: gen_ai_service/providers/openai_client.py - typed, retrying</li> <li>Legacy: <code>openai_interface/</code> \u2013 removed as of Phase 6</li> <li>Phase 1: Utilities &amp; Adapters \u2705 COMPLETE</li> <li> Create token_utils.py - token counting</li> <li> Create response_utils.py - response extraction</li> <li> Create simple_completion.py - migration adapter</li> <li> Add comprehensive tests (33 new tests)</li> <li> Fix hard-coded literals (use policy dataclass)</li> <li>Phase 2-6: Migration \u2705 COMPLETE</li> <li> Phase 2: Migrate core modules (ai_text_processing, journal_processing)</li> <li> Phase 3: Migrate CLI tools</li> <li> Phase 4: Migrate tests</li> <li> Phase 5: Update notebooks</li> <li> Phase 6: Delete legacy code (openai_interface/)</li> </ul>"},{"location":"project/repo-root/todo-list/#priority-2-beta-quality","title":"Priority 2: Beta Quality","text":"<p>Goal: Improve maintainability, user experience, and code quality for beta release.</p>"},{"location":"project/repo-root/todo-list/#6-expand-test-coverage","title":"6. \ud83d\udea7 Expand Test Coverage","text":"<ul> <li>Status: NOT STARTED</li> <li>Current Coverage: ~5% (4 test modules)</li> <li>Target: 50%+ for gen_ai_service</li> <li>Tasks:</li> <li> GenAI service flows: prompt rendering, policy resolution, provider adapters</li> <li> CLI integration tests (option parsing, environment validation)</li> <li> Configuration loading edge cases</li> <li> Error handling scenarios</li> <li> Pattern catalog validation</li> </ul>"},{"location":"project/repo-root/todo-list/#7-consolidate-environment-loading","title":"7. \ud83d\udea7 Consolidate Environment Loading","text":"<ul> <li>Status: NOT STARTED</li> <li>Problem: Multiple modules call <code>load_dotenv()</code> at import time</li> <li>https://github.com/aaronksolomon/tnh-scholar/blob/main/src/tnh_scholar/ai_text_processing/prompts.py</li> <li>https://github.com/aaronksolomon/tnh-scholar/blob/main/src/tnh_scholar/audio_processing/diarization/pyannote_client.py</li> <li>Tasks:</li> <li> Create single startup hook for dotenv loading</li> <li> Use Pydantic Settings consistently</li> <li> Pass configuration objects instead of <code>os.getenv()</code> calls</li> <li> Remove import-time side effects</li> </ul>"},{"location":"project/repo-root/todo-list/#8-clean-up-cli-tool-versions","title":"8. \ud83d\udea7 Clean Up CLI Tool Versions","text":"<ul> <li>Status: PARTIAL (old versions removed, utilities pending)</li> <li>Location: cli_tools/audio_transcribe/</li> <li>Tasks:</li> <li> Remove audio_transcribe0.py</li> <li> Remove audio_transcribe1.py</li> <li> Remove audio_transcribe2.py</li> <li> Keep only current version</li> <li> Create shared utilities (argument parsing, environment validation, logging)</li> </ul>"},{"location":"project/repo-root/todo-list/#9-documentation-reorganization-adr-dd01-adr-dd02","title":"9. \u2705 Documentation Reorganization (ADR-DD01 &amp; ADR-DD02)","text":"<ul> <li>Status: PHASE 1 COMPLETE \u2705 (Parts 1\u20134 \u2705 COMPLETE, Part 8 \u2705 COMPLETE, File Reorganization \u2705 COMPLETE; Parts 5\u20137 deferred to Phase 2)</li> <li>Reference:</li> <li>ADR-DD01: Docs Reorganization Strategy</li> <li>ADR-DD02: Documentation Main Content and Navigation Strategy \u2705 APPROVED</li> <li>Goal: Execute the phased documentation overhaul for <code>docs/</code> tree, keep README \u2248 docs/index with drift monitoring, automate verification. Note: <code>patterns/</code> directory is managed separately (TODO #16).</li> <li>Next Sequence: Part 5 (Archive) \u2192 Part 6 (Gap Filling) \u2192 Part 7 (Standalone Tasks)</li> <li>Checkpoints / Tasks:</li> <li>Inventory + Tagging<ul> <li> Catalog every Markdown file (owner, status: current/needs-update/historical)</li> <li> Add front matter metadata + PromptTemplate terminology notes</li> <li> Identify raw research assets to offload to external storage</li> </ul> </li> <li>Filesystem Reorg (\u2705 COMPLETE)<ul> <li> Create the target hierarchy (overview, getting-started, user-guide, cli-reference, prompt-templates, api-reference, architecture/adr, development, research, docs-ops, archive)</li> <li> Move existing docs into the new layout with stub <code>index.md</code> files</li> <li> Rename all architecture documents for clarity and consistency (ADR naming, design doc naming)</li> <li> Create README.md files for major sections (architecture/, cli/, development/, getting-started/)</li> <li> Remove obsolete CLI reference stubs (auto-generation removed, see TODO #17)</li> <li> Reorganize reference materials into categorized subdirectories</li> <li> Tag archival folders explicitly for mkdocs-literate-nav auto-generation (deferred to Phase 2)</li> </ul> </li> <li>Terminology + README Sweep (Part 3b: \u2705 COMPLETED - ADR-DD02 + ADR-DD03)<ul> <li> 3b (COMPLETED): Designed content architecture for README.md and docs/index.md (ADR-DD02)</li> <li> Implemented drift reporting script (<code>check_readme_docs_drift.py</code>) for non-blocking sync monitoring</li> <li> Established persona-based navigation strategy (Practitioners, Developers, Researchers)</li> <li> Updated markdown standards to enforce exact YAML title \u2194 heading match</li> <li> Pattern \u2192 Prompt terminology standardization (ADR-DD03 Phase 1 \u2705 COMPLETE)</li> <li> Updated all user-facing documentation (README, docs/index.md, getting-started/, user-guide/)</li> <li> Renamed patterns.md \u2192 prompts.md; pattern-system/ \u2192 prompt-system/</li> <li> Added historical terminology note to docs/index.md</li> <li> Retained legacy compatibility: TNH_PATTERN_DIR, --pattern flags</li> <li> Phase 2: CLI documentation updates (deferred post-merge, many tools deprecated)</li> <li> Phase 3: Code refactoring (tracked separately, many modules scheduled for deletion)</li> <li> Add prompt authoring schema guidance (deferred to Part 6)</li> </ul> </li> <li>MkDocs + Automation (\u2705 ALL PARTS COMPLETE)<ul> <li> Install <code>mkdocs-literate-nav</code> and <code>mkdocs-gen-files</code> to dev dependencies</li> <li> Restructure <code>mkdocs.yaml</code> to remove hardcoded nav and use literate-nav plugin</li> <li> Create <code>docs/nav.md</code> as the source-of-truth navigation hierarchy</li> <li> Configure gen-files to auto-generate CLI docs and prompt template catalogs</li> <li> Add doc-index automation (<code>scripts/generate_doc_index.py</code>) and flag generated outputs</li> <li> 4b (COMPLETED): Add doc-generation scripts (<code>generate_cli_docs.py</code>, <code>sync_readme.py</code>) and Makefile <code>docs</code> targets</li> <li> 4c (COMPLETED): Wire CI to run <code>mkdocs build</code> + doc verification + GitHub Pages deployment</li> <li> Add markdownlint to CI/CD (MD025/MD013 ignored via <code>.markdownlint.json</code>)</li> <li> 4d (COMPLETED): Normalize internal documentation links; refactor doc-index generation to single <code>docs/documentation_index.md</code> with relative links</li> <li> 4e (COMPLETED): Enable filesystem-driven nav with mkdocs-literate-nav</li> <li> 4f (COMPLETED - ADR-DD02): Add drift reporting (<code>check_readme_docs_drift.py</code>) with Makefile target and CI integration</li> <li> 4g (PHASE 1 COMPLETE): Documentation testing and validation workflow</li> <li>Phase 1: Quick Wins \u2705 COMPLETE<ul> <li> Enable <code>mkdocs build --strict</code> in <code>docs-verify</code> (fail on warnings)</li> <li> Add link checking with <code>lychee</code> + <code>.lychee.toml</code> (ignore flaky/external as needed)</li> <li> Add <code>codespell</code> with <code>.codespell-ignore.txt</code> (dharma terms/proper nouns); wire into pre-commit/CI</li> <li> Create <code>docs-quickcheck</code> make target: sync_root_docs \u2192 mkdocs --strict \u2192 lychee \u2192 codespell</li> <li> Fixed all 136 MkDocs strict mode warnings (autorefs, griffe type annotations)</li> </ul> </li> <li>Phase 2: Metadata Validation (Beta gate)<ul> <li> Add <code>scripts/check_doc_metadata.py</code> to validate front matter (title/description/status) and warn on empty descriptions</li> <li> Detect orphaned docs not reachable from nav (using generated nav) and report missing descriptions</li> <li> Add metadata check to pre-commit and CI</li> </ul> </li> <li>Phase 3: Coverage &amp; Structure (Prod polish)<ul> <li> Add <code>interrogate</code> for Python docstring coverage (threshold on <code>src/tnh_scholar</code>, skip tests/scripts)</li> <li> Validate ADRs follow template sections (Context/Decision/Consequences) + required front matter</li> <li> Run offline/internal link check on built site (<code>lychee --offline</code> on <code>site/</code>)</li> <li> Optional: add <code>vale</code> with a minimal style guide for docs tone/consistency</li> </ul> </li> </ul> </li> <li>Historical Archive + Discoverability (Phase 2)<ul> <li> Archived historical research artifacts and experiment files</li> <li> Move additional legacy ADRs/prototypes into <code>docs/archive/**</code></li> <li> Create comprehensive archive index + add summary links from primary sections</li> <li> Host raw transcripts externally (S3/KB) and link from summaries</li> </ul> </li> <li>Backlog + Gap Filling<ul> <li> Populate <code>docs/docs-ops/roadmap.md</code> with missing topics (PromptTemplate catalog, workflow playbooks, evaluation guides, KB, deployment, research summaries, doc ops)</li> <li> Open GitHub issues per backlog item with owners/priorities</li> </ul> </li> <li>Documentation Structure Reorganization (\u2705 COMPLETE - Python Community Standards)<ul> <li> Split design-guide.md into Python standard docs:</li> <li> style-guide.md: Code formatting, naming, PEP 8, type annotations</li> <li> design-principles.md: Architectural patterns, modularity, composition</li> <li> Move object-service architecture to canonical location:</li> <li> Moved from development/architecture/ to architecture/object-service/</li> <li> Converted V2 blueprint to ADR-OS01 (adopted V3, deleted V1)</li> <li> Created design-overview.md with high-level summary</li> <li> Updated implementation-status.md with resolved items</li> <li> Create forward-looking prompt architecture:</li> <li> Created prompt-architecture.md (current + planned V2 with PromptCatalog)</li> <li> Moved pattern-core-design.md to archive/ with terminology note</li> <li> Documented VS Code integration requirements</li> <li> Fix all broken links from reorganization:</li> <li> Fixed 35 mkdocs build --strict warnings \u2192 0 warnings \u2705</li> <li> Updated docs/index.md, contributing.md, ADR cross-references</li> <li> Regenerated documentation_index.md</li> <li> Established Python community standard structure:</li> <li>docs/architecture/ = ADRs, design decisions (the \"why\")</li> <li>docs/development/ = Developer guides (the \"how\")</li> <li>docs/project/ = Vision, philosophy (stakeholders)</li> </ul> </li> <li>Outstanding Standalone Tasks (Phase 2 - Future Work)<ul> <li> Created architecture/README.md overview</li> <li> Deprecate outdated CLI examples (deferred post-CLI-refactor, see TODO #17)</li> <li> Add practical user guides for new features post-reorg</li> <li> Expand architecture overview with component diagrams</li> <li> Establish research artifact archival workflow (external storage + summary linking)</li> </ul> </li> <li>Include Root Markdown Files in MkDocs Navigation<ul> <li>Status: \u2705 COMPLETE</li> <li>Priority: MEDIUM (Part of docs-reorg cleanup)</li> <li>Goal: Make root-level config/meta files (README, TODO, CHANGELOG, CONTRIBUTING, DEV_SETUP, release_checklist) discoverable in mkdocs navigation and documentation index</li> <li>Approach: Build-time copy with \"DO NOT EDIT\" warnings</li> <li> Create <code>docs/project/repo-root/</code> directory for project meta-documentation</li> <li> Create <code>scripts/sync_root_docs.py</code> to copy root markdown files</li> <li> Copy root <code>.md</code> files (README, TODO, CHANGELOG, CONTRIBUTING, DEV_SETUP, release_checklist) to <code>docs/project/repo-root/</code></li> <li> Prepend HTML comment warning to each copied file</li> <li> Update Makefile <code>docs</code> target to run sync script before mkdocs build</li> <li> Test documentation build: <code>make docs</code></li> <li> Verify copied files appear in navigation and documentation index</li> <li> Create <code>docs/project/index.md</code> with section overview</li> <li> Wire into gen-files plugin for automatic sync on build</li> </ul> </li> </ul>"},{"location":"project/repo-root/todo-list/#10-type-system-improvements","title":"10. \ud83d\udea7 Type System Improvements","text":"<ul> <li>Status: PARTIAL (see detailed section below)</li> <li>Current: 58 errors across 16 files</li> <li>Tasks: See Type System Improvements section below</li> </ul>"},{"location":"project/repo-root/todo-list/#priority-3-production-readiness","title":"Priority 3: Production Readiness","text":"<p>Goal: Long-term sustainability, advanced features, and production hardening.</p>"},{"location":"project/repo-root/todo-list/#11-refactor-monolithic-modules","title":"11. \ud83d\udea7 Refactor Monolithic Modules","text":"<ul> <li>Status: NOT STARTED</li> <li>Targets:</li> <li> https://github.com/aaronksolomon/tnh-scholar/blob/main/src/tnh_scholar/ai_text_processing/prompts.py (34KB)<ul> <li>Break into: prompt model, repository manager, git helpers, lock helpers</li> <li>Add docstrings and tests for each unit</li> <li>Document front-matter schema</li> </ul> </li> <li> https://github.com/aaronksolomon/tnh-scholar/blob/main/src/tnh_scholar/journal_processing/journal_process.py (28KB)<ul> <li>Identify focused units</li> <li>Extract reusable components</li> </ul> </li> </ul>"},{"location":"project/repo-root/todo-list/#13-complete-provider-abstraction","title":"13. \ud83d\udea7 Complete Provider Abstraction","text":"<ul> <li>Status: NOT STARTED</li> <li>Tasks:</li> <li> Implement Anthropic adapter</li> <li> Add provider-specific error handling</li> <li> Test fallback/retry across providers</li> <li> Provider capability discovery</li> <li> Multi-provider cost optimization</li> </ul>"},{"location":"project/repo-root/todo-list/#14-knowledge-base-implementation","title":"14. \ud83d\udea7 Knowledge Base Implementation","text":"<ul> <li>Status: DESIGN COMPLETE</li> <li>ADR: ADR-K01: Preliminary Architectural Strategy</li> <li>Tasks:</li> <li> Implement Supabase integration</li> <li> Vector search functionality</li> <li> Query capabilities</li> <li> Semantic similarity search</li> </ul>"},{"location":"project/repo-root/todo-list/#15-developer-experience-improvements","title":"15. \ud83d\udea7 Developer Experience Improvements","text":"<ul> <li>Status: PARTIAL (hooks and Makefile exist, automation pending)</li> <li>Tasks:</li> <li> Add pre-commit hooks (Ruff, notebook prep)</li> <li> Create Makefile for common tasks (lint, test, docs, format, setup)</li> <li> Add MyPy to pre-commit hooks</li> <li> Add contribution templates (issue/PR templates)</li> <li> CONTRIBUTING.md exists and documented</li> <li> Release automation</li> <li> Changelog automation</li> </ul>"},{"location":"project/repo-root/todo-list/#16-configuration-data-layout","title":"16. \ud83d\udea7 Configuration &amp; Data Layout","text":"<ul> <li>Status: NOT STARTED</li> <li>Priority: HIGH (blocks pip install)</li> <li>Problem: src/tnh_scholar/init.py raises FileNotFoundError when repo layout missing</li> <li>Tasks:</li> <li> Package pattern assets as resources</li> <li> Make patterns directory optional</li> <li> Move directory checks to CLI entry points only</li> <li> Ensure installed wheels work without patterns/ directory</li> </ul>"},{"location":"project/repo-root/todo-list/#17-prompt-catalog-safety","title":"17. \ud83d\udea7 Prompt Catalog Safety","text":"<ul> <li>Status: NOT STARTED</li> <li>Priority: MEDIUM</li> <li>Problem: Adapter doesn't handle missing keys or invalid front-matter gracefully</li> <li>Tasks:</li> <li> Add manifest validation</li> <li> Implement caching</li> <li> Better error messages (unknown prompt, hash mismatch)</li> <li> Front-matter validation</li> <li> Document pattern schema</li> </ul>"},{"location":"project/repo-root/todo-list/#type-system-improvements","title":"Type System Improvements","text":"<p>Current Status:</p> <ul> <li>Total Type Errors: 58</li> <li>Affected Files: 16</li> <li>Files Checked: 62</li> </ul>"},{"location":"project/repo-root/todo-list/#high-priority-pre-beta","title":"High Priority (Pre-Beta)","text":""},{"location":"project/repo-root/todo-list/#install-missing-type-stubs-completed","title":"Install Missing Type Stubs \u2705 COMPLETED","text":"<ul> <li> Install required type stub packages:</li> <li> types-PyYAML</li> <li> types-requests</li> </ul>"},{"location":"project/repo-root/todo-list/#critical-type-errors","title":"Critical Type Errors","text":"<ul> <li> Fix audio processing boundary type inconsistencies</li> <li> Resolve return type mismatches in <code>audio_processing/audio.py</code></li> <li> Standardize Boundary type usage</li> <li> Fix core text processing type errors</li> <li> Fix str vs list[str] return type in <code>bracket.py</code></li> <li> Resolve object extension error in <code>video_processing.py</code></li> <li> Address function redefinitions in <code>run_oa_batch_jobs.py</code>:</li> <li> Resolve <code>calculate_enqueued_tokens</code> redefinition</li> <li> Fix <code>process_batch_files</code> redefinition</li> <li> Fix <code>main</code> function redefinition</li> </ul>"},{"location":"project/repo-root/todo-list/#medium-priority-beta-stage","title":"Medium Priority (Beta Stage)","text":""},{"location":"project/repo-root/todo-list/#add-missing-type-annotations","title":"Add Missing Type Annotations","text":"<ul> <li> Add variable type annotations:</li> <li> <code>attributes_with_values</code> in clean_parse_tag.py</li> <li> <code>current_page</code> in xml_processing.py</li> <li> <code>covered_lines</code> in ai_text_processing.py</li> <li> <code>seen_names</code> in patterns.py</li> </ul>"},{"location":"project/repo-root/todo-list/#pattern-system-type-improvements","title":"Pattern System Type Improvements","text":"<ul> <li> Fix Pattern class type issues:</li> <li> Resolve <code>apply_template</code> attribute errors</li> <li> Fix <code>name</code> attribute access issues</li> <li> Standardize Pattern type definition</li> </ul>"},{"location":"project/repo-root/todo-list/#low-priority-post-beta","title":"Low Priority (Post-Beta)","text":""},{"location":"project/repo-root/todo-list/#general-type-improvements","title":"General Type Improvements","text":"<ul> <li> Clean up Any return types:</li> <li> Properly type <code>getch</code> handling in user_io_utils.py</li> <li> Type language code returns in lang.py</li> <li> Remove Any returns in ai_text_processing.py</li> <li> Standardize type usage:</li> <li> Implement consistent string formatting in patterns.py</li> <li> Update callable type usage</li> <li> Clean up type hints in openai_interface.py</li> </ul>"},{"location":"project/repo-root/todo-list/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"project/repo-root/todo-list/#phase-1-core-type-safety","title":"Phase 1: Core Type Safety","text":"<ul> <li> Focus on high-priority items affecting core functionality</li> <li> Implement type checking in CI pipeline</li> <li> Document type decisions</li> </ul>"},{"location":"project/repo-root/todo-list/#phase-2-beta-preparation","title":"Phase 2: Beta Preparation","text":"<ul> <li> Address medium-priority items</li> <li> Set up pre-commit type checking hooks</li> <li> Update documentation with type information</li> </ul>"},{"location":"project/repo-root/todo-list/#phase-3-post-beta-cleanup","title":"Phase 3: Post-Beta Cleanup","text":"<ul> <li> Handle low-priority type improvements</li> <li> Implement stricter type checking settings</li> <li> Full type coverage audit</li> </ul>"},{"location":"project/repo-root/todo-list/#typing-guidelines","title":"Typing Guidelines","text":"<p>Standards:</p> <ul> <li> Use explicit types over Any</li> <li> Create type aliases for complex types</li> <li> Document typing decisions</li> <li> Implement consistent Optional handling</li> </ul> <p>References:</p> <ul> <li>Mypy Documentation</li> <li>Python Type Hints</li> <li>Type Checking Best Practices</li> </ul>"},{"location":"project/repo-root/todo-list/#additional-tasks","title":"Additional Tasks","text":""},{"location":"project/repo-root/todo-list/#medium-priority","title":"Medium Priority","text":""},{"location":"project/repo-root/todo-list/#improve-numberedtext-ergonomics","title":"Improve NumberedText Ergonomics","text":"<ul> <li>Location: text_processing/numbered_text.py</li> <li>Problem: Constructor raises for content=None even though typed as optional</li> <li>Tasks:</li> <li> Decide whether empty content should mean \"no lines yet\"</li> <li> Add file-based round-trip tests</li> <li> Trim redundant pytest boilerplate in tests</li> </ul>"},{"location":"project/repo-root/todo-list/#logging-system-scope","title":"Logging System Scope","text":"<ul> <li>Location: logging_config.py</li> <li>Problem: Modules call setup_logging individually</li> <li>Tasks:</li> <li> Define single application bootstrap</li> <li> Document logger acquisition pattern (get_logger only)</li> <li> Create shared CLI bootstrap helper</li> </ul>"},{"location":"project/repo-root/todo-list/#low-priority","title":"Low Priority","text":""},{"location":"project/repo-root/todo-list/#package-api-definition","title":"Package API Definition","text":"<ul> <li>Status: Deferred during prototyping</li> <li>Tasks:</li> <li> Review and document all intended public exports</li> <li> Implement <code>__all__</code> in key <code>__init__.py</code> files</li> <li> Verify exports match documentation</li> </ul>"},{"location":"project/repo-root/todo-list/#repo-hygiene","title":"Repo Hygiene","text":"<ul> <li>Problem: Generated artifacts in repo</li> <li>Files: build/, dist/, site/, current_pip_freeze.txt, mypy_errors.txt, project_directory_tree.txt</li> <li>Tasks:</li> <li> Add to .gitignore</li> <li> Document regeneration process</li> <li> Rely on release pipelines for builds</li> </ul>"},{"location":"project/repo-root/todo-list/#notebook-research-management","title":"Notebook &amp; Research Management","text":"<ul> <li>Location: notebooks/, docs/research/</li> <li>Problem: Valuable but not curated exploratory work</li> <li>Tasks:</li> <li> Adopt naming/linting convention</li> <li> Keep reproducible notebooks in notebooks/experiments</li> <li> Publish vetted analyses to docs/research via nbconvert</li> <li> Archive obsolete notebooks</li> </ul>"},{"location":"project/repo-root/todo-list/#17-comprehensive-cli-reference-documentation","title":"17. \ud83d\udea7 Comprehensive CLI Reference Documentation","text":"<ul> <li>Status: NOT STARTED (deferred post-CLI-refactor)</li> <li>Priority: MEDIUM</li> <li>Context: Removed auto-generated CLI reference stubs (2025-12-03). Renamed <code>docs/cli/</code> to <code>docs/cli-reference/</code> to reflect reference-style content. CLI structure scheduled for overhaul.</li> <li>Blocked By: CLI tool consolidation (TODO #8)</li> <li>Tasks:</li> <li> Review final CLI structure after refactor</li> <li> Create comprehensive CLI reference using actual <code>--help</code> output at all command levels</li> <li> Generate structured documentation for each command:<ul> <li>Command purpose and use cases</li> <li>Full option/argument reference</li> <li>Usage examples</li> <li>Common workflows</li> </ul> </li> <li> Automate CLI reference generation in <code>scripts/generate_cli_docs.py</code></li> <li> Integrate with MkDocs build process</li> <li> Enhance existing <code>docs/cli-reference/</code> structure with comprehensive reference material</li> <li>Notes:</li> <li>Previously had placeholder stubs with minimal content</li> <li>Current <code>docs/cli-reference/</code> contains hand-written per-command reference pages</li> <li>Requires examining actual CLI code structure for comprehensive coverage</li> <li>Should align with user guide examples</li> </ul>"},{"location":"project/repo-root/todo-list/#18-convert-documentation-links-to-absolute-paths","title":"18. \u2705 Convert Documentation Links to Absolute Paths","text":"<ul> <li>Status: COMPLETED (PR #14, commit 85ec6b0)</li> <li>Priority: MEDIUM</li> <li>Context: Enabled MkDocs 1.6+ absolute link validation (2025-12-04). Absolute links (<code>/path/to/file.md</code>) are clearer, easier to maintain, and automation-friendly compared to relative links (<code>../../../path/to/file.md</code>).</li> <li>Reference: ADR-DD01 Addendum 2025-12-04: Absolute Link Strategy</li> <li>Completed: 2025-12-05</li> <li>Tasks:</li> <li> Audit all Markdown files for relative internal links (120 links found across 25 files)</li> <li> Convert relative links to absolute paths (e.g., <code>../../../cli-reference/overview.md</code> \u2192 <code>/cli-reference/overview.md</code>)</li> <li> Update documentation generation scripts to emit absolute links (created <code>scripts/convert_relative_links.py</code>)</li> <li> Verify all links resolve correctly with <code>mkdocs build --strict</code> (passed)</li> <li> Run link checker to validate changes (verified with <code>scripts/verify_doc_links.py</code>)</li> <li> Update markdown standards documentation to mandate absolute links</li> <li> Add Makefile targets for link verification (<code>docs-links</code>, <code>docs-links-apply</code>)</li> <li>Results:</li> <li>964 absolute links now in use across 96 markdown files</li> <li>All internal documentation links use absolute paths</li> <li>MkDocs configured with <code>validation.links.absolute_links: relative_to_docs</code></li> <li>Link verification integrated into docs build process</li> </ul>"},{"location":"project/repo-root/todo-list/#19-document-success-cases","title":"19. \ud83d\udea7 Document Success Cases","text":"<ul> <li>Status: NOT STARTED</li> <li>Priority: MEDIUM</li> <li>Goal: Create comprehensive documentation of TNH Scholar's successful real-world applications</li> <li>Context: Cleanly document proven use cases to demonstrate project value and guide future development</li> <li>Success Cases to Document:</li> <li> Deer Park Monastery Cooking Course<ul> <li>Generating translated SRTs for video recordings</li> <li>Diarization implementation</li> <li>SRT generation workflow</li> </ul> </li> <li> 1950s JVB (Journal of Vietnamese Buddhism) Translation<ul> <li>OCR work on Thay's 1950s editorial work</li> <li>Proof-of-concept translations</li> <li>Historical document processing pipeline</li> </ul> </li> <li> Dharma Talk Transcriptions<ul> <li>Generating polished standalone XML documents from recordings</li> <li>Transcription to structured format workflow</li> </ul> </li> <li> Sr. Dang Nhiem's Dharma Talks<ul> <li>Clean transcription work using audio-transcribe and related tools</li> <li>Audio processing pipeline</li> </ul> </li> <li>Tasks:</li> <li> Create <code>docs/case-studies/</code> directory structure</li> <li> Document each success case with:<ul> <li>Project context and goals</li> <li>Tools and workflows used</li> <li>Technical challenges and solutions</li> <li>Results and outcomes</li> <li>Lessons learned</li> </ul> </li> <li> Add references to relevant code, prompts, and configuration</li> <li> Include sample outputs where appropriate</li> <li> Link from main documentation and README</li> </ul>"},{"location":"project/repo-root/todo-list/#20-notebook-system-overhaul","title":"20. \ud83d\udea7 Notebook System Overhaul","text":"<ul> <li>Status: NOT STARTED</li> <li>Priority: HIGH</li> <li>Goal: Transform notebook collection from exploratory/testing to production-quality examples and convert testing notebooks to proper test cases</li> <li>Context: Current notebooks include valuable work but mix exploration, testing, and examples without clear organization</li> <li>Tasks:</li> <li> Audit &amp; Categorize:<ul> <li> Inventory all notebooks with purpose classification</li> <li> Identify core example notebooks (referencing success cases from TODO #19)</li> <li> Identify testing notebooks to convert to pytest</li> <li> Identify legacy/archival notebooks</li> </ul> </li> <li> Core Example Notebooks (keep and polish):<ul> <li> Fully annotate with current code</li> <li> Ensure working with latest codebase</li> <li> Add clear documentation headers</li> <li> Reference relevant success cases</li> <li> Add to docs as working examples</li> </ul> </li> <li> Testing Notebooks \u2192 Pytest Migration:<ul> <li> Convert notebook-based tests to standard pytest test cases</li> <li> Ensure pytest coverage for all testing scenarios</li> <li> Remove testing notebooks after conversion</li> <li> Update test documentation</li> </ul> </li> <li> Legacy/Archival Notebooks:<ul> <li> Mark clearly as legacy/archival</li> <li> Add context notes for understanding past work</li> <li> Move to <code>notebooks/archive/</code> or similar</li> <li> Document their historical purpose</li> </ul> </li> <li> Documentation Updates:<ul> <li> Update notebook documentation structure</li> <li> Create notebook usage guide</li> <li> Link core examples from user guide</li> <li> Document notebook development workflow</li> </ul> </li> <li>ADR Decision: May require architecture decision record for notebook management strategy</li> </ul>"},{"location":"project/repo-root/todo-list/#progress-summary","title":"Progress Summary","text":"<p>Recently Completed (as of 2025-12-09):</p> <ul> <li>\u2705 Packaging &amp; dependencies fixed</li> <li>\u2705 CI pytest integration</li> <li>\u2705 Library exception handling (removed sys.exit)</li> <li>\u2705 OpenAI client unification (all 6 phases complete)</li> <li>\u2705 Documentation reorganization (Phase 1 complete)</li> <li>\u2705 Pre-commit hooks and Makefile setup</li> <li>\u2705 Documentation links converted to absolute paths (TODO #18)</li> </ul> <p>Current Sprint Focus:</p> <ul> <li>\ud83c\udfaf Implement core stubs (policy, routing, safety)</li> <li>\ud83c\udfaf Expand test coverage to 50%+</li> <li>\ud83c\udfaf Type system improvements (58 errors to resolve)</li> </ul> <p>Beta Blockers:</p> <ul> <li>Configuration &amp; data layout (pattern directory)</li> <li>Core stub implementations (params_policy, model_router, safety_gate)</li> <li>Test coverage expansion</li> </ul>"},{"location":"project/repo-root/todo-list/#notes-for-maintainers","title":"Notes for Maintainers","text":""},{"location":"project/repo-root/todo-list/#test-running","title":"Test Running","text":"<pre><code># Run all tests with coverage\npoetry run pytest --maxfail=1 --cov=tnh_scholar --cov-report=term-missing -v\n\n# Run specific test file\npoetry run pytest tests/gen_ai_service/test_service.py -v\n\n# Run with coverage report\npoetry run pytest --cov=tnh_scholar --cov-report=html\n</code></pre>"},{"location":"project/repo-root/todo-list/#type-checking","title":"Type Checking","text":"<pre><code># Check types\npoetry run mypy src/\n\n# Generate error report\npoetry run mypy src/ &gt; mypy_errors.txt\n</code></pre>"},{"location":"project/repo-root/todo-list/#code-quality","title":"Code Quality","text":"<pre><code># Format code\npoetry run black src/ tests/\n\n# Lint\npoetry run ruff check src/\n\n# Run all checks (as CI does)\npoetry run black --check src/\npoetry run mypy src/\npoetry run ruff check src/\npoetry run pytest --maxfail=1 --cov=tnh_scholar\n</code></pre>"},{"location":"project/repo-root/versioning/","title":"TNH Scholar Versioning Policy","text":""},{"location":"project/repo-root/versioning/#tnh-scholar-versioning-policy","title":"TNH Scholar Versioning Policy","text":"<p>TNH Scholar uses different versioning strategies depending on project maturity.</p>"},{"location":"project/repo-root/versioning/#rapid-prototype-phase-0x-current","title":"Rapid Prototype Phase (0.x) - Current","text":"<p>Status: Active (currently v0.1.4)</p>"},{"location":"project/repo-root/versioning/#core-principles","title":"Core Principles","text":"<p>During the 0.x release series, TNH Scholar follows rapid prototype versioning that prioritizes architectural consistency and fast iteration over backward compatibility:</p> <ol> <li>Breaking changes are acceptable in ANY 0.x release</li> <li>Minor version bumps (0.1.x \u2192 0.2.0) MAY include breaking changes</li> <li>Patch version bumps (0.1.3 \u2192 0.1.4) MAY include breaking changes</li> <li> <p>No backward compatibility guarantees</p> </li> <li> <p>Force refactors in dependent code</p> </li> <li>When core systems change (e.g., prompt system, GenAI service), all dependent modules MUST be updated</li> <li>No dual API support or compatibility shims</li> <li> <p>Breaking changes push all code forward</p> </li> <li> <p>Immediate deprecation and removal</p> </li> <li>Legacy APIs are removed immediately when replaced</li> <li>No deprecation timeline during 0.x</li> <li>Example: <code>TNH_PATTERN_DIR</code> \u2192 <code>TNH_PROMPT_DIR</code> (removed immediately, no migration period)</li> </ol>"},{"location":"project/repo-root/versioning/#why-rapid-prototype-versioning","title":"Why Rapid Prototype Versioning?","text":"<p>Problem with strict semver during prototyping: - Maintaining backward compatibility across major architectural changes creates technical debt - Compatibility shims slow down iteration and create dual-system complexity - Developer time spent on migration paths instead of core features</p> <p>Benefits of rapid prototype versioning: - Architectural improvements can be implemented immediately - All code stays consistent with latest patterns - Faster iteration cycles - Clearer codebase without legacy cruft</p>"},{"location":"project/repo-root/versioning/#version-number-scheme-0x","title":"Version Number Scheme (0.x)","text":"<p>We use <code>MAJOR.MINOR.PATCH</code> notation, but with different semantics than semver:</p> <ul> <li>0.MINOR.PATCH format (e.g., 0.1.4, 0.2.0)</li> <li>MINOR bump (0.1.x \u2192 0.2.0): Significant feature additions or major refactors (may break APIs)</li> <li>PATCH bump (0.1.3 \u2192 0.1.4): Bug fixes, small features, or internal refactors (may break APIs)</li> <li>MAJOR stays at 0 until stable release</li> </ul> <p>Key Difference from Semver: PATCH bumps can include breaking changes during 0.x.</p>"},{"location":"project/repo-root/versioning/#what-we-provide","title":"What We Provide","text":"<p>Even though we don't guarantee compatibility, we provide:</p> <ol> <li>Migration guides for major architectural changes (e.g., ADR-PT04)</li> <li>Clear documentation of breaking changes in CHANGELOG</li> <li>Comprehensive test suites to validate changes</li> <li>ADRs (Architecture Decision Records) explaining major changes</li> </ol>"},{"location":"project/repo-root/versioning/#examples","title":"Examples","text":"<p>Acceptable in 0.1.3 \u2192 0.1.4 (patch): - \u2705 Remove <code>ai_text_processing/prompts.py</code> entirely - \u2705 Change import paths from <code>tnh_scholar.ai_text_processing.prompts</code> to <code>tnh_scholar.prompt_system</code> - \u2705 Remove <code>TNH_PATTERN_DIR</code> environment variable support - \u2705 Change CLI flags from <code>--pattern</code> to <code>--prompt</code> - \u2705 Refactor GenAI service adapter interfaces</p> <p>Acceptable in 0.1.x \u2192 0.2.0 (minor): - \u2705 Major architecture refactor (e.g., prompt system \u2192 object-service compliance) - \u2705 Complete rewrite of subsystems - \u2705 New CLI tools or major feature additions - \u2705 All of the above PLUS large-scale changes</p>"},{"location":"project/repo-root/versioning/#user-impact-communication","title":"User Impact &amp; Communication","text":"<p>For Users (pip install tnh-scholar): - Expect breaking changes in ANY 0.x update - Pin to specific version if stability needed: <code>pip install tnh-scholar==0.1.4</code> - Review CHANGELOG before upgrading - Migration guides provided for major changes</p> <p>For Contributors: - When making breaking changes, update all dependent code in the same PR - Document breaking changes in PR description and CHANGELOG - Update tests to reflect new APIs - No need to maintain backward compatibility during 0.x</p>"},{"location":"project/repo-root/versioning/#post-prototype-phase-10-future","title":"Post-Prototype Phase (1.0+) - Future","text":"<p>Status: Not yet active (target: TBD)</p>"},{"location":"project/repo-root/versioning/#transition-to-semantic-versioning","title":"Transition to Semantic Versioning","text":"<p>Once TNH Scholar reaches 1.0.0, we will adopt strict semantic versioning (semver):</p> <ul> <li>MAJOR.MINOR.PATCH (e.g., 1.2.3)</li> <li>MAJOR (1.x.x \u2192 2.0.0): Breaking changes, backward-incompatible API changes</li> <li>MINOR (1.2.x \u2192 1.3.0): New features, backward-compatible additions</li> <li>PATCH (1.2.3 \u2192 1.2.4): Bug fixes, backward-compatible fixes only</li> </ul>"},{"location":"project/repo-root/versioning/#what-changes-at-10","title":"What Changes at 1.0?","text":"<ol> <li>Stability commitment: Public APIs will not break without major version bump</li> <li>Deprecation policy: Deprecated features maintained for at least one major version</li> <li>Migration guides: Clear upgrade paths for breaking changes</li> <li>LTS consideration: Possible long-term support for major versions</li> </ol>"},{"location":"project/repo-root/versioning/#pre-10-checklist","title":"Pre-1.0 Checklist","text":"<p>Before releasing 1.0.0, we must:</p> <ul> <li> Complete core architecture stabilization (prompt system, GenAI service, transcription)</li> <li> Finalize public API surface</li> <li> Comprehensive test coverage (&gt;80%)</li> <li> Complete documentation for all public APIs</li> <li> Security review</li> <li> Performance benchmarking</li> <li> Production usage validation</li> </ul>"},{"location":"project/repo-root/versioning/#version-history","title":"Version History","text":"Version Date Type Notes 0.1.4 2025-01 Patch Test patches, build fixes 0.1.3 2024-12 Patch Alpha release ... ... ... Earlier versions"},{"location":"project/repo-root/versioning/#related-documentation","title":"Related Documentation","text":"<ul> <li>CHANGELOG.md - Detailed version history</li> <li>CONTRIBUTING.md - Contribution guidelines</li> <li>release_checklist.md - Release process</li> <li>ADR-PT04 - Example breaking change (prompt system refactor)</li> </ul>"},{"location":"project/repo-root/versioning/#faq","title":"FAQ","text":""},{"location":"project/repo-root/versioning/#q-why-not-follow-standard-semver-during-0x","title":"Q: Why not follow standard semver during 0.x?","text":"<p>A: Standard semver allows breaking changes in 0.x minor bumps (0.1.0 \u2192 0.2.0), but typically discourages them in patches (0.1.1 \u2192 0.1.2). However, this still creates pressure to maintain compatibility, slowing down architectural improvements. Our explicit rapid prototype policy makes it clear that ANY 0.x change may break, allowing maximum iteration speed.</p>"},{"location":"project/repo-root/versioning/#q-how-is-this-different-from-0x-means-unstable-in-semver","title":"Q: How is this different from \"0.x means unstable\" in semver?","text":"<p>A: Semver states \"0.x is for initial development, anything may change.\" We make this explicit and add operational guidelines: - Force dependent refactors (no compatibility shims) - Immediate deprecation and removal - Breaking changes acceptable in patches, not just minors</p>"},{"location":"project/repo-root/versioning/#q-when-will-10-be-released","title":"Q: When will 1.0 be released?","text":"<p>A: When core architecture is stable, public APIs are finalized, and the project is production-ready. No specific timeline yet. Follow GitHub milestones for progress.</p>"},{"location":"project/repo-root/versioning/#q-how-do-i-avoid-breaking-changes","title":"Q: How do I avoid breaking changes?","text":"<p>A: Pin to a specific version: <code>pip install tnh-scholar==0.1.4</code>. Review CHANGELOG before upgrading.</p>"},{"location":"project/repo-root/versioning/#q-what-if-im-building-on-top-of-tnh-scholar","title":"Q: What if I'm building on top of TNH Scholar?","text":"<p>A: During 0.x, expect to update your code when upgrading. Consider: - Pin to specific version until 1.0 - Subscribe to release notifications - Review ADRs for architectural changes - Contribute to design discussions to influence stable APIs</p> <p>Last Updated: 2025-12-06 Current Version: 0.1.4 Current Phase: Rapid Prototype (0.x)</p>"},{"location":"research/","title":"Research","text":"<p>Research documents/notes and experiments across the project. Includes initial feasibility studies.</p> <ul> <li>Documentation Index \u2014 complete list of docs and other content for researching TNH scholar itself.</li> </ul> <p>Key starting points:</p> <ul> <li>TNH Scholar Knowledge Base: Design Document</li> <li>Summary Report on Metadata Extraction, Source Parsing, and Model Training</li> <li>Preliminary Feasibility Study</li> <li>Structural-Informed Adaptive Processing (SIAP) Methodology</li> </ul>"},{"location":"research/gpt_development_convos/","title":"GPT Development Convos","text":"<p>Link log of early GPT design, feasibility, and data processing conversations for the project.</p>"},{"location":"research/gpt_development_convos/#design","title":"Design","text":"<ul> <li>Augmented LLM for Thich Nhat Hanh: Initial design and feasibility convo.</li> <li>Feasibility Study Outline: Converting feasibility study to Markdown, other documentation considerations.</li> <li>Project Directory Naming Conventions. Brief convo on file and directory naming conventions for the project. Agreed on all lower case, '_' separated filenames.</li> </ul>"},{"location":"research/gpt_development_convos/#data-processing","title":"Data Processing","text":"<ul> <li>TNHS: Initial Data Processing. First convo on data processing.</li> <li>TNHS: Text Extraction Strategies. First explorations of data extraction code. Epub vs. text; human-machine collaboration strategy.</li> </ul>"},{"location":"research/kb-design-document/","title":"TNH Scholar Knowledge Base: Design Document","text":"<p>Design document for the TNH Scholar knowledge base and semantic search stack.</p>"},{"location":"research/kb-design-document/#project-overview","title":"Project Overview","text":"<p>The TNH Scholar Knowledge Base project aims to create an AI-driven searchable knowledge base for the works of Thich Nhat Hanh, enabling researchers to perform semantic searches across a comprehensive corpus of teachings. The system will support complex queries ranging from thematic exploration to perspective synthesis while maintaining scholarly accuracy and source attribution.</p>"},{"location":"research/kb-design-document/#core-objectives","title":"Core Objectives","text":"<ul> <li>Primary Goal: Enable semantic search across Thich Nhat Hanh's complete works with accurate source attribution</li> <li>Target Users: Monastics and senior community members for research purposes (internal use only)</li> <li>Output Focus: Concrete passage references rather than synthesized responses (initially)</li> <li>Technical Approach: Leverage off-the-shelf systems and technologies where possible</li> </ul>"},{"location":"research/kb-design-document/#query-types-to-support","title":"Query Types to Support","text":"<p>The system must handle diverse query patterns:</p> <ol> <li>Factual Retrieval: \"Where is the best description of touching the earth practice?\"</li> <li>Thematic Exploration: \"Find all works referencing climate change and transcending separate self\"</li> <li>Perspective Analysis: \"What does Thay say about soldiers, police officers, and nonviolence?\"</li> <li>Qualitative Assessment: \"What were some of Thay's most difficult/profound/loving moments?\"</li> <li>Cross-Domain Inquiry: \"Does Thay ever talk about physics and dharma teachings?\"</li> <li>Synthetic Analysis (future): \"What would Thay say about the Israel Palestine conflict today?\"</li> </ol>"},{"location":"research/kb-design-document/#content-scope-and-processing","title":"Content Scope and Processing","text":""},{"location":"research/kb-design-document/#source-materials","title":"Source Materials","text":"<ul> <li>Text Sources: Hundreds of texts in Vietnamese and English (predominant languages)</li> <li>Audio/Video Sources: Potentially thousands of sources requiring transcription</li> <li>Processing Approach: All sources converted to digital text with unified metadata structure</li> <li>Transcription Capability: Robust transcription tools already available in the project</li> </ul>"},{"location":"research/kb-design-document/#content-processing-pipeline","title":"Content Processing Pipeline","text":""},{"location":"research/kb-design-document/#1-text-normalization","title":"1. Text Normalization","text":"<ul> <li>Consistent formatting, punctuation, and character handling</li> <li>Language-specific processing for Vietnamese and English</li> <li>Special character and encoding standardization</li> </ul>"},{"location":"research/kb-design-document/#2-transcription-integration","title":"2. Transcription Integration","text":"<ul> <li>Audio/video content processed through existing transcription tools</li> <li>Quality assurance for transcribed content</li> <li>Timestamping preservation for multimedia sources</li> </ul>"},{"location":"research/kb-design-document/#3-metadata-enrichment","title":"3. Metadata Enrichment","text":"<p>Development of consistent metadata schema including:</p> <p>Bibliographic Information:</p> <ul> <li>Source identification and publication details</li> <li>Date, author/speaker, and publication context</li> <li>Original format and processing information</li> </ul> <p>Content Classification:</p> <ul> <li>Topics, themes, and spiritual practices</li> <li>Target audiences and contexts</li> <li>Relationship to other works in corpus</li> </ul> <p>Structural Metadata:</p> <ul> <li>Position within larger works</li> <li>Section and subsection organization</li> <li>Cross-references and citations</li> </ul> <p>Technical Metadata:</p> <ul> <li>Processing timestamps and methods</li> <li>Language identification and translation links</li> <li>Quality scores and confidence measures</li> </ul>"},{"location":"research/kb-design-document/#4-text-segmentation-strategy","title":"4. Text Segmentation Strategy","text":"<p>Key Decision Point: Determining optimal chunk size for retrieval</p> <p>Options for consideration:</p> <ul> <li>Paragraph-level chunks: Balance between context and precision</li> <li>Section-based chunks: Preserving logical content boundaries  </li> <li>Sliding window approaches: Overlapping chunks to prevent context loss</li> <li>Hierarchical chunking: Multiple granularities (section \u2192 paragraph \u2192 sentence)</li> </ul>"},{"location":"research/kb-design-document/#search-and-retrieval-architecture","title":"Search and Retrieval Architecture","text":""},{"location":"research/kb-design-document/#vector-embeddings-technical-foundation","title":"Vector Embeddings: Technical Foundation","text":"<p>Vector embeddings convert text into high-dimensional numerical representations where semantically similar content occupies nearby positions in vector space. This enables semantic search beyond keyword matching.</p> <p>Key Concepts:</p> <ul> <li>Text converted to vectors (lists of numbers) representing meaning</li> <li>Similar concepts have mathematically similar vectors</li> <li>Search performed by finding nearest neighbors in vector space</li> <li>Typical dimensions: 768-3072 numbers per text segment</li> </ul> <p>Example Flow:</p> <pre><code>Text: \"The practice of mindful breathing\"\n\u2192 Embedding Model \u2192 [0.016, -0.028, 0.044, ..., 0.037]\n</code></pre>"},{"location":"research/kb-design-document/#embedding-model-options","title":"Embedding Model Options","text":""},{"location":"research/kb-design-document/#commercial-solutions","title":"Commercial Solutions","text":"<ul> <li>OpenAI text-embedding-3-large: 3,072 dimensions, high quality, API-based</li> <li>Cohere Embed: Multilingual support, commercial API</li> <li>Azure AI Embeddings: Enterprise integration capabilities</li> </ul>"},{"location":"research/kb-design-document/#open-source-solutions","title":"Open Source Solutions","text":"<ul> <li>Sentence-BERT (SBERT): Wide model variety, multilingual options available</li> <li>E5 Models: Microsoft's efficient embeddings with strong performance</li> <li>BGE Models: Multilingual embeddings with strong cross-language capabilities</li> <li>Multilingual Models: Specialized for Vietnamese-English bilingual corpus</li> </ul> <p>Key Decision Point: Commercial vs. Open Source Embedding Strategy</p> <ul> <li>Commercial: Higher quality, faster development, ongoing costs</li> <li>Open Source: Full control, customization potential, infrastructure overhead</li> </ul>"},{"location":"research/kb-design-document/#search-paradigms","title":"Search Paradigms","text":""},{"location":"research/kb-design-document/#1-vector-based-semantic-search","title":"1. Vector-Based Semantic Search","text":"<ul> <li>Approach: Dense vector embeddings with nearest neighbor search</li> <li>Strengths: Captures conceptual relationships, handles paraphrasing</li> <li>Limitations: Computationally expensive, may lose precision on technical terms</li> <li>Best For: Thematic queries, conceptual exploration</li> </ul>"},{"location":"research/kb-design-document/#2-keyword-based-search","title":"2. Keyword-Based Search","text":"<ul> <li>Approach: Traditional BM25/TF-IDF algorithms with boolean operators</li> <li>Strengths: Precise terminology matching, computationally efficient</li> <li>Limitations: Misses semantic relationships, language-dependent</li> <li>Best For: Exact phrase matching, specific terminology</li> </ul>"},{"location":"research/kb-design-document/#3-hybrid-search-systems","title":"3. Hybrid Search Systems","text":"<ul> <li>Approach: Combining vector and keyword search with result fusion</li> <li>Strengths: Balances precision and recall, handles diverse query types</li> <li>Limitations: Complex implementation, requires careful tuning</li> <li>Best For: Production systems serving varied query patterns</li> </ul> <p>Recommended Approach: Start with pure vector search, evolve to hybrid system based on user feedback and query patterns.</p>"},{"location":"research/kb-design-document/#storage-and-infrastructure-options","title":"Storage and Infrastructure Options","text":""},{"location":"research/kb-design-document/#vector-database-solutions","title":"Vector Database Solutions","text":"<p>Specialized Vector Databases:</p> <ul> <li>Pinecone: Fully managed, simple API, commercial service</li> <li>Weaviate: Open-source, schema support, self-hosted or cloud</li> <li>Milvus: High-performance, enterprise features, complex setup</li> <li>Qdrant: Fast performance, good filtering, moderate complexity</li> <li>Chroma: Simple open-source, ideal for prototypes</li> <li>PGVector: PostgreSQL extension, leverages existing database skills</li> </ul> <p>Traditional Search with Vector Extensions:</p> <ul> <li>Elasticsearch: Mature ecosystem, hybrid search capabilities, complex configuration</li> <li>Solr: Similar to Elasticsearch, vector plugins available</li> </ul>"},{"location":"research/kb-design-document/#infrastructure-decision-matrix","title":"Infrastructure Decision Matrix","text":"Solution Setup Complexity Performance Cost Model Best For Pinecone Low High Usage-based Rapid prototyping Elasticsearch Medium High Infrastructure Hybrid search needs Chroma Low Medium Self-hosted Initial development Weaviate Medium High Flexible Production deployment"},{"location":"research/kb-design-document/#advanced-retrieval-techniques","title":"Advanced Retrieval Techniques","text":""},{"location":"research/kb-design-document/#passage-ranking-and-re-ranking","title":"Passage Ranking and Re-ranking","text":"<p>Methods to improve relevance of returned results:</p> <ul> <li>Cross-Encoders: Direct query-passage relevance scoring</li> <li>LLM-based Reranking: Large language models for relevance judgment  </li> <li>Learning to Rank: Machine learning approaches using multiple features</li> <li>Multi-stage Retrieval: Initial retrieval followed by sophisticated ranking</li> </ul>"},{"location":"research/kb-design-document/#query-processing-enhancement","title":"Query Processing Enhancement","text":"<ul> <li>Query Expansion: Adding related terms to improve recall</li> <li>Query Rewriting: Reformulating queries for better matching</li> <li>Retrieval Augmented Generation for Queries: LLM-generated query improvements</li> <li>Intent Classification: Understanding query type to optimize search strategy</li> </ul>"},{"location":"research/kb-design-document/#implementation-pathways","title":"Implementation Pathways","text":""},{"location":"research/kb-design-document/#fully-commercial-pathway","title":"Fully Commercial Pathway","text":"<p>Components:</p> <ul> <li>Embedding: OpenAI text-embedding-3-large API</li> <li>Storage: Pinecone or Azure Cognitive Search</li> <li>Query Processing: OpenAI API for query understanding</li> <li>Infrastructure: Cloud providers (AWS, Azure, GCP)</li> </ul> <p>Cost Estimate: $500-2000/month depending on usage Timeline: 2-4 weeks for initial prototype Advantages: Rapid development, managed infrastructure, high-quality embeddings Disadvantages: Recurring costs, vendor dependency, data privacy considerations</p>"},{"location":"research/kb-design-document/#fully-open-source-pathway","title":"Fully Open Source Pathway","text":"<p>Components:</p> <ul> <li>Embedding: SBERT, E5, or BGE models (self-hosted)</li> <li>Storage: Elasticsearch, Qdrant, or Milvus (self-hosted)</li> <li>Query Processing: Open source rerankers, self-hosted LLMs</li> <li>Infrastructure: Self-managed servers</li> </ul> <p>Cost Estimate: Primarily hardware and operational overhead Timeline: 6-12 weeks for initial prototype Advantages: No recurring API costs, full control, complete customization Disadvantages: Higher development effort, infrastructure management, technical expertise required</p>"},{"location":"research/kb-design-document/#hybrid-approach-recommended-for-prototyping","title":"Hybrid Approach (Recommended for Prototyping)","text":"<p>Phase 1 - Rapid Prototype:</p> <ul> <li>Commercial embedding API + simple vector database</li> <li>Focus on core functionality validation</li> <li>Minimal infrastructure overhead</li> </ul> <p>Phase 2 - Production System:</p> <ul> <li>Evaluate commercial vs. open source based on Phase 1 learnings</li> <li>Implement sophisticated ranking and query processing</li> <li>Scale infrastructure based on usage patterns</li> </ul>"},{"location":"research/kb-design-document/#evaluation-framework","title":"Evaluation Framework","text":""},{"location":"research/kb-design-document/#technical-metrics","title":"Technical Metrics","text":"<p>Relevance Metrics:</p> <ul> <li>Precision/Recall: Classical search performance measures</li> <li>Mean Average Precision (MAP): Standard information retrieval metric</li> <li>Normalized Discounted Cumulative Gain (nDCG): Ranking quality assessment</li> <li>Mean Reciprocal Rank (MRR): Position of first relevant result</li> </ul> <p>System Performance:</p> <ul> <li>Query response time and throughput</li> <li>Index size and memory requirements</li> <li>Computational resource utilization</li> <li>System availability and reliability</li> </ul>"},{"location":"research/kb-design-document/#user-centered-evaluation","title":"User-Centered Evaluation","text":"<p>Relevance Assessment:</p> <ul> <li>Human judgment of search result quality</li> <li>Task completion rates for researchers</li> <li>Side-by-side comparison of different approaches</li> <li>Feedback collection and analysis systems</li> </ul> <p>Usage Analytics:</p> <ul> <li>Query patterns and frequency analysis</li> <li>Result click-through and usage patterns</li> <li>User satisfaction surveys and interviews</li> <li>Feature usage and adoption metrics</li> </ul>"},{"location":"research/kb-design-document/#evaluation-data-requirements","title":"Evaluation Data Requirements","text":"<p>Test Query Development:</p> <ul> <li>Representative query sets across all supported types</li> <li>Ground truth relevance judgments</li> <li>Edge cases and challenging queries</li> <li>Multilingual query examples</li> </ul> <p>Benchmark Creation:</p> <ul> <li>Gold standard query-document pairs</li> <li>Cross-validation datasets for model training</li> <li>Hold-out test sets for final evaluation</li> <li>Adversarial examples for robustness testing</li> </ul>"},{"location":"research/kb-design-document/#open-decision-points-and-areas-for-exploration","title":"Open Decision Points and Areas for Exploration","text":""},{"location":"research/kb-design-document/#1-chunking-strategy-selection","title":"1. Chunking Strategy Selection","text":"<p>Decision Required: Optimal text segment size and boundaries Options:</p> <ul> <li>Paragraph-level with context preservation</li> <li>Section-based following logical boundaries</li> <li>Sliding window with overlap</li> <li>Hierarchical multi-granularity approach</li> </ul> <p>Exploration Needed:</p> <ul> <li>Performance testing with different chunk sizes</li> <li>User feedback on result granularity preferences</li> <li>Context preservation vs. precision trade-offs</li> </ul>"},{"location":"research/kb-design-document/#2-multilingual-handling-approach","title":"2. Multilingual Handling Approach","text":"<p>Decision Required: Strategy for Vietnamese-English bilingual corpus Options:</p> <ul> <li>Separate indices for each language</li> <li>Cross-lingual embeddings for unified search</li> <li>Translation-based query expansion</li> <li>Language-specific optimization</li> </ul> <p>Exploration Needed:</p> <ul> <li>Cross-language retrieval effectiveness</li> <li>Translation quality impact on search results</li> <li>User preference for language-specific vs. unified results</li> </ul>"},{"location":"research/kb-design-document/#3-metadata-schema-design","title":"3. Metadata Schema Design","text":"<p>Decision Required: Comprehensive metadata structure for all content types Critical Elements:</p> <ul> <li>Standardization across diverse source materials</li> <li>Balance between detail and usability</li> <li>Automatic vs. manual metadata generation</li> <li>Evolution and versioning strategy</li> </ul> <p>Exploration Needed:</p> <ul> <li>Analysis of existing source material organization</li> <li>User requirements for filtering and faceting</li> <li>Automated metadata extraction capabilities</li> </ul>"},{"location":"research/kb-design-document/#4-commercial-vs-open-source-technology-mix","title":"4. Commercial vs. Open Source Technology Mix","text":"<p>Decision Required: Optimal balance of commercial and open source components Considerations:</p> <ul> <li>Budget constraints and cost predictability</li> <li>Data privacy and control requirements</li> <li>Development timeline and resource availability</li> <li>Long-term maintenance and scaling needs</li> </ul> <p>Exploration Needed:</p> <ul> <li>Pilot testing of different technology combinations</li> <li>Total cost of ownership analysis</li> <li>Performance and quality comparisons</li> <li>Risk assessment for vendor dependencies</li> </ul>"},{"location":"research/kb-design-document/#5-advanced-feature-development-priority","title":"5. Advanced Feature Development Priority","text":"<p>Decision Required: Roadmap for sophisticated features beyond basic retrieval Potential Features:</p> <ul> <li>Synthetic query answering (\"What would Thay say about...\")</li> <li>Cross-reference and citation analysis</li> <li>Temporal analysis of teaching evolution</li> <li>Thematic clustering and visualization</li> <li>Collaborative annotation and correction systems</li> </ul> <p>Exploration Needed:</p> <ul> <li>User interviews to prioritize feature importance</li> <li>Technical feasibility assessment for advanced features</li> <li>Resource requirements for feature development</li> <li>Integration complexity with core search functionality</li> </ul>"},{"location":"research/kb-design-document/#6-query-response-pair-integration-strategy","title":"6. Query-Response Pair Integration Strategy","text":"<p>Decision Required: Optimal use of existing query-response pair prototype Options:</p> <ul> <li>Training data for custom embedding fine-tuning</li> <li>Evaluation benchmarks for system performance</li> <li>Re-ranking model training data</li> <li>Query expansion and reformulation examples</li> </ul> <p>Exploration Needed:</p> <ul> <li>Quality assessment of existing query-response pairs</li> <li>Expansion strategies for broader coverage</li> <li>Integration methods with chosen search architecture</li> <li>Contribution to overall system performance</li> </ul>"},{"location":"research/kb-design-document/#implementation-phases","title":"Implementation Phases","text":""},{"location":"research/kb-design-document/#phase-1-foundation-development-4-6-weeks","title":"Phase 1: Foundation Development (4-6 weeks)","text":"<p>Objectives:</p> <ul> <li>Establish core document processing pipeline</li> <li>Implement basic vector search functionality</li> <li>Create initial metadata schema</li> <li>Develop evaluation framework</li> </ul> <p>Deliverables:</p> <ul> <li>Document ingestion and processing system</li> <li>Basic search interface for internal testing</li> <li>Initial performance benchmarks</li> <li>Technology stack validation</li> </ul>"},{"location":"research/kb-design-document/#phase-2-enhanced-retrieval-6-8-weeks","title":"Phase 2: Enhanced Retrieval (6-8 weeks)","text":"<p>Objectives:</p> <ul> <li>Implement sophisticated ranking and filtering</li> <li>Optimize query processing and understanding</li> <li>Expand metadata richness and utility</li> <li>Integrate user feedback mechanisms</li> </ul> <p>Deliverables:</p> <ul> <li>Production-quality search system</li> <li>Comprehensive evaluation results</li> <li>User training materials and documentation</li> <li>Performance optimization and scaling plan</li> </ul>"},{"location":"research/kb-design-document/#phase-3-advanced-features-8-12-weeks","title":"Phase 3: Advanced Features (8-12 weeks)","text":"<p>Objectives:</p> <ul> <li>Develop synthetic query capabilities</li> <li>Implement collaborative features</li> <li>Create analytical and visualization tools</li> <li>Establish long-term maintenance procedures</li> </ul> <p>Deliverables:</p> <ul> <li>Feature-complete knowledge base system</li> <li>Advanced query processing capabilities</li> <li>Analytics dashboard and reporting tools</li> <li>Comprehensive system documentation</li> </ul>"},{"location":"research/kb-design-document/#success-criteria","title":"Success Criteria","text":""},{"location":"research/kb-design-document/#technical-success-metrics","title":"Technical Success Metrics","text":"<ul> <li>Query response time under 2 seconds for 95% of searches</li> <li>Relevance scores above 0.8 for top-3 results on benchmark queries</li> <li>System availability above 99% during operating hours</li> <li>Support for concurrent users without performance degradation</li> </ul>"},{"location":"research/kb-design-document/#user-success-metrics","title":"User Success Metrics","text":"<ul> <li>User satisfaction scores above 4.0/5.0 in system evaluation</li> <li>Task completion rates above 85% for research queries</li> <li>Adoption rate above 90% among target user community</li> <li>Positive feedback on system utility and accuracy</li> </ul>"},{"location":"research/kb-design-document/#content-success-metrics","title":"Content Success Metrics","text":"<ul> <li>Coverage of 95% of available source materials</li> <li>Metadata completeness above 90% for all indexed content</li> <li>Cross-language retrieval accuracy comparable to single-language performance</li> <li>Successful handling of all defined query types</li> </ul> <p>This design document provides a comprehensive framework for developing the TNH Scholar Knowledge Base while highlighting critical decision points that require further investigation and stakeholder input. The modular approach allows for iterative development and refinement based on user feedback and technical validation.</p>"},{"location":"research/metadata-summary-report/","title":"Summary Report on Metadata Extraction, Source Parsing, and Model Training for TNH-Scholar","text":"<p>Summary of metadata extraction lessons, tooling, and training implications across Thich Nhat Hanh sources.</p>"},{"location":"research/metadata-summary-report/#overview","title":"Overview","text":"<p>The process of extracting and handling metadata for training models from Thich Nhat Hanh\u2019s works has proven to be a complex but feasible task. While initial expectations were that this would be straightforward, we\u2019ve learned that every book and text source has its own formatting and structural peculiarities, requiring tailored approaches for each. As we move forward with building the three primary models (search, conversation, and translation), it\u2019s clear that managing metadata correctly will be essential for effective training and retrieval.</p>"},{"location":"research/metadata-summary-report/#1-key-learnings-on-metadata-extraction-source-parsing","title":"1. Key Learnings on Metadata Extraction &amp; Source Parsing","text":""},{"location":"research/metadata-summary-report/#challenges-identified","title":"Challenges Identified","text":"<ul> <li>Inconsistent formatting: Each book (or other source) has unique formatting, especially when comparing different types of publications (e.g., EPUBs vs PDFs).</li> <li>Non-uniform structure: Some books have chapters, others have only sections or exercises, while some include quotes and author signatures that are not always easily distinguishable from other text elements.</li> <li>Manual intervention: Parsing tools help automate much of the extraction, but each book requires some degree of human intervention or book-specific rules.</li> </ul>"},{"location":"research/metadata-summary-report/#tools-used-so-far","title":"Tools Used So Far","text":"<ul> <li>ebooklib: Used to read and extract content from EPUB books, providing access to the structural elements present in the ebook\u2019s HTML.</li> <li>BeautifulSoup: For parsing the HTML content extracted from EPUBs, allowing for extraction based on tags and attributes (e.g., <code>&lt;p&gt;</code>, <code>&lt;blockquote&gt;</code>, and class names).</li> <li>Regular Expressions (Regex): Used for detecting patterns in text, such as chapter titles, quotes, and headings.</li> <li>SpaCy: Considered for creating custom NLP models for detecting metadata like \"paragraphs\" or \"quotes,\" but this would require additional training data.</li> </ul>"},{"location":"research/metadata-summary-report/#potential-tools-for-further-exploration","title":"Potential Tools for Further Exploration","text":"<ul> <li>Prodigy: A powerful annotation tool designed for training NLP models. It can be used to manually annotate text for metadata like \"heading,\" \"paragraph,\" \"quote,\" etc. This helps create a labeled dataset that can train models to automatically identify such elements.</li> <li>Label Studio: An open-source data labeling tool that supports text, image, and audio annotation. It can be used to manually annotate and label different structural elements like chapters, quotes, and sections for model training or evaluation.</li> <li>LayoutParser: A deep learning-based tool for document layout analysis. It can be useful for extracting structural information from scanned PDFs or documents with more complex layouts. This would be particularly useful if you're dealing with non-EPUB formatted books or sources.</li> <li>Tesseract OCR: An open-source optical character recognition (OCR) engine, useful for converting scanned images and PDFs into text that can then be processed for structural extraction. If you have scanned books, Tesseract can be the starting point for transforming them into searchable text.</li> <li>PDFMiner: A library for extracting text and metadata from PDFs. It can be used when working with PDF versions of books and helps retain the structure of the content for further processing.</li> <li>Hugging Face Transformers: A versatile library that offers pre-trained models for text classification, zero-shot classification, and other tasks. It can be leveraged to automate the detection of structural elements, such as headings, quotes, or sections, without the need for extensive manual training.</li> <li>Fairseq: An open-source sequence-to-sequence learning library from Facebook AI. It can be explored for training custom translation models or for tasks that require document-level context understanding across large texts.</li> <li>OpenRefine: A tool for cleaning and transforming data. It could be useful for preprocessing and standardizing the extracted text metadata, ensuring consistency across your dataset.</li> <li>Whoosh: A fast, featureful full-text indexing and searching library. Once the metadata is extracted, Whoosh can be used to build a lightweight search engine for indexing and retrieving relevant text chunks.</li> <li>NLTK</li> <li>Textacy</li> <li>Tika</li> <li>Polyglot</li> <li>Gensim</li> <li>Doccano</li> </ul>"},{"location":"research/metadata-summary-report/#current-approaches-for-metadata-extraction","title":"Current Approaches for Metadata Extraction","text":""},{"location":"research/metadata-summary-report/#a-rule-based-extraction","title":"A. Rule-Based Extraction","text":"<ul> <li>Strategy: Use HTML tags and class attributes to map elements like chapters, paragraphs, headings, quotes, and exercises to custom markers (e.g., <code>&lt;&lt;Chapter&gt;&gt;</code>, <code>&lt;&lt;Quote&gt;&gt;</code>).</li> <li>Tools: BeautifulSoup, regex.</li> <li>Pros: Can automate a large portion of the process for well-structured EPUBs.</li> <li>Cons: Requires manual tweaking for each source, not suitable for handling inconsistencies or malformed HTML.</li> </ul>"},{"location":"research/metadata-summary-report/#b-custom-metadata-labeling","title":"B. Custom Metadata Labeling","text":"<ul> <li>Strategy: Create custom training data to fine-tune NLP models for metadata labeling, detecting structural elements like \"quote,\" \"exercise,\" \"chapter,\" and \"paragraph.\"</li> <li>Tools: SpaCy for NER (Named Entity Recognition).</li> <li>Pros: Once trained, the model can generalize to new books, reducing the need for manual intervention.</li> <li>Cons: Requires significant upfront work in annotating data, time-consuming.</li> </ul>"},{"location":"research/metadata-summary-report/#c-pdf-extraction-last-resort","title":"C. PDF Extraction (Last Resort)","text":"<ul> <li>Strategy: For books only available as PDFs, use OCR tools to extract text, then apply regex or rule-based extraction.</li> <li>Tools: PDFMiner, Tesseract OCR.</li> <li>Pros: Works when EPUB or other formats are not available.</li> <li>Cons: PDF extraction tends to lose structural fidelity, making it harder to recover headings and paragraphs.</li> </ul>"},{"location":"research/metadata-summary-report/#lessons-learned","title":"Lessons Learned","text":"<ul> <li>Structural extraction complexity: Parsing books is harder than expected, and managing structural elements (headings, chapters, etc.) requires a flexible, multi-step approach.</li> <li>Time and resource investment: Extracting metadata is time-consuming, and managing multiple books will require templated rules or automation combined with manual oversight.</li> </ul>"},{"location":"research/metadata-summary-report/#2-approaches-to-train-3-primary-models","title":"2. Approaches to Train 3 Primary Models","text":"<p>We aim to create three primary models: search, conversational, and translation. Each model requires a different approach to training, with metadata playing various roles in enhancing the model\u2019s functionality.</p>"},{"location":"research/metadata-summary-report/#a-search-model-fine-tuned-bert","title":"A. Search Model (Fine-Tuned BERT)","text":"<p>Goal: Retrieve relevant passages when users search for topics (e.g., \"mindfulness and compassion\").</p> <p>Training Strategy:</p> <ul> <li>Text Segmentation: Fine-tune BERT on text chunks (paragraphs, sections) without metadata.</li> <li>Metadata Use: Metadata like \"Introduction,\" \"Quote,\" or \"Chapter\" can be used to rank results or filter them during retrieval but should be excluded during training.</li> <li>Inclusion Criteria: Include all relevant content, including quotes and cited sutras.</li> </ul> <p>Considerations:</p> <ul> <li>Metadata helps provide context in the search results (e.g., \u201cThis passage is from the introduction to Love in Action\u201d).</li> <li>Ranking by metadata (e.g., prioritize results from exercises or introductions) will improve user experience.</li> </ul>"},{"location":"research/metadata-summary-report/#b-conversational-model-fine-tuned-gpt-like","title":"B. Conversational Model (Fine-Tuned GPT-like)","text":"<p>Goal: Simulate conversations with the text in Thay\u2019s voice, responding naturally to user questions.</p> <p>Training Strategy:</p> <ul> <li>Voice Consistency: Exclude quotes, sutras, or references that are not directly in Thich Nhat Hanh\u2019s voice. Focus on Thay\u2019s personal writings and speeches.</li> <li>Metadata Use: Metadata such as \"Chapter\" or \"Section\" might not be as useful here. Instead, focus on ensuring that only relevant content is included for training.</li> <li>Fine-tuning: Use conversation datasets to improve the model\u2019s ability to respond in Thay\u2019s style.</li> </ul> <p>Considerations:</p> <ul> <li>Metadata isn\u2019t as critical for training but will be important when parsing the data (i.e., excluding non-Thay voice content).</li> </ul>"},{"location":"research/metadata-summary-report/#c-translation-model-fine-tuned-marianmt-or-mbart","title":"C. Translation Model (Fine-Tuned MarianMT or mBART)","text":"<p>Goal: Provide high-quality translations between English, Vietnamese, and French for Thich Nhat Hanh\u2019s teachings.</p> <p>Training Strategy:</p> <ul> <li>Parallel Texts: Align bilingual or multilingual texts from Thich Nhat Hanh\u2019s works.</li> <li>Metadata Use: Metadata should be stripped from the training data, focusing purely on the text for translation.</li> <li>Fine-tuning: Use existing pre-trained translation models (MarianMT, mBART) and fine-tune on the specific language pairs and content relevant to Thich Nhat Hanh\u2019s works.</li> </ul> <p>Considerations:</p> <ul> <li>Metadata is largely irrelevant for the translation task but could be helpful when cross-referencing translations back to the original text.</li> </ul>"},{"location":"research/metadata-summary-report/#3-summary-of-current-tools-and-their-roles","title":"3. Summary of Current Tools and Their Roles","text":"Tool Purpose Role in Project ebooklib EPUB parsing Extracts content and structure from EPUB books. BeautifulSoup HTML parsing Processes HTML from EPUBs to extract structured elements. Regex Pattern matching Identifies chapters, quotes, and headings via patterns. SpaCy Custom NLP model for metadata extraction Can be used for fine-tuning to detect specific metadata entities. PDFMiner PDF text extraction Alternative for PDF books (fallback). Tesseract OCR for scanned documents Used if only image-based PDFs are available. Hugging Face Model fine-tuning BERT for search, GPT-like models for conversation, MarianMT for translation."},{"location":"research/metadata-summary-report/#4-important-considerations-going-forward","title":"4. Important Considerations Going Forward","text":"<ul> <li>Refining Extraction: Further experiments with rule-based extraction or custom models for metadata will be essential.</li> <li>Model-Specific Training: The distinction between training data for search, conversation, and translation must be carefully managed (e.g., include sutras for search, exclude for conversational training).</li> <li>Proof of Concept Focus: While full automation is a long-term goal, manual handling will be required in the short term, especially for book-specific edge cases.</li> </ul>"},{"location":"research/preliminary-feasibility-study/","title":"Preliminary Feasibility Study","text":"<p>Feasibility study exploring an interactive translation, search, and conversation system built on Thich Nhat Hanh\u2019s teachings.</p>"},{"location":"research/preliminary-feasibility-study/#an-interactive-study-and-translation-system-based-on-thich-nhat-hanhs-teachings","title":"An Interactive Study and Translation System Based on Thich Nhat Hanh\u2019s Teachings","text":""},{"location":"research/preliminary-feasibility-study/#1-project-overview","title":"1. Project Overview","text":"<p>This project aims to create a multipart system using large language models (LLMs) trained on Thich Nhat Hanh\u2019s Teachings. The system will serve primarily Plum Village monastics, and secondarily, mindfulness practitioners, providing:</p> <ul> <li> <p>A query-based text search system to retrieve relevant teachings.</p> </li> <li> <p>An interactive model for exploring Thich Nhat Hanh\u2019s teachings through text-based dialogue.</p> </li> <li> <p>A multilingual translation engine, initially supporting English, Vietnamese, and French, with potential to expand to other languages such as Mandarin, Spanish, Japanese, etc.</p> </li> </ul> <p>This project is designed as a foundational system, which can be expanded in the future to handle deeper conversational tasks and additional languages as technological advancements and resources become available.</p>"},{"location":"research/preliminary-feasibility-study/#2-scope-and-objectives","title":"2. Scope and Objectives","text":""},{"location":"research/preliminary-feasibility-study/#primary-objectives","title":"Primary Objectives:","text":"<ol> <li>Text Query and Search (BERT-Based):<ul> <li>A system where users can input queries on mindfulness topics (e.g., \u201cmindfulness and compassion\u201d) and receive relevant resources (text, video, or audio excerpts).</li> <li>Prioritize contextual understanding to ensure relevant and accurate search results from Thich Nhat Hanh\u2019s teachings.</li> </ul> </li> <li>Interactive Textual Exploration (GPT-3-Based):<ul> <li>An exploratory interactive tool for users to engage with Thich Nhat Hanh\u2019s teachings in a conversational format.</li> <li>GPT-3 will be fine-tuned to respond meaningfully to user input, guiding exploration of teachings.</li> </ul> </li> <li>Multilingual Translation Engine (mBART-Based, Expanding to Multiple Languages):<ul> <li>A translation tool focusing initially on English, Vietnamese, and French.</li> <li>Capacity to expand later into other languages such as Mandarin, Spanish, and others.</li> <li>Emphasis on nuanced, Plum Village style language to ensure accurate and meaningful translation, especially for complex content.</li> </ul> </li> </ol>"},{"location":"research/preliminary-feasibility-study/#3-research-component-model-evaluation","title":"3. Research Component: Model Evaluation","text":"<p>Given the rapid development of models in NLP, this project will include a research phase to evaluate and choose the most appropriate models for the three primary system components.</p>"},{"location":"research/preliminary-feasibility-study/#31-model-research-for-text-query-and-search","title":"3.1 Model Research for Text Query and Search:","text":"<ul> <li>BERT (Bidirectional Encoder Representations from Transformers): BERT excels at understanding context within text, making it ideal for retrieving relevant passages based on search queries.</li> <li>Evaluation: Review BERT variations, such as multilingual BERT, to assess their suitability for understanding context in a multilingual corpus of teachings.</li> <li>Alternatives: Other Transformer-based models such as RoBERTa or Longformer will be evaluated for handling large datasets or longer documents.</li> </ul>"},{"location":"research/preliminary-feasibility-study/#32-model-research-for-interactive-dialogue","title":"3.2 Model Research for Interactive Dialogue:","text":"<ul> <li>GPT-3: The project will leverage GPT-3 for generating conversational responses based on user input. GPT-3 is powerful for generating coherent, meaningful interactions.</li> <li>Evaluation: The research will assess DialoGPT (a dialogue-optimized version of GPT-2) as an alternative, especially for its focus on human-like conversations.</li> <li>Factors to Consider: Trade-offs between response quality (GPT-3\u2019s strength) and fine-tuning flexibility (DialoGPT\u2019s ease of customization for dialogue).</li> </ul>"},{"location":"research/preliminary-feasibility-study/#33-model-research-for-translation-engine","title":"3.3 Model Research for Translation Engine:","text":"<ul> <li>mBART (Multilingual BART): The research will focus on mBART due to its strength in handling complex contextual relationships, which is essential for translating spiritual texts.</li> <li>Evaluation: Compare mBART with MarianMT, which offers efficiency and lower computational demands. mBART\u2019s superior handling of nuanced language will be balanced against the resource-efficient MarianMT, which is specifically designed for translation tasks.</li> <li>Initial Language Focus: Fine-tune the translation engine for English, Vietnamese, and French, with a plan to expand to other languages as needed.</li> </ul>"},{"location":"research/preliminary-feasibility-study/#4-technical-feasibility","title":"4. Technical Feasibility","text":""},{"location":"research/preliminary-feasibility-study/#41-data-collection-and-preparation","title":"4.1 Data Collection and Preparation","text":"<ul> <li>Text Corpus: Collect, preprocess, and check for accuracy, Thich Nhat Hanh\u2019s writings (books, articles, Dharma talks, interviews) in English, Vietnamese, and French. The initial training set will be equivalent to approximately 100 books, sourced from talks, interviews, and books across the three languages, with the majority of the volume in Vietnamese. This dataset will serve as the foundation for query, translation, and interactive systems.</li> <li>Audio/Visual Corpus: Include video/audio resources with transcriptions to support multimedia queries. Audio-to-text conversion models may be considered in the future.</li> <li>Translation Data: Collect parallel texts for English-Vietnamese and English-French pairs to fine-tune the translation engine. This dataset will serve as the foundation for expanding into additional languages.</li> </ul>"},{"location":"research/preliminary-feasibility-study/#42-model-fine-tuning-and-deployment","title":"4.2 Model Fine-Tuning and Deployment","text":"<ul> <li>Cloud Infrastructure: Use cloud-based services like Google Cloud or AWS to fine-tune models and ensure scalability. The GPT-3 system can be trained through the Open AI API interface. Pre-trained models significantly reduce training time and resources, but fine-tuning will still require access to GPUs/TPUs.</li> <li>Deployment: Hugging Face provides tools and APIs for easy model deployment. This infrastructure will be used for both training and serving models.</li> </ul>"},{"location":"research/preliminary-feasibility-study/#43-computational-resources","title":"4.3 Computational Resources","text":"<ul> <li>Fine-tuning pre-trained models like BERT, and mBART will require significant computational resources, but this is mitigated by using pre-trained models as a foundation. GPT-3 can be trained entirely on the Open AI platform.</li> <li>Ongoing resource needs will depend on the scalability of translation tasks and the number of supported languages.</li> </ul>"},{"location":"research/preliminary-feasibility-study/#5-cost-feasibility","title":"5. Cost Feasibility","text":""},{"location":"research/preliminary-feasibility-study/#51-initial-costs","title":"5.1 Initial Costs:","text":"<ul> <li>Fine-tuning pre-trained models and deploying them on cloud infrastructure.</li> <li>Data preprocessing, audio to text conversion, and cleaning of the text corpus.</li> <li>Training GPT-3 on the initial dataset (equivalent to 100 books) will cost approximately $300. This does not include other Open AI platform costs, but provides a ballpark figure for the initial training phase.</li> <li>Translation engine setup for initial languages (English, Vietnamese, French).</li> </ul>"},{"location":"research/preliminary-feasibility-study/#52-ongoing-costs","title":"5.2 Ongoing Costs:","text":"<ul> <li>Hosting the models, ongoing training as the dataset grows, and maintaining cloud infrastructure for serving requests.</li> <li>Expanding the translation system to include additional languages as needed.</li> </ul>"},{"location":"research/preliminary-feasibility-study/#53-mitigation","title":"5.3 Mitigation:","text":"<ul> <li>Starting with smaller datasets and key languages (English, Vietnamese, French) reduces initial costs and allows for gradual scaling.</li> </ul>"},{"location":"research/preliminary-feasibility-study/#6-operational-feasibility","title":"6. Operational Feasibility","text":""},{"location":"research/preliminary-feasibility-study/#61-user-interface-and-experience","title":"6.1 User Interface and Experience:","text":"<ul> <li>Develop a user-friendly interface allowing monastics and practitioners to search, interact, and request translations of texts.</li> <li>Ensure accessibility across various devices (web and mobile) to facilitate widespread use among the Plum Village community and beyond.</li> </ul>"},{"location":"research/preliminary-feasibility-study/#62-testing-and-feedback","title":"6.2 Testing and Feedback:","text":"<ul> <li>Conduct pilot testing with a small group of Plum Village monastics to refine the system.</li> <li>Collect feedback to fine-tune the query system, conversational responses, and translation accuracy.</li> </ul>"},{"location":"research/preliminary-feasibility-study/#7-future-scalability-and-expansion","title":"7. Future Scalability and Expansion","text":"<ol> <li>Language Expansion: As new bilingual data becomes available, expand the translation system to support languages like Mandarin, Spanish, Japanese, and Korean.</li> <li>Advanced Conversational Capabilities: Explore future opportunities to refine the interactive system, potentially scaling toward a more advanced conversational model that reflects deeper spiritual dialogues.</li> <li>Multimedia Integration: Add real-time transcription of audio content and expand the multimedia query engine to handle video/audio teachings.</li> </ol>"},{"location":"research/preliminary-feasibility-study/#8-ethical-and-legal-considerations","title":"8. Ethical and Legal Considerations","text":"<ol> <li>Data Ownership: Ensure permissions are secured for digitizing and using Thich Nhat Hanh\u2019s teachings.</li> <li>Ethical AI: Implement guidelines to prevent misrepresentation or biased interpretations of the teachings. Ensure transparency in how models handle sensitive spiritual content.</li> </ol>"},{"location":"research/preliminary-feasibility-study/#conclusion","title":"Conclusion","text":"<p>The project is technically feasible using pre-trained models such as BERT, GPT-3, and mBART, with a strong focus on accuracy for translations and queries. It can scale over time to include additional languages and advanced functionalities. Starting with smaller, well-defined goals (English, Vietnamese, French translations; query and interactive systems) ensures cost-effective deployment and lays the groundwork for future expansion.</p>"},{"location":"research/rag-research-directions/","title":"RAG Research Directions for TNH Scholar","text":"<p>This document sketches medium\u2013term research directions for retrieval-augmented generation (RAG) inside the TNH Scholar project. It assumes:</p> <ul> <li>A growing corpus of text from Thich Nhat Hanh (TNH), Plum Village, and related Buddhist canons (e.g., CBETA, Taish\u014d).</li> <li>A layered architecture in which LLMs act as reasoning and explanation engines, while retrievers and indexes act as external memory.</li> <li>A long-term goal of grounded, citation-rich Dharma assistance, not purely conversational or speculative answers.</li> </ul> <p>The aim here is not to specify implementation details, but to identify research themes, walking-skeleton experiments, and relevant external work that can inform TNH Scholar\u2019s roadmap.</p>"},{"location":"research/rag-research-directions/#1-background-why-rag-for-tnh-scholar","title":"1. Background: Why RAG for TNH Scholar?","text":"<p>TNH Scholar\u2019s domain is:</p> <ul> <li>Knowledge-intensive: thousands of pages of canonical and commentarial material; fine-grained doctrinal distinctions; multiple schools and historical layers.</li> <li>Multilingual and multi-era: classical Chinese, Vietnamese, P\u0101li/Sanskrit, French, English, and modern commentary across decades.</li> <li>High-stakes conceptually: misquoting or hallucinating scriptures, mis-attributing teachings, or collapsing doctrinal nuance is not acceptable.</li> </ul> <p>Vanilla LLMs are powerful but limited for this domain:</p> <ul> <li>Their internal \u201cparametric\u201d memory is opaque and not easily updated; it may or may not contain accurate Buddhist sources.</li> <li>They may hallucinate sutra passages, attributions, and historical details without clear references.</li> <li>They struggle to provide verifiable links back into a specific TNH / CBETA / PV corpus under version control.</li> </ul> <p>RAG directly addresses this by combining:</p> <ul> <li>A retriever + index over the curated Buddhist corpora (TNH books, CBETA, etc.), and</li> <li>A generator (LLM) that uses retrieved passages as grounding context, with explicit citations and provenance.</li> </ul> <p>In this framing, RAG is not an add-on; it is the primary way TNH Scholar connects \u201cintelligence\u201d (LLM) with \u201cmemory\u201d (canon + metadata).</p>"},{"location":"research/rag-research-directions/#2-rag-building-blocks-in-the-tnh-scholar-context","title":"2. RAG Building Blocks in the TNH Scholar Context","text":"<p>At a high level, most RAG systems have three components. TNH Scholar can use the same decomposition but with domain-specific constraints.</p>"},{"location":"research/rag-research-directions/#21-retrieval-layer","title":"2.1 Retrieval layer","text":"<p>Core responsibilities:</p> <ul> <li>Encode text (sentences, paragraphs, sections) into dense vectors, ideally with domain-tuned multilingual embeddings.</li> <li>Support semantic search across multiple corpora and languages.</li> <li>Respect rich metadata filters:</li> <li>Canon / collection (e.g., CBETA X, Taish\u014d, TNH books, PV talks).</li> <li>Language and translation alignment.</li> <li>Genre (sutra, commentary, letter, Dharma talk, journal, etc.).</li> <li>Time period and location when available.</li> </ul> <p>Medium-term research leverages bi-encoders and domain fine-tuning, but the PoC can rely on pre-trained multilingual models (e.g., MPNet, E5) plus careful chunking and metadata design.</p>"},{"location":"research/rag-research-directions/#22-generation-layer","title":"2.2 Generation layer","text":"<p>Core responsibilities:</p> <ul> <li>Take user queries + retrieved passages and produce:</li> <li>Clear, accessible explanations.</li> <li>Short comparisons (e.g., \u201cLotus Sutra vs TNH on compassion\u201d).</li> <li>Citations into the TNH Scholar corpus.</li> <li>Respect stylistic and ethical constraints:</li> <li>No fabricated citations or sutra names.</li> <li>Clear separation between \u201cwhat the texts say\u201d and \u201cinterpretive framing.\u201d</li> <li>Capacity to answer at multiple levels (beginner, experienced practitioner, academic).</li> </ul> <p>This layer will increasingly be instruction-tuned and/or fine-tuned to TNH Scholar\u2019s norms, but early prototypes can use off-the-shelf high-quality LLMs with strong prompt design.</p>"},{"location":"research/rag-research-directions/#23-orchestration-and-reasoning-layer","title":"2.3 Orchestration and reasoning layer","text":"<p>Core responsibilities:</p> <ul> <li>Decide when to retrieve (not every question needs a heavy RAG pipeline).</li> <li>Decide what to retrieve:</li> <li>Which corpus? Which language? What level of detail?</li> <li>Should we prefer TNH\u2019s own explanations, classical sources, or both?</li> <li>Support multi-step tasks:</li> <li>First retrieve background on \u201cmindfulness of breathing,\u201d</li> <li>then retrieve TNH\u2019s own commentary,</li> <li>then synthesize and compare.</li> </ul> <p>Research here connects to agentic patterns (e.g., ReAct) and self-reflective RAG (e.g., Self-RAG).</p>"},{"location":"research/rag-research-directions/#3-priority-research-directions","title":"3. Priority Research Directions","text":"<p>This section outlines concrete directions that are both scientifically interesting and directly applicable to TNH Scholar.</p>"},{"location":"research/rag-research-directions/#31-multilingual-canon-aware-embeddings","title":"3.1 Multilingual, Canon-Aware Embeddings","text":"<p>Goal: Build embedding spaces that respect both semantic content and canonical structure across languages.</p> <p>Key questions:</p> <ul> <li>How well do off-the-shelf multilingual models (e.g., MPNet, E5) capture:</li> <li>Buddhist terms of art (e.g., \u015b\u016bnyat\u0101, interbeing, bodhicitta),</li> <li>doctrinal categories (Four Noble Truths, dependent origination, etc.),</li> <li>cross-lingual parity between Chinese / Vietnamese / English?</li> <li>What is the benefit of fine-tuning bi-encoders on:</li> <li>Manually curated query\u2013passage pairs (e.g., TNH Scholar training data).</li> <li>Parallel corpora (Chinese \u2194 Vietnamese \u2194 English sutra translations).</li> <li>TNH-specific writings with aligned translations.</li> </ul> <p>Possible experiments:</p> <ul> <li>Start with generic multilingual embeddings and measure retrieval quality on hand-crafted evaluation sets (e.g., \u201cmindfulness of breathing passages across languages\u201d).</li> <li>Introduce contrastive fine-tuning using query\u2013passage pairs and evaluate improvement in:</li> <li>Top-k recall for doctrinal questions.</li> <li>Cross-lingual retrieval accuracy (e.g., English query \u2192 Chinese or Vietnamese source).</li> </ul>"},{"location":"research/rag-research-directions/#32-self-querying-and-metadata-aware-retrieval","title":"3.2 Self-Querying and Metadata-Aware Retrieval","text":"<p>Goal: Allow the LLM to co-design the retrieval query, including semantic intent and metadata filters.</p> <p>External work on self-query retrievers shows that LLMs can:</p> <ul> <li>Parse a natural-language query into:</li> <li>A semantic search vector.</li> <li>A structured metadata filter (e.g., <code>author = TNH</code>, <code>language = EN</code>, <code>year &gt; 1990</code>). </li> <li>Improve retrieval quality when document metadata is rich.</li> </ul> <p>TNH Scholar is a natural fit because its documents have (or will have):</p> <ul> <li>Per-text and per-section metadata: author, era, collection, language, genre.</li> <li>Cross-references between canonical sources and TNH commentary.</li> </ul> <p>Possible experiments:</p> <ul> <li>Build a minimal self-querying retriever that:</li> <li>Uses an LLM to propose a metadata filter from the user question.</li> <li>Combines this with vector search in the TNH / CBETA index.</li> <li>Evaluate whether self-querying improves:</li> <li>Precision of citations for targeted questions (\u201cWhere does Thay discuss climate and interbeing?\u201d).</li> <li>User satisfaction for queries that implicitly assume a particular corpus or timeframe.</li> </ul>"},{"location":"research/rag-research-directions/#33-agentic-rag-react-style-reasoning-and-tool-use","title":"3.3 Agentic RAG: ReAct-Style Reasoning and Tool Use","text":"<p>Goal: Equip TNH Scholar with a light-weight reason\u2013act loop, so that it can break down complex queries and call tools (retrievers, databases) step by step.</p> <p>The ReAct pattern (Reason + Act) prompts an LLM to:</p> <ul> <li>Interleave \u201cthought\u201d steps (natural-language reasoning) with</li> <li>\u201caction\u201d steps (tool calls, e.g., search, metadata lookup).</li> </ul> <p>For TNH Scholar, this could look like:</p> <ol> <li>User asks: \u201cCompare how the Lotus Sutra and Thich Nhat Hanh describe compassionate action.\u201d</li> <li>Agent reasoning:</li> <li>Identify: need passages from Lotus Sutra and TNH\u2019s writings on compassion in action.</li> <li>Issue two searches: one over CBETA/Taish\u014d, one over TNH books.</li> <li>Retrieve top passages, maybe in different languages.</li> <li>Agent then synthesizes similarities and differences, with citations.</li> </ol> <p>Possible experiments:</p> <ul> <li>Implement a walking-skeleton ReAct agent with a small set of tools:</li> <li><code>search_cbeta</code>, <code>search_tnh</code>, <code>fetch_metadata</code>.</li> <li>Constrain its output to always include citations and a short explanation of its reasoning steps (for internal debugging and evaluation).</li> <li>Compare ReAct-style answers with single-shot RAG answers on multi-hop questions (e.g., \u201ccompare,\u201d \u201ctrace evolution of an idea,\u201d \u201crelate canonical verse to modern commentary\u201d).</li> </ul>"},{"location":"research/rag-research-directions/#34-self-reflective-rag-and-faithfulness","title":"3.4 Self-Reflective RAG and Faithfulness","text":"<p>Goal: Reduce hallucinations and ensure that generated answers are faithful to retrieved sources.</p> <p>Self-RAG proposes training a model that can:</p> <ul> <li>Decide when to retrieve, and when parametric knowledge is enough.</li> <li>Insert special reflection tokens to:</li> <li>Check whether retrieved passages are relevant.</li> <li>Critique its own draft answer against the evidence. </li> </ul> <p>TNH Scholar can adapt these ideas without necessarily re-training large models from scratch:</p> <ul> <li>Define a prompt schema where the LLM:</li> <li>First writes a draft answer.</li> <li>Then explicitly checks each key claim against retrieved passages.</li> <li>Marks claims as \u201cdirectly supported,\u201d \u201cinterpretive but consistent,\u201d or \u201cnot supported.\u201d</li> <li>Optionally, enforce a \u201cno unsupported claim\u201d mode where the assistant must either:</li> <li>Provide a citation, or</li> <li>Clearly label content as opinion or extrapolation.</li> </ul> <p>Possible experiments:</p> <ul> <li>Develop a small suite of \u201cgotcha\u201d questions (e.g., tempt the model to fabricate a sutra name or misattribute a quote), and see how a self-reflective RAG prompt mitigates these errors.</li> <li>Explore whether lightweight fine-tuning on TNH Scholar reflection tasks improves faithfulness metrics.</li> </ul>"},{"location":"research/rag-research-directions/#35-evaluation-frameworks-for-dharma-oriented-rag","title":"3.5 Evaluation Frameworks for Dharma-Oriented RAG","text":"<p>Goal: Move beyond generic QA metrics and evaluate TNH Scholar along Dharma-appropriate dimensions.</p> <p>Inspired by RAG evaluation work in knowledge-intensive domains (e.g., scientific QA, healthcare), TNH Scholar can define:</p> <ul> <li>Faithfulness: Are claims supported by cited passages? Are citations correct and non-fabricated?</li> <li>Coverage: Does the system surface a reasonable sample of relevant sources (not just one favorite passage)?</li> <li>Doctrinal accuracy: For monastic/teacher evaluators, does the answer respect traditional interpretations and avoid serious doctrinal mistakes?</li> <li>Pedagogical suitability: Is the explanation appropriate for the requested audience (beginner / practitioner / researcher)?</li> </ul> <p>Possible components:</p> <ul> <li>A small gold-standard evaluation set curated by practitioners:</li> <li>Questions, ideal answer sketches, and illustrative source passages.</li> <li>A citation correctness checker:</li> <li>Human-in-the-loop at first; later possibly supported by automated overlap metrics (e.g., answer-text vs evidence-text similarity).</li> <li>Periodic \u201cDharma review\u201d sessions where monastics or senior practitioners score system outputs and feed back into model alignment and retrieval tuning.</li> </ul>"},{"location":"research/rag-research-directions/#36-data-and-annotation-programs","title":"3.6 Data and Annotation Programs","text":"<p>Goal: Create reusable supervision signals to improve retrieval and generation over time.</p> <p>Key data types:</p> <ul> <li>Query\u2013passage pairs for retriever fine-tuning:</li> <li>Human-generated queries for important passages.</li> <li>GPT-assisted synthetic queries filtered and curated by humans.</li> <li>Parallel corpora for multilingual alignment:</li> <li>Sutra passages across Chinese/Vietnamese/English.</li> <li>TNH works where multiple translations exist.</li> <li>Explanatory pairs for generation tuning:</li> <li>Short explanations or commentaries linked to source text.</li> <li>Structured responses (e.g., \u201ccontext,\u201d \u201ckey teaching,\u201d \u201capplication to daily life\u201d).</li> </ul> <p>Possible experiments:</p> <ul> <li>Integrate existing TNH Scholar pipelines (e.g., GPT-based query generation) into a formal dataset for retriever training.</li> <li>Explore curriculum-style training:</li> <li>Start with simple factual questions.</li> <li>Progress to conceptual and comparative questions.</li> <li>Eventually move to multi-hop questions involving multiple sources and eras.</li> </ul>"},{"location":"research/rag-research-directions/#4-walking-skeleton-plan-high-level","title":"4. Walking Skeleton Plan (High-Level)","text":"<p>This section sketches a phased approach that stays faithful to the \u201cwalking skeleton\u201d philosophy: minimal viable functionality first, with clear anchors for future complexity.</p>"},{"location":"research/rag-research-directions/#phase-0-poc-search-current","title":"Phase 0: PoC Search (Current)","text":"<ul> <li>Status: In progress.</li> <li>Components:</li> <li>Basic vector index over a small CBETA subset and/or TNH texts.</li> <li>Off-the-shelf multilingual model (e.g., MPNet) for embeddings.</li> <li>Simple similarity search and ranking.</li> <li>Goal:</li> <li>Verify that cross-lingual search already surfaces meaningful passages for queries like \u201ccompassionate action,\u201d \u201cinterbeing,\u201d or \u201cmindfulness of breathing.\u201d</li> </ul>"},{"location":"research/rag-research-directions/#phase-1-baseline-rag","title":"Phase 1: Baseline RAG","text":"<ul> <li>Add a generation step on top of retrieval:</li> <li>Retrieve top-k passages.</li> <li>Feed them into an LLM with a structured prompt to produce:<ul> <li>A short explanation.</li> <li>A list of citations.</li> </ul> </li> <li>Keep the orchestration logic simple:</li> <li>Always retrieve for now.</li> <li>Single corpus (e.g., TNH English texts or a curated CBETA subset).</li> </ul>"},{"location":"research/rag-research-directions/#phase-2-metadata-self-querying","title":"Phase 2: Metadata &amp; Self-Querying","text":"<ul> <li>Introduce metadata-rich indexing:</li> <li>Author, language, genre, canonical collection.</li> <li>Implement a self-querying retriever:</li> <li>LLM proposes semantic query + metadata filters.</li> <li>Use this to improve retrieval for focused questions (e.g., \u201cTNH on climate and interbeing,\u201d \u201cLotus Sutra references only\u201d).</li> </ul>"},{"location":"research/rag-research-directions/#phase-3-agentic-rag-react-style-walking-skeleton","title":"Phase 3: Agentic RAG (ReAct-Style Walking Skeleton)","text":"<ul> <li>Add a lightweight ReAct-style agent with 2\u20133 tools:</li> <li><code>search_cbeta</code>, <code>search_tnh</code>, <code>lookup_metadata</code>.</li> <li>Teach the agent (via prompting) to:</li> <li>Break down complex questions.</li> <li>Call tools in a small number of steps.</li> <li>Produce answers with explicit citations and a short \u201creasoning summary\u201d (for debugging and evaluation).</li> </ul>"},{"location":"research/rag-research-directions/#phase-4-self-reflective-evaluated-rag","title":"Phase 4: Self-Reflective &amp; Evaluated RAG","text":"<ul> <li>Experiment with self-reflective prompts inspired by Self-RAG:</li> <li>Draft answer \u2192 retrieve/reflect \u2192 revise answer with evidence labels.</li> <li>Define and implement an evaluation harness:</li> <li>Faithfulness and citation correctness metrics.</li> <li>Human evaluation protocol with monastics / experienced practitioners.</li> <li>Consider small-scale fine-tuning on TNH Scholar-specific reflection tasks if early experiments are promising.</li> </ul>"},{"location":"research/rag-research-directions/#5-bibliography-and-external-resources","title":"5. Bibliography and External Resources","text":"<p>This section lists selected external work that is especially relevant to TNH Scholar\u2019s RAG roadmap. It is intentionally opinionated and incomplete; the goal is to anchor future deep dives, not to be exhaustive.</p>"},{"location":"research/rag-research-directions/#51-primary-rag-research","title":"5.1 Primary RAG Research","text":"<ol> <li>Lewis et al., 2020 \u2013 Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks    The original RAG paper, introducing a framework that combines a dense retriever with a sequence-to-sequence generator over a Wikipedia index. Useful as a conceptual baseline and for understanding early design trade-offs.  </li> <li>ArXiv abstract </li> <li>Gao et al., 2023/2024 \u2013 Retrieval-Augmented Generation for Large Language Models: A Survey    A comprehensive survey of RAG for LLMs, organizing the field into Naive, Advanced, and Modular RAG, and discussing retrieval, generation, and augmentation techniques as separate modules. Highly relevant for TNH Scholar\u2019s architectural planning and evaluation design.  </li> <li>ArXiv survey </li> <li>PDF </li> </ol>"},{"location":"research/rag-research-directions/#52-self-reflective-and-faithfulness-oriented-rag","title":"5.2 Self-Reflective and Faithfulness-Oriented RAG","text":"<ol> <li>Asai et al., 2023 \u2013 Self-RAG: Learning to Retrieve, Generate, and Critique    Proposes a framework where a single LM learns to decide when to retrieve, how to use retrieved passages, and how to critique its own generations using reflection tokens. Important for TNH Scholar\u2019s goals around reducing hallucinations and explicitly grounding answers in Dharma sources.  </li> <li>Project page </li> <li>ArXiv / OpenReview entry </li> <li>OpenReview page </li> </ol>"},{"location":"research/rag-research-directions/#53-agentic-patterns-and-tool-use","title":"5.3 Agentic Patterns and Tool Use","text":"<ol> <li>Yao et al., 2022/2023 \u2013 ReAct: Synergizing Reasoning and Acting in Language Models    Introduces the ReAct pattern, where LLMs interleave natural-language reasoning with tool calls. Provides a template for TNH Scholar to build agents that can search multiple corpora, inspect metadata, and iteratively refine their answers.  </li> <li>ArXiv paper </li> <li>PDF </li> <li>Project page </li> <li>Google AI blog summary </li> </ol>"},{"location":"research/rag-research-directions/#54-metadata-aware-and-self-querying-retrieval","title":"5.4 Metadata-Aware and Self-Querying Retrieval","text":"<ol> <li>Self-Query Retriever (various implementations and guides)    Practical articles describing how to use LLMs to generate structured queries (including metadata filters) for vector stores. While implementation details vary (LangChain, custom frameworks, etc.), the core idea\u2014LLM-driven metadata-aware retrieval\u2014is directly applicable to TNH Scholar\u2019s canonical corpora.  </li> <li>Enhancing RAG Performance with Metadata: The Power of Self-Query Retrievers </li> <li>RAG X \u2014 Self Query Retriever </li> <li>Advanced RAG Techniques: What They Are &amp; How to Use Them </li> </ol>"},{"location":"research/rag-research-directions/#55-practitioner-overviews-and-emerging-tooling","title":"5.5 Practitioner Overviews and Emerging Tooling","text":"<ol> <li>Advanced RAG Engineering Guides and Blogs    A growing ecosystem of practitioner-focused articles and frameworks (e.g., Haystack, LlamaIndex, LangChain, vendor-specific stacks) is exploring: multi-hop RAG, hybrid dense+sparse retrieval, re-ranking, and evaluation tooling. These are not Dharma-specific, but they provide implementation patterns and pitfalls that can inform TNH Scholar\u2019s engineering phase.  </li> <li>Example: Advanced RAG Techniques: What They Are &amp; How to Use Them (already listed above).</li> </ol> <p>This document is intentionally high-level and research-oriented. As TNH Scholar matures, each subsection can be expanded into:</p> <ul> <li>A design document (for a specific RAG component or experiment).</li> <li>An ADR (capturing specific architectural and tooling decisions).</li> <li>A set of notebooks or scripts in the <code>research/</code> or <code>experiments/</code> tree, linked back here for provenance.</li> </ul>"},{"location":"research/siap-methodology/","title":"Structural-Informed Adaptive Processing (SIAP) Methodology","text":"<p>Methodology for structure-aware adaptive processing that selects AI strategies based on content fingerprints.</p>"},{"location":"research/siap-methodology/#core-philosophy","title":"Core Philosophy","text":"<p>Information processing should be guided by structural understanding, not just algorithmic capability. Rather than applying uniform processing to heterogeneous content, we first assess structure, then select and configure processing strategies accordingly.</p>"},{"location":"research/siap-methodology/#methodology-components","title":"Methodology Components","text":""},{"location":"research/siap-methodology/#1-structural-assessment-phase","title":"1. Structural Assessment Phase","text":"<p>Principle: Understand before processing</p> <ul> <li>Content fingerprinting: Identify structural patterns (speaker segments, document sections, data distributions)</li> <li>Context classification: Categorize content type (formal presentation, conversation, technical documentation)</li> <li>Resource profiling: Assess computational/cost constraints</li> <li>Quality requirements: Define accuracy vs speed vs cost trade-offs</li> </ul>"},{"location":"research/siap-methodology/#2-strategy-stratification","title":"2. Strategy Stratification","text":"<p>Principle: Hierarchical processing strategies with explicit trade-offs</p> <ul> <li>Coarse strategies: Fast, cheap, broad coverage (your 1-2 minute sampling)</li> <li>Fine strategies: Expensive, precise, targeted (utterance-level detection)</li> <li>Fallback cascades: Graceful escalation paths when coarse methods insufficient</li> <li>Hybrid approaches: Combining multiple strategies based on confidence thresholds</li> </ul>"},{"location":"research/siap-methodology/#3-adaptive-configuration","title":"3. Adaptive Configuration","text":"<p>Principle: Strategy parameters adapt to content structure</p> <ul> <li>Content-aware tuning: Sampling rates based on content type</li> <li>Dynamic thresholds: Confidence levels that trigger strategy escalation</li> <li>Resource budgets: Cost/time constraints that shape strategy selection</li> <li>Quality targets: Accuracy requirements that determine processing depth</li> </ul>"},{"location":"research/siap-methodology/#4-meta-level-orchestration","title":"4. Meta-level Orchestration","text":"<p>Principle: Strategy selection becomes itself an automated decision</p> <ul> <li>Strategy selection agents: AI systems that choose processing approaches</li> <li>Performance feedback loops: Strategies improve based on outcome quality</li> <li>Cost optimization: Automatic trade-off decisions based on constraints</li> <li>Human oversight: Escalation to human judgment for edge cases</li> </ul>"},{"location":"research/siap-methodology/#5-human-agent-collaboration-framework","title":"5. Human-Agent Collaboration Framework","text":"<p>Principle: Leverage human domain expertise with agent processing capability</p> <ul> <li>Domain knowledge injection: Humans provide structural insights (dharma talk patterns)</li> <li>Strategy design: Humans design strategy hierarchies, agents execute them</li> <li>Quality assessment: Human judgment validates strategy effectiveness</li> <li>Continuous refinement: Collaborative improvement of strategy selection</li> </ul>"},{"location":"research/siap-methodology/#implementation-architecture","title":"Implementation Architecture","text":""},{"location":"research/siap-methodology/#content-structure-analyzer","title":"Content Structure Analyzer","text":"<pre><code>class ContentAnalyzer:\n    def assess_structure(self, content) -&gt; StructuralProfile\n    def classify_content_type(self, content) -&gt; ContentType\n    def estimate_processing_requirements(self, content) -&gt; ResourceProfile\n</code></pre>"},{"location":"research/siap-methodology/#strategy-configuration-engine","title":"Strategy Configuration Engine","text":"<pre><code>class StrategyEngine:\n    def select_strategy(self, profile: StructuralProfile) -&gt; ProcessingStrategy\n    def configure_parameters(self, strategy, constraints) -&gt; StrategyConfig\n    def create_fallback_chain(self, primary_strategy) -&gt; List[ProcessingStrategy]\n</code></pre>"},{"location":"research/siap-methodology/#meta-level-orchestrator","title":"Meta-level Orchestrator","text":"<pre><code>class ProcessingOrchestrator:\n    def execute_strategy(self, content, strategy) -&gt; ProcessingResult\n    def evaluate_quality(self, result) -&gt; QualityMetrics\n    def escalate_if_needed(self, result, fallback_chain) -&gt; ProcessingResult\n</code></pre>"},{"location":"research/siap-methodology/#application-domains","title":"Application Domains","text":""},{"location":"research/siap-methodology/#audiovideo-processing","title":"Audio/Video Processing","text":"<ul> <li>Structure: Speaker segments, scene changes, audio quality regions</li> <li>Strategies: Coarse sampling \u2192 targeted analysis \u2192 full processing</li> <li>Adaptation: Content type (interview, lecture, music) drives strategy</li> </ul>"},{"location":"research/siap-methodology/#document-processing","title":"Document Processing","text":"<ul> <li>Structure: Sections, tables, figures, text density</li> <li>Strategies: Layout analysis \u2192 content extraction \u2192 semantic processing</li> <li>Adaptation: Document type (academic, legal, technical) shapes approach</li> </ul>"},{"location":"research/siap-methodology/#data-analysis-pipelines","title":"Data Analysis Pipelines","text":"<ul> <li>Structure: Data distributions, missing patterns, correlation structures</li> <li>Strategies: Statistical profiling \u2192 targeted modeling \u2192 full analysis</li> <li>Adaptation: Data characteristics determine processing complexity</li> </ul>"},{"location":"research/siap-methodology/#key-benefits","title":"Key Benefits","text":""},{"location":"research/siap-methodology/#cost-efficiency","title":"Cost Efficiency","text":"<ul> <li>Avoids over-processing uniform content</li> <li>Applies expensive methods only where needed</li> <li>Optimizes resource allocation based on content structure</li> </ul>"},{"location":"research/siap-methodology/#quality-optimization","title":"Quality Optimization","text":"<ul> <li>Matches processing depth to content requirements</li> <li>Provides fallback mechanisms for edge cases</li> <li>Enables human expertise injection at critical points</li> </ul>"},{"location":"research/siap-methodology/#scalability","title":"Scalability","text":"<ul> <li>Strategy selection scales independently of content volume</li> <li>Meta-level agents can handle increasing complexity</li> <li>Human involvement focuses on high-value decisions</li> </ul>"},{"location":"research/siap-methodology/#adaptability","title":"Adaptability","text":"<ul> <li>New content types automatically trigger strategy development</li> <li>Performance feedback improves strategy selection over time</li> <li>Human domain expertise continuously refines approaches</li> </ul>"},{"location":"research/siap-methodology/#strategic-implications","title":"Strategic Implications","text":"<p>This methodology suggests that the future of AI processing lies not in better algorithms alone, but in better strategy selection and configuration systems. The most effective AI systems will be those that can assess information structure, select appropriate processing strategies, and collaborate with humans to continuously refine their approach.</p> <p>Your language detection optimization exemplifies this perfectly: leveraging structural knowledge (speaker consistency) to optimize strategy (coarse sampling) with configurable fallbacks (fine-grained detection) that could be orchestrated by meta-level decision making.</p> <p>Does this framework capture the essence of what you're seeing in your processing pipeline development?</p>"},{"location":"research/gpt4o-search-query-testing/","title":"Gpt4O Search Query Testing","text":"<p>Table of Contents:</p> <p>1-3 Word Queries - Prompt experiments for generating search query and passage pairs used to train retrieval models.</p> <p>This file auto-generated.</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/","title":"1-3 Word Queries","text":"<p>Prompt experiments for generating search query and passage pairs used to train retrieval models.</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#input-test-1","title":"Input test 1:","text":"<p>Generate 30 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The queries should be a mix of short (30%), medium (50%), and long lengths (20%): - 30% short queries (1-3 words) - 50% medium queries (4-10 words) - 20% long queries (10+ words)</p> <p>The project focuses on helping users find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Text:</p> <p>title Transformation and Healing: Sutra on the Four Establishments of Mindfulness \\title</p> <p>section Mindfulness Exercises \\section</p> <p>exercise Exercise 7| Parts of the Body exercise</p> <p>Further, the practitioner meditates on his very own body from the soles of the feet upwards and then from the hair on top of the head downwards, a body contained inside the skin and full of all the impurities which belong to the body: \u2018Here is the hair of the head, the hairs on the body, the nails, teeth, skin, flesh, sinews, bones, bone marrow, kidneys, heart, liver, diaphragm, spleen, lungs, intestines, bowels, excrement, bile, phlegm, pus, blood, sweat, fat, tears, grease, saliva, mucus, synovic fluid, urine.\u2019</p> <p>This exercise brings us into even deeper contact with our body. Here we observe the body in all its parts, from the hair on the head to the skin on the soles of the feet. In the process of our observation, we scan all the parts of the body, including the brain, heart, lungs, gall bladder, spleen, blood, urine, and so forth. The Buddha gives us the example of a farmer pouring the contents of a sack filled with a variety of seeds onto the floor and then observing and identifying each kind of seed: \u201cThis is rice, these are beans, these are sesame seeds.\u201d</p> <p>We use our conscious breathing in order to observe mindfully all the parts of the body. For example: \u201cBreathing in, I am aware of the hair on my head. Breathing out, I know that this is the hair on my head.\u201d Breathing consciously helps us dwell in mindfulness more easily and sustain the work of observing each part of the body. In addition to the conscious breathing, we can use the method of silently calling each part of the body by name to enable these parts to become increasingly clear in the light of mindfulness.</p> <p>Why do we need to observe in mindfulness the different parts of the body? First of all, it is to be in contact with the body. We often have the impression that we\u2019re already totally in touch with our body, but often we\u2019re wrong. Between us and our body there can be a large separation, and our body remains a stranger to us. Sometimes we hate our body. There are even people who see their body as a prison and a place of punishment. To come back to our body is to become familiar with it and to establish harmony with it. We know that if our body isn\u2019t happy, we\u2019re not happy, and so we want our body to be calm and peaceful. To do so, we come back to our body and make peace with it.</p> <p>We can try touching the different parts of our body to make their acquaintance. We should touch each part in an affectionate and caring way. For several decades, our eyes, feet, and heart have done their work devotedly and faithfully with us and for us, but we never really give them much attention or express our gratitude to them. It\u2019s necessary to establish a close relationship with our body.</p> <p>The second reason for mindfully observing the different parts of the body is that each part can be the door to liberation and awakening. At first we\u2019ll only recognize the presence of the part of the body being observed, but later we\u2019ll come to see its true nature. Every hair on our head and every cell in our body contains the entire universe. Observing the interdependent nature of a single hair can help us to see into the nature of the universe.</p> <p>The exercise of observing every part of the body begins with the hair on the head and goes down to the skin on the soles of the feet. Sometimes we observe just one part of the body deeply, such as our eyes, heart, or toe. In the process of observation from the head to the feet, some observations may spring up in our mind. For example, as we pass our heart, we may think, \u201cMy friend John has a heart condition. I must visit him soon to see if he\u2019s all right.\u201d We can note these observations and then continue with the work of observing the remaining parts of the body. Later we can return to those observations.</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#ouput","title":"Ouput","text":"<p>Here are 20 query-text pairs for training a BERT-based search model on Thich Nhat Hanh\u2019s work, focusing on the \u201cTransformation and Healing: Sutra on the Four Establishments of Mindfulness,\u201d specifically the mindfulness exercise on \u201cParts of the Body.\u201d</p> <p>Short Queries (30%):</p> <pre><code>1.  Query: Mindfulness breathing\n</code></pre> <p>Text: \u201cWe use our conscious breathing in order to observe mindfully all the parts of the body\u2026\u201d     2.  Query: Body observation Text: \u201cThe practitioner meditates on his very own body from the soles of the feet upwards\u2026\u201d     3.  Query: Contact with the body Text: \u201cFirst of all, it is to be in contact with the body. We often have the impression that we\u2019re already totally in touch with our body, but often we\u2019re wrong\u2026\u201d     4.  Query: Parts of the body meditation Text: \u201cWe scan all the parts of the body, including the brain, heart, lungs, gall bladder, spleen, blood, urine\u2026\u201d     5.  Query: Affectionate touch Text: \u201cWe should touch each part in an affectionate and caring way. For several decades, our eyes, feet, and heart have done their work devotedly\u2026\u201d     6.  Query: Conscious breathing Text: \u201cBreathing consciously helps us dwell in mindfulness more easily and sustain the work of observing each part of the body\u2026\u201d</p> <p>Medium Queries (50%):     7. Query: How to reconnect with the body Text: \u201cTo come back to our body is to become familiar with it and to establish harmony with it. We know that if our body isn\u2019t happy, we\u2019re not happy\u2026\u201d</p> <pre><code>8.  Query: Meditation on the body\u2019s true nature\n</code></pre> <p>Text: \u201cAt first we\u2019ll only recognize the presence of the part of the body being observed, but later we\u2019ll come to see its true nature\u2026\u201d     9.  Query: Importance of mindful observation Text: \u201cThe second reason for mindfully observing the different parts of the body is that each part can be the door to liberation and awakening\u2026\u201d     10. Query: Separation from the body Text: \u201cBetween us and our body there can be a large separation, and our body remains a stranger to us. Sometimes we hate our body\u2026\u201d     11. Query: Calling parts of the body by name Text: \u201cIn addition to the conscious breathing, we can use the method of silently calling each part of the body by name to enable these parts to become increasingly clear\u2026\u201d     12. Query: Observing interdependence in the body Text: \u201cEvery hair on our head and every cell in our body contains the entire universe. Observing the interdependent nature of a single hair can help us to see into the nature of the universe\u2026\u201d     13. Query: Mindfulness and body parts Text: \u201cWhy do we need to observe in mindfulness the different parts of the body? First of all, it is to be in contact with the body\u2026\u201d     14. Query: Examples from Buddha\u2019s teachings on mindfulness Text: \u201cThe Buddha gives us the example of a farmer pouring the contents of a sack filled with a variety of seeds onto the floor and then observing and identifying each kind of seed\u2026\u201d     15. Query: Importance of making peace with the body Text: \u201cWe want our body to be calm and peaceful. To do so, we come back to our body and make peace with it\u2026\u201d     16. Query: Touching the body with care Text: \u201cWe can try touching the different parts of our body to make their acquaintance. We should touch each part in an affectionate and caring way\u2026\u201d</p> <p>Long Queries (20%):     17. Query: Why should we mindfully observe different parts of the body and how does this help us? Text: \u201cThe second reason for mindfully observing the different parts of the body is that each part can be the door to liberation and awakening. At first we\u2019ll only recognize the presence of the part of the body being observed, but later we\u2019ll come to see its true nature\u2026\u201d</p> <pre><code>18. Query: What are the practices for observing the body in mindfulness as taught by Thich Nhat Hanh?\n</code></pre> <p>Text: \u201cWe scan all the parts of the body, including the brain, heart, lungs, gall bladder, spleen, blood, urine, and so forth. The Buddha gives us the example of a farmer pouring the contents of a sack filled with a variety of seeds onto the floor and then observing and identifying each kind of seed\u2026\u201d     19. Query: How does conscious breathing help in the mindful observation of the body? Text: \u201cWe use our conscious breathing in order to observe mindfully all the parts of the body. For example: \u2018Breathing in, I am aware of the hair on my head. Breathing out, I know that this is the hair on my head.\u2019 Breathing consciously helps us dwell in mindfulness more easily and sustain the work of observing each part of the body\u2026\u201d     20. Query: What is the method of observing each part of the body as taught in Buddhist mindfulness exercises? Text: \u201cThis exercise brings us into even deeper contact with our body. Here we observe the body in all its parts, from the hair on the head to the skin on the soles of the feet. In the process of our observation, we scan all the parts of the body, including the brain, heart, lungs, gall bladder, spleen, blood, urine, and so forth\u2026\u201d</p> <p>These query-text pairs provide a balanced mix of short, medium, and long queries to help the model understand different types of search intents while reflecting key themes and detailed teachings from the text.</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#input-2-prompt","title":"Input 2 Prompt","text":"<p>Generate 20 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The queries should be: - roughly 30% short queries (~ 1-3 words) - roughly 50% medium queries (~ 4-7 words) - roughly 20% long queries (~ 7-11 words)</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Text:</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#input-3-prompt","title":"Input 3 Prompt","text":"<p>Generate 40 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The queries should be: - 40% short queries (1-3 words) - 50% medium queries (4-7 words) - 20% long queries (7-11 words)</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Output in pythonic as a list of tuples: (query, text). Truncate the text in the pair to the first 10 words.</p> <p>Text:</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#input-4-prompt","title":"Input 4 Prompt","text":"<p>Generate 40 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The queries should be: - 40% short queries (1-3 words) - 50% medium queries (4-7 words) - 20% long queries (7-11 words)</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Output in pythonic as a list of tuples: (query, text). Truncate the text in the pair to the first 10 words.</p> <p>Text:</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#input-5-prompt","title":"Input 5 Prompt","text":"<p>Generate 40 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The queries should be: - 40% short queries (1-3 words) - 50% medium queries (4-7 words) - 10% long queries (7-12 words)</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Output in pythonic as a list of tuples: (query, text). Truncate the text in the pair to the first 10 words.</p> <p>Text:</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#input-6-prompt","title":"Input 6 Prompt","text":"<p>Generate 40 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. </p> <p>15 queries: 1-3 words in length 20 queries: 4-7 words in length 5 queries: 7-12 words in length</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, but also some detailed information specific to the text.</p> <p>Output in pythonic as a list of tuples: (query, text). Truncate the text in the pair to the first 10 words.</p> <p>Text:</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#output-6","title":"Output 6","text":"<p>`queries_text_pairs = [     # 1-3 word queries     (\"mindfulness\", \"Further, the practitioner meditates on his very own body from\"),     (\"body parts\", \"Here is the hair of the head, the hairs\"),     (\"conscious breathing\", \"We use our conscious breathing in order to observe mindfully all\"),     (\"Buddha example\", \"The Buddha gives us the example of a farmer pouring\"),     (\"body awareness\", \"Breathing in, I am aware of the hair on\"),     (\"inner peace\", \"We know that if our body isn\u2019t happy, we\u2019re\"),     (\"harmony body\", \"To come back to our body is to become familiar\"),     (\"affectionate touch\", \"We can try touching the different parts of our body\"),     (\"liberation door\", \"The second reason for mindfully observing the different parts\"),     (\"interdependent nature\", \"Every hair on our head and every cell in\"),     (\"cell universe\", \"Every hair on our head and every cell in\"),     (\"scanning body\", \"In the process of our observation, we scan all\"),     (\"bodily gratitude\", \"We never really give them much attention or express\"),     (\"observing heart\", \"As we pass our heart, we may think, 'My'\"),     (\"mindful observation\", \"Why do we need to observe in mindfulness the different\"),</p> <pre><code># 4-7 word queries\n(\"how to practice conscious breathing\", \"We use our conscious breathing in order to observe\"),\n(\"awareness of head to feet\", \"Breathing in, I am aware of the hair\"),\n(\"Buddha's teachings on body mindfulness\", \"The Buddha gives us the example of a farmer\"),\n(\"building harmony with one's body\", \"To come back to our body is to\"),\n(\"reasons for mindful body observation\", \"Why do we need to observe in mindfulness the\"),\n(\"touching the body mindfully\", \"We can try touching the different parts of our\"),\n(\"grateful for bodily functions\", \"For several decades, our eyes, feet, and heart\"),\n(\"using breath for body awareness\", \"Breathing consciously helps us dwell in mindfulness more\"),\n(\"body parts as door to awakening\", \"The second reason for mindfully observing the different\"),\n(\"making peace with the body\", \"We know that if our body isn\u2019t happy\"),\n(\"why mindful breathing matters\", \"Breathing consciously helps us dwell in mindfulness more\"),\n(\"how mindfulness affects body awareness\", \"Why do we need to observe in mindfulness the\"),\n(\"relationship between body and happiness\", \"We know that if our body isn\u2019t happy\"),\n(\"using mindfulness to connect body\", \"To come back to our body is to\"),\n(\"importance of mindful body touch\", \"We can try touching the different parts of our\"),\n(\"observing body leads to insight\", \"Observing the interdependent nature of a single hair\"),\n(\"example of mindful body observation\", \"The Buddha gives us the example of a\"),\n(\"mindfulness of body leads to peace\", \"We know that if our body isn\u2019t happy\"),\n(\"contact with body through mindfulness\", \"First of all, it is to be in\"),\n(\"what mindfulness reveals about the body\", \"At first we\u2019ll only recognize the presence of\"),\n\n# 7-12 word queries\n(\"breathing and observing each part of the body mindfully\", \"Breathing consciously helps us dwell in mindfulness more easily\"),\n(\"mindful observation of body from head to feet\", \"In the process of our observation, we scan\"),\n(\"using mindfulness to become familiar with the body\", \"To come back to our body is to\"),\n(\"observing the interdependent nature of each body part\", \"Observing the interdependent nature of a single hair\"),\n(\"what Buddha teaches about observing the body's nature\", \"The Buddha gives us the example of a farmer\")\n</code></pre> <p>]`</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#input-7-prompt","title":"Input 7 prompt","text":"<p>Generate 30 query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. </p> <p>10 queries: 1-3 words in length 12 queries: 4-7 words in length 8 queries: full sentence questions</p> <p>The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text, some broad context questions beyond the scope relevant to Plum Village, and also some detailed information specific to the text.</p> <p>Output in pythonic as a list of tuples: (query, text). Truncate the text in the pair to the first 10 words.</p> <p>Text:</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#input-8-prompt-modified","title":"Input 8 prompt (modified!)","text":"<p>Generate query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text (without too much semantic overlap), some broad context questions generally relevant to Plum Village (not directly from the text), and also some detailed information specific to the text. Metadata, such as tagged titles, etc. can be ignored for this semantic based search training.</p> <p>12 queries: 1-3 words in length 12 queries: 4-7 words in length 8 queries: full sentence questions</p> <p>Give the result in a code block as csv form: query, text. </p> <p>Text:</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#output-for-prompt-8","title":"Output for Prompt 8","text":"<p>queries_text_pairs = [     # 1-3 word queries     (\"body mindfulness\", \"Further, the practitioner meditates on his very own body...\"),     (\"conscious breathing\", \"We use our conscious breathing in order to observe...\"),     (\"body parts\", \"In the process of our observation, we scan all the parts...\"),     (\"bodily impurities\", \"Further, the practitioner meditates on his very own body...\"),     (\"loving body\", \"We should touch each part in an affectionate and caring...\"),     (\"liberation\", \"Each part can be the door to liberation and awakening...\"),     (\"interdependence\", \"Every hair on our head and every cell in our body...\"),     (\"affectionate touch\", \"We should touch each part in an affectionate and caring...\"),     (\"Buddha's example\", \"The Buddha gives us the example of a farmer pouring...\"),     (\"physical harmony\", \"To come back to our body is to become familiar with it...\"),     (\"body awareness\", \"Breathing in, I am aware of the hair on my head...\"),     (\"observing parts\", \"The exercise of observing every part of the body begins...\"),</p> <pre><code># 4-7 word queries\n(\"scanning body parts through mindfulness\", \"In the process of our observation, we scan all the parts...\"),\n(\"observing body to establish harmony\", \"To come back to our body is to become familiar with it...\"),\n(\"how to mindfully observe body parts\", \"We use our conscious breathing in order to observe...\"),\n(\"practicing mindfulness for bodily awareness\", \"Breathing in, I am aware of the hair on my head...\"),\n(\"why practice mindfulness for body\", \"Why do we need to observe in mindfulness the different...\"),\n(\"meditating on physical impurities\", \"Further, the practitioner meditates on his very own body...\"),\n(\"building affection towards your body\", \"We should touch each part in an affectionate and caring...\"),\n(\"example of Buddha on mindfulness\", \"The Buddha gives us the example of a farmer pouring...\"),\n(\"observing body as a mindfulness exercise\", \"The exercise of observing every part of the body begins...\"),\n(\"liberation through bodily awareness\", \"Each part can be the door to liberation and awakening...\"),\n(\"practicing gratitude towards body parts\", \"For several decades, our eyes, feet, and heart have...\"),\n(\"understanding interdependence through mindfulness\", \"Every hair on our head and every cell in our body...\"),\n\n# Full sentence queries\n(\"How does observing the body lead to mindfulness?\", \"We use our conscious breathing in order to observe...\"),\n(\"What is the Buddha\u2019s example on mindfulness?\", \"The Buddha gives us the example of a farmer pouring...\"),\n(\"Why is it important to observe the body?\", \"Why do we need to observe in mindfulness the different...\"),\n(\"How can we establish harmony with our body?\", \"To come back to our body is to become familiar with it...\"),\n(\"What role does conscious breathing play in mindfulness?\", \"Breathing in, I am aware of the hair on my head...\"),\n(\"How does mindfulness lead to liberation?\", \"Each part can be the door to liberation and awakening...\"),\n(\"What is the importance of touching the body affectionately?\", \"We should touch each part in an affectionate and caring...\"),\n(\"How does mindfulness reveal the interdependent nature of the universe?\", \"Every hair on our head and every cell in our body...\")\n</code></pre> <p>]</p>"},{"location":"research/gpt4o-search-query-testing/queries-1-3-words/#working-prompt","title":"Working prompt","text":"<p>Generate query, text pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. The project focuses on helping people find info about Plum Village practices as well as researching Thay's teachings and Buddhist teachings. The generated queries should capture the key concepts and themes from the text (without too much semantic overlap), some broad context questions generally relevant to Plum Village (not directly from the text), and also some detailed information specific to the text. Metadata, such as tagged titles, etc. can be ignored for this semantic based search training.</p> <p>12 queries: 1-3 words in length 12 queries: 4-7 words in length 8 queries: full sentence questions</p> <p>Give the result in a code block as csv form: query, text. </p> <p>Text:</p>"},{"location":"research/gpt4o-translation-experiments/","title":"Gpt4O Translation Experiments","text":"<p>Table of Contents:</p> <p>Passage Test - Spot-checks comparing GPT-4o translation outputs with bilingual baselines on short passages.</p> <p>This file auto-generated.</p>"},{"location":"research/gpt4o-translation-experiments/passage_test/","title":"Passage Test","text":"<p>Spot-checks comparing GPT-4o translation outputs with bilingual baselines on short passages.</p>"},{"location":"research/gpt4o-translation-experiments/passage_test/#gpt-4o-prompt-1","title":"gpt-4o prompt 1","text":"<p>you are thich nhat hanh.</p> <p>translate this these sentences into Vietnamese:</p> <p>This exercise helps us see the impermanent and decomposing nature of our body. The objects of our mindful observation are the nine stages of the decomposition of a corpse. When we first read this, we may feel that this is not a pleasant meditation. But the effect of this practice can be very great.</p>"},{"location":"research/gpt4o-translation-experiments/passage_test/#out","title":"Out","text":"<p>B\u00e0i t\u1eadp n\u00e0y gi\u00fap ch\u00fang ta th\u1ea5y r\u00f5 t\u00ednh v\u00f4 th\u01b0\u1eddng v\u00e0 s\u1ef1 ph\u00e2n h\u1ee7y c\u1ee7a th\u00e2n th\u1ec3 ch\u00fang ta. C\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng quan s\u00e1t ch\u00e1nh ni\u1ec7m l\u00e0 ch\u00edn giai \u0111o\u1ea1n ph\u00e2n h\u1ee7y c\u1ee7a m\u1ed9t t\u1eed thi. Khi \u0111\u1ecdc \u0111i\u1ec1u n\u00e0y l\u1ea7n \u0111\u1ea7u, ch\u00fang ta c\u00f3 th\u1ec3 c\u1ea3m th\u1ea5y r\u1eb1ng \u0111\u00e2y kh\u00f4ng ph\u1ea3i l\u00e0 m\u1ed9t thi\u1ec1n t\u1eadp d\u1ec5 ch\u1ecbu. Nh\u01b0ng t\u00e1c d\u1ee5ng c\u1ee7a th\u1ef1c t\u1eadp n\u00e0y c\u00f3 th\u1ec3 r\u1ea5t l\u1edbn.</p>"},{"location":"research/gpt4o-translation-experiments/passage_test/#prompt-2-back-into-gpt-4o-translate-to-english","title":"prompt 2 -- back into gpt-4o translate to English","text":"<p>This exercise helps us to clearly see the impermanence and decay of our bodies. The objects of mindful observation are the nine stages of a decaying corpse. When reading this for the first time, we might feel that this is not a pleasant meditation practice. However, the effects of this practice can be profound.</p>"},{"location":"research/gpt4o-translation-experiments/passage_test/#vietai-en2vi-translation-mtet-translation","title":"VietAI EN2vi translation MTet translation","text":"<p>B\u00e0i t\u1eadp n\u00e0y gi\u00fap ta th\u1ea5y \u0111\u01b0\u1ee3c b\u1ea3n ch\u1ea5t v\u00f4 th\u01b0\u1eddng v\u00e0 ph\u00e2n h\u1ee7y c\u1ee7a th\u00e2n. \u0110\u1ed1i t\u01b0\u1ee3ng quan s\u00e1t ch\u00e1nh ni\u1ec7m c\u1ee7a ta l\u00e0 ch\u00edn giai \u0111o\u1ea1n ph\u00e2n h\u1ee7y x\u00e1c ch\u1ebft. Khi \u0111\u1ecdc \u0111\u1ebfn \u0111\u00e2y, ta c\u00f3 th\u1ec3 c\u1ea3m th\u1ea5y \u0111\u00e2y kh\u00f4ng ph\u1ea3i l\u00e0 m\u1ed9t thi\u1ec1n \u0111\u1ecbnh d\u1ec5 ch\u1ecbu. Nh\u01b0ng hi\u1ec7u qu\u1ea3 c\u1ee7a vi\u1ec7c th\u1ef1c h\u00e0nh n\u00e0y c\u00f3 th\u1ec3 r\u1ea5t l\u1edbn lao.</p>"},{"location":"user-guide/","title":"User Guide","text":"<p>Table of Contents:</p> <p>Best Practices - This guide outlines recommended practices for using TNH Scholar effectively.</p> <p>User Guide Overview - Practical guide for using TNH Scholar as a tool user or workflow designer, covering main workflows and how the pieces fit together.</p> <p>TNH Scholar Prompt System - This document describes the TNH Scholar Prompt System (formerly called patterns). The system allows for template-based prompting of AI interactions, with version control and concurrent access management.</p> <p>This file auto-generated.</p>"},{"location":"user-guide/best-practices/","title":"Best Practices","text":"<p>This guide outlines recommended practices for using TNH Scholar effectively.</p>"},{"location":"user-guide/best-practices/#general-guidelines","title":"General Guidelines","text":""},{"location":"user-guide/best-practices/#text-processing","title":"Text Processing","text":""},{"location":"user-guide/best-practices/#1-input-preparation","title":"1. Input Preparation","text":"<ul> <li>Ensure text files use consistent line endings</li> <li>Remove any special formatting or control characters</li> <li>Use UTF-8 encoding for all text files</li> <li>Consider running <code>nfmt</code> on input files to standardize formatting</li> </ul>"},{"location":"user-guide/best-practices/#2-language-handling","title":"2. Language Handling","text":"<ul> <li>Specify language codes explicitly when known</li> <li>Use ISO 639-1 two-letter codes (e.g., 'en', 'vi')</li> <li>Allow auto-detection only for simple cases</li> </ul>"},{"location":"user-guide/best-practices/#3-file-management","title":"3. File Management","text":"<ul> <li>Keep original files backed up</li> <li>Use descriptive file names</li> <li>Maintain consistent directory structure</li> <li>Store intermediate results when running long pipelines</li> </ul>"},{"location":"user-guide/best-practices/#command-line-tools","title":"Command-Line Tools","text":""},{"location":"user-guide/best-practices/#tnh-fab","title":"TNH-FAB","text":""},{"location":"user-guide/best-practices/#1-pattern-selection","title":"1. Pattern Selection","text":"<ul> <li>Use default patterns for initial testing</li> <li>Create custom patterns for specific needs</li> <li>Test patterns with small samples first</li> <li>Document pattern modifications</li> </ul>"},{"location":"user-guide/best-practices/#2-pipeline-design","title":"2. Pipeline Design","text":"<ul> <li>Break complex processing into steps</li> <li>Use intermediate files for long pipelines</li> <li>Validate output at each stage</li> <li>Consider using <code>tee</code> for debugging</li> </ul> <p>Example of good pipeline practice:</p> <pre><code># Good: Save intermediate results\ncat input.txt | \\\n  tnh-fab punctuate &gt; punctuated.txt &amp;&amp; \\\n  tnh-fab section punctuated.txt &gt; sections.json &amp;&amp; \\\n  tnh-fab process -p format_xml -s sections.json punctuated.txt &gt; final.xml\n\n# Not recommended: Direct pipeline without saves\ncat input.txt | tnh-fab punctuate | tnh-fab section | tnh-fab process -p format_xml\n</code></pre>"},{"location":"user-guide/best-practices/#3-error-handling","title":"3. Error Handling","text":"<ul> <li>Check command exit codes</li> <li>Save error output for debugging</li> <li>Use verbose mode for troubleshooting</li> <li>Keep log files organized</li> </ul>"},{"location":"user-guide/best-practices/#audio-transcribe","title":"Audio-Transcribe","text":""},{"location":"user-guide/best-practices/#1-audio-processing","title":"1. Audio Processing","text":"<ul> <li>Use appropriate chunk sizes for content</li> <li>Consider silence detection for natural breaks</li> <li>Monitor transcription quality</li> <li>Save intermediate audio chunks</li> </ul>"},{"location":"user-guide/best-practices/#2-youtube-downloads","title":"2. YouTube Downloads","text":"<ul> <li>Verify video availability before batch processing</li> <li>Use CSV files for bulk operations</li> <li>Include timestamps when needed</li> <li>Save downloaded audio files</li> </ul>"},{"location":"user-guide/best-practices/#pattern-development","title":"Pattern Development","text":""},{"location":"user-guide/best-practices/#1-pattern-design","title":"1. Pattern Design","text":"<ul> <li>Keep patterns focused and single-purpose</li> <li>Include clear documentation</li> <li>Test with various input types</li> <li>Patterns are automatically version controlled using git.</li> <li>Use git tools to inspect and manage versions.</li> </ul>"},{"location":"user-guide/best-practices/#2-template-variables","title":"2. Template Variables","text":"<ul> <li>Use descriptive variable names</li> <li>Provide default values when appropriate</li> <li>Document required variables</li> <li>Test variable combinations</li> </ul> <p>Example pattern structure:</p> <pre><code>---\ndescription: Example pattern for formatting\nversion: 1.0\nauthor: TNH Scholar\n---\nProcess this text according to these parameters:\n\nLanguage: {{ language }}\nStyle: {{ style_convention }}\nReview Count: {{ review_count }}\n\nAdditional instructions...\n</code></pre>"},{"location":"user-guide/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/best-practices/#1-resource-management","title":"1. Resource Management","text":"<ul> <li>Monitor token usage</li> <li>Use appropriate batch sizes</li> <li>Consider chunking for large files</li> <li>Cache intermediate results</li> </ul>"},{"location":"user-guide/best-practices/#2-api-usage","title":"2. API Usage","text":"<ul> <li>Implement rate limiting</li> <li>Handle API errors gracefully</li> <li>Monitor usage and costs</li> <li>Use appropriate models for tasks</li> </ul>"},{"location":"user-guide/best-practices/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"user-guide/best-practices/#1-input-validation","title":"1. Input Validation","text":"<ul> <li>Test with various file sizes</li> <li>Include different languages</li> <li>Check character encodings</li> <li>Verify line endings</li> </ul>"},{"location":"user-guide/best-practices/#2-output-validation","title":"2. Output Validation","text":"<ul> <li>Verify file formats</li> <li>Check content integrity</li> <li>Validate XML structure</li> <li>Compare with expected results</li> </ul>"},{"location":"user-guide/best-practices/#3-process-validation","title":"3. Process Validation","text":"<ul> <li>Monitor system resources</li> <li>Track processing time</li> <li>Log error conditions</li> <li>Document edge cases</li> </ul>"},{"location":"user-guide/best-practices/#security-considerations","title":"Security Considerations","text":""},{"location":"user-guide/best-practices/#1-api-keys","title":"1. API Keys","text":"<ul> <li>Use environment variables</li> <li>Never commit keys to version control</li> <li>Rotate keys regularly</li> <li>Monitor API usage</li> </ul>"},{"location":"user-guide/best-practices/#2-file-handling","title":"2. File Handling","text":"<ul> <li>Validate input files</li> <li>Use secure file permissions</li> <li>Clean up temporary files</li> <li>Handle sensitive content appropriately</li> </ul>"},{"location":"user-guide/best-practices/#documentation","title":"Documentation","text":""},{"location":"user-guide/best-practices/#1-code-comments","title":"1. Code Comments","text":"<ul> <li>Document complex logic</li> <li>Explain pattern usage</li> <li>Note assumptions</li> <li>Include examples</li> </ul>"},{"location":"user-guide/best-practices/#2-process-documentation","title":"2. Process Documentation","text":"<ul> <li>Document workflows</li> <li>Create usage examples</li> <li>Update for changes</li> <li>Include troubleshooting guides</li> </ul>"},{"location":"user-guide/overview/","title":"User Guide Overview","text":"<p>This User Guide describes how to use TNH Scholar as a tool user or workflow designer. It focuses on practical flows, concrete decisions, and how the pieces fit together, without requiring you to understand every internal design document.</p> <p>If you are new to the project, you may want to read the TNH Scholar index first, then return here when you are ready to dive into concrete workflows.</p>"},{"location":"user-guide/overview/#roles-and-typical-usage","title":"Roles and Typical Usage","text":"<p>Most people who interact with TNH Scholar do so in one of these roles:</p> <ul> <li> <p>Tool user   Runs the CLI commands to process specific audio, text, or video inputs, and reviews the outputs.</p> </li> <li> <p>Workflow designer   Chains together multiple tools (and sometimes GenAIService calls) into repeatable flows for a community or project.</p> </li> <li> <p>Developer or maintainer   Extends the codebase, adds new tools, or modifies existing ones.</p> </li> </ul> <p>This guide is aimed primarily at tool users and workflow designers. Developers should also see the development docs.</p>"},{"location":"user-guide/overview/#the-main-workflows","title":"The Main Workflows","text":"<p>The current CLI and service layer support three broad types of workflows.</p>"},{"location":"user-guide/overview/#1-audio-and-video-to-clean-text","title":"1. Audio and Video to Clean Text","text":"<p>Goal: Start from a recorded Dharma talk or teaching session and end with a clean, reviewable transcript.</p> <p>Typical steps:</p> <ol> <li>Transcribe audio with <code>audio-transcribe</code> </li> <li>Input: audio or video file (for example, <code>.wav</code>, <code>.mp3</code>, <code>.mp4</code>).  </li> <li> <p>Output: timestamped transcript (often JSON and/or text).</p> </li> <li> <p>Normalize formatting with <code>nfmt</code> </p> </li> <li>Input: transcript text.  </li> <li> <p>Output: normalized plain text, with consistent line wrapping, spacing, and punctuation.</p> </li> <li> <p>Optional: apply structure or tagging with <code>tnh-fab</code> or GenAIService-based flows  </p> </li> <li>Add markers for paragraphs, headings, quotes, or exercises.  </li> <li>Prepare the text for metadata or translation workflows.</li> </ol> <p>Relevant documentation:</p> <ul> <li>CLI Overview</li> <li>audio-transcribe and nfmt CLI guides</li> </ul>"},{"location":"user-guide/overview/#2-existing-text-to-structured-metadata-rich-text","title":"2. Existing Text to Structured, Metadata-Rich Text","text":"<p>Goal: Take texts that already exist (OCR, EPUB, PDF-derived, or plain text) and make them structured, tagged, and ready for search, translation, or archival use.</p> <p>Typical steps:</p> <ol> <li>Normalize and clean </li> <li> <p>Use <code>nfmt</code> or equivalent preprocessing to remove obvious noise and enforce consistent formatting.</p> </li> <li> <p>Apply patterns and prompts with <code>tnh-fab</code> </p> </li> <li> <p>Use domain-specific patterns or prompts to:  </p> <ul> <li>Identify headings and sections,  </li> <li>Tag poems, plays, quotes, exercises, or notes,  </li> <li>Insert metadata or footnote markers.</li> </ul> </li> <li> <p>Review and refine </p> </li> <li>Humans review the output, correct tagging, and adjust patterns as needed.  </li> <li>The corrected text becomes a better training or reference dataset for future workflows.</li> </ol> <p>Relevant documentation:</p> <ul> <li>Prompt System Architecture</li> <li>Additional prompt design docs: ADR-PT03</li> <li>tnh-fab CLI guide</li> </ul>"},{"location":"user-guide/overview/#3-prepared-text-to-model-ready-chunks","title":"3. Prepared Text to Model-Ready Chunks","text":"<p>Goal: Convert cleaned and structured text into units suitable for:</p> <ul> <li>Vector embedding and semantic search,</li> <li>Translation via GenAIService or other models,</li> <li>Evaluation and QA workflows.</li> </ul> <p>Typical steps:</p> <ol> <li>Segment text into chunks </li> <li> <p>Apply rules based on token length, semantic boundaries, or structural markers (for example, sections, paragraphs, stanzas).</p> </li> <li> <p>Estimate token usage with <code>token-count</code> </p> </li> <li>Check that individual chunks fit model limits.  </li> <li> <p>Plan batch sizes and costs for large-scale processing.</p> </li> <li> <p>Run AI workflows via GenAIService or other orchestration tools  </p> </li> <li>For example, translation, query-text pair generation, or similarity search indexing.</li> </ol> <p>Relevant documentation:</p> <ul> <li>Chunking and diarization design docs: Diarization System Design</li> <li>GenAI Service design documents</li> <li>token-count CLI guide</li> </ul>"},{"location":"user-guide/overview/#choosing-the-right-tool","title":"Choosing the Right Tool","text":"<p>When deciding which tool or workflow to use, consider:</p> <ul> <li>Type of input </li> <li>Audio or video \u2192 start with <code>audio-transcribe</code>.  </li> <li> <p>Text or OCR output \u2192 start with <code>nfmt</code> and/or <code>tnh-fab</code>.</p> </li> <li> <p>Target output </p> </li> <li>Human-readable transcript \u2192 focus on <code>audio-transcribe</code> + <code>nfmt</code>.  </li> <li>Machine-usable chunks for search or translation \u2192 include chunking logic and <code>token-count</code>.  </li> <li> <p>Rich, tagged editions \u2192 lean on <code>tnh-fab</code> and relevant prompt patterns.</p> </li> <li> <p>Review requirements </p> </li> <li>For archival or publication-ready materials, assume human review is mandatory.  </li> <li>For internal experimentation, you may tolerate more automation, but provenance still matters.</li> </ul> <p>The CLI Overview includes a quick decision table for common scenarios.</p>"},{"location":"user-guide/overview/#provenance-and-human-oversight","title":"Provenance and Human Oversight","text":"<p>A central principle of TNH Scholar is that all AI-assisted outputs must be traceable and reviewable. In practice this means:</p> <ul> <li>Keeping original sources (audio, scans, raw text) accessible and referenced.  </li> <li>Recording which tools, prompts, and models were used to generate any derived artifact.  </li> <li>Encouraging review workflows where humans accept, modify, or reject AI-suggested changes.</li> </ul> <p>The internal GenAIService and prompt system are designed to support these requirements. See:</p> <ul> <li>Prompt System Architecture</li> <li>Prompt Fingerprints and Provenance</li> </ul>"},{"location":"user-guide/overview/#where-to-go-next","title":"Where to Go Next","text":"<p>Suggested next readings:</p> <ul> <li>For concrete commands and options:</li> <li>CLI Overview</li> <li> <p>CLI guides under cli</p> </li> <li> <p>For design and architecture background:</p> </li> <li>Architecture Overview</li> <li> <p>GenAIService ADRs such as ADR-A12</p> </li> <li> <p>For contributing and development:  </p> </li> <li>Contributing to TNH Scholar (Prototype Phase) </li> <li>Human\u2013AI Software Engineering Principles</li> </ul>"},{"location":"user-guide/prompt-system/","title":"TNH Scholar Prompt System","text":"<p>This document describes the TNH Scholar Prompt System (formerly called patterns). The system allows for template-based prompting of AI interactions, with version control and concurrent access management.</p> <p>It is designed to interface with tnh-fab a multi-command text processing tool.</p> <p>Additional tools which use prompts may be developed for the project.</p> <p>The prompt system provides a version-controlled, concurrent-safe way to manage text processing templates. It is built around Jinja2 templates with Git-based versioning and file locking for safety.</p>"},{"location":"user-guide/prompt-system/#core-components","title":"Core Components","text":""},{"location":"user-guide/prompt-system/#prompt","title":"Prompt","text":"<p>A Prompt represents a single text processing template with:</p> <ul> <li>Instructions (as a Jinja2 template)</li> <li>Default template values</li> <li>Metadata in YAML frontmatter (optional) which may include default template values</li> </ul> <p>Example prompt file:</p> <pre><code>---\ndescription: Example prompt\nversion: 1.0\nlanguage: English\n---\nProcess this text in {{ language }} using {{ style_convention }} formatting.\n</code></pre> <p>In this example prompt the default <code>language</code> template variable is specified as English. If not supplied through a template or other means, this default value will be used. Setting default values when possible in the frontmatter is a good practice and allows prompts to run with less specifications.</p>"},{"location":"user-guide/prompt-system/#prompt-files","title":"Prompt Files","text":"<ul> <li>Stored as .md files</li> <li>Include optional YAML frontmatter</li> <li>Use Jinja2 template syntax</li> <li>Support template variables</li> </ul>"},{"location":"user-guide/prompt-system/#using-prompts","title":"Using Prompts","text":""},{"location":"user-guide/prompt-system/#through-tnh-fab-cli","title":"Through TNH-FAB CLI","text":"<p>The most common way to use prompts is through the TNH-FAB command-line tool:</p> <pre><code># Basic prompt processing\ntnh-fab process -p prompt_name input.txt\n\n# Process with sections\ntnh-fab process -p format_xml -s sections.json input.txt\n\n# Process by paragraphs\ntnh-fab process -p format_xml -g input.txt\n\n# Process with template values\ntnh-fab process -p format_xml -t template.yaml input.txt\n</code></pre> <p>Note: The <code>-p</code> flag uses legacy \"pattern\" terminology for backwards compatibility with existing code.</p> <p>Each TNH-FAB command (punctuate, section, translate, process) uses specific prompts:</p> <ul> <li>punctuate: Uses punctuation prompts (default: 'default_punctuate')</li> <li>section: Uses section analysis prompts (default: 'default_section')</li> <li>translate: Uses translation prompts (default: 'default_line_translation')</li> <li>process: Requires explicit prompt specification</li> </ul>"},{"location":"user-guide/prompt-system/#programmatic-usage","title":"Programmatic Usage","text":"<p>For developers building tools that use the prompt system:</p> <pre><code>from tnh_scholar.ai_text_processing import Pattern, PatternManager\n\n# Initialize pattern manager\npattern_manager = PatternManager(pattern_dir)\n\n# Load a pattern\npattern = pattern_manager.load_pattern(\"my_pattern\")\n\n# Apply template values\nresult = pattern.apply_template({\n    \"language\": \"English\",\n    \"style_convention\": \"APA\"\n})\n</code></pre>"},{"location":"user-guide/prompt-system/#prompt-location","title":"Prompt Location","text":"<p>By default, prompts are stored in the user's home directory under:</p> <pre><code>~/.config/tnh-scholar/patterns/\n</code></pre> <p>(Pattern is the legacy name that will be moved soon).</p> <p>This location can be customized by setting the <code>TNH_PATTERN_DIR</code> environment variable:</p> <pre><code># In .bashrc, .zshrc, or similar:\nexport TNH_PATTERN_DIR=/path/to/patterns\n</code></pre> <p>(or loaded through a <code>.env</code> file for development installations.)</p> <p>The prompt system will:</p> <ol> <li>First check for <code>TNH_PATTERN_DIR</code> environment variable</li> <li>If not set, use the default ~/.config/tnh-scholar/patterns</li> <li>Create the pattern directory if it doesn't exist</li> </ol> <p>When using a prompt/pattern name with tnh-fab commands (e.g., <code>tnh-fab process -p my_pattern</code>), the system searches for a corresponding .md file (e.g., <code>my_pattern.md</code>) in the pattern directory and its subdirectories.</p>"},{"location":"user-guide/prompt-system/#default-promptpatterns","title":"Default Prompt/Patterns","text":"<p>Through the setup utility, tnh-setup, the user has the option to download and install several default and example patterns.</p> <p>Note that tnh-fab expects the following patterns to be in the patterns directory for default use:</p> <ul> <li>default_punctuate.md - Default punctuation pattern</li> <li>default_section.md - Default section analysis pattern</li> <li>default_line_translation.md - Default translation pattern</li> </ul> <p>These provide basic functionality but can be customized or overridden by creating patterns with the same names in your pattern directory.</p>"},{"location":"user-guide/prompt-system/#pattern-integration","title":"Pattern Integration","text":"<p>The prompt system can be integrated into other tools and workflows:</p> <ul> <li>Custom text processing applications</li> <li>Web services</li> <li>Analysis pipelines</li> <li>Batch processing systems</li> </ul>"},{"location":"user-guide/prompt-system/#template-variables","title":"Template Variables","text":"<p>Templates support variables through Jinja2 syntax:</p> <ul> <li>Use <code>{{ variable }}</code> for simple substitution</li> <li>Values provided when applying template</li> <li>Default values can be specified in Pattern</li> </ul>"},{"location":"user-guide/prompt-system/#pattern-storage-and-management","title":"Pattern Storage and Management","text":""},{"location":"user-guide/prompt-system/#pattern-manager","title":"Pattern Manager","text":"<p>The PatternManager provides the main interface for:</p> <ul> <li>Loading patterns by name</li> <li>Saving new patterns</li> <li>Version control integration</li> <li>Concurrent access management</li> </ul>"},{"location":"user-guide/prompt-system/#pattern-locations","title":"Pattern Locations","text":"<p>Patterns are stored in a directory specified as either:</p> <ul> <li> <p><code>$HOME/.config/tnh-scholar/patterns</code> (default search location)</p> </li> <li> <p>A custom directory specified by TNH_PATTERN_DIR environment variable, which can also be configured in a .env file.</p> </li> </ul>"},{"location":"user-guide/prompt-system/#version-control","title":"Version Control","text":"<p>Patterns are automatically version controlled:</p> <ul> <li>Git-backed storage</li> <li>Automatic commits on changes</li> <li>History tracking</li> <li>Change validation</li> </ul>"},{"location":"user-guide/prompt-system/#concurrent-access","title":"Concurrent Access","text":"<p>The system provides safe concurrent access through:</p> <ul> <li>File-level locking</li> <li>Lock cleanup</li> <li>Stale lock detection</li> <li>Safe access patterns</li> </ul>"},{"location":"user-guide/prompt-system/#creating-patterns","title":"Creating Patterns","text":"<p>Patterns must have:</p> <ol> <li>Unique name</li> <li>Valid Jinja2 template content</li> <li>Optional default template values</li> </ol> <p>Example pattern creation:</p> <pre><code>from tnh_scholar.ai_text_processing import Pattern\n\npattern = Pattern(\n    name=\"example_pattern\",\n    instructions=\"Process {{ text }} using {{ style }}\",\n    default_template_fields={\"style\": \"default\"}\n)\n</code></pre>"},{"location":"user-guide/prompt-system/#pattern-file-format","title":"Pattern File Format","text":"<p>A pattern file (<code>example.md</code>):</p> <pre><code>---\ndescription: Example processing pattern\nversion: 1.0\nauthor: TNH Scholar\n---\nPlease process this text according to these parameters:\n\nLanguage: {{ language }}\nStyle: {{ style_convention }}\nReview Count: {{ review_count }}\n\nApply standard formatting while maintaining original meaning.\n</code></pre>"},{"location":"user-guide/prompt-system/#error-handling","title":"Error Handling","text":"<p>The system handles common errors:</p> <ul> <li>Missing patterns</li> <li>Invalid template syntax</li> <li>Concurrent access conflicts</li> <li>Version control issues</li> </ul>"},{"location":"user-guide/prompt-system/#technical-details","title":"Technical Details","text":""},{"location":"user-guide/prompt-system/#file-locking","title":"File Locking","text":"<ul> <li>Uses system-level file locking</li> <li>Automatic lock cleanup</li> <li>Timeout handling</li> <li>Safe concurrent access</li> </ul>"},{"location":"user-guide/prompt-system/#version-control_1","title":"Version Control","text":"<ul> <li>Git-based backend</li> <li>Automatic commit messages</li> <li>Change tracking</li> <li>History preservation</li> </ul>"},{"location":"user-guide/prompt-system/#pattern-validation","title":"Pattern Validation","text":"<p>All patterns are validated for:</p> <ul> <li>Template syntax</li> <li>Required variables</li> <li>Unique naming</li> <li>Content format</li> </ul>"},{"location":"user-guide/prompt-system/#limitations","title":"Limitations","text":"<p>Current implementation:</p> <ul> <li>Single repository per PatternManager</li> <li>File-based storage only</li> <li>Local Git repository</li> <li>Synchronous operations</li> </ul>"},{"location":"user-guide/prompt-system/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/prompt-system/#1-pattern-naming","title":"1. Pattern Naming","text":"<ul> <li>Use descriptive names</li> <li>Include purpose in name</li> <li>Follow lowercase_with_underscores format</li> </ul>"},{"location":"user-guide/prompt-system/#2-template-content","title":"2. Template Content","text":"<ul> <li>Document required variables</li> <li>Include usage examples</li> <li>Provide default values</li> <li>Use clear template syntax</li> </ul>"},{"location":"user-guide/prompt-system/#3-pattern-management","title":"3. Pattern Management","text":"<ul> <li>Regular pattern updates</li> <li>Version control usage</li> <li>Proper error handling</li> <li>Pattern testing</li> </ul>"}]}