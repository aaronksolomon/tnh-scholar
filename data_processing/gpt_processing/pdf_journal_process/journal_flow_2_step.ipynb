{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from data_processing.gpt_processing import (\n",
    "    set_api_client, \n",
    "    generate_messages, \n",
    "    create_jsonl_file_for_batch, \n",
    "    start_batch, \n",
    "    get_batch_response,\n",
    "    get_completed_batches,\n",
    "    set_model_settings,\n",
    "    get_batch_status,\n",
    "    get_active_batches,\n",
    "    get_all_batch_info,\n",
    "    token_count,\n",
    "    run_immediate_chat_process,\n",
    "    run_single_oa_batch,\n",
    "    get_last_batch_response\n",
    ")\n",
    "\n",
    "from data_processing.xml_processing import ( \n",
    "    save_pages_to_xml,\n",
    "    split_xml_pages\n",
    ")\n",
    "\n",
    "from data_processing.text_processing import (\n",
    "    get_text_from_file,\n",
    "    write_text_to_file\n",
    ")\n",
    "from pathlib import Path\n",
    "%aimport time\n",
    "%aimport json\n",
    "%aimport datetime\n",
    "%aimport logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API client\n",
    "client = set_api_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings = {\n",
    "    \"gpt-4o\": {\n",
    "        \"max_tokens\": 5000,\n",
    "        \"context_limit\": 20000,  # Total context limit for the model\n",
    "        \"temperature\": 0.25\n",
    "    },\n",
    "    \"gpt-3.5-turbo\": {\n",
    "        \"max_tokens\": 4096,  # Set conservatively to avoid errors\n",
    "        \"context_limit\": 16384  # Same as gpt-4o\n",
    "        }\n",
    "    }\n",
    "\n",
    "set_model_settings(model_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BATCH_RETRIES = 20  # Number of retries\n",
    "BATCH_RETRY_DELAY = 5  # seconds to wait before retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "project_dir = Path(\"/Users/phapman/Desktop/tnh-scholar/\")\n",
    "data_dir = project_dir / \"data_processing\"\n",
    "journal_dir = data_dir / \"processed_journal_data\"\n",
    "journal_name = \"phat-giao-viet-nam-1956-01\"\n",
    "working_dir = journal_dir / journal_name\n",
    "input_xml = working_dir / f\"TEST2_full_cleaned_{journal_name}.xml\"\n",
    "translated_xml_path = journal_dir / f\"translation_{journal_name}.xml\"\n",
    "section_batch_jsonl = working_dir / \"section_batch.jsonl\"\n",
    "translate_batch_jsonl = working_dir / \"translation_batch.jsonl\"\n",
    "section_metadata_out = working_dir / \"section_metadata.json\"\n",
    "raw_json_metadata_path = working_dir / \"raw_metadata_response.txt\"\n",
    "logfile = data_dir / \"gpt_processing\" / \"processing_info.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the logger\n",
    "def setup_logger(log_file_path):\n",
    "    \"\"\"\n",
    "    Configures the logger to write to a log file and the console.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file_path, encoding=\"utf-8\"),\n",
    "            logging.StreamHandler()  # Optional: to log to the console as well\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_logger(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_section = \"\"\"\n",
    "You are a highly skilled assistant processing a Vietnamese Buddhist journal scanned from OCR. Use the title: \"Journal of Vietnamese Buddhism.\"\n",
    "You will be determining the journal sections by page number. You will also generate metadata for the full text and each section. \n",
    "You will return this metadata in JSON format.\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the text and divide it into sections based on logical breaks, such as headings, topic changes, or clear shifts in content.\n",
    "2. Ensure every page is part of  a section, even if that section is titled \"blank page\" or \"title page,\" for example.\n",
    "3. For each section, provide:\n",
    "   - The original title in Vietnamese (`section_title_vi`).\n",
    "   - The translated title in English (`section_title_en`).\n",
    "   - The author's name if it is available (`section_author`). \n",
    "   - A one-paragraph summary of the section in English (`section_summary`).\n",
    "   - A list of keywords for the section that are related to its content, these can be proper names, specific concepts, or contextual information.\n",
    "   - The section's start and end page numbers (`start_page` and `end_page`).\n",
    "   - Use \"null\" for any data that is not available (such as author name) for the section.\n",
    "\n",
    "4. Return the output as a JSON object with the following schema:\n",
    "{\n",
    "    \"journal_summary\": \"A one-page summary of the whole journal in English.\",\n",
    "    \"sections\": [\n",
    "        {\n",
    "            \"section_title_vi\": \"Original title in Vietnamese\",\n",
    "            \"section_title_en\": \"Translated title in English\",\n",
    "            \"section_author\": \"Name of the author of the section\",\n",
    "            \"section_summary\": \"One-paragraph summary of the section in English\",\n",
    "            \"section_keywords\": \"A list of keywords for the section\",\n",
    "            \"start_page\":  X,\n",
    "            \"end_page\":  Y\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\n",
    "5.  Ensure the JSON is well-formed and adheres strictly to the provided schema.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_translate = \"\"\"\n",
    "You are Thich Nhat Hanh translating from Vietnamese to English for your experienced students. \n",
    "The text is based on an OCR scan of a journal you edited from 1956-1958. Use the title: \"Journal of Vietnamese Buddhism\" for the journal when it is referenced.\n",
    "You will be translating a single section of the journal and will be provided with the section title in English. \n",
    "You want your students to understand the text in its larger historical context, in the context of Vietnamese Buddhism, and in the context of your own life.\n",
    "Translate for the most meaningful, typical, and eloquent English interpretation. \n",
    "Make corrections in the text only where necessary (for example if words are missing) to create logical flow . \n",
    "Keep pages together: each translated page must match its original page source as pages will be studied side by side with the original Vietnamese.\n",
    "Infer paragraphs and text structure from the text layout.\n",
    "Add XML tags for clarity. Use only the following tags: \n",
    "\n",
    "   <p> for paragraphs.\n",
    "   <section> for major sections.\n",
    "   <subsection> for subsections.\n",
    "   <title> for main titles of sections and subsections. \n",
    "   <subtitle> for subtitles of sections and subsections. \n",
    "   <heading> for headings that do not mark titles or subtitles\n",
    "   <TOC> for tables of contents\n",
    "   <author> for authors of sections or subsections\n",
    "   <ol> <ul> <li> for lists\n",
    "   <i> for italics. \n",
    "   <b> for bold.\n",
    "   <notes>\n",
    "   <translation-notes>\n",
    "\n",
    "You may use <notes> at the end of the section for notes that already exist in the text, or for interesting elements you wish to call attention to.\n",
    "You may add <translation-notes> at the end of the section as a commentary to summarize your translation choices. \n",
    "For <translation-notes>, you may include information on Sino-Vietnamese, complex, unusual, poetic, or other interesting terms, and significant corrections to the text. \n",
    "In the <translation-notes> include the original Vietnamese terms for reference.\n",
    "\n",
    "All titles, XML sections, text, and terms should be translated--do not leave any terms or expressions in Vietnamese, except names of Vietnamese people.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def deserialize_json(serialized_data):\n",
    "    \"\"\"\n",
    "    Converts a serialized JSON string into a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "        serialized_data (str): The JSON string to deserialize.\n",
    "\n",
    "    Returns:\n",
    "        dict: The deserialized Python dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert the JSON string into a dictionary\n",
    "        return json.loads(serialized_data)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to deserialize JSON: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "# Define the schema\n",
    "journal_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"journal_summary\": {\"type\": \"string\"},\n",
    "        \"sections\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"section_title_vi\": {\"type\": \"string\"},\n",
    "                    \"section_title_en\": {\"type\": \"string\"},\n",
    "                    \"section_author\": {\"type\": [\"string\", \"null\"]},\n",
    "                    \"section_summary\": {\"type\": \"string\"},\n",
    "                    \"section_keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"start_page\": {\"type\": \"integer\", \"minimum\": 1},\n",
    "                    \"end_page\": {\"type\": \"integer\", \"minimum\": 1}\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"section_title_vi\",\n",
    "                    \"section_title_en\",\n",
    "                    \"section_summary\",\n",
    "                    \"section_keywords\",\n",
    "                    \"start_page\",\n",
    "                    \"end_page\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"journal_summary\", \"sections\"]\n",
    "}\n",
    "\n",
    "def validate_and_clean_data(data, schema):\n",
    "    \"\"\"\n",
    "    Recursively validate and clean AI-generated data to fit the given schema.\n",
    "    Any missing fields are filled with defaults, and extra fields are ignored.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The AI-generated data to validate and clean.\n",
    "        schema (dict): The schema defining the required structure.\n",
    "\n",
    "    Returns:\n",
    "        dict: The cleaned data adhering to the schema.\n",
    "    \"\"\"\n",
    "    def clean_value(value, field_schema):\n",
    "        \"\"\"\n",
    "        Clean a single value based on its schema, attempting type conversions where necessary.\n",
    "        \"\"\"\n",
    "        field_type = field_schema[\"type\"]\n",
    "\n",
    "        # Handle type: string\n",
    "        if field_type == \"string\":\n",
    "            if isinstance(value, str):\n",
    "                return value\n",
    "            elif value is not None:\n",
    "                return str(value)\n",
    "            return \"unset\"\n",
    "\n",
    "        # Handle type: integer\n",
    "        elif field_type == \"integer\":\n",
    "            if isinstance(value, int):\n",
    "                return value\n",
    "            elif isinstance(value, str) and value.isdigit():\n",
    "                return int(value)\n",
    "            try:\n",
    "                return int(float(value))  # Handle cases like \"2.0\"\n",
    "            except (ValueError, TypeError):\n",
    "                return 0\n",
    "\n",
    "        # Handle type: array\n",
    "        elif field_type == \"array\":\n",
    "            if isinstance(value, list):\n",
    "                item_schema = field_schema.get(\"items\", {})\n",
    "                return [clean_value(item, item_schema) for item in value]\n",
    "            elif isinstance(value, str):\n",
    "                # Try splitting comma-separated strings into a list\n",
    "                return [v.strip() for v in value.split(\",\")]\n",
    "            return []\n",
    "\n",
    "        # Handle type: object\n",
    "        elif field_type == \"object\":\n",
    "            if isinstance(value, dict):\n",
    "                return validate_and_clean_data(value, field_schema)\n",
    "            return {}\n",
    "\n",
    "        # Handle nullable strings\n",
    "        elif field_type == [\"string\", \"null\"]:\n",
    "            if value is None or isinstance(value, str):\n",
    "                return value\n",
    "            return str(value)\n",
    "\n",
    "        # Default case for unknown or unsupported types\n",
    "        return \"unset\"\n",
    "\n",
    "    def clean_object(obj, obj_schema):\n",
    "        \"\"\"\n",
    "        Clean a dictionary object based on its schema.\n",
    "        \"\"\"\n",
    "        if not isinstance(obj, dict):\n",
    "            print(f\"Expected dict but got: \\n{type(obj)}: {obj}\\nResetting to empty dict.\")\n",
    "            return {}\n",
    "        cleaned = {}\n",
    "        properties = obj_schema.get(\"properties\", {})\n",
    "        for key, field_schema in properties.items():\n",
    "            # Set default value for missing fields\n",
    "            cleaned[key] = clean_value(obj.get(key), field_schema)\n",
    "        return cleaned\n",
    "\n",
    "    # Handle the top-level object\n",
    "    if schema[\"type\"] == \"object\":\n",
    "        cleaned_data = clean_object(data, schema)\n",
    "        return cleaned_data\n",
    "    else:\n",
    "        raise ValueError(\"Top-level schema must be of type 'object'.\")\n",
    "\n",
    "def validate_and_save_metadata(output_file_path: Path, json_metadata_serial: str, schema):\n",
    "    \"\"\"\n",
    "    Validates and cleans journal data against the schema, then writes it to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data (str): The journal data as a serialized JSON string to validate and clean.\n",
    "        schema (dict): The schema defining the required structure.\n",
    "        output_file_path (str): Path to the output JSON file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if successfully written to the file, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean the data to fit the schema\n",
    "        data = deserialize_json(json_metadata_serial)\n",
    "        cleaned_data = validate_and_clean_data(data, schema)\n",
    "\n",
    "        # Write the cleaned data to the specified JSON file\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Cleaned data successfully written to {output_file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during validation or writing: {e}\")\n",
    "        raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the save_processed_journal_metadata function\n",
    "def save_processed_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Save processed journal metadata, including title, summary, and sections, to a JSON file.\n",
    "    \"\"\"\n",
    "    output_file = project_dir / \"processed_journal_metadata.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(f\"Processed journal metadata saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_groups_from_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Extracts page groups from the section metadata for use with `split_xml_pages`.\n",
    "\n",
    "    Parameters:\n",
    "        metadata (dict): The section metadata containing sections with start and end pages.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: A list of tuples, each representing a page range (start_page, end_page).\n",
    "    \"\"\"\n",
    "    page_groups = []\n",
    "\n",
    "    # Ensure metadata contains sections\n",
    "    if \"sections\" not in metadata or not isinstance(metadata[\"sections\"], list):\n",
    "        raise ValueError(\"Metadata does not contain a valid 'sections' key with a list of sections.\")\n",
    "\n",
    "    for section in metadata[\"sections\"]:\n",
    "        try:\n",
    "            # Extract start and end pages\n",
    "            start_page = section.get(\"start_page\")\n",
    "            end_page = section.get(\"end_page\")\n",
    "\n",
    "            # Ensure both start_page and end_page are integers\n",
    "            if not isinstance(start_page, int) or not isinstance(end_page, int):\n",
    "                raise ValueError(f\"Invalid page range in section: {section}\")\n",
    "\n",
    "            # Add the tuple to the page groups list\n",
    "            page_groups.append((start_page, end_page))\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in section metadata: {e}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing section metadata: {e}\")\n",
    "\n",
    "    return page_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_batch_info(batch, logger):\n",
    "    \"\"\"\n",
    "    Log the batch object and its metadata using the logger.\n",
    "\n",
    "    Args:\n",
    "        batch: The Batch object returned by start_batch.\n",
    "        logger: The logger instance for logging.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Serialize batch object to a dictionary\n",
    "        batch_data = batch.to_dict() if hasattr(batch, \"to_dict\") else batch.__dict__\n",
    "\n",
    "        # Add metadata\n",
    "        batch_data[\"timestamp\"] = datetime.datetime.now().isoformat()\n",
    "\n",
    "        # Log the batch data as JSON\n",
    "        logger.info(\"Batch Info: %s\", json.dumps(batch_data, indent=4))\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to log batch: %s\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polling function for batch completion\n",
    "def poll_batch_for_response(batch_id, interval=10):\n",
    "    \"\"\"\n",
    "    Poll the batch status until it completes or fails.\n",
    "    Raises an exception if the batch fails.\n",
    "    Returns the batch response if successful.\n",
    "    \"\"\"\n",
    "    print(f\"Polling batch status for batch ID {batch_id} ...\")\n",
    "    while True:\n",
    "        time.sleep(interval)\n",
    "        batch_status = get_batch_status(batch_id)\n",
    "\n",
    "        if not batch_status:\n",
    "            raise RuntimeError(f\"Batch ID {batch_id} not found.\")\n",
    "\n",
    "        if batch_status == \"completed\":\n",
    "            print(\"Batch processing completed successfully.\")\n",
    "            return get_batch_response(batch_id)\n",
    "        \n",
    "        elif batch_status == \"failed\":\n",
    "            raise RuntimeError(f\"Batch ID {batch_id} failed during processing.\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"Batch status: {batch_status}. Retrying in {interval} seconds...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sectioning(input_xml_path, output_json_path, raw_output_path, journal_name, max_retries=MAX_BATCH_RETRIES, retry_delay=BATCH_RETRY_DELAY):\n",
    "    \"\"\"\n",
    "    Splits the journal content into sections using GPT, with retries for both starting and completing the batch.\n",
    "    \"\"\"\n",
    "    journal_pages = get_text_from_file(input_xml_path)\n",
    "\n",
    "    # Create GPT messages for sectioning\n",
    "    user_message_wrapper = lambda text: f\"{text}\"\n",
    "    messages = generate_messages(system_message_section, user_message_wrapper, [journal_pages])\n",
    "\n",
    "    # Create JSONL file for batch processing\n",
    "    jsonl_file = create_jsonl_file_for_batch(messages, section_batch_jsonl, json_mode=True)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Try to start the batch\n",
    "            batch = start_batch(jsonl_file, description=f\"Batch for sectioning journal: {journal_name} | input file: {input_xml_path}\")\n",
    "            batch_id = batch.id\n",
    "            if not batch_id:\n",
    "                raise RuntimeError(\"Batch started but no ID was returned.\")\n",
    "\n",
    "            print(f\"Batch for sectioning started successfully on attempt {attempt + 1}. ID: {batch_id}\")\n",
    "\n",
    "            # Poll for batch completion\n",
    "            json_results = poll_batch_for_response(batch_id)\n",
    "            if json_results:\n",
    "                break # exit retry loop\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown error in polling for batch response.\", exc_info=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}. Retrying batch process in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "    else:\n",
    "        logger.error(\"Failed to complete batch sectioning after maximum retries.\")\n",
    "        raise RuntimeError(\"Error: Failed to complete batch sectioning after maximum retries.\")\n",
    "\n",
    "    # save raw result\n",
    "    try:\n",
    "        write_text_to_file(raw_output_path, json_results, force=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"failed to write raw response file: {raw_output_path}\")\n",
    "        raise\n",
    "\n",
    "    # If successful, try to validate and save metadata and exit loop\n",
    "    try:\n",
    "        valid = validate_and_save_metadata(output_json_path, json_results, journal_schema)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred while validating and saving metadata for journal {journal_name}: '{output_json_path}' (batch ID: {batch_id}).\", exc_info=True)\n",
    "        raise\n",
    "    \n",
    "    if valid:\n",
    "        logger.info(f\"Successfully processed {journal_name}: {input_xml_path} with batch: {batch_id} and saved metadata to {output_json_path} \")\n",
    "        return output_json_path\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Translation\n",
    "def batch_translate(input_xml_path, metadata_path, journal_name, xml_output_path, max_retries=MAX_BATCH_RETRIES, retry_delay=BATCH_RETRY_DELAY):\n",
    "    \"\"\"\n",
    "    Translates the journal sections using the GPT model.\n",
    "    Saves the translated content back to XML.\n",
    "\n",
    "    Args:\n",
    "        input_xml_path (str): Path to the input XML file.\n",
    "        metadata_path (str): Path to the metadata JSON file.\n",
    "        max_retries (int): Maximum number of retries for batch operations.\n",
    "        retry_delay (int): Delay in seconds between retries.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the process succeeds, False otherwise.\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "        f\"starting translation batch {journal_name}...\",\n",
    "        extra={\n",
    "            \"input_xml\": input_xml_path,\n",
    "            \"metadata_path\": metadata_path,\n",
    "            \"journal_name\": journal_name\n",
    "        }\n",
    "    )\n",
    "    try: # data initialization:\n",
    "        # get metadata\n",
    "        section_metadata = deserialize_json(metadata_path)\n",
    "        section_title = section_metadata.section_title_en\n",
    "\n",
    "        # Extract page groups and split XML content\n",
    "        page_groups = extract_page_groups_from_metadata(section_metadata)\n",
    "        xml_content = get_text_from_file(input_xml_path)\n",
    "        sections = split_xml_pages(xml_content, page_groups)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error with full traceback\n",
    "        logger.error(\n",
    "            \"Could not initialize data for translation batching {journal_name}\", exc_info=True)\n",
    "        raise  # Re-raise the exception to escalate\n",
    "\n",
    "    # Create GPT messages for translation\n",
    "\n",
    "    user_message_wrapper = lambda section: f\"Translate this section with title {section_title}:\\n{section}\"\n",
    "    messages = generate_messages(system_message_translate, user_message_wrapper, sections)\n",
    "\n",
    "    # Create JSONL file for batch processing\n",
    "    \n",
    "    jsonl_file = create_jsonl_file_for_batch(messages, translate_batch_jsonl)\n",
    "    if not jsonl_file:\n",
    "        logger.error(\n",
    "            \"Failed to create JSONL file for translation batch.\",\n",
    "            exc_info=True  # Logs the exception traceback if one exists\n",
    "        )\n",
    "        raise RuntimeError(\"Failed to create JSONL file for translation batch.\")\n",
    "    \n",
    "    for attempt in range(max_retries): # batching logic requires multiple retries due to issues with API:\n",
    "        try:\n",
    "            # Start the batch\n",
    "            batch = start_batch(jsonl_file, description=\"Batch for translating journal\")\n",
    "            batch_id = batch.get(\"id\")\n",
    "            if not batch_id:\n",
    "                raise RuntimeError(\"Batch started but no ID was returned.\")\n",
    "            \n",
    "            print(f\"Batch for translation started successfully on attempt {attempt + 1}. ID: {batch_id}\")\n",
    "\n",
    "            # Poll for batch completion\n",
    "            print(\"Polling for batch completion...\")\n",
    "            results = poll_batch_for_response(batch_id)\n",
    "\n",
    "            if results:\n",
    "                break # exit the retry loop\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown error. No results from batch polling.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Attempt {attempt + 1} failed during translation for journal '{input_xml_path}'. Retrying in {retry_delay} seconds...\",\n",
    "                exc_info=True\n",
    "            )\n",
    "            time.sleep(retry_delay)\n",
    "    else:\n",
    "        logger.error(f\"Failed to complete translation after {max_retries} retries for journal '{input_xml_path}'.\")\n",
    "        raise RuntimeError(\"Unable to run translate batch.\")\n",
    "        \n",
    "    # Save translated content back to XML\n",
    "    try: \n",
    "        print(\"Saving translated content back to XML...\")\n",
    "        translated_sections = []\n",
    "        for i, translated_content in enumerate(results):\n",
    "            translated_sections.append(translated_content)\n",
    "\n",
    "        save_pages_to_xml(translated_sections, xml_output_path, overwrite=True)\n",
    "        print(f\"Translated journal saved to {xml_output_path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Failed to save translation data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/phapman/Desktop/tnh-scholar/data_processing/processed_journal_data/phat-giao-viet-nam-1956-01/TEST2_full_cleaned_phat-giao-viet-nam-1956-01.xml')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch sectioning...\n",
      "model: gpt-4o\n",
      "max_tokens: 5000\n",
      "temperature: 0.25\n",
      "response_format: {'type': 'json_object'}\n",
      "JSONL file created at: /Users/phapman/Desktop/tnh-scholar/data_processing/processed_journal_data/phat-giao-viet-nam-1956-01/section_batch.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:33:06,483 - INFO - HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "2024-11-28 19:33:07,627 - INFO - HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Initiated: 11-28-2024 19:33:05 PST | section_batch.jsonl | Batch for sectioning journal: phat-giao-viet-nam-1956-01 | input file: /Users/phapman/Desktop/tnh-scholar/data_processing/processed_journal_data/phat-giao-viet-nam-1956-01/TEST2_full_cleaned_phat-giao-viet-nam-1956-01.xml\n",
      "Batch for sectioning started successfully on attempt 1. ID: batch_674935f36c408190b685bec894577e3c\n",
      "Polling batch status for batch ID batch_674935f36c408190b685bec894577e3c ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:33:17,887 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_674935f36c408190b685bec894577e3c \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Batch ID batch_674935f36c408190b685bec894577e3c failed during processing.. Retrying batch process in 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:33:23,397 - INFO - HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "2024-11-28 19:33:24,014 - INFO - HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Initiated: 11-28-2024 19:33:22 PST | section_batch.jsonl | Batch for sectioning journal: phat-giao-viet-nam-1956-01 | input file: /Users/phapman/Desktop/tnh-scholar/data_processing/processed_journal_data/phat-giao-viet-nam-1956-01/TEST2_full_cleaned_phat-giao-viet-nam-1956-01.xml\n",
      "Batch for sectioning started successfully on attempt 2. ID: batch_67493603c5748190b1b783e83805285a\n",
      "Polling batch status for batch ID batch_67493603c5748190b1b783e83805285a ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:33:34,201 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_67493603c5748190b1b783e83805285a \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 2 failed: Batch ID batch_67493603c5748190b1b783e83805285a failed during processing.. Retrying batch process in 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:33:39,832 - INFO - HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "2024-11-28 19:33:40,406 - INFO - HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Initiated: 11-28-2024 19:33:39 PST | section_batch.jsonl | Batch for sectioning journal: phat-giao-viet-nam-1956-01 | input file: /Users/phapman/Desktop/tnh-scholar/data_processing/processed_journal_data/phat-giao-viet-nam-1956-01/TEST2_full_cleaned_phat-giao-viet-nam-1956-01.xml\n",
      "Batch for sectioning started successfully on attempt 3. ID: batch_6749361435788190a9391a6839d845aa\n",
      "Polling batch status for batch ID batch_6749361435788190a9391a6839d845aa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:33:50,606 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_6749361435788190a9391a6839d845aa \"HTTP/1.1 200 OK\"\n",
      "2024-11-28 19:33:50,748 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_6749361435788190a9391a6839d845aa \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:33:51,252 - INFO - HTTP Request: GET https://api.openai.com/v1/files/file-9F8Ku2oW6NHdWfZp4DbBxH/content \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during validation or writing: 'list' object has no attribute 'get'\n",
      "Batch sectioning completed and metadata saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sectioning\n",
    "print(\"Starting batch sectioning...\")\n",
    "metadata_path = batch_sectioning(input_xml, section_metadata_out, raw_json_metadata_path, journal_name)\n",
    "if metadata_path:\n",
    "    batch_translate\n",
    "\n",
    "# # Step 2: Translation\n",
    "# print(\"Starting batch translation...\")\n",
    "# batch_translation(sectioned_xml, translated_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 21:15:49,819 - INFO - HTTP Request: GET https://api.openai.com/v1/batches?limit=30 \"HTTP/1.1 200 OK\"\n",
      "2024-11-28 21:15:50,287 - INFO - HTTP Request: GET https://api.openai.com/v1/batches?limit=30&after=batch_6748f738f6948190b4b987ac65cf2a1d \"HTTP/1.1 200 OK\"\n",
      "2024-11-28 21:15:50,778 - INFO - HTTP Request: GET https://api.openai.com/v1/batches?limit=30&after=batch_6736d4d981f48190beda7f734d6a75d9 \"HTTP/1.1 200 OK\"\n",
      "2024-11-28 21:15:50,976 - INFO - HTTP Request: GET https://api.openai.com/v1/batches?limit=30&after=batch_672b85772df8819085957fa62c4e3020 \"HTTP/1.1 200 OK\"\n",
      "2024-11-28 21:15:51,265 - INFO - HTTP Request: GET https://api.openai.com/v1/batches?limit=30&after=batch_672b091c02088190b0d4e5d9275ec258 \"HTTP/1.1 200 OK\"\n",
      "2024-11-28 21:15:51,588 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_6749361435788190a9391a6839d845aa \"HTTP/1.1 200 OK\"\n",
      "2024-11-28 21:15:52,007 - INFO - HTTP Request: GET https://api.openai.com/v1/files/file-9F8Ku2oW6NHdWfZp4DbBxH/content \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result = get_last_batch_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"journal_summary\": \"This Vietnamese journal explores the deep-rooted connection between Buddhism and Vietnamese culture, tracing its historical influence and emphasizing the importance of maintaining Buddhist values in modern society. It also discusses the proper path for Vietnamese Buddhists, emphasizing the need for clear understanding and awareness in navigating life's challenges.\",\n",
      "    \"sections\": [\n",
      "        {\n",
      "            \"section_title_vi\": \"PHẬT GIÁO VIỆT NAM NGUYỆT SAN SỐ 1\",\n",
      "            \"section_title_en\": \"Vietnamese Buddhism Monthly Issue 1\",\n",
      "            \"section_author\": null,\n",
      "            \"section_summary\": \"This is the title page of the journal, indicating the publication as the first issue of the Vietnamese Buddhism Monthly, released on the 15th of August in the year of the Monkey.\",\n",
      "            \"section_keywords\": [\"Vietnamese Buddhism\", \"Monthly Issue\", \"Publication Date\"],\n",
      "            \"start_page\": 1,\n",
      "            \"end_page\": 1\n",
      "        },\n",
      "        {\n",
      "            \"section_title_vi\": \"PHẬT-GIÁO VIỆT - NAM\",\n",
      "            \"section_title_en\": \"Vietnamese Buddhism\",\n",
      "            \"section_author\": null,\n",
      "            \"section_summary\": \"This section discusses the historical and cultural integration of Buddhism into Vietnamese society over fifteen centuries. It highlights Buddhism's role in shaping Vietnamese national identity and culture, emphasizing its adaptability and deep connection with Vietnamese values and traditions.\",\n",
      "            \"section_keywords\": [\"Buddhism\", \"Vietnamese Culture\", \"History\", \"National Identity\"],\n",
      "            \"start_page\": 2,\n",
      "            \"end_page\": 4\n",
      "        },\n",
      "        {\n",
      "            \"section_title_vi\": \"HƯỚNG ĐI CỦA NGƯỜI PHẬT TỬ VIỆT NAM\",\n",
      "            \"section_title_en\": \"The Path for Vietnamese Buddhists\",\n",
      "            \"section_author\": null,\n",
      "            \"section_summary\": \"This section outlines the proper path for Vietnamese Buddhists, stressing the importance of having a clear understanding and awareness in life. It warns against blind faith and emphasizes the need for correct perception and wisdom to avoid life's pitfalls and achieve true understanding.\",\n",
      "            \"section_keywords\": [\"Buddhist Path\", \"Awareness\", \"Correct Perception\", \"Wisdom\"],\n",
      "            \"start_page\": 5,\n",
      "            \"end_page\": 5\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data successfully written to /Users/phapman/Desktop/tnh-scholar/data_processing/processed_journal_data/phat-giao-viet-nam-1956-01/section_metadata.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_and_save_metadata(section_metadata_out, result[0], journal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Sectioning\n",
    "# def batch_sectioning(input_xml_path, output_xml_path):\n",
    "#     \"\"\"\n",
    "#     Splits the journal content into sections using the GPT model.\n",
    "#     Saves the sectioned content back to XML.\n",
    "#     \"\"\"\n",
    "#     # Load the input XML\n",
    "#     journal_pages = load_xml(input_xml_path)\n",
    "#     pages_content = [page.text for page in journal_pages]\n",
    "\n",
    "#     # Create GPT messages for sectioning\n",
    "#     user_message_wrapper = lambda text: f\"Divide this content into sections:\\n{text}\"\n",
    "#     messages = generate_messages(system_message_section, user_message_wrapper, pages_content)\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, section_batch_jsonl)\n",
    "\n",
    "#     # Start the batch\n",
    "#     batch = start_batch(jsonl_file, description=\"Batch for sectioning journal\")\n",
    "#     batch_id = batch.get(\"id\")\n",
    "#     if not batch_id:\n",
    "#         print(\"Error: Failed to start batch for sectioning.\")\n",
    "#         return None\n",
    "\n",
    "#     print(f\"Batch for sectioning started with ID: {batch_id}\")\n",
    "\n",
    "#     # Poll for batch completion\n",
    "#     results = poll_batch_status(batch_id)\n",
    "#     if not results:\n",
    "#         print(\"Error: Failed to retrieve sectioning batch results.\")\n",
    "#         return None\n",
    "\n",
    "#     # Save sectioned content back to XML\n",
    "#     for i, section_content in enumerate(results):\n",
    "#         journal_pages[i].text = section_content  # Replace original content with sectioned content\n",
    "\n",
    "#     save_xml(journal_pages, output_xml_path)\n",
    "#     print(f\"Sectioned journal saved to {output_xml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from gpt_processing.gpt_interface import (\n",
    "#     set_api_client, \n",
    "#     generate_messages, \n",
    "#     create_jsonl_file_for_batch, \n",
    "#     start_batch, \n",
    "#     get_batch_response\n",
    "# )\n",
    "# from data_processing.xml_processing import (\n",
    "#     load_xml, \n",
    "#     save_xml, \n",
    "#     extract_sections_from_xml\n",
    "# )\n",
    "\n",
    "# # Initialize OpenAI client\n",
    "# set_api_client()\n",
    "\n",
    "# # File paths\n",
    "# INPUT_XML = \"input_journal.xml\"\n",
    "# SECTIONED_XML = \"sectioned_journal.xml\"\n",
    "# TRANSLATED_XML = \"translated_journal.xml\"\n",
    "# BATCH_SECTION_JSONL = \"section_batch.jsonl\"\n",
    "# BATCH_TRANSLATE_JSONL = \"translate_batch.jsonl\"\n",
    "\n",
    "# # System messages\n",
    "# SYSTEM_MESSAGE_SECTION = \"\"\"\n",
    "# You are a helpful assistant. Divide the text into meaningful sections and add XML tags:\n",
    "# <section> for major sections, <subsection> for subsections, <title> for titles, and <p> for paragraphs.\n",
    "# \"\"\"\n",
    "# SYSTEM_MESSAGE_TRANSLATE = \"\"\"\n",
    "# You are Thich Nhat Hanh translating from Vietnamese to English. Provide meaningful translations with appropriate XML tags:\n",
    "# <section>, <subsection>, <title>, <p>.\n",
    "# \"\"\"\n",
    "\n",
    "# # Step 1: Sectioning\n",
    "# def batch_sectioning(input_xml, output_xml):\n",
    "#     # Load the input XML and extract pages or chunks\n",
    "#     journal_pages = load_xml(input_xml)\n",
    "#     pages_content = [page.text for page in journal_pages]  # Assuming .text contains the text of each page\n",
    "\n",
    "#     # Create GPT messages for sectioning\n",
    "#     user_message_wrapper = lambda text: f\"Divide this content into sections:\\n{text}\"\n",
    "#     messages = generate_messages(SYSTEM_MESSAGE_SECTION, user_message_wrapper, pages_content)\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, BATCH_SECTION_JSONL)\n",
    "\n",
    "#     # Start batch\n",
    "#     batch = start_batch(jsonl_file, description=\"Batch for sectioning journal\")\n",
    "#     batch_id = batch.get(\"id\")\n",
    "#     if not batch_id:\n",
    "#         print(\"Error: Failed to start batch for sectioning.\")\n",
    "#         return None\n",
    "\n",
    "#     print(f\"Batch for sectioning started with ID: {batch_id}\")\n",
    "\n",
    "#     # Poll for batch completion and retrieve results\n",
    "#     results = get_batch_response(batch_id)\n",
    "#     if not results:\n",
    "#         print(\"Error: Failed to retrieve sectioning batch results.\")\n",
    "#         return None\n",
    "\n",
    "#     # Save the sectioned content back to XML\n",
    "#     for i, section_content in enumerate(results):\n",
    "#         journal_pages[i].text = section_content  # Replace original content with sectioned content\n",
    "\n",
    "#     save_xml(journal_pages, output_xml)\n",
    "#     print(f\"Sectioned journal saved to {output_xml}\")\n",
    "\n",
    "# # Step 2: Translation\n",
    "# def batch_translation(input_xml, output_xml):\n",
    "#     # Load the sectioned XML and extract sections or chunks for translation\n",
    "#     sections = extract_sections_from_xml(input_xml)\n",
    "\n",
    "#     # Create GPT messages for translation\n",
    "#     user_message_wrapper = lambda section: f\"Translate this section:\\n{section}\"\n",
    "#     messages = generate_messages(SYSTEM_MESSAGE_TRANSLATE, user_message_wrapper, sections)\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, BATCH_TRANSLATE_JSONL)\n",
    "\n",
    "#     # Start batch\n",
    "#     batch = start_batch(jsonl_file, description=\"Batch for translating journal\")\n",
    "#     batch_id = batch.get(\"id\")\n",
    "#     if not batch_id:\n",
    "#         print(\"Error: Failed to start batch for translation.\")\n",
    "#         return None\n",
    "\n",
    "#     print(f\"Batch for translation started with ID: {batch_id}\")\n",
    "\n",
    "#     # Poll for batch completion and retrieve results\n",
    "#     results = get_batch_response(batch_id)\n",
    "#     if not results:\n",
    "#         print(\"Error: Failed to retrieve translation batch results.\")\n",
    "#         return None\n",
    "\n",
    "#     # Save the translated content back to XML\n",
    "#     for i, translated_content in enumerate(results):\n",
    "#         sections[i].text = translated_content  # Replace original content with translated content\n",
    "\n",
    "#     save_xml(sections, output_xml)\n",
    "#     print(f\"Translated journal saved to {output_xml}\")\n",
    "\n",
    "# # Main process\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Step 1: Sectioning\n",
    "#     print(\"Starting batch sectioning...\")\n",
    "#     batch_sectioning(INPUT_XML, SECTIONED_XML)\n",
    "\n",
    "#     # Step 2: Translation\n",
    "#     print(\"Starting batch translation...\")\n",
    "#     batch_translation(SECTIONED_XML, TRANSLATED_XML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function schema for function calling\n",
    "# function_schemas = [\n",
    "#     {\n",
    "#         \"name\": \"save_processed_metadata\",\n",
    "#         \"description\": \"Save metadata for a processed vietnamese journal, including sections and summaries, that will later be translated\",\n",
    "#         \"parameters\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"properties\": {\n",
    "#                 \"journal_summary\": {\"type\": \"string\", \"description\": \"A one-page summary of the journal in English.\"},\n",
    "#                 \"sections\": {\n",
    "#                     \"type\": \"array\",\n",
    "#                     \"items\": {\n",
    "#                         \"type\": \"object\",\n",
    "#                         \"properties\": {\n",
    "#                             \"section_title_vi\": {\"type\": \"string\", \"description\": \"The original title of the section in Vietnamese.\"},\n",
    "#                             \"section_title_en\": {\"type\": \"string\", \"description\": \"The translated title of the section in English.\"},\n",
    "#                             \"section_summary\": {\"type\": \"string\", \"description\": \"A one paragraph summary of the section in English.\"},\n",
    "#                             \"page_range\": {\n",
    "#                                 \"type\": \"array\",\n",
    "#                                 \"items\": {\"type\": \"integer\"},\n",
    "#                                 \"minItems\": 2,\n",
    "#                                 \"maxItems\": 2,\n",
    "#                                 \"description\": \"The start and end page numbers of the section.\"\n",
    "#                             }\n",
    "#                         },\n",
    "#                         \"required\": [\"section_title_en\", \"section_title_vi\", \"section_summary\", \"page_range\"]\n",
    "#                     }\n",
    "#                 }\n",
    "#             },\n",
    "#             \"required\": [\"journal_summary\", \"sections\"]\n",
    "#         }\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Translation\n",
    "# def batch_translate(input_xml_path, metadata_path):\n",
    "#     \"\"\"\n",
    "#     Translates the journal sections using the GPT model.\n",
    "#     Saves the translated content back to XML.\n",
    "#     \"\"\"\n",
    "#     # Load the sectioned XML\n",
    "#     section_metadata = #load json data from metadata_path and deserialize\n",
    "\n",
    "#     # use the function split_xml_to_pages to get sections for translation:\n",
    "#     sections = split_xml_pages(...)\n",
    "\n",
    "#     # Create GPT messages for translation\n",
    "#     user_message_wrapper = lambda section: f\"Translate this section:\\n{section}\"\n",
    "#     messages = generate_messages(system_message_translate, user_message_wrapper, sections)\n",
    "\n",
    "#     # convert the blocks below to a series of nested try blocks with multiple attempts as in batch_section():\n",
    "#     # add appropriate logging to match batch_section():\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, translate_batch_jsonl)\n",
    "\n",
    "#     # Start the batch\n",
    "#     batch = start_batch(jsonl_file, description=\"Batch for translating journal\")\n",
    "#     batch_id = batch.get(\"id\")\n",
    "#     if not batch_id:\n",
    "#         print(\"Error: Failed to start batch for translation.\")\n",
    "#         return None\n",
    "\n",
    "#     print(f\"Batch for translation started with ID: {batch_id}\")\n",
    "\n",
    "#     # Poll for batch completion\n",
    "#     results = poll_batch_for_response(batch_id)\n",
    "#     if not results:\n",
    "#         print(\"Error: Failed to retrieve translation batch results.\")\n",
    "#         return None\n",
    "\n",
    "#     # Save translated content back to XML\n",
    "#     translated_sections = []\n",
    "#     for i, translated_content in enumerate(results):\n",
    "#         translated_sections.append(translated_content)  # Replace original content with translated content\n",
    "\n",
    "#     save_pages_to_xml(translated_sections, translated_xml)\n",
    "#     print(f\"Translated journal saved to {translated_xml}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # old system message\n",
    "\n",
    "# system_message_section = \"\"\"\n",
    "# You are a highly skilled assistant processing a Vietnamese journal scanned from OCR. \n",
    "# You will be determining the journal sections by page number. You will also generate summaries for the full text and each section. \n",
    "# You will return this metadata in JSON format.\n",
    "\n",
    "# Instructions:\n",
    "# 1. Analyze the text and divide it into sections based on logical breaks, such as headings, topic changes, or clear shifts in content.\n",
    "# 2. Ensure every page is part of  a section, even if that section is titled \"blank page\" or \"title page,\" for example.\n",
    "# 3. For each section, provide:\n",
    "#    - The original title in Vietnamese (`section_title_vi`).\n",
    "#    - The translated title in English (`section_title_en`).\n",
    "#    - The author's name if it is available (`section_author`). \n",
    "#    - A one-paragraph summary of the section in English (`section_summary`).\n",
    "#    - A list of keywords for the section that are related to its content, these can be proper names, specific concepts, or contextual information.\n",
    "#    - The section's start and end page numbers (`start_page` and `end_page`).\n",
    "#    - Use \"null\" for any data that is not available (such as author name) for the section.\n",
    "\n",
    "# 4. Return the output as a JSON object with the following schema:\n",
    "# {\n",
    "#     \"journal_summary\": \"A one-page summary of the whole journal in English.\",\n",
    "#     \"sections\": [\n",
    "#         {\n",
    "#             \"section_title_vi\": \"Original title in Vietnamese\",\n",
    "#             \"section_title_en\": \"Translated title in English\",\n",
    "#             \"section_author\": \"Name of the author of the section\",\n",
    "#             \"section_summary\": \"One-paragraph summary of the section in English\",\n",
    "#             \"section_keywords\": \"A list of keywords for the section\",\n",
    "#             \"start_page\":  X,\n",
    "#             \"end_page\":  Y\n",
    "#         },\n",
    "#         ...\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# 5.  Ensure the JSON is well-formed and adheres strictly to the provided schema.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Translation\n",
    "# def batch_translate(input_xml_path, output_xml_path):\n",
    "#     \"\"\"\n",
    "#     Translates the journal sections using the GPT model.\n",
    "#     Saves the translated content back to XML.\n",
    "#     \"\"\"\n",
    "#     # Load the sectioned XML\n",
    "#     section_metadata = \n",
    "\n",
    "#     # Create GPT messages for translation\n",
    "#     user_message_wrapper = lambda section: f\"Translate this section:\\n{section}\"\n",
    "#     messages = generate_messages(system_message_translate, user_message_wrapper, sections)\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, translate_batch_jsonl)\n",
    "\n",
    "#     # Start the batch\n",
    "#     batch = start_batch(jsonl_file, description=\"Batch for translating journal\")\n",
    "#     batch_id = batch.get(\"id\")\n",
    "#     if not batch_id:\n",
    "#         print(\"Error: Failed to start batch for translation.\")\n",
    "#         return None\n",
    "\n",
    "#     print(f\"Batch for translation started with ID: {batch_id}\")\n",
    "\n",
    "#     # Poll for batch completion\n",
    "#     results = poll_batch_status(batch_id)\n",
    "#     if not results:\n",
    "#         print(\"Error: Failed to retrieve translation batch results.\")\n",
    "#         return None\n",
    "\n",
    "#     # Save translated content back to XML\n",
    "#     for i, translated_content in enumerate(results):\n",
    "#         sections[i].text = translated_content  # Replace original content with translated content\n",
    "\n",
    "#     save_pages_to_xml(sections, output_xml_path)\n",
    "#     print(f\"Translated journal saved to {output_xml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 17:17:12,289 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AYjv0xkI0XcjvFgsIoqkv6onw62F9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The immediate future of AI is poised to be characterized by several key trends and developments:\\n\\n1. **Continued Advancements in Machine Learning**: Expect ongoing improvements in algorithms, leading to more efficient, robust, and interpretable AI systems. Techniques like transfer learning, reinforcement learning, and unsupervised learning will gain prominence, making AI systems more adaptable and capable of complex tasks with less data.\\n\\n2. **Explainability and Transparency**: With increased AI integration in critical decision-making processes, there will be heightened demand for models that provide clear explanations of their outputs. This aligns with growing regulatory pressures and the need for trustworthiness in AI systems.\\n\\n3. **Ethical AI and Governance**: The focus will intensify on creating frameworks for ethical AI, addressing issues like bias, privacy, and accountability. Companies and governments will work towards establishing clearer policies and standards for AI deployment.\\n\\n4. **Edge AI**: The shift toward edge computing involves deploying AI technologies directly on devices (\"on the edge\") instead of relying solely on centralized cloud systems. This approach reduces latency, improves privacy, and can be more cost-effective, driving broader IoT and real-time analytics applications.\\n\\n5. **Democratization of AI**: Tools and platforms that make AI more accessible to non-experts are advancing, lowering the barriers to entry in AI development and implementation. This democratization will enable more businesses to leverage AI, fueling innovation across industries.\\n\\n6. **Specialized AI Applications**: AI will continue to make inroads into specialized applications such as healthcare (driving diagnostics and personalized medicine), finance (automating trading and fraud detection), and autonomous systems (from vehicles to drones).\\n\\n7. **Hybrid AI Models**: Combining various AI approaches, such as integrating symbolic reasoning with neural networks, will gain traction. These hybrid models aim to leverage the strengths of different methodologies, paving the way for more versatile and powerful AI systems.\\n\\n8. **Collaboration between AI and Human Experts**: Rather than replacing human roles, AI systems will increasingly be designed to augment human capabilities, assisting experts in making more informed decisions, and streamlining workflows.\\n\\n9. **Sustainability**: There will be a stronger emphasis on reducing the environmental impact of AI, particularly in relation to energy-intensive training processes. Research into more sustainable AI practices will expand, focusing on efficiency and resource conservation.\\n\\n10. **Increased Focus on Continuous Learning**: AI systems capable of continuous learning will be developed, enabling systems to adapt and evolve as new data and scenarios arise, rather than relying solely on traditional retraining cycles.\\n\\nAs these trends evolve, the field of AI will continue to redefine itself, opening up new possibilities and challenges. Building systems that are not only powerful but also ethical and sustainable will be crucial in shaping the trajectory of AI\\'s immediate future.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732843026, model='gpt-4o-2024-08-06', object='chat.completion', service_tier=None, system_fingerprint='fp_7f6be3efb0', usage=CompletionUsage(completion_tokens=563, prompt_tokens=40, total_tokens=603, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "set_api_client()\n",
    "msgs = generate_messages(\"you are assisting a software engineering/researcher looking to develop new AI platforms and processes.\", lambda x: x, [\"why is AI suddenly successful?\", \"What is the (immediate) future of AI?\"])\n",
    "run_immediate_chat_process(msgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "model_settings = {\n",
    "    \"gpt-4o\": {\n",
    "        \"max_tokens\": 3000,\n",
    "        \"context_limit\": 20000,  # Total context limit for the model\n",
    "        \"temperature\": 1.3\n",
    "    }}\n",
    "\n",
    "set_model_settings(model_settings)\n",
    "batch_id = run_single_oa_batch([\"what is the square root of 2?\", \"why is the sky blue?\"], \"you are are explaining complex ideas to a 9 year old child.\")\n",
    "\n",
    "poll_batch_for_response(batch_id, 10)\n",
    "\n",
    "msgs = generate_messages(\"you are assisting a software engineering/researcher looking to develop new AI platforms and processes.\", lambda x: x, [\"why is AI suddenly successful?\", \"What is the (immediate) future of AI?\"])\n",
    "run_immediate_chat_process(msgs[1])\n",
    "\n",
    "get_last_batch_response()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
