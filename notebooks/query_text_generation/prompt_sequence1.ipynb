{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport openai, pandas, time, os, re, math, ast, tiktoken\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.text_processing import get_text_from_file, set_working_directory, get_working_directory\n",
    "from data_processing.text_processing import normalize_quotes\n",
    "\n",
    "set_working_directory(\"../../books/private_books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_QUERY_RATIO = 0.007\n",
    "MEDIUM_QUERY_RATIO = 0.007\n",
    "SHORT_QUERY_RATIO = 0.007\n",
    "MAX_QUERY_COUNT = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "def token_count(text):\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instructions = \"\"\"You are a thorough, insightful, and consistent assistant generating (query, text) pairs to train a BERT-based search model on Thich Nhat Hanh's works. \n",
    "Generate queries for a range of audiences, from beginners in the Plum Village tradition to advanced monastics. \n",
    "\n",
    "Use metadata, such as titles, quotes, or gathas, as cues to identify central themes or key concepts in the text. \n",
    "Queries should capture essential topics, themes, or questions (without too much semantic overlap), including both broad scope questions, and questions about specific insights or details. \n",
    "\n",
    "For extended queries, explore deeper, complex, or philosophical connections to Buddhist Teachings and Thich Nhat Hanh's life and teachings. \n",
    "Novel or unexpected queries can also be considered.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old text: modified with GPT help Nov 5, 2024\n",
    "You are a thorough, insightful, and consistent assistant generating (query, text) pairs for a project aimed at training a BERT-based search model on finding relevant passages in the works of Thich Nhat Hanh. \n",
    "Think of queries from a wide range of people: those new to and curious about the Plum Village tradition up to experienced monastics researching Thay's life, teachings, or deep Buddhist principles.\n",
    "The generated queries should capture the key concepts and themes from the text (without too much semantic overlap), some broad context questions generally relevant to Plum Village (not directly from the text), and also some detailed, or complex information specific to the text. \n",
    "Metadata, such as tagged titles, quotes, etc. can be used to guide generation. Queries specific to Named Entities in the text may also be relevant.\n",
    "For the longer queries, focus on more complex or philosophical aspects of the text or connections to broader Buddhist teachings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New system message, generated with chat gpt help on Nov 5, 2024\n",
    "\n",
    "You are a thorough, insightful assistant generating (query, text) pairs to train a BERT-based search model on Thich Nhat Hanh's works. \n",
    "Generate queries for a range of audiences, from beginners in the Plum Village tradition to advanced monastics. \n",
    "\n",
    "Use metadata, such as titles, quotes, or gathas, as cues to identify central themes or key concepts in the text. Queries should capture essential topics, themes, or questions (without too much semantic overlap), including both broad context questions and detailed insights. \n",
    "\n",
    "For extended queries, explore deeper, complex, or philosophical connections to Thich Nhat Hanhâ€™s life and teachings, as well as broader Buddhist principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count(system_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 4 queries (1-3 words), 3 queries (4-6 words), and 3 full-sentence questions based on this text:\n",
    "\n",
    "<section level=\"3\" type=\"exercise\">\n",
    "<title>Exercise 7| Parts of the Body</title>\n",
    "[more text here]\n",
    "\n",
    "Only return the list of pairs in this format: [(query, text), (query, text), ...]. Avoid Python syntax, backticks, or any code elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input_wrapper = \"\"\"Generate {short_query_count} queries (1-3 words), {medium_query_count} queries (4-6 words), and {long_query_count} full-sentence questions based on this text:\n",
    "---\n",
    "\n",
    "{text_segment}\n",
    "\n",
    "---\n",
    "The output will be imported into python using ast. Do not include any Python code syntax, variables, triple backticks or other code elements. \n",
    "Only return the list content, with each pair in the form (query, text) as a tuple.\n",
    "For example: [(\"query\", \"text\"), (\"query\", \"text\"), ...]. Where \"query\" is your specific query, and \"text\" is the unique matching phrase or sentence from the text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"\"\"            <section level=\"3\" type=\"exercise\">\n",
    "                <title>Exercise 7| Parts of the Body</title>\n",
    "                <sutra-quote>Further, the practitioner meditates on his very own body from the soles\n",
    "                    of the\n",
    "                    feet upwards and then from the hair on top of the head downwards, a body\n",
    "                    contained inside the skin and full of all the impurities which belong to the\n",
    "                    body: 'Here is the hair of the head, the hairs on the body, the nails, teeth,\n",
    "                    skin, flesh, sinews, bones, bone marrow, kidneys, heart, liver, diaphragm,\n",
    "                    spleen, lungs, intestines, bowels, excrement, bile, phlegm, pus, blood, sweat,\n",
    "                    fat, tears, grease, saliva, mucus, synovic fluid, urine.'</sutra-quote>\n",
    "                <p>This exercise brings us into even deeper contact with our body. Here we observe\n",
    "                    the body in all its parts, from the hair on the head to the skin on the soles of\n",
    "                    the feet. In the process of our observation, we scan all the parts of the body,\n",
    "                    including the brain, heart, lungs, gall bladder, spleen, blood, urine, and so\n",
    "                    forth. The Buddha gives us the example of a farmer pouring the contents of a\n",
    "                    sack filled with a variety of seeds onto the floor and then observing and\n",
    "                    identifying each kind of seed: \"This is rice, these are beans, these are sesame\n",
    "                    seeds.\"</p>\n",
    "                <p>We use our conscious breathing in order to observe mindfully all the parts of the\n",
    "                    body. For example: \"Breathing in, I am aware of the hair on my head. Breathing\n",
    "                    out, I know that this is the hair on my head.\" Breathing consciously helps us\n",
    "                    dwell in mindfulness more easily and sustain the work of observing each part of\n",
    "                    the body. In addition to the conscious breathing, we can use the method of\n",
    "                    silently calling each part of the body by name to enable these parts to become\n",
    "                    increasingly clear in the light of mindfulness.</p>\n",
    "                <p>Why do we need to observe in mindfulness the different parts of the body? First\n",
    "                    of all, it is to be in contact with the body. We often have the impression that\n",
    "                    we're already totally in touch with our body, but often we're wrong. Between us\n",
    "                    and our body there can be a large separation, and our body remains a stranger to\n",
    "                    us. Sometimes we hate our body. There are even people who see their body as a\n",
    "                    prison and a place of punishment. To come back to our body is to become familiar\n",
    "                    with it and to establish harmony with it. We know that if our body isn't happy,\n",
    "                    we're not happy, and so we want our body to be calm and peaceful. To do so, we\n",
    "                    come back to our body and make peace with it.</p>\n",
    "                <p>We can try touching the different parts of our body to make their acquaintance.\n",
    "                    We should touch each part in an affectionate and caring way. For several\n",
    "                    decades, our eyes, feet, and heart have done their work devotedly and faithfully\n",
    "                    with us and for us, but we never really give them much attention or express our\n",
    "                    gratitude to them. It's necessary to establish a close relationship with our\n",
    "                    body.</p>\n",
    "                <p>The second reason for mindfully observing the different parts of the body is that\n",
    "                    each part can be the door to liberation and awakening. At first we'll only\n",
    "                    recognize the presence of the part of the body being observed, but later we'll\n",
    "                    come to see its true nature. Every hair on our head and every cell in our body\n",
    "                    contains the entire universe. Observing the interdependent nature of a single\n",
    "                    hair can help us to see into the nature of the universe.</p>\n",
    "                <p>The exercise of observing every part of the body begins with the hair on the head\n",
    "                    and goes down to the skin on the soles of the feet. Sometimes we observe just\n",
    "                    one part of the body deeply, such as our eyes, heart, or toe. In the process of\n",
    "                    observation from the head to the feet, some observations may spring up in our\n",
    "                    mind. For example, as we pass our heart, we may think, \"My friend John has a\n",
    "                    heart condition. I must visit him soon to see if he's all right.\" We can note\n",
    "                    these observations and then continue with the work of observing the remaining\n",
    "                    parts of the body. Later we can return to those observations.</p>\n",
    "            </section>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 / 977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = normalize_quotes(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"Simple word counter based on whitespace.\"\"\"\n",
    "    return len(re.findall(r'\\w+', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_query_counts(tokens):\n",
    "    lq, mq, sq = (ceil(LONG_QUERY_RATIO * tokens), ceil(MEDIUM_QUERY_RATIO * tokens), ceil(SHORT_QUERY_RATIO * tokens))\n",
    "    lq, mq, sq = min(MAX_QUERY_COUNT, lq), min(MAX_QUERY_COUNT, mq), min(MAX_QUERY_COUNT, sq)\n",
    "    return lq, mq, sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_query_counts(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_messages(text_segment):\n",
    "\n",
    "    tokens = token_count(text_segment)\n",
    "\n",
    "    long_count, medium_count, short_count = calc_query_counts(tokens)\n",
    "    \n",
    "    messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_instructions\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_input_wrapper.format(text_segment=text_segment,\n",
    "                                                         long_query_count=long_count,\n",
    "                                                         medium_query_count=medium_count,\n",
    "                                                         short_query_count=short_count \n",
    "                                                        )\n",
    "                }\n",
    "            ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(text_segment):\n",
    "\n",
    "    messages = generate_messages(text_segment)\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"gpt-4o\",\n",
    "        )\n",
    "        \n",
    "        return chat_completion\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_content(completion):\n",
    "   return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = generate_messages(test_input)\n",
    "print(messages[0]['content'])\n",
    "print(messages[1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = generate_queries(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_completion_content(completion)\n",
    "print(output)\n",
    "#queries_list = ast.literal_eval(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_xml_str = get_text_from_file(\"TH_working4.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "def process_sections(section):\n",
    "    \"\"\"\n",
    "    Collects section data with paragraph and word counts.\n",
    "    \"\"\"\n",
    "    section_data = []\n",
    "    \n",
    "    # Extract section attributes\n",
    "    level = section.get(\"level\")\n",
    "    section_type = section.get(\"type\", \"\")\n",
    "    title = section.findtext(\"title\", default=\"\")\n",
    "\n",
    "    # Count paragraphs and words within this section\n",
    "    paragraphs = section.findall(\"p\")\n",
    "    paragraph_count = len(paragraphs)\n",
    "    word_count = sum(count_words(p.text) for p in paragraphs if p.text)\n",
    "\n",
    "    # Append section data\n",
    "    section_data.append({\n",
    "        \"level\": level,\n",
    "        \"type\": section_type,\n",
    "        \"title\": title,\n",
    "        \"paragraph_count\": paragraph_count,\n",
    "        \"word_count\": word_count\n",
    "    })\n",
    "    \n",
    "    # Process nested sections recursively\n",
    "    for sub_section in section.findall(\"section\"):\n",
    "        section_data.extend(process_sections(sub_section))\n",
    "    \n",
    "    return section_data\n",
    "\n",
    "# Parse XML and iterate over top-level sections\n",
    "root = etree.fromstring(book_xml_str)\n",
    "\n",
    "# Collect all section data\n",
    "all_section_data = []\n",
    "for section in root.findall(\".//section\"):\n",
    "    all_section_data.extend(process_sections(section))\n",
    "\n",
    "# Example output for debugging\n",
    "for section_info in all_section_data:\n",
    "    print(section_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import os\n",
    "\n",
    "\n",
    "def generate_chunks(xml_filename, ignore_list=None):\n",
    "    \"\"\"\n",
    "    Generates fine-grained and broad-scope chunks from XML data, applying filters.\n",
    "\n",
    "    Fine-grained chunks are each level 2 section.\n",
    "    Broad-scope chunks are entire level 1 sections with nested level 2 sections.\n",
    "    Sections with types in the ignore list or with zero paragraphs and no subsections are excluded.\n",
    "\n",
    "    Args:\n",
    "    - xml_filename: Name of the XML file within the working directory.\n",
    "    - ignore_list: List of keywords to ignore based on 'type' (case insensitive).\n",
    "\n",
    "    Returns:\n",
    "    - fine_grained_chunks: List of strings, each containing a level 2 section as XML.\n",
    "    - broad_scope_chunks: List of strings, each containing a level 1 section as XML with all nested content.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the full path to the XML file\n",
    "    wdir = get_working_directory()\n",
    "\n",
    "    if wdir:\n",
    "        xml_file = os.path.join(wdir, xml_filename)\n",
    "    else:\n",
    "        xml_file = xml_filename\n",
    "    \n",
    "    # Parse XML and prepare containers for chunks\n",
    "    tree = etree.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    ignore_list = ignore_list or []\n",
    "    fine_grained_chunks = []\n",
    "    broad_scope_chunks = []\n",
    "\n",
    "    # Helper function to check if a section should be ignored\n",
    "    def should_ignore(section):\n",
    "        section_type = section.get(\"type\", \"\").lower()\n",
    "        paragraph_count = len(section.findall(\"p\"))\n",
    "        has_subsections = len(section.findall(\"section\")) > 0\n",
    "        \n",
    "        # Ignore if type matches any keyword in ignore list or if it has zero paragraphs and no subsections\n",
    "        return (\n",
    "            any(keyword.lower() in section_type for keyword in ignore_list) or\n",
    "            (paragraph_count == 0 and not has_subsections)\n",
    "        )\n",
    "\n",
    "    # Collect level 2 sections as fine-grained chunks\n",
    "    for level_2_section in root.findall(\".//section[@level='2']\"):\n",
    "        if not should_ignore(level_2_section):\n",
    "            fine_grained_chunks.append(etree.tostring(level_2_section, encoding='unicode'))\n",
    "    \n",
    "    # Collect entire level 1 sections as broad-scope chunks\n",
    "    for level_1_section in root.findall(\".//section[@level='1']\"):\n",
    "        if not should_ignore(level_1_section):\n",
    "            broad_scope_chunks.append(etree.tostring(level_1_section, encoding='unicode'))\n",
    "    \n",
    "    return fine_grained_chunks, broad_scope_chunks\n",
    "\n",
    "# Example usage\n",
    "xml_filename = \"TH_working4.xml\"\n",
    "ignore_keywords = [\"bibliographic-data\"]\n",
    "\n",
    "fine_grained_chunks, broad_scope_chunks = generate_chunks(xml_filename, ignore_list=ignore_keywords)\n",
    "\n",
    "# Print samples for inspection\n",
    "print(\"Fine-Grained Chunks Sample:\", fine_grained_chunks[:3])\n",
    "print(\"Broad-Scope Chunks Sample:\", broad_scope_chunks[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fine_grained_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(broad_scope_chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgc = [count_words(chunk) for chunk in fine_grained_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_tokens = [token_count(chunk) for chunk in fine_grained_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_grained_chunks[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc = [count_words(chunk) for chunk in broad_scope_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_tokens = [token_count(chunk) for chunk in broad_scope_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_scope_chunks[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_totals = [calc_query_counts(tc) for tc in fg_tokens]\n",
    "fg_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([x+y+z for (x,y,z) in fg_totals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_totals = [calc_query_counts(tc) for tc in bs_tokens]\n",
    "bc_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([x+y+z for (x,y,z) in bc_totals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "150*3 * 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8 / 718, 10 / 718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages_from_chunks(chunk_list):\n",
    "    messages = []\n",
    "    for chunk in chunk_list:\n",
    "        messages.append(generate_messages(chunk))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = build_messages_from_chunks(broad_scope_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[1][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def create_jsonl_file_for_batch(messages, output_file_path=\"batch_requests.jsonl\"):\n",
    "    \"\"\"\n",
    "    Creates a JSONL file for batch processing, with each request using the same system message and different user messages.\n",
    "\n",
    "    Args:\n",
    "        messages: to be sent for completion\n",
    "        output_file_path (str): The path where the .jsonl file will be saved.\n",
    "    \n",
    "    Returns:\n",
    "        str: The path to the generated .jsonl file.\n",
    "    \"\"\"\n",
    "    requests = []\n",
    "    for i, message in enumerate(messages):\n",
    "        request_obj = {\n",
    "            \"custom_id\": f\"request-{i+1}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"messages\": message,\n",
    "                \"max_tokens\": 3000\n",
    "            },\n",
    "        }\n",
    "        requests.append(request_obj)\n",
    "\n",
    "    # Write requests to JSONL file\n",
    "    with open(output_file_path, \"w\") as f:\n",
    "        for request in requests:\n",
    "            json.dump(request, f)\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate fine grained batch request file:\n",
    "chunks_to_build = fine_grained_chunks\n",
    "messages_fg = build_messages_from_chunks(chunks_to_build)\n",
    "create_jsonl_file_for_batch(messages_fg, \"batch_requests_fine_grained.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_to_test = broad_scope_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token_count(chunk) for chunk in chunks_to_test]\n",
    "sum(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = build_messages_from_chunks(chunks_to_test)\n",
    "create_jsonl_file_for_batch(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file = client.files.create(\n",
    "  file=open(\"batch_requests.jsonl\", \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"first (query,text) generation task run.\"\n",
    "    }\n",
    ")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.retrieve(batch.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def retrieve_batch_results(batch):\n",
    "    \"\"\"\n",
    "    Retrieves the status of a batch job and returns the result if completed.\n",
    "    Parses the JSON result file, collects the output messages (query-text pairs),\n",
    "    and converts them to Python lists.\n",
    "    \n",
    "    Args:\n",
    "    - batch (Batch): The batch object to retrieve status and results for.\n",
    "\n",
    "    Returns:\n",
    "    - If completed: A list of lists containing query-text pairs.\n",
    "    - If not completed: A string with the batch status.\n",
    "    \"\"\"\n",
    "    # Check the batch status\n",
    "    batch_status = client.batches.retrieve(batch.id)\n",
    "    if batch_status.status != 'completed':\n",
    "        return f\"Batch status: {batch_status.status}\"\n",
    "\n",
    "    # Retrieve the output file contents\n",
    "    file_id = batch_status.output_file_id\n",
    "    file_response = client.files.content(file_id)\n",
    "\n",
    "    # Parse the JSON lines in the output file\n",
    "    results = []\n",
    "    for line in file_response.text.splitlines():\n",
    "        data = json.loads(line)  # Parse each line as JSON\n",
    "        response_body = data.get(\"response\", {}).get(\"body\", {})\n",
    "        if response_body:\n",
    "            # Convert the content field in message to a list of tuples\n",
    "            content = response_body[\"choices\"][0][\"message\"][\"content\"]\n",
    "            try:\n",
    "                # Safely evaluate the string to convert it to a Python list of tuples\n",
    "                query_text_pairs = ast.literal_eval(content)\n",
    "                if isinstance(query_text_pairs, list):\n",
    "                    results.append(query_text_pairs)\n",
    "            except (SyntaxError, ValueError):\n",
    "                continue\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bs_chunks = retrieve_batch_results(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fg_chunks = output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fg_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fg_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bs_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_grained_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_data_to_json(data, filename):\n",
    "    \"\"\"\n",
    "    Writes a list of data to a file in JSONL format, where each line is a\n",
    "    JSON object corresponding to an element in the list.\n",
    "\n",
    "    Parameters:\n",
    "    data (list): A list of JSON-serializable elements to write to the file.\n",
    "    filename (str): The name of the file to write the JSONL data to.\n",
    "\n",
    "    Example:\n",
    "    >>> write_data_to_json([{\"key1\": \"value1\"}, {\"key2\": \"value2\"}], \"output.jsonl\")\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")  # Newline after each JSON object for JSONL format\n",
    "\n",
    "# Example usage\n",
    "# Assuming your list is already JSON-serializable\n",
    "# my_data = [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}, {\"name\": \"Charlie\"}]\n",
    "# write_data_to_json(my_data, \"output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(query_text_list):\n",
    "    return [[query, text] for query, text in query_text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[query, text] for query, text in output_fg_chunks[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listify(output_fg_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_data = [list(item[0], item[1]) for item in zip(fine_grained_chunks, output_fg_chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output_structure(output_chunks):\n",
    "    \"\"\"\n",
    "    Checks that each element in the nested list `output_chunks` is a pair (has exactly two items).\n",
    "    Prints the index and content of any elements that do not match the expected structure.\n",
    "    \n",
    "    Parameters:\n",
    "    output_chunks (list): A list of lists, each containing tuples/lists expected to be of length 2.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if all elements are pairs; False if any issues are found.\n",
    "    \"\"\"\n",
    "    issues_found = False\n",
    "    \n",
    "    for i, query_text_list in enumerate(output_chunks):\n",
    "        for j, item in enumerate(query_text_list):\n",
    "            if len(item) != 2:\n",
    "                print(f\"Problematic element at output_chunks[{i}][{j}]: {item}\")\n",
    "                issues_found = True\n",
    "                \n",
    "    if not issues_found:\n",
    "        print(\"All elements have the correct structure (pairs).\")\n",
    "    return not issues_found  # Returns True if no issues were found, False otherwise\n",
    "\n",
    "# Example usage:\n",
    "# result = check_output_structure(output_fg_chunks)\n",
    "# if result:\n",
    "#     print(\"Structure is as expected.\")\n",
    "# else:\n",
    "#     print(\"Found structural issues in the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_output_structure(output_chunks):\n",
    "    \"\"\"\n",
    "    Repairs each element in `output_chunks` to ensure it is a tuple pair. If an item is not a tuple,\n",
    "    it is converted to a tuple with an empty string as the second element. If a tuple has more than\n",
    "    2 elements, a warning is issued and it is truncated to the first two elements.\n",
    "    \n",
    "    Parameters:\n",
    "    output_chunks (list): A list of lists, each containing items expected to be tuples of length 2.\n",
    "    \n",
    "    Returns:\n",
    "    list: A modified copy of `output_chunks` with all elements as tuple pairs of length 2.\n",
    "    \"\"\"\n",
    "    repaired_chunks = []\n",
    "    \n",
    "    for query_text_list in output_chunks:\n",
    "        repaired_list = []\n",
    "        for item in query_text_list:\n",
    "            if isinstance(item, tuple):\n",
    "                if len(item) > 2:\n",
    "                    print(f\"Warning: Truncating item with more than 2 elements: {item}\")\n",
    "                    repaired_list.append((item[0], item[1]))\n",
    "                else:\n",
    "                    repaired_list.append(item)\n",
    "            else:\n",
    "                # Convert non-tuple items into a tuple pair with an empty string\n",
    "                repaired_list.append((item, \"\"))\n",
    "                \n",
    "        repaired_chunks.append(repaired_list)\n",
    "    \n",
    "    return repaired_chunks\n",
    "\n",
    "# Example usage:\n",
    "# repaired_output_fg_chunks = repair_output_structure(output_fg_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output_structure(output_fg_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repaired_fg = repair_output_structure(output_fg_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repaired_fg[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output_structure(repaired_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fg_chunks[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_out_lists = [listify(query_text_list) for query_text_list in repaired_fg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_out_lists = [listify(query_text_list) for query_text_list in output_bs_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bs_out_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_out_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_out_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data_to_json(fg_out_lists, \"TH_fine_grain_query_text_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data_to_json(bs_out_lists, \"TH_broad_scope_query_text_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_out_lists[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_for_seg6 = [\n",
    "  (\"How does observing the body's processes help practitioners achieve mindfulness and understanding?\", \"Observing the impermanent, selfless, and interdependent nature of all that is doesn't lead us to feel aversion for life. On the contrary, it helps us see the preciousness of all that lives. Liberation doesn't mean running away from or destroying life.\"),\n",
    "  (\"What role does insight play in recognizing the impermanent and interdependent nature of the body?\", \"The words recognition, insight, clarity, and realization here mean that the practitioner recognizes, sees, sheds light on, and realizes the impermanent and interdependent nature of the body and all that is, by means of the mindful observation of the body.\"),\n",
    "  (\"How is the practice of observing the body articulated in different versions of the sutras?\", \"In the second version of the sutra, the description of each body meditation exercise is as follows: This is how the practitioner is aware of body as body, both within and without, and establishes mindfulness in the body with understanding, insight, clarity, and realization.\"),\n",
    "  (\"Why is it important to observe the impermanence and selflessness of the Five Aggregates?\", \"In the same way, the ordinary man caught in dualistic conceptions is accustomed to thinking that the Five Aggregates are the root of his suffering, but in fact the root of suffering is the lack of understanding about the impermanent, selfless, and interdependent nature of the Five Aggregates.\"),\n",
    "  (\"How can mindfulness of the body's impermanence increase our appreciation for life?\", \"To observe the impermanence of things is not to reject them, but to be in contact with them with deep understanding, without being caught in desire and attachment.\"),\n",
    "  (\"What misconceptions about Buddhism and non-attachment are addressed in this text?\", \"Many people present Buddhism as a path that denies life, that transcends the world of the Five Aggregates... To present Buddhism in this way is no different from saying that the object of our practice is to arrive at the absence of life or nothingness.\"),\n",
    "  (\"Why is it essential to distinguish between desire that nourishes life and desire that leads to suffering?\", \"So we can say that to eat and drink so that the body is strong and healthy is to walk on the path of emancipation, while to eat and drink in a way that causes our body and others to suffer is to go against the way of liberation.\"),\n",
    "  (\"How does the Buddha's appreciation of beauty relate to his teachings on impermanence?\", \"The Buddha was not afraid of beautiful things, because he was able to see the impermanent nature of everything, beautiful or ugly. He didn't chase after things, and he didn't run away from them either.\"),\n",
    "  (\"In what ways do misinterpretations about the root of suffering impact Buddhist practice?\", \"There are people who, because of their incorrect understanding of what the root of suffering is, instead of dealing with their attitude of attachment, think they have to deal with their organs of sense and the aggregates, and so they fear form, sound, smell, taste, touch, and objects of mind and feel aversion for the body, feelings, perceptions, mental formations, and consciousness.\"),\n",
    "  (\"What does the example of the dog and the clod of earth illustrate about suffering and attachment?\", \"In the same way, the ordinary man caught in dualistic conceptions is accustomed to thinking that the Five Aggregates are the root of his suffering, but in fact the root of suffering is the lack of understanding about the impermanent, selfless, and interdependent nature of the Five Aggregates.\"),\n",
    "  (\"Why is it said that reality is not to be found in terms of existence or nonexistence?\", \"In the Kaccayana Gotta Sutta, the Buddha also taught that reality is not to be found in terms of existence or nonexistence. His meaning is perfectly clear: suffering is not brought about by life, the Five Skandhas, or the selfless and interdependent nature of all that is.\"),\n",
    "  (\"How does the understanding of interdependence and selflessness contribute to liberation?\", \"Only when, thanks to mindful observation, we realize the impermanent, selfless, and interdependent nature of all that is, can we achieve freedom and liberation.\"),\n",
    "  (\"In what ways does recognizing impermanence liberate a practitioner from attachment and sorrow?\", \"Because we can see the impermanent nature of the flowers, we can appreciate all the more the beauty of each flower. To observe the impermanence of things is not to reject them, but to be in contact with them with deep understanding, without being caught in desire and attachment.\"),\n",
    "  (\"What is Thich Nhat Hanh's perspective on the relationship between Buddhism and everyday enjoyment such as eating and drinking?\", \"If we've had nothing to eat for three days, we feel like eating. Is that desire? Is the natural desire for the indispensable elements of life a desire we need to destroy?... To eat when hungry, to drink when thirsty, is that to go against the path which leads to emancipation?\"),\n",
    "  (\"How does understanding the impermanent nature of desires lead to freedom?\", \"In identifying the mind of desire, in observing the nature of that mind and the nature of the object of desire, we'll see the impermanence, selflessness, and interdependence of it, and we'll no longer be dominated by that state of mind.\"),\n",
    "  (\"How does the practice of peace and joy fit into the broader context of mindfulness?\", \"The tenth exercise is taken from the second version of the sutra (see Appendix). It is a practice of peace and joy.\"),\n",
    "  (\"Why is the Buddha portrayed with a heart of love and a smile in opposition to an emaciated arhat?\", \"The image of the bodhisattva is very close to the image of the Buddha entering life with a heart of love and compassion and a smile on his lips.\"),\n",
    "  (\"How does mindfulness of desire differ from being dominated by desire?\", \"To know how to appreciate a beautiful sunset is not desire, if we 'remain established in the observation, free and not caught up in any worldly consideration.'\"),\n",
    "  (\"How can we apply the teachings of the impermanence of flowers to our daily life?\", \"If we cut flowers from our garden to place on the altar, that is because we acknowledge the beauty of those flowers... When the flowers wilt in a few days, we won't suffer or feel sad.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fg_chunks[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_repair = output_fg_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(repair_for_seg6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_repair[6][28:] = repair_for_seg6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_repair[6][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_repair[6][29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_repair[6][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_out_lists = [listify(query_text_list) for query_text_list in test_repair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_out_lists[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data_to_json(fg_out_lists, \"TH_fine_grain_query_text_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
