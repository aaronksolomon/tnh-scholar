{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport yt_dlp\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "%aimport whisper\n",
    "%aimport logging\n",
    "from typing import List, Dict\n",
    "%aimport json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"audio_extraction_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.gpt_processing import token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_storage_dir = Path(\"test_extracted_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio_yt(url: str, output_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Downloads audio from a YouTube video using yt_dlp.YoutubeDL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the YouTube video.\n",
    "        output_dir (Path): Directory to save the downloaded audio file.\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the downloaded audio file.\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': str(output_dir / '%(title)s.%(ext)s'),  # Save in the output directory\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "        downloaded_file = Path(ydl.prepare_filename(result)).with_suffix('.mp3')\n",
    "        return downloaded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_into_chunks(audio_file: Path, chunk_duration: int, output_dir: Path = None) -> Path:\n",
    "    \"\"\"\n",
    "    Splits an audio file into chunks of a specified time duration.\n",
    "\n",
    "    Args:\n",
    "        audio_file (Path): Path to the input audio file.\n",
    "        chunk_duration (int): Duration of each chunk in milliseconds (e.g., 10 * 60 * 1000 for 10 minutes).\n",
    "        output_dir (Path): Path to the directory where chunks will be saved. \n",
    "                           If None, a subdirectory is created in the same directory as the input file.\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the directory containing the chunks.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    total_duration = len(audio)  # Total duration in milliseconds\n",
    "\n",
    "    # Create output directory based on filename\n",
    "    if output_dir is None:\n",
    "        output_dir = audio_file.parent / f\"{audio_file.stem}_chunks\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Split the audio into chunks\n",
    "    for i, start in enumerate(range(0, total_duration, chunk_duration)):\n",
    "        chunk = audio[start:start + chunk_duration]\n",
    "        chunk_path = output_dir / f\"chunk_{i + 1}.mp3\"\n",
    "        chunk.export(chunk_path, format=\"mp3\")\n",
    "        print(f\"Exported: {chunk_path}\")\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_boundaries(audio_file: Path, model_size: str = 'tiny') -> List[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Use Whisper to detect sentence boundaries in the audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_file (Path): Path to the audio file.\n",
    "        model_size (str): Whisper model size (e.g., 'tiny', 'base', 'small').\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, float]]: List of timestamps with sentence boundaries. Each entry contains:\n",
    "            - \"start\": Start time of the sentence (in seconds).\n",
    "            - \"end\": End time of the sentence (in seconds).\n",
    "            - \"text\": Sentence text.\n",
    "    \"\"\"\n",
    "    # Load the Whisper model\n",
    "    logger.info(\"Loading model...\")\n",
    "    model = whisper.load_model(model_size)\n",
    "    logger.info(f\"Model '{model_size}' loaded.\")\n",
    "\n",
    "    # Transcribe the audio file\n",
    "    result = model.transcribe(str(audio_file), task=\"transcribe\", word_timestamps=True)\n",
    "    \n",
    "    # Extract sentence boundaries from segments\n",
    "    sentence_boundaries = []\n",
    "    for segment in result['segments']:\n",
    "        sentence_boundaries.append({\n",
    "            \"start\": segment['start'],\n",
    "            \"end\": segment['end'],\n",
    "            \"text\": segment['text']\n",
    "        })\n",
    "    \n",
    "    return sentence_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_at_boundaries(audio_file: Path, boundaries: List[Dict[str, float]], output_dir: Path  = None, max_duration: int = 10 * 60) -> Path:\n",
    "    \"\"\"\n",
    "    Split the audio file into chunks close to a specified duration while respecting boundaries.\n",
    "\n",
    "    Args:\n",
    "        audio_file (Path): Path to the audio file.\n",
    "        boundaries (List[Dict[str, float]]): List of boundaries with start and end times.\n",
    "        output_dir (Path): Directory to save the chunks.\n",
    "        max_duration (int): Maximum duration for each chunk in seconds (default is 10 minutes).\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the directory containing the chunks.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    \n",
    "    # Create output directory based on filename\n",
    "    if output_dir is None:\n",
    "        output_dir = audio_file.parent / f\"{audio_file.stem}_chunks\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True) \n",
    "       \n",
    "    # Initialize variables\n",
    "    current_chunk = AudioSegment.empty()\n",
    "    current_start = boundaries[0][\"start\"]\n",
    "    chunk_count = 1\n",
    "\n",
    "    for i, boundary in enumerate(boundaries):\n",
    "        # Calculate segment duration\n",
    "        segment_start_ms = boundary[\"start\"] * 1000\n",
    "        segment_end_ms = boundary[\"end\"] * 1000\n",
    "        segment = audio[segment_start_ms:segment_end_ms]\n",
    "        \n",
    "        # Add segment to the current chunk if it fits\n",
    "        if len(current_chunk) + len(segment) <= max_duration * 1000:\n",
    "            current_chunk += segment\n",
    "        else:\n",
    "            # Export the current chunk\n",
    "            chunk_path = output_dir / f\"chunk_{chunk_count}.mp3\"\n",
    "            current_chunk.export(chunk_path, format=\"mp3\")\n",
    "            logger.info(f\"Exported: {chunk_path}\")\n",
    "            \n",
    "            # Start a new chunk\n",
    "            chunk_count += 1\n",
    "            current_chunk = segment\n",
    "            current_start = boundary[\"start\"]\n",
    "    \n",
    "    # Export the final chunk\n",
    "    if len(current_chunk) > 0:\n",
    "        chunk_path = output_dir / f\"chunk_{chunk_count}.mp3\"\n",
    "        current_chunk.export(chunk_path, format=\"mp3\")\n",
    "        print(f\"Exported: {chunk_path}\")\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_chunks(\n",
    "    directory: Path, \n",
    "    output_file: Path, \n",
    "    jsonl_file: Path, \n",
    "    model: str = \"whisper-1\", \n",
    "    prompt: str = \"\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Processes all audio chunks in the specified directory using OpenAI's transcription API,\n",
    "    saves the transcription objects into a JSONL file, and stitches the transcriptions\n",
    "    into a single text file.\n",
    "\n",
    "    Args:\n",
    "        directory (Path): Path to the directory containing audio chunks.\n",
    "        output_file (Path): Path to the output file to save the stitched transcription.\n",
    "        jsonl_file (Path): Path to save the transcription objects as a JSONL file.\n",
    "        model (str): The transcription model to use (default is \"whisper-1\").\n",
    "        prompt (str): Optional prompt to provide context for better transcription.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    jsonl_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Collect all audio chunks in the directory\n",
    "    audio_files = sorted(directory.glob(\"*.mp3\"))  # Sort files by name for sequential processing\n",
    "    logger.info(f\"Audio files found:\\n\\t{audio_files}\")\n",
    "\n",
    "    # Initialize the output content\n",
    "    stitched_transcription = []\n",
    "\n",
    "    # Open the JSONL file for writing\n",
    "    with jsonl_file.open(\"w\", encoding=\"utf-8\") as jsonl_out:\n",
    "        # Process each audio chunk\n",
    "        for audio_file in audio_files:\n",
    "            logger.info(f\"Processing {audio_file.name}...\")\n",
    "            try:\n",
    "                with audio_file.open(\"rb\") as file:\n",
    "                    transcript = client.audio.transcriptions.create(\n",
    "                        model=model,\n",
    "                        prompt=prompt,\n",
    "                        file=file\n",
    "                    )\n",
    "                    # Save the full response object to the JSONL file\n",
    "                    jsonl_out.write(json.dumps(transcript.to_dict()) + \"\\n\")\n",
    "                    \n",
    "                    # Append the transcribed text to the stitched output\n",
    "                    stitched_transcription.append(transcript.text)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {audio_file.name}: {e}\", exc_info=True)\n",
    "                \n",
    "\n",
    "    # Write the stitched transcription to the output file\n",
    "    with output_file.open(\"w\", encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(\" \".join(stitched_transcription))\n",
    "\n",
    "    logger.info(f\"Stitched transcription saved to {output_file}\")\n",
    "    logger.info(f\"Full transcript objects saved to {jsonl_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_audio_yt(\"https://www.youtube.com/watch?v=SEc28BCHgu8&ab_channel=DeerParkMonastery\", audio_storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = audio_storage_dir / \"Taking Care of Our Fear ｜ Br. Phap Luu ｜ 2024-11-06.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = detect_boundaries(audio_file_path, \"tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_dir = audio_storage_dir / \"test_output_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_audio_at_boundaries(audio_file_path, boundaries, test_output_dir, max_duration=3 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunk_duration_ms = 8 * 60 * 1000  # 10 minutes\n",
    "output_directory = split_audio_into_chunks(audio_file_path, chunk_duration_ms)\n",
    "print(f\"Chunks saved to: {output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_directory = audio_storage_dir / \"test_output_dir\"\n",
    "output_transcription_file = Path(\"taking_care_of_fear_transcript.txt\")\n",
    "output_jsonl_file = Path(\"taking_care_of_fear_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "process_audio_chunks(\n",
    "    directory=chunks_directory,\n",
    "    output_file=output_transcription_file,\n",
    "    jsonl_file=output_jsonl_file,\n",
    "    prompt=\"Dharma, Deer Park, Thay, Thich Nhat Hanh, Bodhicitta, Bodhisattva, Mahayana\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Open the file and perform transcription\n",
    "with audio_file_path.open(\"rb\") as audio_file:\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        response_format=\"verbose_json\",\n",
    "        prompt=\"Dharma, Deer Park, Thay, Thich Nhat Hanh, Bodhicitta, Bodhisattva, Mahayana\",\n",
    "        file=chunks_directory / \"chunk_1.mp3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.segments[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.audio.transcription_verbose import TranscriptionVerbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formatting prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the following text into paragraphs. Make minimal corrections to grammar if required for logical flow. Make no other changes; add no content. Output the final text only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count(transcript.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
