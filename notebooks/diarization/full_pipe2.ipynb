{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Diarization and Transcription Pipeline\n",
    "\n",
    "This notebook provides a simple workflow to:\n",
    "1. Perform speaker diarization using PyAnnote\n",
    "2. Extract speaker-specific audio\n",
    "3. Transcribe speaker audio with audio-transcribe\n",
    "4. Convert JSONL to SRT with timeline mapping\n",
    "\n",
    "Note: Requires tnh-scholar package to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from tnh_scholar.audio_processing.diarization import (\n",
    "    diarize,\n",
    "    resume_diarization,\n",
    ")\n",
    "from tnh_scholar.audio_processing.diarization.audio import AudioHandler\n",
    "from tnh_scholar.audio_processing.diarization.config import (\n",
    "    ChunkConfig,\n",
    "    DiarizationConfig,\n",
    "    LanguageConfig,\n",
    "    SpeakerConfig,\n",
    ")\n",
    "from tnh_scholar.audio_processing.diarization.models import AugDiarizedSegment\n",
    "from tnh_scholar.audio_processing.diarization.pyannote_adapter import PyannoteAdapter\n",
    "from tnh_scholar.audio_processing.diarization.strategies import LanguageProbe, WhisperLanguageDetector\n",
    "from tnh_scholar.audio_processing.diarization.strategies.speaker_blocker import group_speaker_blocks\n",
    "from tnh_scholar.audio_processing.diarization.strategies.time_gap import TimeGapChunker\n",
    "from tnh_scholar.audio_processing.diarization.timeline_mapper import TimelineMapper\n",
    "from tnh_scholar.audio_processing.diarization.viewer import close_segment_viewer, launch_segment_viewer\n",
    "from tnh_scholar.audio_processing.timed_object.timed_text import Granularity, TimedText\n",
    "from tnh_scholar.audio_processing.transcription import patch_whisper_options\n",
    "from tnh_scholar.audio_processing.transcription.srt_processor import (\n",
    "    SRTConfig,\n",
    "    SRTProcessor,\n",
    ")\n",
    "from tnh_scholar.audio_processing.transcription.text_segment_builder import TextSegmentBuilder\n",
    "from tnh_scholar.audio_processing.transcription.transcription_service import (\n",
    "    TranscriptionResult,\n",
    "    TranscriptionServiceFactory,\n",
    ")\n",
    "from tnh_scholar.audio_processing.utils import (\n",
    "    get_audio_from_file,\n",
    "    get_segment_audio,\n",
    "    play_diarization_segment,\n",
    ")\n",
    "from tnh_scholar.utils.file_utils import (\n",
    "    write_str_to_file,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Handle warnings with traceback\n",
    "def warn_with_traceback(message, category, filename, lineno, file=None, line=None):\n",
    "    log = file if hasattr(file, 'write') else sys.stderr\n",
    "    traceback.print_stack(file=log)\n",
    "    log.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
    "\n",
    "warnings.showwarning = warn_with_traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from tnh_scholar.logging_config import setup_logging\n",
    "\n",
    "setup_logging(log_level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these values\n",
    "# Path to the directory containing audio files\n",
    "\n",
    "BASE_DIR = Path.home() / \"Desktop/tnh-scholar/audio_transcriptions\"\n",
    "\n",
    "# Audio file to process (run this notebook once per file)\n",
    "AUDIO_FILE_STR = \"farm_convo_spencer.flac\"\n",
    "\n",
    "DIARIZATION_FILE_STR = AUDIO_FILE_STR\n",
    "\n",
    "SPEAKER_COUNT = None # Must be 1, 2 or None. If speakers > 2 use None for best result.\n",
    "\n",
    "GENERATE_NEW_DIARIZATION = False\n",
    "\n",
    "DIARIZE_SINGLE_SPEAKER = False\n",
    "\n",
    "SRT_INCLUDE_SPEAKER = True\n",
    "\n",
    "LANGUAGE = 'en'\n",
    "\n",
    "TARGET_CHUNK_TIME = 2 * 60  # seconds\n",
    "\n",
    "MIN_CHUNK_TIME = 10 # seconds\n",
    "\n",
    "TRANSCRIBER = \"whisper\"\n",
    "\n",
    "completed = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = \"\" # read_str_from_file(BASE_DIR / \"sr_bamboo_metadata.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarize_config = DiarizationConfig(\n",
    "    chunk = ChunkConfig(\n",
    "        target_duration=TARGET_CHUNK_TIME * 1000,\n",
    "        min_duration= MIN_CHUNK_TIME * 1000, \n",
    "    ),\n",
    "    speaker = SpeakerConfig(\n",
    "        single_speaker=DIARIZE_SINGLE_SPEAKER,\n",
    "    ),\n",
    "    language = LanguageConfig(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "audio_file_path = BASE_DIR / AUDIO_FILE_STR\n",
    "diarize_audio_file_path = BASE_DIR / DIARIZATION_FILE_STR\n",
    "\n",
    "file_ext_str = audio_file_path.suffix\n",
    "\n",
    "if not audio_file_path.exists():\n",
    "    raise FileNotFoundError(f\"No file found: {audio_file_path}\")\n",
    "\n",
    "diarization_results_path = diarize_audio_file_path.parent / \"raw_diarization_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diarization_result(file_path):\n",
    "    \"\"\"Load diarization result from JSON file or sample data.\"\"\"\n",
    "    if not file_path:\n",
    "        raise ValueError(\"File_path must be provided.\")\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PyAnnote diarization\n",
    "if GENERATE_NEW_DIARIZATION:\n",
    "    completed = False\n",
    "    print(f\"Starting diarization for {diarize_audio_file_path}...\")\n",
    "    result = diarize(diarize_audio_file_path, num_speakers=SPEAKER_COUNT, output_path=diarization_results_path)\n",
    "\n",
    "    # If the job is still running, you'll get a job ID\n",
    "    if isinstance(result, str):\n",
    "        job_id = result\n",
    "        print(f\"Diarization job started with ID: {job_id}\")\n",
    "        print(\"Wait for completion and then run the next cell with this job ID\")\n",
    "    else:\n",
    "        completed = True\n",
    "        print(\"Diarization process finished on initial run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this if you got a job ID in the previous cell\n",
    "# Replace with your actual job ID from the previous step\n",
    "# job_id = \"your-job-id-here\"  # e.g., \"994c79b7-5f32-4715-aa34-33f00e216369\"\n",
    "\n",
    "# Check status\n",
    "\n",
    "if not completed:\n",
    "    status = check_job_status(job_id)\n",
    "    print(f\"Current status: {status.get('status', 'unknown')}\")\n",
    "\n",
    "    # Resume if needed\n",
    "    if status.get('status') != 'succeeded':\n",
    "        print(\"Resuming diarization...\")\n",
    "        result = resume_diarization(audio_file_path, job_id)\n",
    "        print(\"Diarization completed\")\n",
    "    else:\n",
    "        print(\"Diarization already completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diarize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_audio = get_audio_from_file(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_options_aai = {\"language_code\": LANGUAGE, \"language_detection\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_service = TranscriptionServiceFactory.create_service(provider=TRANSCRIBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = ts_service.transcribe(audio_file_path, transcription_options_aai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_seg = transcript.utterance_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert full_seg is not None\n",
    "new_seg = TimedText(segments=full_seg.segments, granularity=Granularity.SEGMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert full_seg is not None\n",
    "full_out = new_seg.export_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = BASE_DIR / audio_file_path.with_suffix(\".txt\")\n",
    "write_str_to_file(path_out, full_out, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_config = SRTConfig(include_speaker=SRT_INCLUDE_SPEAKER) \n",
    "srt_processor = SRTProcessor(srt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_out = srt_processor.generate(full_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(srt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the diarization results\n",
    "\n",
    "print(f\"Loading diarization results from {diarization_results_path}\")\n",
    "chunker = TimeGapChunker(config=diarize_config)\n",
    "segment_adapter = PyannoteAdapter(config=diarize_config)\n",
    "result = load_diarization_result(file_path=diarization_results_path)\n",
    "data = result['output']\n",
    "segments = segment_adapter.to_segments(data)\n",
    "chunk_list = chunker.extract(segments)\n",
    "\n",
    "for chunk in chunk_list:\n",
    "    print(f\"  chunk: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarize_raw = data['diarization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarize_raw[0]['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_list = [seg for seg in segments if seg.duration_sec > 4.0]\n",
    "long_list_info = [\n",
    "    (i, \n",
    "    seg.duration_sec, seg.start.to_seconds(), \n",
    "    seg.end.to_seconds(), seg.speaker\n",
    "    ) \n",
    "    for i, seg in enumerate(long_list) \n",
    "]\n",
    "long_list_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_blocks = group_speaker_blocks(segments, config=diarize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speaker_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = launch_segment_viewer(speaker_blocks[:250], audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_segment_viewer(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(block.speaker, block.duration) for block in speaker_blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 167\n",
    "seg = segments[test_idx]\n",
    "print(seg)\n",
    "play_diarization_segment(seg, base_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = WhisperLanguageDetector()\n",
    "\n",
    "probe = LanguageProbe(\n",
    "    config=diarize_config, \n",
    "    detector=detector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_audio = get_segment_audio(seg, base_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug_seg = AugDiarizedSegment.from_segment(segments[test_idx], audio=seg_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe.segment_language(aug_segment=aug_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "from openai import RateLimitError\n",
    "\n",
    "segments_to_probe = long_list\n",
    "\n",
    "def probe_segment_safe(probe, aug_segment):\n",
    "    try:\n",
    "        return probe.segment_language(aug_segment=aug_segment)\n",
    "    except RateLimitError:\n",
    "        print(\"Rate limit hit, sleeping and retrying...\")\n",
    "        time.sleep(10)  # Wait and retry\n",
    "        try:\n",
    "            return probe.segment_language(aug_segment=aug_segment)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed again: {e}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: probe all segments in long_list (or chunk_list, or your own list)\n",
    "max_workers = 1000  # Adjust based on your rate limit\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(\n",
    "        probe_segment_safe, \n",
    "        probe, \n",
    "        AugDiarizedSegment.from_segment(seg, audio=get_segment_audio(seg, base_audio))\n",
    "        )\n",
    "        for seg in segments_to_probe]  # Adjust range as needed\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        results.append(future.result())\n",
    "\n",
    "print(\"Language probe results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chunk.accumulated_time for chunk in chunk_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(chunk.segments) for chunk in chunk_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract speaker audio segments\n",
    "print(\"Extracting speaker audio segments to local ByteIO objects\")\n",
    "audio_handler = AudioHandler()\n",
    "total_chunks = len(chunk_list) \n",
    "\n",
    "for i, chunk in enumerate(chunk_list, start=1):\n",
    "    print(f\"Building chunk {i} of {total_chunks}\")\n",
    "    audio_handler.build_audio_chunk(chunk, audio_file=audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_list[0].total_duration_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list = [chunk.audio for chunk in chunk_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_chunk = audio_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio_mp4(aud_chunk.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_service = TranscriptionServiceFactory.create_service(provider=TRANSCRIBER)\n",
    "\n",
    "transcription_options_whisper = {\n",
    "    \"language\": LANGUAGE, \"timestamp_granularities\": [\"word\"], \"prompt\": metadata\n",
    "    }\n",
    "transcription_options_whisper = patch_whisper_options(\n",
    "    transcription_options_whisper, file_extension=file_ext_str\n",
    "    )\n",
    "transcription_options_aai = {\"language_code\": LANGUAGE, \"language_detection\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_options = transcription_options_whisper \\\n",
    "    if TRANSCRIBER == 'whisper' \\\n",
    "        else transcription_options_aai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_to_process = chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts: List[TranscriptionResult]= []\n",
    "for i, chunk in enumerate(chunks_to_process, start=1):\n",
    "    print(f\"processing chunk: {i}\")\n",
    "    audio = chunk.audio\n",
    "    if not audio:\n",
    "        raise ValueError(\"No audio data for chunk.\")\n",
    "    audio_obj = audio.data\n",
    "    print(f\"Running transcript generation with {TRANSCRIBER} service...\")\n",
    "    print(f\"Audio file: {audio_obj}\")\n",
    "    transcript = ts_service.transcribe(audio_obj, transcription_options)\n",
    "    print(transcript)\n",
    "    transcripts.append(transcript)\n",
    "    \n",
    "print(\"Transcription loop complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.raw_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = TimelineMapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings = []\n",
    "for chunk, transcript in zip(chunk_list, transcripts):\n",
    "    tt = transcript.word_timing\n",
    "    if tt is not None:\n",
    "        new_timing = mapper.remap(tt, chunk)\n",
    "        timings.append(new_timing)\n",
    "    else:\n",
    "        raise ValueError(\"No timed text for words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_timing = TimedText.merge(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_builder = TextSegmentBuilder(max_duration_ms=4*1000, target_characters=42, ignore_speaker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_seg = segment_builder.create_segments(complete_timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_config = SRTConfig(include_speaker=SRT_INCLUDE_SPEAKER) \n",
    "srt_processor = SRTProcessor(srt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_out = srt_processor.generate(full_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(srt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio_mp4(aud_chunk.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(str(audio_file_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"srt_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ext = \".srt\"\n",
    "new_stem = f\"{audio_file_path.stem}_{test_str}\"\n",
    "srt_path = audio_file_path.with_name(new_stem + new_ext)\n",
    "\n",
    "\n",
    "write_str_to_file(srt_path, srt_out, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF PROCESS PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing: Translate all final SRT files to English\n",
    "print(\"\\n===== Translating SRT files to English =====\")\n",
    "\n",
    "        \n",
    "# Find all _final.srt files in each speaker directory\n",
    "for srt_file in BASE_DIR.glob(\"*.srt\"):\n",
    "    print(f\"file: {srt_file}\")\n",
    "          \n",
    "    en_srt_file = srt_file.with_name(f\"{srt_file.stem}_en.srt\")\n",
    "    \n",
    "    # Skip if English version already exists\n",
    "    if en_srt_file.exists():\n",
    "        print(f\"English SRT already exists: {en_srt_file}\")\n",
    "        continue\n",
    "        \n",
    "    # Run srt-translate\n",
    "    cmd = f\"srt-translate '{srt_file}' -o '{en_srt_file}' -t en\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "        print(f\"Successfully translated: {srt_file} -> {en_srt_file}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error translating {srt_file}: {e}\")\n",
    "\n",
    "print(\"===== Translation complete =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Settings ---\n",
    "srt_folder = BASE_DIR  # <-- Change this to your actual folder\n",
    "srt_processor = SRTProcessor()\n",
    "\n",
    "# --- Processing Loop ---\n",
    "for srt_file in srt_folder.glob(\"*.srt\"):\n",
    "    # Read original SRT content\n",
    "    srt_content = srt_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Parse to TimedText\n",
    "    timed_text = srt_processor.parse(srt_content)\n",
    "\n",
    "    # Re-generate SRT without speaker labels\n",
    "    cleaned_srt = srt_processor.generate(timed_text, include_speaker=False)\n",
    "\n",
    "    # Rename original file to *_sp.srt\n",
    "    speaker_file = srt_file.with_stem(f\"{srt_file.stem}_sp\")\n",
    "    srt_file.rename(speaker_file)\n",
    "\n",
    "    # Save cleaned SRT under original filename\n",
    "    srt_file.write_text(cleaned_srt, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Cleaning and renaming completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process each speaker's audio\n",
    "# for speaker, blocks in mapped_blocks.items():\n",
    "#     speaker_audio_path = export_dir / f\"{speaker}.mp3\"\n",
    "#     speaker_output_dir = export_dir / \"audio_transcriptions\" / speaker\n",
    "#     audio_transcribe_output_dir = export_dir / \"audio_transcriptions\"\n",
    "#     ensure_directory_exists(speaker_output_dir)\n",
    "    \n",
    "#     print(f\"\\nProcessing {speaker}...\")\n",
    "    \n",
    "#     # Run audio-transcribe on the speaker's audio file\n",
    "#     cmd = f\"audio-transcribe -f {speaker_audio_path} --output_dir {audio_transcribe_output_dir} --split --transcribe\"\n",
    "#     print(f\"Running: {cmd}\")\n",
    "#     subprocess.run(cmd, shell=True, check=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
