{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "import json\n",
    "from google.cloud import vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root_dir = Path(\"/Users/phapman/Desktop/tnh-scholar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(pdf_path: Path, bucket_name: str, gcs_destination: str) -> str:\n",
    "    \"\"\"\n",
    "    Uploads a local PDF file to Google Cloud Storage.\n",
    "    \n",
    "    Parameters:\n",
    "        pdf_path (Path): Path to the local PDF.\n",
    "        bucket_name (str): GCS bucket name.\n",
    "        gcs_destination (str): Path in the GCS bucket.\n",
    "    \n",
    "    Returns:\n",
    "        str: The GCS URI of the uploaded PDF.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(gcs_destination)\n",
    "    blob.upload_from_filename(str(pdf_path))\n",
    "    gcs_uri = f\"gs://{bucket_name}/{gcs_destination}\"\n",
    "    print(f\"Uploaded {pdf_path} to {gcs_uri}\")\n",
    "    return gcs_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_pdf(gcs_uri: str, output_gcs_uri: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends a batch processing request to Google Vision API for document text detection.\n",
    "    \n",
    "    Parameters:\n",
    "        gcs_uri (str): URI of the PDF in GCS.\n",
    "        output_gcs_uri (str): GCS URI for storing the result.\n",
    "    \n",
    "    Returns:\n",
    "        str: The operation name for tracking the job.\n",
    "    \"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    # Input configuration\n",
    "    input_config = vision.InputConfig(\n",
    "        gcs_source=vision.GcsSource(uri=gcs_uri),\n",
    "        mime_type=\"application/pdf\"\n",
    "    )\n",
    "\n",
    "    # Output configuration for asynchronous results\n",
    "    output_config = vision.GcsDestination(uri=output_gcs_uri)\n",
    "\n",
    "    # Feature type: DOCUMENT_TEXT_DETECTION\n",
    "    features = [vision.Feature(type=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)]\n",
    "\n",
    "    # File request (Corrected: `output_config` goes into `AsyncAnnotateFileRequest`)\n",
    "    async_request = vision.AsyncAnnotateFileRequest(\n",
    "        input_config=input_config,\n",
    "        features=features,\n",
    "        output_config=vision.OutputConfig(gcs_destination=output_config, batch_size=1)\n",
    "    )\n",
    "\n",
    "    # Async batch request\n",
    "    operation = client.async_batch_annotate_files(requests=[async_request])\n",
    "    print(f\"Started batch operation: {operation.operation.name}\")\n",
    "    return operation.operation.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def poll_operation_status_with_retry(operation_name: str, poll_interval: int = 10, max_retries: int = 30) -> None:\n",
    "    \"\"\"\n",
    "    Polls the status of an asynchronous batch operation with retries.\n",
    "    \n",
    "    Parameters:\n",
    "        operation_name (str): Name of the operation to check.\n",
    "        poll_interval (int): Time (in seconds) to wait between retries.\n",
    "        max_retries (int): Maximum number of retries before giving up.\n",
    "    \"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        operation = client.transport.operations_client.get_operation(operation_name)\n",
    "        \n",
    "        if operation.done:\n",
    "            if operation.HasField('error'):\n",
    "                print(f\"Operation failed with error: {operation.error.message}\")\n",
    "                return operation\n",
    "            elif operation.HasField('response'):\n",
    "                print(\"Operation completed successfully.\")\n",
    "                return True # Exit the loop upon success or failure\n",
    "            else:\n",
    "                print(\"Unknown operation status.\")\n",
    "                return operation\n",
    "        else:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries}: Operation still in progress...\")\n",
    "            time.sleep(poll_interval)\n",
    "\n",
    "    print(\"Polling timed out. The operation may still be in progress.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_parse_results(output_gcs_uri: str, local_output_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    Downloads and parses batch processing results from GCS.\n",
    "    \n",
    "    Parameters:\n",
    "        output_gcs_uri (str): URI of the GCS folder containing results.\n",
    "        local_output_dir (Path): Local directory to save the results.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name, prefix = output_gcs_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    pages = []\n",
    "\n",
    "    local_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Downloading results to {local_output_dir}...\")\n",
    "\n",
    "    for blob in blobs:\n",
    "        local_file_path = local_output_dir / blob.name.split(\"/\")[-1]\n",
    "        blob.download_to_filename(str(local_file_path))\n",
    "        print(f\"Downloaded: {blob.name}\")\n",
    "\n",
    "        # Parse JSON output\n",
    "        with open(local_file_path, \"r\") as f:\n",
    "            result = json.load(f)\n",
    "            for page in result.get(\"responses\", []):\n",
    "                pages.append(page.get(\"fullTextAnnotation\", {}).get(\"text\", \"\"))\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_results(output_gcs_uri: str):\n",
    "    \"\"\"\n",
    "    Downloads and parses batch processing results from GCS directly into memory.\n",
    "\n",
    "    Parameters:\n",
    "        output_gcs_uri (str): URI of the GCS folder containing results.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of texts extracted from the results.\n",
    "    \"\"\"\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name, prefix = output_gcs_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    pages = []\n",
    "\n",
    "    print(\"Downloading and parsing results in memory...\")\n",
    "\n",
    "    for blob in blobs:\n",
    "        print(f\"Processing: {blob.name}\")\n",
    "        \n",
    "        # Read blob content directly into memory\n",
    "        content = blob.download_as_bytes()\n",
    "        \n",
    "        # Parse JSON from memory\n",
    "        result = json.loads(content)\n",
    "        for page in result.get(\"responses\", []):\n",
    "            text = page.get(\"fullTextAnnotation\", {}).get(\"text\", \"\")\n",
    "            if text:\n",
    "                pages.append(text)\n",
    "\n",
    "    return result, pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir_path = project_root_dir / \"data_processing/PDF/Phat_Giao_journals\"\n",
    "pdf_path = pdf_dir_path / \"phat-giao-viet-nam-1956-01.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"test-bucket-tnh-translation\"\n",
    "gcs_destination = \"vietnamese_docs/vietnamese.pdf\"\n",
    "output_gcs_uri = \"gs://test-bucket-tnh-translation/vietnamese_docs/output/\"\n",
    "local_output_dir = Path(\"./local_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Upload PDF to GCS\n",
    "# gcs_uri = upload_to_gcs(pdf_path, bucket_name, gcs_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Start batch processing\n",
    "# operation_name = batch_process_pdf(gcs_uri, output_gcs_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Poll the operation status (repeat until done)\n",
    "result = poll_operation_status_with_retry(operation_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Download and parse results\n",
    "full_data, pages = download_results(output_gcs_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = full_data['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0]['text_annotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ocr_json(json_path):\n",
    "    \"\"\"\n",
    "    Parses a Google Vision OCR JSON output file to extract blocks and paragraphs with their bounding boxes.\n",
    "    Navigates through the hierarchy starting from the 'responses' key.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the Google Vision OCR output JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a block with its text and paragraphs. \n",
    "              Each block contains:\n",
    "                - 'block_text': Full text of the block\n",
    "                - 'bounding_box': Bounding box of the block\n",
    "                - 'paragraphs': List of paragraphs, each containing:\n",
    "                    - 'paragraph_text': Full text of the paragraph\n",
    "                    - 'bounding_box': Bounding box of the paragraph\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        ocr_data = json.load(f)\n",
    "\n",
    "    parsed_data = []\n",
    "\n",
    "    # Navigate to 'responses' -> 'fullTextAnnotation' -> 'pages' -> 'blocks'\n",
    "    responses = ocr_data.get('responses', [])\n",
    "    for response in responses:\n",
    "        full_text_annotation = response.get('fullTextAnnotation', {})\n",
    "        pages = full_text_annotation.get('pages', [])\n",
    "        \n",
    "        for page in pages:\n",
    "            for block in page.get('blocks', []):\n",
    "                block_text = []\n",
    "                block_bounding_box = block.get('boundingBox', {})\n",
    "                paragraphs = []\n",
    "\n",
    "                for paragraph in block.get('paragraphs', []):\n",
    "                    paragraph_text = []\n",
    "                    paragraph_bounding_box = paragraph.get('boundingBox', {})\n",
    "\n",
    "                    for word in paragraph.get('words', []):\n",
    "                        word_text = ''.join(symbol.get('text', '') for symbol in word.get('symbols', []))\n",
    "                        paragraph_text.append(word_text)\n",
    "\n",
    "                    paragraphs.append({\n",
    "                        'paragraph_text': ' '.join(paragraph_text),\n",
    "                        'bounding_box': paragraph_bounding_box\n",
    "                    })\n",
    "                    block_text.extend(paragraph_text)\n",
    "\n",
    "                parsed_data.append({\n",
    "                    'block_text': ' '.join(block_text),\n",
    "                    'bounding_box': block_bounding_box,\n",
    "                    'paragraphs': paragraphs\n",
    "                })\n",
    "\n",
    "    return parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_origin_point(element):\n",
    "    \"\"\"\n",
    "    Extracts the origin point (first vertex) of the bounding box.\n",
    "\n",
    "    Args:\n",
    "        element (dict): A block or paragraph dictionary containing a 'bounding_box' key.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The (x, y) origin point of the bounding box, or None if not found.\n",
    "    \"\"\"\n",
    " \n",
    "    bounding_box = element.get(\"bounding_box\", {})\n",
    "    vertices = bounding_box.get(\"normalizedVertices\", [])\n",
    "\n",
    "    if vertices and len(vertices) > 0:\n",
    "        point = vertices[0].get(\"x\", 0), vertices[0].get(\"y\", 0)\n",
    "        return point\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xml_from_blocks(parsed_data, include_attributes=[]):\n",
    "    \"\"\"\n",
    "    Converts parsed block and paragraph data into XML-structured text with optional attributes.\n",
    "\n",
    "    Args:\n",
    "        parsed_data (list): A list of dictionaries, each representing a block with text, bounding boxes, \n",
    "                            and paragraphs (output from parse_ocr_json).\n",
    "        include_attributes (list): A list of attributes to include in the tags. Example: [\"bounding_box\", \"origin_point\"].\n",
    "\n",
    "    Returns:\n",
    "        str: XML-structured text representing the data.\n",
    "    \"\"\"\n",
    "    from xml.etree.ElementTree import Element, SubElement, tostring\n",
    "    from xml.dom.minidom import parseString\n",
    "\n",
    "    assert isinstance(include_attributes, list)\n",
    "\n",
    "    # Define a mapping of attribute names to helper functions\n",
    "    attribute_extractors = {\n",
    "        \"origin_point\": get_origin_point,\n",
    "    }\n",
    "\n",
    "    # Root element\n",
    "    root = Element(\"document\")\n",
    "\n",
    "    # Process each block\n",
    "    for block in parsed_data:\n",
    "        # Prepare attributes for the block\n",
    "        block_attributes = {\n",
    "            key: str(attribute_extractors[key](block))\n",
    "            for key in include_attributes if key in attribute_extractors\n",
    "        }\n",
    "        block_element = SubElement(root, \"block\", attrib=block_attributes)\n",
    "\n",
    "        # Add paragraphs within the block\n",
    "        for paragraph in block.get(\"paragraphs\", []):\n",
    "            # Prepare attributes for the paragraph\n",
    "            paragraph_attributes = {\n",
    "                key: str(attribute_extractors[key](paragraph))\n",
    "                for key in include_attributes if key in attribute_extractors\n",
    "            }\n",
    "            paragraph_element = SubElement(\n",
    "                block_element, \"p\", attrib=paragraph_attributes\n",
    "            )\n",
    "            paragraph_element.text = paragraph.get(\"paragraph_text\", \"\")\n",
    "\n",
    "    # Generate pretty XML string\n",
    "    rough_string = tostring(root, encoding=\"unicode\")\n",
    "    pretty_xml = parseString(rough_string).toprettyxml(indent=\"  \")\n",
    "    return pretty_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = local_output_dir / \"output-1-to-1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = parse_ocr_json(local_output_dir / \"output-3-to-3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(build_xml_from_blocks(result, include_attributes=[\"origin_point\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9628349 * 1157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(build_xml_from_blocks(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" ?>\n",
    "<document>\n",
    "  <block>\n",
    "    <p>IPIHIAT - GIAO</p>\n",
    "    <p>VIET = NAM</p>\n",
    "  </block>\n",
    "  <block>\n",
    "    <p>NG.VI</p>\n",
    "  </block>\n",
    "  <block>\n",
    "    <p>TỪ NGÀY 10 Tỳ - Ni - Đa - Lưu - Chi sang nước ta đến nay ,</p>\n",
    "    <p>kề ra đã đến mười lăm thế kỷ . Phật - Giáo đã ở lại cùng chúng ta một ngàn năm trăm năm , và đã cùng dân tộc Việt - Nam chịu chung bao nhiêu thắng - trầm vinh nhục ,</p>\n",
    "    <p>Phật - Giáo Việt - Nam quả là một nền Phật - Giáo dân tộc .</p>\n",
    "  </block>\n",
    "  <block>\n",
    "    <p>PHẬT - GIÁO VIỆT NAM</p>\n",
    "  </block>\n",
    "  <block>\n",
    "    <p>Phật - Giáo Việt Nam không</p>\n",
    "    <p>phải chỉ là một tồn - giáo tin ngưỡng mà bất cứ thời nào , ở đâu , cũng chỉ biết có sứ - mạng của lồn - giáo tín ngưỡng . Không 1 Ở bất cứ nước nào trên thế giới cũng vậy , khi bước chân đến , Đạo Phật cũng thích nghỉ ngay với phong - lục , khí hậu , nhân tính đề biển thành một lối sống cho quần chúng . Ở Việt Nam cũng thể . Phật - Giáo đã hòa hợp trong cá tính dân tộc ta , đã cùng dân tộc ta xây dựng một văn hóa quốc gia độc - lập .</p>\n",
    "  </block>\n",
    "  <block>\n",
    "    <p>3</p>\n",
    "  </block>\n",
    "</document>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
