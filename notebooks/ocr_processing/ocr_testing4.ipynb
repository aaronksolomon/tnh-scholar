{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from xml.sax.saxutils import escape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = vision.ImageAnnotatorClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_processed_pdf(directory, filename):\n",
    "    annotated_images = []\n",
    "    unannotated_images = []  # Unannotated images\n",
    "    text_pages = []\n",
    "    word_locations_list = []\n",
    "\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    print(f\"beginning to process: {file_path}\")\n",
    "\n",
    "    # Load PDF and extract image bytes from a given page\n",
    "    doc = fitz.open(file_path)\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        print(f\"processing page: {page_num}...\")\n",
    "        \n",
    "        page = doc.load_page(page_num)  # Load the first page\n",
    "        images = page.get_images(full=True)\n",
    "        xref = images[0][0]  # Get the first image reference\n",
    "        base_image = doc.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]  # Get the raw JPEG byte content\n",
    "\n",
    "        # Convert to PIL Image for further processing if needed\n",
    "        pil_image = Image.open(io.BytesIO(image_bytes))\n",
    "        \n",
    "        # Save unannotated image before drawing\n",
    "        unannotated_images.append(pil_image.copy())\n",
    "\n",
    "        # Annotate with vision\n",
    "        image = vision.Image(content=image_bytes)\n",
    "        response = client.text_detection(image=image)\n",
    "        text_annotations = response.text_annotations\n",
    "\n",
    "        # Build labeled image\n",
    "        font_size = 15  \n",
    "        font_path=\"/System/Library/Fonts/Supplemental/Arial.ttf\"\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "        draw = ImageDraw.Draw(pil_image)\n",
    "        for i, text_obj in enumerate(text_annotations):\n",
    "            # Get the bounding box vertices\n",
    "            vertices = [(vertex.x, vertex.y) for vertex in text_obj.bounding_poly.vertices]\n",
    "\n",
    "            # Draw a polygon based on the bounding box\n",
    "            if len(vertices) == 4:\n",
    "                draw.polygon(vertices, outline=\"red\", width=2)\n",
    "                if i > 0: # first bounding box is whole text region\n",
    "                    draw.text(vertices[0], text_obj.description, fill=\"red\", font=font)\n",
    "\n",
    "        full_page_text = text_annotations[0].description  # always the first text_annotation object\n",
    "        word_locations = text_annotations[1:]\n",
    "        word_locations_list.append(word_locations)\n",
    "        text_pages.append(full_page_text)\n",
    "\n",
    "\n",
    "    return text_pages, word_locations_list, annotated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def save_annotated_text_and_image_data(root_dir, data_location, text_data, image_data):\n",
    "    \"\"\"\n",
    "    Creates a directory structure to save images, text data, and an HTML gallery for viewing.\n",
    "    \n",
    "    Parameters:\n",
    "        root_dir (str): The root directory to store data.\n",
    "        data_location (str): Directory location under root_dir for storing files.\n",
    "        text_data (list of str): List of extracted text for each page.\n",
    "        image_data (list of tuples): List of tuples, where each tuple contains the image filename (str) \n",
    "                                     and the image data (as bytes or PIL image).\n",
    "                                     \n",
    "    \"\"\"\n",
    "    # Create directory paths\n",
    "    base_dir = os.path.join(root_dir, data_location)\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    images_dir = os.path.join(base_dir, \"images\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    \n",
    "    # File paths for JSONL and HTML files\n",
    "    jsonl_file_path = os.path.join(base_dir, \"extracted_text.jsonl\")\n",
    "    html_file_path = os.path.join(base_dir, \"index.html\")\n",
    "    \n",
    "    # Step 1: Save text data to JSONL\n",
    "    with open(jsonl_file_path, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "        for i, text in enumerate(text_data):\n",
    "            entry = {\n",
    "                \"image\": f\"images/page{i+1}.jpg\",  # Image filename\n",
    "                \"text\": text\n",
    "            }\n",
    "            jsonl_file.write(json.dumps(entry) + \"\\n\")\n",
    "    \n",
    "    # Step 2: Save images to the images directory\n",
    "    for i, img in enumerate(image_data):\n",
    "        image_path = os.path.join(images_dir, f\"page{i+1}.jpg\")\n",
    "        img.save(image_path, format=\"JPEG\")\n",
    "    \n",
    "    # Step 3: Generate HTML file for viewing\n",
    "    with open(html_file_path, \"w\", encoding=\"utf-8\") as html_file:\n",
    "        html_file.write(\"<html><head><title>Annotated Image Gallery</title></head><body>\\n\")\n",
    "        html_file.write(\"<h1>Annotated Image Gallery</h1>\\n\")\n",
    "        \n",
    "        for i, text in enumerate(text_data):\n",
    "            img_filename = f\"images/page{i+1}.jpg\"\n",
    "            html_file.write(\"<div class='image-entry'>\\n\")\n",
    "            html_file.write(f\"  <img src='{img_filename}' alt='Page {i+1}' style='width:100%; max-width:600px;'>\\n\")\n",
    "            html_file.write(f\"  <textarea readonly style='width:100%; height:150px;'>{text}</textarea>\\n\")\n",
    "            html_file.write(\"</div><br>\\n\")\n",
    "        \n",
    "        html_file.write(\"</body></html>\")\n",
    "\n",
    "    print(f\"Data successfully saved in {base_dir}\")\n",
    "\n",
    "# Example usage:\n",
    "# save_annotated_text_and_image_data(root_dir=\"my_data\", data_location=\"ocr_project\", text_data=[\"text1\", \"text2\"], image_data=[(\"img1.jpg\", img_bytes), (\"img2.jpg\", img_bytes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_xml_text_file(root_dir, title_file, text_data):\n",
    "    \"\"\"\n",
    "    Generates and saves an XML file with a title and pages, where each page contains text data.\n",
    "    \n",
    "    Parameters:\n",
    "        root_dir (str): The root directory where the XML file will be saved.\n",
    "        title_file (str): The title to be used in the XML file and filename.\n",
    "        text_data (list of str): List of extracted text for each page.\n",
    "    \"\"\"\n",
    "    # Define the XML file path\n",
    "    xml_file_path = os.path.join(root_dir, title_file, f\"full_OCR_text_{title_file}.xml\")\n",
    "    os.makedirs(os.path.dirname(xml_file_path), exist_ok=True)\n",
    "    \n",
    "    # Start writing the XML file\n",
    "    with open(xml_file_path, \"w\", encoding=\"utf-8\") as xml_file:\n",
    "        # Write XML declaration and root element with title\n",
    "        xml_file.write(\"<?xml version='1.0' encoding='UTF-8'?>\\n\")\n",
    "        xml_file.write(f\"<document>\\n  <title>{escape(title_file)}</title>\\n\")\n",
    "        \n",
    "        # Add each page with its content, escaping the text for XML safety\n",
    "        for i, text in enumerate(text_data):\n",
    "            # Escape text content to handle special characters\n",
    "            escaped_text = escape(text)\n",
    "            # Write each page as <page page=\"1\">...</page>\n",
    "            xml_file.write(f\"  <page page='{i+1}'>\\n\")\n",
    "            xml_file.write(f\"    {escaped_text}\\n\")\n",
    "            xml_file.write(\"  </page>\\n\")\n",
    "        \n",
    "        # Close the root element\n",
    "        xml_file.write(\"</document>\")\n",
    "\n",
    "    print(f\"XML file successfully saved at {xml_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pdf_process(pdf_dir, pdf_filename, output_dir):\n",
    "    current_title, ext = os.path.splitext(pdf_filename)\n",
    "    assert ext == \".pdf\", f\"Expected .pdf, got {ext}\"\n",
    "    text_pages, marked_images = build_processed_pdf(pdf_dir, pdf_filename)\n",
    "    save_annotated_text_and_image_data(output_dir, current_title, text_pages, marked_images)\n",
    "    save_xml_text_file(output_dir, current_title, text_pages)\n",
    "    return text_pages, marked_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to read pdfs from\n",
    "pdf_dir = \"../PDF/Phat_Giao_journals\"\n",
    "\n",
    "# directory to save output\n",
    "output_dir = \"../processed_journal_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, words, images = build_processed_pdf(pdf_dir, \"phat-giao-viet-nam-1956-01.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(words[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_page_words(word_info):\n",
    "    \n",
    "    converted_info = []\n",
    "    for word in word_info:\n",
    "        text = word.description\n",
    "        x = [vertex.x for vertex in word.bounding_poly.vertices]\n",
    "        y = [vertex.y for vertex in word.bounding_poly.vertices]\n",
    "        assert len(x) == 4, \"not four pointed bounding box.\"\n",
    "\n",
    "        width_bottom = x[1] - x[0] # point order is: bottom-left, bottom-right, top-right, top-left\n",
    "        width_top = x[2] - x[3]\n",
    "        height_left = y[3] - y[0]\n",
    "        height_right = y[2] - y[1] # this according to the layout of bounding poly\n",
    "\n",
    "        if width_bottom != width_top:\n",
    "            print(f\"Warning bounding box width mismatch for: {text}, skipping.\")\n",
    "            continue\n",
    "        if height_left != height_right:\n",
    "            print(f\"Warning bounding box height mismatch for: {text}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        new_info = [text, (x[0], y[0]), width_bottom, height_left]\n",
    "        converted_info.append(new_info)\n",
    "    \n",
    "    return converted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_info = convert_page_words(words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_lines(words):\n",
    "    \"\"\"\n",
    "    Collate OCR words into lines based on y-coordinates and line height, \n",
    "    assuming the words are already sorted by their y-coordinates.\n",
    "\n",
    "    Args:\n",
    "        words (list): List of words with their positions and dimensions \n",
    "                      in the form [text, (x, y), width, height].\n",
    "    \n",
    "    Returns:\n",
    "        list: Collated lines in the form \n",
    "              [text_line, (start_x, start_y), (end_x, start_y), line_height].\n",
    "    \"\"\"\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    # Sort words by y-coordinate (descending for top-to-bottom layout)\n",
    "    # words = sorted(words, key=lambda w: (w[1][1], w[1][0]))\n",
    "        \n",
    "    collated_lines = []\n",
    "    current_line = []\n",
    "    current_y = words[0][1][1]\n",
    "    current_height = words[0][3]\n",
    "\n",
    "    for word in words:\n",
    "        text, (x, y), width, height = word\n",
    "        # Calculate tolerance for y-coordinate based on current line height\n",
    "        tolerance = current_height * 0.75\n",
    "        \n",
    "        # Check if the word fits in the current line\n",
    "        if abs(y - current_y) <= tolerance:\n",
    "            current_line.append(word)\n",
    "        else:\n",
    "            # Finalize the current line\n",
    "            if current_line:\n",
    "                line_text = \" \".join(w[0] for w in current_line)\n",
    "                start_x = current_line[0][1][0]\n",
    "                start_y = current_line[0][1][1]\n",
    "                end_x = current_line[-1][1][0] + current_line[-1][2]\n",
    "                collated_lines.append([line_text, (start_x, start_y), (end_x, start_y), current_height])\n",
    "            \n",
    "            # Start a new line\n",
    "            current_line = [word]\n",
    "            current_y = y\n",
    "            current_height = height\n",
    "    \n",
    "    # Add the last line\n",
    "    if current_line:\n",
    "        line_text = \" \".join(w[0] for w in current_line)\n",
    "        start_x = current_line[0][1][0]\n",
    "        start_y = current_line[0][1][1]\n",
    "        end_x = current_line[-1][1][0] + current_line[-1][2]\n",
    "        collated_lines.append([line_text, (start_x, start_y), (end_x, start_y), current_height])\n",
    "    \n",
    "    return collated_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_lines(new_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pages, marked_images = full_pdf_process(pdf_dir, \"phat-giao-viet-nam-1956-25-26.pdf\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files =  os.listdir(pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_list = [all_files[2]] + all_files[6:16] + all_files[17:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in px_list:\n",
    "        #print(filename)  # Do something with each file path\n",
    "        full_pdf_process(pdf_dir, filename, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
