{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from data_processing.gpt_processing import (\n",
    "    generate_messages, \n",
    "    create_jsonl_file_for_batch, \n",
    "    start_batch,\n",
    "    start_batch_with_retries, \n",
    "    get_batch_response,\n",
    "    get_completed_batches,\n",
    "    set_model_settings,\n",
    "    get_batch_status,\n",
    "    get_active_batches,\n",
    "    get_all_batch_info,\n",
    "    token_count,\n",
    "    run_immediate_chat_process,\n",
    "    run_single_batch,\n",
    "    get_last_batch_response,\n",
    ")\n",
    "\n",
    "from data_processing.xml_processing import ( \n",
    "    save_pages_to_xml,\n",
    "    split_xml_on_pagebreaks,\n",
    "    join_xml_data_to_doc\n",
    ")\n",
    "\n",
    "from data_processing.text_processing import (\n",
    "    get_text_from_file,\n",
    "    write_text_to_file\n",
    ")\n",
    "%aimport time\n",
    "%aimport json\n",
    "%aimport datetime\n",
    "%aimport logging\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from math import floor\n",
    "from datetime import datetime\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "project_dir = Path(\"/Users/phapman/Desktop/tnh-scholar/\")\n",
    "data_dir = project_dir / \"data_processing\"\n",
    "journal_dir = data_dir / \"processed_journal_data\"\n",
    "journal_name = \"phat-giao-viet-nam-1956-02\"\n",
    "working_dir = journal_dir / journal_name\n",
    "input_xml = working_dir / f\"full_cleaned_{journal_name}.xml\"\n",
    "translation_xml_path = working_dir / f\"translation_{journal_name}.xml\"\n",
    "section_batch_jsonl = working_dir / \"section_batch.jsonl\"\n",
    "translate_batch_jsonl = working_dir / \"translation_batch.jsonl\"\n",
    "section_metadata_out = working_dir / \"section_metadata.json\"\n",
    "raw_json_metadata_path = working_dir / \"raw_metadata_response.txt\"\n",
    "logfile = data_dir / \"gpt_processing\" / \"processing_info.log\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MAX_TOKEN_LIMIT = 20000\n",
    "MAX_BATCH_RETRIES = 20  # Number of retries\n",
    "BATCH_RETRY_DELAY = 5  # seconds to wait before retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the logger\n",
    "def setup_logger(log_file_path):\n",
    "    \"\"\"\n",
    "    Configures the logger to write to a log file and the console.\n",
    "    \"\"\"\n",
    "    # Remove existing handlers\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",  # Include logger name\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file_path, encoding=\"utf-8\"),\n",
    "            logging.StreamHandler()  # Optional: to log to the console as well\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Suppress DEBUG/INFO logs for specific noisy modules\n",
    "    modules_to_suppress = [\"httpx\", \"httpcore\", \"urllib3\", \"openai\"]\n",
    "    for module in modules_to_suppress:\n",
    "        logger = logging.getLogger(module)\n",
    "        logger.setLevel(logging.WARNING)  # Suppress DEBUG and INFO logs\n",
    "\n",
    "    \n",
    "    return logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_logger(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings = {\n",
    "    \"gpt-4o\": {\n",
    "        \"max_tokens\": 5000,\n",
    "        \"context_limit\": 20000,  # Total context limit for the model\n",
    "        \"temperature\": 0.25\n",
    "    }\n",
    "}\n",
    "set_model_settings(model_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_section = \"\"\"\n",
    "You are a highly skilled assistant processing a Vietnamese Buddhist journal scanned from OCR. Use the title: \"Journal of Vietnamese Buddhism.\"\n",
    "You will be determining the journal sections by page number. You will also generate metadata for the full text and each section. \n",
    "You will return this metadata in JSON format.\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the text and divide it into sections based on logical breaks, such as headings, topic changes, or clear shifts in content.\n",
    "2. Ensure every page is part of  a section, even if that section is titled \"blank page\" or \"title page,\" for example.\n",
    "3. For each section, provide:\n",
    "   - The original title in Vietnamese (`section_title_vi`).\n",
    "   - The translated title in English (`section_title_en`).\n",
    "   - The author's name if it is available (`section_author`). \n",
    "   - A one-paragraph summary of the section in English (`section_summary`).\n",
    "   - A list of keywords for the section that are related to its content, these can be proper names, specific concepts, or contextual information.\n",
    "   - The section's start and end page numbers (`start_page` and `end_page`).\n",
    "   - Use \"null\" for any data that is not available (such as author name) for the section.\n",
    "\n",
    "4. Return the output as a JSON object with the following schema:\n",
    "{\n",
    "    \"journal_summary\": \"A one-page summary of the whole journal in English.\",\n",
    "    \"sections\": [\n",
    "        {\n",
    "            \"title_vi\": \"Original title in Vietnamese\",\n",
    "            \"title_en\": \"Translated title in English\",\n",
    "            \"author\": \"Name of the author of the section\",\n",
    "            \"summary\": \"One-paragraph summary of the section in English\",\n",
    "            \"keywords\": \"A list of keywords for the section\",\n",
    "            \"start_page\":  X,\n",
    "            \"end_page\":  Y\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\n",
    "5.  Ensure the JSON is well-formed and adheres strictly to the provided schema.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_translate = \"\"\"\n",
    "You are Thich Nhat Hanh translating from Vietnamese to English for your experienced students. \n",
    "The text is based on an OCR scan of a journal you edited from 1956-1958. Use the title: \"Journal of Vietnamese Buddhism\" for the journal when it is referenced.\n",
    "You will be translating a single section of the journal and will be provided with the section title in English. \n",
    "You want your students to understand the text in its larger historical context, in the context of Vietnamese Buddhism, and in the context of your own life.\n",
    "Translate for the most meaningful, typical, and eloquent English interpretation that is simple, yet poetic. Translate literally, don't add any content. \n",
    "Notes on the text can be added in the <notes>.\n",
    "Make corrections in the text only where necessary (for example if words are missing) to create logical flow. Note all corrections in the <translation-notes>. \n",
    "Do not change <pagebreak> tag postioning. Each translated page must match its original page source as pages will be studied side by side with the original Vietnamese.\n",
    "Infer paragraphs and text structure from the text layout.\n",
    "Add XML tags for clarity, using only the following tags: \n",
    "\n",
    "   <section> for major sections.\n",
    "   <subsection> for subsections.\n",
    "   <title> for main titles of sections and subsections. \n",
    "   <subtitle> for subtitles of sections and subsections. \n",
    "   <heading> for headings that do not mark titles or subtitles\n",
    "   <p> for paragraphs.\n",
    "   <br/> for linebreaks that add meaning such as in poems or other structures.\n",
    "   <TOC> for tables of contents\n",
    "   <author> for authors of sections or subsections\n",
    "   <ol> <ul> <li> for lists\n",
    "   <i> for italics. \n",
    "   <b> for bold.\n",
    "   <notes>\n",
    "   <translation-notes>\n",
    "\n",
    "You may use <notes> at the end of the section for notes on historical, cultural, spiritual, or other interesting elements of the text.\n",
    "You may add <translation-notes> at the end of the section as a commentary to summarize your translation choices. \n",
    "For <translation-notes>, you may include information on Sino-Vietnamese, complex, unusual, poetic, or other interesting terms, and significant corrections to the text. \n",
    "In the <translation-notes> include the original Vietnamese terms for reference.\n",
    "\n",
    "IMPORTANT: All titles, XML sections, text, and terms should be translated. Do not however, translate names of people; leave names in Vietnamese with diacritics.\n",
    "IMPORTANT: Return pure XML with no formatting marks such as xml or ```.\n",
    "IMPORTANT: The returned XML should begin and end with <section> tags.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def deserialize_json(serialized_data: str):\n",
    "    \"\"\"\n",
    "    Converts a serialized JSON string into a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "        serialized_data (str): The JSON string to deserialize.\n",
    "\n",
    "    Returns:\n",
    "        dict: The deserialized Python dictionary.\n",
    "    \"\"\"\n",
    "    if not isinstance(serialized_data, str):\n",
    "        logger.error(f\"String input required for deserialize_json. Received: {type(serialized_data)}\")\n",
    "        raise ValueError(\"String input required.\")\n",
    "\n",
    "    try:\n",
    "        # Convert the JSON string into a dictionary\n",
    "        return json.loads(serialized_data)\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to deserialize JSON: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "journal_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"journal_summary\": {\"type\": \"string\"},\n",
    "        \"sections\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"title_vi\": {\"type\": \"string\"},\n",
    "                    \"title_en\": {\"type\": \"string\"},\n",
    "                    \"author\": {\"type\": [\"string\", \"null\"]},\n",
    "                    \"summary\": {\"type\": \"string\"},\n",
    "                    \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"start_page\": {\"type\": \"integer\", \"minimum\": 1},\n",
    "                    \"end_page\": {\"type\": \"integer\", \"minimum\": 1}\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"title_vi\",\n",
    "                    \"title_en\",\n",
    "                    \"summary\",\n",
    "                    \"keywords\",\n",
    "                    \"start_page\",\n",
    "                    \"end_page\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"journal_summary\", \"sections\"]\n",
    "}\n",
    "\n",
    "def validate_and_clean_data(data, schema):\n",
    "    \"\"\"\n",
    "    Recursively validate and clean AI-generated data to fit the given schema.\n",
    "    Any missing fields are filled with defaults, and extra fields are ignored.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The AI-generated data to validate and clean.\n",
    "        schema (dict): The schema defining the required structure.\n",
    "\n",
    "    Returns:\n",
    "        dict: The cleaned data adhering to the schema.\n",
    "    \"\"\"\n",
    "    def clean_value(value, field_schema):\n",
    "        \"\"\"\n",
    "        Clean a single value based on its schema, attempting type conversions where necessary.\n",
    "        \"\"\"\n",
    "        field_type = field_schema[\"type\"]\n",
    "\n",
    "        # Handle type: string\n",
    "        if field_type == \"string\":\n",
    "            if isinstance(value, str):\n",
    "                return value\n",
    "            elif value is not None:\n",
    "                return str(value)\n",
    "            return \"unset\"\n",
    "\n",
    "        # Handle type: integer\n",
    "        elif field_type == \"integer\":\n",
    "            if isinstance(value, int):\n",
    "                return value\n",
    "            elif isinstance(value, str) and value.isdigit():\n",
    "                return int(value)\n",
    "            try:\n",
    "                return int(float(value))  # Handle cases like \"2.0\"\n",
    "            except (ValueError, TypeError):\n",
    "                return 0\n",
    "\n",
    "        # Handle type: array\n",
    "        elif field_type == \"array\":\n",
    "            if isinstance(value, list):\n",
    "                item_schema = field_schema.get(\"items\", {})\n",
    "                return [clean_value(item, item_schema) for item in value]\n",
    "            elif isinstance(value, str):\n",
    "                # Try splitting comma-separated strings into a list\n",
    "                return [v.strip() for v in value.split(\",\")]\n",
    "            return []\n",
    "\n",
    "        # Handle type: object\n",
    "        elif field_type == \"object\":\n",
    "            if isinstance(value, dict):\n",
    "                return validate_and_clean_data(value, field_schema)\n",
    "            return {}\n",
    "\n",
    "        # Handle nullable strings\n",
    "        elif field_type == [\"string\", \"null\"]:\n",
    "            if value is None or isinstance(value, str):\n",
    "                return value\n",
    "            return str(value)\n",
    "\n",
    "        # Default case for unknown or unsupported types\n",
    "        return \"unset\"\n",
    "\n",
    "    def clean_object(obj, obj_schema):\n",
    "        \"\"\"\n",
    "        Clean a dictionary object based on its schema.\n",
    "        \"\"\"\n",
    "        if not isinstance(obj, dict):\n",
    "            print(f\"Expected dict but got: \\n{type(obj)}: {obj}\\nResetting to empty dict.\")\n",
    "            return {}\n",
    "        cleaned = {}\n",
    "        properties = obj_schema.get(\"properties\", {})\n",
    "        for key, field_schema in properties.items():\n",
    "            # Set default value for missing fields\n",
    "            cleaned[key] = clean_value(obj.get(key), field_schema)\n",
    "        return cleaned\n",
    "\n",
    "    # Handle the top-level object\n",
    "    if schema[\"type\"] == \"object\":\n",
    "        cleaned_data = clean_object(data, schema)\n",
    "        return cleaned_data\n",
    "    else:\n",
    "        raise ValueError(\"Top-level schema must be of type 'object'.\")\n",
    "\n",
    "def validate_and_save_metadata(output_file_path: Path, json_metadata_serial: str, schema):\n",
    "    \"\"\"\n",
    "    Validates and cleans journal data against the schema, then writes it to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data (str): The journal data as a serialized JSON string to validate and clean.\n",
    "        schema (dict): The schema defining the required structure.\n",
    "        output_file_path (str): Path to the output JSON file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if successfully written to the file, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean the data to fit the schema\n",
    "        data = deserialize_json(json_metadata_serial)\n",
    "        cleaned_data = validate_and_clean_data(data, schema)\n",
    "\n",
    "        # Write the parsed data to the specified JSON file\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "        logger.info(f\"Parsed and validated metadata successfully written to {output_file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during validation or writing: {e}\")\n",
    "        raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_groups_from_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Extracts page groups from the section metadata for use with `split_xml_pages`.\n",
    "\n",
    "    Parameters:\n",
    "        metadata (dict): The section metadata containing sections with start and end pages.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: A list of tuples, each representing a page range (start_page, end_page).\n",
    "    \"\"\"\n",
    "    page_groups = []\n",
    "\n",
    "    # Ensure metadata contains sections\n",
    "    if \"sections\" not in metadata or not isinstance(metadata[\"sections\"], list):\n",
    "        raise ValueError(\"Metadata does not contain a valid 'sections' key with a list of sections.\")\n",
    "\n",
    "    for section in metadata[\"sections\"]:\n",
    "        try:\n",
    "            # Extract start and end pages\n",
    "            start_page = section.get(\"start_page\")\n",
    "            end_page = section.get(\"end_page\")\n",
    "\n",
    "            # Ensure both start_page and end_page are integers\n",
    "            if not isinstance(start_page, int) or not isinstance(end_page, int):\n",
    "                raise ValueError(f\"Invalid page range in section: {section}\")\n",
    "\n",
    "            # Add the tuple to the page groups list\n",
    "            page_groups.append((start_page, end_page))\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in section metadata: {e}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing section metadata: {e}\")\n",
    "\n",
    "    logger.debug(f\"page groups found: {page_groups}\")\n",
    "\n",
    "    return page_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sectioning(input_xml_path, journal_name):\n",
    "    \"\"\"\n",
    "    Splits the journal content into sections using GPT, with retries for both starting and completing the batch.\n",
    "\n",
    "    Args:\n",
    "        input_xml_path (str): Path to the input XML file.\n",
    "        output_json_path (str): Path to save validated metadata JSON.\n",
    "        raw_output_path (str): Path to save the raw batch results.\n",
    "        journal_name (str): Name of the journal being processed.\n",
    "        max_retries (int): Maximum number of retries for batch processing.\n",
    "        retry_delay (int): Delay in seconds between retries.\n",
    "\n",
    "    Returns:\n",
    "        str: the result of the batch sectioning process as a serialized json object. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Starting sectioning batch for {journal_name} with file:\\n\\t{input_xml_path}\")\n",
    "        # Load journal content\n",
    "        journal_pages = get_text_from_file(input_xml_path)\n",
    "\n",
    "        # Create GPT messages for sectioning\n",
    "        user_message_wrapper = lambda text: f\"{text}\"\n",
    "        messages = generate_messages(system_message_section, user_message_wrapper, [journal_pages])\n",
    "\n",
    "        # Create JSONL file for batch processing\n",
    "        jsonl_file = create_jsonl_file_for_batch(messages, section_batch_jsonl, json_mode=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Failed to initialize batch sectioning data for journal '{journal_name}'.\",\n",
    "            extra={\"input_xml_path\": input_xml_path},\n",
    "            exc_info=True\n",
    "        )\n",
    "        raise RuntimeError(f\"Error initializing batch sectioning data for journal '{journal_name}'.\") from e\n",
    "\n",
    "    response = start_batch_with_retries(jsonl_file, description=f\"Batch for sectioning journal: {journal_name} | input file: {input_xml_path}\")\n",
    "    \n",
    "    if response:\n",
    "        json_result = response[0]  # should return json, just one batch so first response\n",
    "        # Log success and return output json\n",
    "        logger.info(f\"Successfully batch sectioned journal '{journal_name}' with input file: {input_xml_path}.\")\n",
    "        return json_result\n",
    "    else:\n",
    "        logger.error(\"Section batch failed to get response.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sectioning_data(output_json_path: Path, raw_output_path: Path, serial_json: str):\n",
    "    try:\n",
    "        write_text_to_file(raw_output_path, serial_json, force=True)\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Failed to write raw response file for journal '{journal_name}'.\",\n",
    "            extra={\"raw_output_path\": raw_output_path},\n",
    "            exc_info=True\n",
    "        )\n",
    "        raise RuntimeError(f\"Failed to write raw response file for journal '{journal_name}'.\") from e\n",
    "\n",
    "    # Validate and save metadata\n",
    "    try:\n",
    "        valid = validate_and_save_metadata(output_json_path, serial_json, journal_schema)\n",
    "        if not valid:\n",
    "            raise RuntimeError(f\"Validation failed for metadata of journal '{journal_name}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error occurred while validating and saving metadata for journal '{journal_name}'.\",\n",
    "            extra={\"output_json_path\": output_json_path},\n",
    "            exc_info=True\n",
    "        )\n",
    "        raise RuntimeError(f\"Validation error for journal '{journal_name}'.\") from e\n",
    "\n",
    "    return output_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_data_for_tx_batch(section_data_to_send, max_token_list):\n",
    "    \n",
    "    max_retries = MAX_BATCH_RETRIES\n",
    "    retry_delay = BATCH_RETRY_DELAY\n",
    "\n",
    "    # Build file for batch translation processing:\n",
    "    try:\n",
    "        # Create GPT messages for translation\n",
    "        user_message_wrapper = lambda section_info: f\"Translate this section with title '{section_info.title}':\\n{section_info.content}\"\n",
    "        messages = generate_messages(system_message_translate, user_message_wrapper, section_data_to_send)\n",
    "\n",
    "        # Create batch file\n",
    "        jsonl_file = create_jsonl_file_for_batch(messages, translate_batch_jsonl, max_token_list=max_token_list)\n",
    "        if not jsonl_file:\n",
    "            raise RuntimeError(\"Failed to create JSONL file for translation batch.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating JSONL file for journal '{journal_name}'.\", exc_info=True)\n",
    "        raise RuntimeError(\"Error creating JSONL file for translation batch.\") from e\n",
    "\n",
    "    translation_data = start_batch_with_retries(jsonl_file, description=f\"Batch for translating journal '{journal_name}'\")\n",
    "    \n",
    "    logger.info(f\"Successfully translated section batch.\")\n",
    "\n",
    "    return translation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sections(section_contents, section_metadata):        \n",
    "    \"\"\"build up sections in batches to translate \"\"\"\n",
    "\n",
    "    section_mdata = section_metadata['sections']\n",
    "    if len(section_contents) != len(section_mdata):\n",
    "            raise RuntimeError(\"Section length mismatch.\")\n",
    "    \n",
    "    #collate metadata and section content, calculate max_tokens per section:\n",
    "    section_data_to_send = []    \n",
    "    max_token_list = []\n",
    "    current_token_count = 0\n",
    "    collected_translations = []\n",
    "    section_last_index = len(section_mdata) - 1\n",
    "\n",
    "    for i, section_info in enumerate(section_mdata):\n",
    "        section_content = section_contents[i]\n",
    "        max_tokens = floor(token_count(section_content) * 1.3) + 1000\n",
    "        max_token_list.append(max_tokens)\n",
    "        current_token_count += max_tokens\n",
    "        section_data = SimpleNamespace(\n",
    "            title=section_info[\"title_en\"], \n",
    "            content=section_content\n",
    "        )\n",
    "        section_data_to_send.append(section_data)\n",
    "        logger.debug(f\"section {i}: {section_data.title} added for batch processing.\")\n",
    "\n",
    "        if current_token_count >= MAX_TOKEN_LIMIT or i == section_last_index:\n",
    "             # send sections for batch processing since token limit reached.\n",
    "             batch_result = send_data_for_tx_batch(section_data_to_send, max_token_list)\n",
    "             collected_translations.extend(batch_result)\n",
    "\n",
    "            # reset containers to start building up next batch.\n",
    "             section_data_to_send = []\n",
    "             max_token_list = []\n",
    "             current_token_count = 0\n",
    "    \n",
    "    return collected_translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Translation\n",
    "def batch_translate(input_xml_path, metadata_path, journal_name, max_retries=MAX_BATCH_RETRIES, retry_delay=BATCH_RETRY_DELAY):\n",
    "    \"\"\"\n",
    "    Translates the journal sections using the GPT model.\n",
    "    Saves the translated content back to XML.\n",
    "\n",
    "    Args:\n",
    "        input_xml_path (str): Path to the input XML file.\n",
    "        metadata_path (str): Path to the metadata JSON file.\n",
    "        journal_name (str): Name of the journal.\n",
    "        xml_output_path (str): Path to save the translated XML.\n",
    "        max_retries (int): Maximum number of retries for batch operations.\n",
    "        retry_delay (int): Delay in seconds between retries.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the process succeeds, False otherwise.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting translation batch for journal '{journal_name}':\\n\\twith file: {input_xml_path}\\n\\tmetadata: {metadata_path}\")\n",
    "    \n",
    "    # Data initialization:\n",
    "    try:\n",
    "        # load metadata\n",
    "        serial_json = get_text_from_file(metadata_path)\n",
    "\n",
    "        section_metadata = deserialize_json(serial_json)\n",
    "        if not section_metadata:\n",
    "            raise RuntimeError(f\"Metadata could not be loaded from {metadata_path}.\")\n",
    "\n",
    "        # Extract page groups and split XML content\n",
    "        page_groups = extract_page_groups_from_metadata(section_metadata)\n",
    "        xml_content = get_text_from_file(input_xml_path)\n",
    "        section_contents = split_xml_on_pagebreaks(xml_content, page_groups)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize data for translation batching for journal '{journal_name}'.\", exc_info=True)\n",
    "        raise RuntimeError(f\"Error during data initialization for journal '{journal_name}'.\") from e\n",
    "        \n",
    "    translation_data = translate_sections(section_contents, section_metadata)\n",
    "    return translation_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_translation_data(xml_output_path: Path, translation_data):\n",
    "# Save translated content back to XML\n",
    "    try:\n",
    "        logger.info(f\"Saving translated content to XML for journal '{journal_name}'.\")\n",
    "        join_xml_data_to_doc(xml_output_path, translation_data, overwrite=True)\n",
    "        logger.info(f\"Translated journal saved successfully to:\\n\\t{xml_output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Failed to save translation data for journal '{journal_name}'.\",\n",
    "            extra={\"xml_output_path\": xml_output_path},\n",
    "            exc_info=True\n",
    "        )\n",
    "        raise RuntimeError(f\"Failed to save translation data for journal '{journal_name}'.\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_xml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sectioning\n",
    "metadata_serial_json = batch_sectioning(input_xml, journal_name)\n",
    "metadata_path = save_sectioning_data(section_metadata_out, raw_json_metadata_path, metadata_serial_json)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings = {\n",
    "    \"gpt-4o\": {\n",
    "        \"max_tokens\": 5000,\n",
    "        \"context_limit\": 20000,  # Total context limit for the model\n",
    "        \"temperature\": 0.75\n",
    "    }\n",
    "}\n",
    "set_model_settings(model_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metadata_path:\n",
    "    translation_data = batch_translate(input_xml, metadata_path, journal_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_translation_data(translation_xml_path, translation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_last_batch_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_and_save_metadata(section_metadata_out, result[0], journal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_sectioning(input_xml_path, output_json_path, raw_output_path, journal_name, max_retries=MAX_BATCH_RETRIES, retry_delay=BATCH_RETRY_DELAY):\n",
    "#     \"\"\"\n",
    "#     Splits the journal content into sections using GPT, with retries for both starting and completing the batch.\n",
    "#     \"\"\"\n",
    "#     journal_pages = get_text_from_file(input_xml_path)\n",
    "\n",
    "#     # Create GPT messages for sectioning\n",
    "#     user_message_wrapper = lambda text: f\"{text}\"\n",
    "#     messages = generate_messages(system_message_section, user_message_wrapper, [journal_pages])\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, section_batch_jsonl, json_mode=True)\n",
    "\n",
    "#     for attempt in range(max_retries):\n",
    "#         try:\n",
    "#             # Try to start the batch\n",
    "#             batch = start_batch(jsonl_file, description=f\"Batch for sectioning journal: {journal_name} | input file: {input_xml_path}\")\n",
    "#             batch_id = batch.id\n",
    "#             if not batch_id:\n",
    "#                 raise RuntimeError(\"Batch started but no ID was returned.\")\n",
    "\n",
    "#             print(f\"Batch for sectioning started successfully on attempt {attempt + 1}. ID: {batch_id}\")\n",
    "\n",
    "#             # Poll for batch completion\n",
    "#             json_results = poll_batch_for_response(batch_id)\n",
    "#             if json_results:\n",
    "#                 break # exit retry loop\n",
    "#             else:\n",
    "#                 raise RuntimeError(\"Unknown error in polling for batch response.\", exc_info=True)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Attempt {attempt + 1} failed: {e}. Retrying batch process in {retry_delay} seconds...\")\n",
    "#             time.sleep(retry_delay)\n",
    "#     else:\n",
    "#         logger.error(\"Failed to complete batch sectioning after maximum retries.\")\n",
    "#         raise RuntimeError(\"Error: Failed to complete batch sectioning after maximum retries.\")\n",
    "\n",
    "#     # save raw result\n",
    "#     try:\n",
    "#         write_text_to_file(raw_output_path, json_results, force=True)\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"failed to write raw response file: {raw_output_path}\")\n",
    "#         raise\n",
    "\n",
    "#     # If successful, try to validate and save metadata and exit loop\n",
    "#     try:\n",
    "#         valid = validate_and_save_metadata(output_json_path, json_results, journal_schema)\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error occurred while validating and saving metadata for journal {journal_name}: '{output_json_path}' (batch ID: {batch_id}).\", exc_info=True)\n",
    "#         raise\n",
    "    \n",
    "#     if valid:\n",
    "#         logger.info(f\"Successfully processed {journal_name}: {input_xml_path} with batch: {batch_id} and saved metadata to {output_json_path} \")\n",
    "#         return output_json_path\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Sectioning\n",
    "# def batch_sectioning(input_xml_path, output_xml_path):\n",
    "#     \"\"\"\n",
    "#     Splits the journal content into sections using the GPT model.\n",
    "#     Saves the sectioned content back to XML.\n",
    "#     \"\"\"\n",
    "#     # Load the input XML\n",
    "#     journal_pages = load_xml(input_xml_path)\n",
    "#     pages_content = [page.text for page in journal_pages]\n",
    "\n",
    "#     # Create GPT messages for sectioning\n",
    "#     user_message_wrapper = lambda text: f\"Divide this content into sections:\\n{text}\"\n",
    "#     messages = generate_messages(system_message_section, user_message_wrapper, pages_content)\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, section_batch_jsonl)\n",
    "\n",
    "#     # Start the batch\n",
    "#     batch = start_batch(jsonl_file, description=\"Batch for sectioning journal\")\n",
    "#     batch_id = batch.get(\"id\")\n",
    "#     if not batch_id:\n",
    "#         print(\"Error: Failed to start batch for sectioning.\")\n",
    "#         return None\n",
    "\n",
    "#     print(f\"Batch for sectioning started with ID: {batch_id}\")\n",
    "\n",
    "#     # Poll for batch completion\n",
    "#     results = poll_batch_status(batch_id)\n",
    "#     if not results:\n",
    "#         print(\"Error: Failed to retrieve sectioning batch results.\")\n",
    "#         return None\n",
    "\n",
    "#     # Save sectioned content back to XML\n",
    "#     for i, section_content in enumerate(results):\n",
    "#         journal_pages[i].text = section_content  # Replace original content with sectioned content\n",
    "\n",
    "#     save_xml(journal_pages, output_xml_path)\n",
    "#     print(f\"Sectioned journal saved to {output_xml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from gpt_processing.gpt_interface import (\n",
    "#     set_api_client, \n",
    "#     generate_messages, \n",
    "#     create_jsonl_file_for_batch, \n",
    "#     start_batch, \n",
    "#     get_batch_response\n",
    "# )\n",
    "# from data_processing.xml_processing import (\n",
    "#     load_xml, \n",
    "#     save_xml, \n",
    "#     extract_sections_from_xml\n",
    "# )\n",
    "\n",
    "# # Initialize OpenAI client\n",
    "# set_api_client()\n",
    "\n",
    "# # File paths\n",
    "# INPUT_XML = \"input_journal.xml\"\n",
    "# SECTIONED_XML = \"sectioned_journal.xml\"\n",
    "# TRANSLATED_XML = \"translated_journal.xml\"\n",
    "# BATCH_SECTION_JSONL = \"section_batch.jsonl\"\n",
    "# BATCH_TRANSLATE_JSONL = \"translate_batch.jsonl\"\n",
    "\n",
    "# # System messages\n",
    "# SYSTEM_MESSAGE_SECTION = \"\"\"\n",
    "# You are a helpful assistant. Divide the text into meaningful sections and add XML tags:\n",
    "# <section> for major sections, <subsection> for subsections, <title> for titles, and <p> for paragraphs.\n",
    "# \"\"\"\n",
    "# SYSTEM_MESSAGE_TRANSLATE = \"\"\"\n",
    "# You are Thich Nhat Hanh translating from Vietnamese to English. Provide meaningful translations with appropriate XML tags:\n",
    "# <section>, <subsection>, <title>, <p>.\n",
    "# \"\"\"\n",
    "\n",
    "# # Step 1: Sectioning\n",
    "# def batch_sectioning(input_xml, output_xml):\n",
    "#     # Load the input XML and extract pages or chunks\n",
    "#     journal_pages = load_xml(input_xml)\n",
    "#     pages_content = [page.text for page in journal_pages]  # Assuming .text contains the text of each page\n",
    "\n",
    "#     # Create GPT messages for sectioning\n",
    "#     user_message_wrapper = lambda text: f\"Divide this content into sections:\\n{text}\"\n",
    "#     messages = generate_messages(SYSTEM_MESSAGE_SECTION, user_message_wrapper, pages_content)\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, BATCH_SECTION_JSONL)\n",
    "\n",
    "#     # Start batch\n",
    "#     batch = start_batch(jsonl_file, description=\"Batch for sectioning journal\")\n",
    "#     batch_id = batch.get(\"id\")\n",
    "#     if not batch_id:\n",
    "#         print(\"Error: Failed to start batch for sectioning.\")\n",
    "#         return None\n",
    "\n",
    "#     print(f\"Batch for sectioning started with ID: {batch_id}\")\n",
    "\n",
    "#     # Poll for batch completion and retrieve results\n",
    "#     results = get_batch_response(batch_id)\n",
    "#     if not results:\n",
    "#         print(\"Error: Failed to retrieve sectioning batch results.\")\n",
    "#         return None\n",
    "\n",
    "#     # Save the sectioned content back to XML\n",
    "#     for i, section_content in enumerate(results):\n",
    "#         journal_pages[i].text = section_content  # Replace original content with sectioned content\n",
    "\n",
    "#     save_xml(journal_pages, output_xml)\n",
    "#     print(f\"Sectioned journal saved to {output_xml}\")\n",
    "\n",
    "# # Step 2: Translation\n",
    "# def batch_translation(input_xml, output_xml):\n",
    "#     # Load the sectioned XML and extract sections or chunks for translation\n",
    "#     sections = extract_sections_from_xml(input_xml)\n",
    "\n",
    "#     # Create GPT messages for translation\n",
    "#     user_message_wrapper = lambda section: f\"Translate this section:\\n{section}\"\n",
    "#     messages = generate_messages(SYSTEM_MESSAGE_TRANSLATE, user_message_wrapper, sections)\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, BATCH_TRANSLATE_JSONL)\n",
    "\n",
    "#     # Start batch\n",
    "#     batch = start_batch(jsonl_file, description=\"Batch for translating journal\")\n",
    "#     batch_id = batch.get(\"id\")\n",
    "#     if not batch_id:\n",
    "#         print(\"Error: Failed to start batch for translation.\")\n",
    "#         return None\n",
    "\n",
    "#     print(f\"Batch for translation started with ID: {batch_id}\")\n",
    "\n",
    "#     # Poll for batch completion and retrieve results\n",
    "#     results = get_batch_response(batch_id)\n",
    "#     if not results:\n",
    "#         print(\"Error: Failed to retrieve translation batch results.\")\n",
    "#         return None\n",
    "\n",
    "#     # Save the translated content back to XML\n",
    "#     for i, translated_content in enumerate(results):\n",
    "#         sections[i].text = translated_content  # Replace original content with translated content\n",
    "\n",
    "#     save_xml(sections, output_xml)\n",
    "#     print(f\"Translated journal saved to {output_xml}\")\n",
    "\n",
    "# # Main process\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Step 1: Sectioning\n",
    "#     print(\"Starting batch sectioning...\")\n",
    "#     batch_sectioning(INPUT_XML, SECTIONED_XML)\n",
    "\n",
    "#     # Step 2: Translation\n",
    "#     print(\"Starting batch translation...\")\n",
    "#     batch_translation(SECTIONED_XML, TRANSLATED_XML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function schema for function calling\n",
    "# function_schemas = [\n",
    "#     {\n",
    "#         \"name\": \"save_processed_metadata\",\n",
    "#         \"description\": \"Save metadata for a processed vietnamese journal, including sections and summaries, that will later be translated\",\n",
    "#         \"parameters\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"properties\": {\n",
    "#                 \"journal_summary\": {\"type\": \"string\", \"description\": \"A one-page summary of the journal in English.\"},\n",
    "#                 \"sections\": {\n",
    "#                     \"type\": \"array\",\n",
    "#                     \"items\": {\n",
    "#                         \"type\": \"object\",\n",
    "#                         \"properties\": {\n",
    "#                             \"section_title_vi\": {\"type\": \"string\", \"description\": \"The original title of the section in Vietnamese.\"},\n",
    "#                             \"section_title_en\": {\"type\": \"string\", \"description\": \"The translated title of the section in English.\"},\n",
    "#                             \"section_summary\": {\"type\": \"string\", \"description\": \"A one paragraph summary of the section in English.\"},\n",
    "#                             \"page_range\": {\n",
    "#                                 \"type\": \"array\",\n",
    "#                                 \"items\": {\"type\": \"integer\"},\n",
    "#                                 \"minItems\": 2,\n",
    "#                                 \"maxItems\": 2,\n",
    "#                                 \"description\": \"The start and end page numbers of the section.\"\n",
    "#                             }\n",
    "#                         },\n",
    "#                         \"required\": [\"section_title_en\", \"section_title_vi\", \"section_summary\", \"page_range\"]\n",
    "#                     }\n",
    "#                 }\n",
    "#             },\n",
    "#             \"required\": [\"journal_summary\", \"sections\"]\n",
    "#         }\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Translation\n",
    "# def batch_translate(input_xml_path, metadata_path):\n",
    "#     \"\"\"\n",
    "#     Translates the journal sections using the GPT model.\n",
    "#     Saves the translated content back to XML.\n",
    "#     \"\"\"\n",
    "#     # Load the sectioned XML\n",
    "#     section_metadata = #load json data from metadata_path and deserialize\n",
    "\n",
    "#     # use the function split_xml_to_pages to get sections for translation:\n",
    "#     sections = split_xml_pages(...)\n",
    "\n",
    "#     # Create GPT messages for translation\n",
    "#     user_message_wrapper = lambda section: f\"Translate this section:\\n{section}\"\n",
    "#     messages = generate_messages(system_message_translate, user_message_wrapper, sections)\n",
    "\n",
    "#     # convert the blocks below to a series of nested try blocks with multiple attempts as in batch_section():\n",
    "#     # add appropriate logging to match batch_section():\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, translate_batch_jsonl)\n",
    "\n",
    "#     # Start the batch\n",
    "#     batch = start_batch(jsonl_file, description=\"Batch for translating journal\")\n",
    "#     batch_id = batch.get(\"id\")\n",
    "#     if not batch_id:\n",
    "#         print(\"Error: Failed to start batch for translation.\")\n",
    "#         return None\n",
    "\n",
    "#     print(f\"Batch for translation started with ID: {batch_id}\")\n",
    "\n",
    "#     # Poll for batch completion\n",
    "#     results = poll_batch_for_response(batch_id)\n",
    "#     if not results:\n",
    "#         print(\"Error: Failed to retrieve translation batch results.\")\n",
    "#         return None\n",
    "\n",
    "#     # Save translated content back to XML\n",
    "#     translated_sections = []\n",
    "#     for i, translated_content in enumerate(results):\n",
    "#         translated_sections.append(translated_content)  # Replace original content with translated content\n",
    "\n",
    "#     save_pages_to_xml(translated_sections, translated_xml)\n",
    "#     print(f\"Translated journal saved to {translated_xml}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # old system message\n",
    "\n",
    "# system_message_section = \"\"\"\n",
    "# You are a highly skilled assistant processing a Vietnamese journal scanned from OCR. \n",
    "# You will be determining the journal sections by page number. You will also generate summaries for the full text and each section. \n",
    "# You will return this metadata in JSON format.\n",
    "\n",
    "# Instructions:\n",
    "# 1. Analyze the text and divide it into sections based on logical breaks, such as headings, topic changes, or clear shifts in content.\n",
    "# 2. Ensure every page is part of  a section, even if that section is titled \"blank page\" or \"title page,\" for example.\n",
    "# 3. For each section, provide:\n",
    "#    - The original title in Vietnamese (`section_title_vi`).\n",
    "#    - The translated title in English (`section_title_en`).\n",
    "#    - The author's name if it is available (`section_author`). \n",
    "#    - A one-paragraph summary of the section in English (`section_summary`).\n",
    "#    - A list of keywords for the section that are related to its content, these can be proper names, specific concepts, or contextual information.\n",
    "#    - The section's start and end page numbers (`start_page` and `end_page`).\n",
    "#    - Use \"null\" for any data that is not available (such as author name) for the section.\n",
    "\n",
    "# 4. Return the output as a JSON object with the following schema:\n",
    "# {\n",
    "#     \"journal_summary\": \"A one-page summary of the whole journal in English.\",\n",
    "#     \"sections\": [\n",
    "#         {\n",
    "#             \"section_title_vi\": \"Original title in Vietnamese\",\n",
    "#             \"section_title_en\": \"Translated title in English\",\n",
    "#             \"section_author\": \"Name of the author of the section\",\n",
    "#             \"section_summary\": \"One-paragraph summary of the section in English\",\n",
    "#             \"section_keywords\": \"A list of keywords for the section\",\n",
    "#             \"start_page\":  X,\n",
    "#             \"end_page\":  Y\n",
    "#         },\n",
    "#         ...\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# 5.  Ensure the JSON is well-formed and adheres strictly to the provided schema.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Translation\n",
    "# def batch_translate(input_xml_path, output_xml_path):\n",
    "#     \"\"\"\n",
    "#     Translates the journal sections using the GPT model.\n",
    "#     Saves the translated content back to XML.\n",
    "#     \"\"\"\n",
    "#     # Load the sectioned XML\n",
    "#     section_metadata = \n",
    "\n",
    "#     # Create GPT messages for translation\n",
    "#     user_message_wrapper = lambda section: f\"Translate this section:\\n{section}\"\n",
    "#     messages = generate_messages(system_message_translate, user_message_wrapper, sections)\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, translate_batch_jsonl)\n",
    "\n",
    "#     # Start the batch\n",
    "#     batch = start_batch(jsonl_file, description=\"Batch for translating journal\")\n",
    "#     batch_id = batch.get(\"id\")\n",
    "#     if not batch_id:\n",
    "#         print(\"Error: Failed to start batch for translation.\")\n",
    "#         return None\n",
    "\n",
    "#     print(f\"Batch for translation started with ID: {batch_id}\")\n",
    "\n",
    "#     # Poll for batch completion\n",
    "#     results = poll_batch_status(batch_id)\n",
    "#     if not results:\n",
    "#         print(\"Error: Failed to retrieve translation batch results.\")\n",
    "#         return None\n",
    "\n",
    "#     # Save translated content back to XML\n",
    "#     for i, translated_content in enumerate(results):\n",
    "#         sections[i].text = translated_content  # Replace original content with translated content\n",
    "\n",
    "#     save_pages_to_xml(sections, output_xml_path)\n",
    "#     print(f\"Translated journal saved to {output_xml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Translation\n",
    "# def batch_translate(input_xml_path, metadata_path, journal_name, xml_output_path, max_retries=MAX_BATCH_RETRIES, retry_delay=BATCH_RETRY_DELAY):\n",
    "#     \"\"\"\n",
    "#     Translates the journal sections using the GPT model.\n",
    "#     Saves the translated content back to XML.\n",
    "\n",
    "#     Args:\n",
    "#         input_xml_path (str): Path to the input XML file.\n",
    "#         metadata_path (str): Path to the metadata JSON file.\n",
    "#         max_retries (int): Maximum number of retries for batch operations.\n",
    "#         retry_delay (int): Delay in seconds between retries.\n",
    "\n",
    "#     Returns:\n",
    "#         bool: True if the process succeeds, False otherwise.\n",
    "#     \"\"\"\n",
    "#     logger.info(\n",
    "#         f\"starting translation batch {journal_name}...\",\n",
    "#         extra={\n",
    "#             \"input_xml\": input_xml_path,\n",
    "#             \"metadata_path\": metadata_path,\n",
    "#             \"journal_name\": journal_name\n",
    "#         }\n",
    "#     )\n",
    "#     try: # data initialization:\n",
    "#         # get metadata\n",
    "#         section_metadata = deserialize_json(metadata_path)\n",
    "#         section_title = section_metadata.section_title_en\n",
    "\n",
    "#         # Extract page groups and split XML content\n",
    "#         page_groups = extract_page_groups_from_metadata(section_metadata)\n",
    "#         xml_content = get_text_from_file(input_xml_path)\n",
    "#         sections = split_xml_pages(xml_content, page_groups)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         # Log the error with full traceback\n",
    "#         logger.error(\n",
    "#             \"Could not initialize data for translation batching {journal_name}\", exc_info=True)\n",
    "#         raise  # Re-raise the exception to escalate\n",
    "\n",
    "#     # Create GPT messages for translation\n",
    "\n",
    "#     user_message_wrapper = lambda section: f\"Translate this section with title {section_title}:\\n{section}\"\n",
    "#     messages = generate_messages(system_message_translate, user_message_wrapper, sections)\n",
    "\n",
    "#     # Create JSONL file for batch processing\n",
    "    \n",
    "#     jsonl_file = create_jsonl_file_for_batch(messages, translate_batch_jsonl)\n",
    "#     if not jsonl_file:\n",
    "#         logger.error(\n",
    "#             \"Failed to create JSONL file for translation batch.\",\n",
    "#             exc_info=True  # Logs the exception traceback if one exists\n",
    "#         )\n",
    "#         raise RuntimeError(\"Failed to create JSONL file for translation batch.\")\n",
    "    \n",
    "#     for attempt in range(max_retries): # batching logic requires multiple retries due to issues with API:\n",
    "#         try:\n",
    "#             # Start the batch\n",
    "#             batch = start_batch(jsonl_file, description=\"Batch for translating journal\")\n",
    "#             batch_id = batch.get(\"id\")\n",
    "#             if not batch_id:\n",
    "#                 raise RuntimeError(\"Batch started but no ID was returned.\")\n",
    "            \n",
    "#             print(f\"Batch for translation started successfully on attempt {attempt + 1}. ID: {batch_id}\")\n",
    "\n",
    "#             # Poll for batch completion\n",
    "#             print(\"Polling for batch completion...\")\n",
    "#             results = poll_batch_for_response(batch_id)\n",
    "\n",
    "#             if results:\n",
    "#                 break # exit the retry loop\n",
    "#             else:\n",
    "#                 raise RuntimeError(\"Unknown error. No results from batch polling.\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             logger.error(\n",
    "#                 f\"Attempt {attempt + 1} failed during translation for journal '{input_xml_path}'. Retrying in {retry_delay} seconds...\",\n",
    "#                 exc_info=True\n",
    "#             )\n",
    "#             time.sleep(retry_delay)\n",
    "#     else:\n",
    "#         logger.error(f\"Failed to complete translation after {max_retries} retries for journal '{input_xml_path}'.\")\n",
    "#         raise RuntimeError(\"Unable to run translate batch.\")\n",
    "        \n",
    "#     # Save translated content back to XML\n",
    "#     try: \n",
    "#         print(\"Saving translated content back to XML...\")\n",
    "#         translated_sections = []\n",
    "#         for i, translated_content in enumerate(results):\n",
    "#             translated_sections.append(translated_content)\n",
    "\n",
    "#         save_pages_to_xml(translated_sections, xml_output_path, overwrite=True)\n",
    "#         print(f\"Translated journal saved to {xml_output_path}\")\n",
    "#     except Exception as e:\n",
    "#         raise RuntimeError(\"Failed to save translation data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "set_api_client()\n",
    "msgs = generate_messages(\"you are assisting a software engineering/researcher looking to develop new AI platforms and processes.\", lambda x: x, [\"why is AI suddenly successful?\", \"What is the (immediate) future of AI?\"])\n",
    "run_immediate_chat_process(msgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "model_settings = {\n",
    "    \"gpt-4o\": {\n",
    "        \"max_tokens\": 3000,\n",
    "        \"context_limit\": 20000,  # Total context limit for the model\n",
    "        \"temperature\": 1.3\n",
    "    }}\n",
    "\n",
    "set_model_settings(model_settings)\n",
    "batch_id = run_single_oa_batch([\"what is the square root of 2?\", \"why is the sky blue?\"], \"you are are explaining complex ideas to a 9 year old child.\")\n",
    "\n",
    "poll_batch_for_response(batch_id, 10)\n",
    "\n",
    "msgs = generate_messages(\"you are assisting a software engineering/researcher looking to develop new AI platforms and processes.\", lambda x: x, [\"why is AI suddenly successful?\", \"What is the (immediate) future of AI?\"])\n",
    "run_immediate_chat_process(msgs[1])\n",
    "\n",
    "get_last_batch_response()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
