{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "%aimport json\n",
    "from pathlib import Path\n",
    "%aimport re\n",
    "from typing import List, Dict\n",
    "%aimport os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.text_processing import get_text_from_file, set_working_directory, get_working_directory\n",
    "from data_processing.text_processing import normalize_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.text_processing import write_text_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.gpt_processing import token_count, set_api_client, get_api_client, get_active_batches\n",
    "from data_processing.gpt_processing import generate_messages, run_immediate_chat_process, create_jsonl_file_for_batch, start_batch\n",
    "from data_processing.gpt_processing import get_completed_batches, get_batch_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_TOKEN_LIMIT = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xml_text(text, chunk_token_limit=CHUNK_TOKEN_LIMIT):\n",
    "    \"\"\"\n",
    "    Splits an XML document into chunks based on <page> tags, ensuring that no chunk\n",
    "    exceeds the specified token limit.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The XML document as a string.\n",
    "    - chunk_token_limit (int): The maximum number of tokens allowed per chunk.\n",
    "    \n",
    "    Returns:\n",
    "    - List[str]: A list of XML strings, each representing a chunk of pages.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert text to bytes\n",
    "    text_bytes = text.encode(\"utf-8\")\n",
    "    \n",
    "    try:\n",
    "        # Parse the XML text into an element tree\n",
    "        root = etree.fromstring(text_bytes)\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        # Extract the line number and column number of the error\n",
    "        line_number = e.lineno\n",
    "        column_number = e.offset\n",
    "        \n",
    "        # Find the offending line in the original text\n",
    "        lines = text.splitlines()\n",
    "        error_line = lines[line_number - 1] if line_number - 1 < len(lines) else \"Unknown line\"\n",
    "        \n",
    "        print(f\"XMLSyntaxError: {e}\")\n",
    "        print(f\"Offending line {line_number}, column {column_number}: {error_line}\")\n",
    "        return []  # Return an empty list or handle the error as needed\n",
    "    \n",
    "    # Initialize variables for splitting the document\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "    \n",
    "    # Iterate over each <page> element in the document\n",
    "    for page in root.findall(\".//page\"):\n",
    "        # Convert the current page element back to a string for token counting\n",
    "        page_text = etree.tostring(page, encoding=\"unicode\")\n",
    "        \n",
    "        # Count the tokens in the page\n",
    "        page_token_count = token_count(page_text)\n",
    "        \n",
    "        # Check if adding this page would exceed the token limit for the current chunk\n",
    "        if current_token_count + page_token_count > chunk_token_limit:\n",
    "            # If so, finalize the current chunk and start a new one\n",
    "            chunks.append(\"<document>\" + \"\".join(current_chunk) + \"</document>\")\n",
    "            current_chunk = []\n",
    "            current_token_count = 0\n",
    "            \n",
    "        # Add the current page to the chunk\n",
    "        current_chunk.append(page_text)\n",
    "        current_token_count += page_token_count\n",
    "    \n",
    "    # Add the last chunk if there are remaining pages\n",
    "    if current_chunk:\n",
    "        chunks.append(\"<document>\" + \"\".join(current_chunk) + \"</document>\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_string_clean = \"\"\"Clean this text: {text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_wrap_function_clean(text_block):\n",
    "    return user_message_string_clean.format(text=text_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_clean = \"\"\"\n",
    "You are a meticulous and consistent world expert at cleaning OCR-generated Vietnamese text. \n",
    "You are cleaning text from a 1950's Buddhist Journal. \n",
    "The text will be given in XML, and the output text should be in matching XML. \n",
    "Your goal is to minimally modify the text to generate a cleaned version.\n",
    "IMPORTANT: Do not move any text from one page to another.  \n",
    "Do not remove any text from the main body of the text. \n",
    "Formatting markers (such as footers) at the end of the text can be adjusted or removed as needed for clarity. \n",
    "You can use patterns in the text blocks (given by page) to infer patterns in the text.\n",
    "You can use the semantic meaning of the text to infer corrections—but make no semantic changes. \n",
    "You can also add diacritical marks if they are missing or clearly inaccurate. \n",
    "Do not change any proper names, except to add missing diacritical marks if the context is clear.  \n",
    "This particular text has a title marker: \"Phat Giao Viet Nam,\" and also a publishing mark near the end of each page of text. \n",
    "The publishing mark is something like \"Tu Vien HUE QUANG\" + \"TRUNG TAM DICH THUAT HAN NOM\" and is very difficult for the OCR to process. \n",
    "Text corresponding to these marks (or part thereof) and page numbers can be omitted.\n",
    "Output the corrected text only with no comments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing gpt-4o-mini system message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are an intelligent, meticulous, and consistent expert at cleaning OCR-generated Vietnamese text. \n",
    "The text will be given in XML, and the output text should be in matching XML. \n",
    "Your goal is to minimally modify the text to generate a cleaned version. \n",
    "Do not remove any text from the main content.  \n",
    "Formatting markers at the beginning and end of the text can be adjusted or removed as needed for clarity. \n",
    "You can use the semantic meaning of the text to infer corrections—but make no semantic changes. \n",
    "You can also add diacritical marks if they are missing or clearly inaccurate and can be determined by context.\n",
    "Do not change any proper names, except to add missing diacritical marks if the context is clear.  \n",
    "This particular text has a title marker: \"Phat Giao Viet Nam,\" and also a publishing mark near the end of each page of text. \n",
    "The publishing mark is something like \"Tu Vien HUE QUANG\" + \"TRUNG TAM DICH THUAT HAN NOM\" and is often incomplete.\n",
    "Text corresponding to these marks (or part thereof) and page numbers can be omitted.\n",
    "Output the corrected text only with no comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unittest.mock import Mock\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Create a mock client with files.create and batches.create methods\n",
    "# mock_client = Mock()\n",
    "\n",
    "# # Mock the behavior of files.create to return a mock file object with an id\n",
    "# mock_client.files.create.return_value.id = \"mock_file_id\"\n",
    "\n",
    "# # Mock the behavior of batches.create to return a dictionary as if it were a batch object\n",
    "# mock_client.batches.create.return_value = {\"id\": \"mock_batch_id\", \"status\": \"created\"}\n",
    "\n",
    "# # Test the function\n",
    "# jsonl_file = \"journal_cleaning_batches/clean_batch_phat-giao-viet-nam-1956-01.jsonl\"\n",
    "# result = start_batch(mock_client, jsonl_file)\n",
    "\n",
    "# print(result)  # Expected output: {'id': 'mock_batch_id', 'status': 'created'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = start_batch(my_client, \"journal_cleaning_batches/test_batch.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = start_batch(my_client, \"journal_cleaning_batches/TEST_clean_batch_phat-giao-viet-nam-1956-01.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process files in sequence and generate cleaning batch JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# processed_journals = \"../../processed_journal_data\"\n",
    "# batch_ouput_prefix = \"./journal_cleaning_batches/\"\n",
    "\n",
    "# for path in Path(processed_journals).iterdir():\n",
    "#     if path.is_dir():\n",
    "#         journal_name = path.name\n",
    "#         for subpath in Path(path).iterdir():\n",
    "#             regex = re.compile(r\"^full_OCR_.*\\.xml\")\n",
    "#             if subpath.is_file() and regex.search(subpath.name):\n",
    "#                 print(subpath.name)\n",
    "#                 try:\n",
    "#                     ocr_text = get_text_from_file(subpath.name, path)\n",
    "#                     print(f\"{ocr_text[:90]}...\")\n",
    "#                     chunks = split_xml_text(ocr_text)\n",
    "#                     clean_message_seq = generate_messages(system_message_clean, user_wrap_function_clean, chunks)\n",
    "#                     create_jsonl_file_for_batch(clean_message_seq, batch_ouput_prefix + \"clean_batch_\" + journal_name + \".jsonl\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"{e}\\nfailed.. \\nContinuing.\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send batches to API client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_client = set_api_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job_dir = \"./journal_cleaning_batches\"\n",
    "batch_files = os.listdir(batch_job_dir)\n",
    "batch_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(batch_job_dir, 'clean_batch_phat-giao-viet-nam-1956-05-06.jsonl')\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## completed batches:\n",
    "10, 25-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_batch(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_27 = start_batch(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_05_06 = start_batch(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_api_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_active_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = get_completed_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_batch_info(batch_info):\n",
    "    output_id = batch_info['output_file_id']\n",
    "    file = batch_client.files.content(output_id)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_batch_response(completed[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cleaned_text = \"\\n\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_working_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text_to_file(\"full_cleaned_phat-giao-viet-nam-1956-05-06\", full_cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_chunks = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_job_path = Path(\"./journal_cleaning_batches\")\n",
    "\n",
    "# batch_info = []\n",
    "# for path_obj in batch_job_path.iterdir():\n",
    "#     regex = re.compile(r\"^clean_batch_.*\\.jsonl\")\n",
    "#     if path_obj.is_file() and regex.search(path_obj.name):\n",
    "#         batch_file = batch_job_path / path_obj.name\n",
    "#         print(batch_file)\n",
    "#         try:\n",
    "#             batch = start_batch(batch_file)\n",
    "#             batch_info.append(batch)\n",
    "#         except Exception as e:\n",
    "#             print(f\"{e}\\nfailed.. \\nContinuing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_translate = \"\"\"\n",
    "You are translating as Thich Nhat Hanh from Vietnamese to English for your experienced students. \n",
    "The text is from 1956-1958. Use the title: \"Vietnamese Buddhist Journal.\" \n",
    "You want experienced students to understand the material and its historical and cultural context—in particular, as it relates to the life and teaching of Thich Nhat Hanh.\n",
    "\n",
    "- Give a full translation that flows well in English in the style of Thich Nhat Hanh.\n",
    "- Format the text clearly and cleanly.\n",
    "- Add standard XML metadata tags that will improve the readability and clarity of the content.\n",
    "- Example tags: <p> <section> <title> <subtitle> <contents> <author> <li> <i> <b> etc.\n",
    "- Titles and XML sections should also be translated.\n",
    "- Do not make comments within the text itself. \n",
    "- Leave all names of people in original Vietnamese with diacritics. \n",
    "- Each section of the text should have its own dedicated `<footnotes>` section.\n",
    "- Use Footnotes liberally to explain elements of Vietnamese Buddhism or Buddhism in general, elements of Vietnamese culture and history, or elements relative to the life, teachings, and practice of Thich Nhat Hanh.\n",
    "- When translating complex terms (Sanskrit, Sino-Vietnamese, or French) mark these with <i></i> tags and give an explanation in the footnotes.  \n",
    "- For any terms footnoted, give the original Vietnamese, Sino-Vietnamese, Sanskrit, or French in the footnote. \n",
    "- If a paragraph logically spans two pages, it can be moved to the most logical page to allow translation to flow seamlessly.\n",
    "- Due to the size of the journal, you may be translating a piece of the document in the middle which will be split from its preceding pages or sections.\n",
    "\n",
    "Footnote Formatting:\n",
    "Inline Footnote Markers:\n",
    "   - In the main text, footnotes should be marked numerically in brackets, such as “[1]”.\n",
    "   - Enclose each inline footnote marker within a `<footnote number=\"n\">` tag, where `n` is the footnote number.\n",
    "   - Example of a footnoted paragraph:\n",
    "     `<p>This paragraph has a footnote. This is a sentence in the paragraph.</p><footnote number=\"1\">[1]</footnote>`\n",
    "\n",
    "Footnote Section:\n",
    "   - At the end of each logical section, include a `<footnotes>` section where explanations for each footnote of that section are listed.\n",
    "   - Each footnote in the `<footnotes>` section should use a `<footnote number=\"n\">` tag, with `n` matching the inline reference.\n",
    "\n",
    "Numbering:\n",
    "   - Footnote numbers should restart at 1 for each new section of text.\n",
    "\n",
    "Here's an example of text containing two footnotes, followed by its `<footnotes>`:\n",
    "\n",
    "`<p>This is the first sentence with a footnote.<footnote number=\"1\">[1]</footnote> Here is another statement that needs a footnote.<footnote number=\"2\">[2]</footnote></p>`\n",
    "\n",
    "`<footnotes>`\n",
    "    `<footnote number=\"1\">This is the explanation of the first footnote</footnote>`\n",
    "    `<footnote number=\"2\">This is the explanation of the second footnote</footnote>`\n",
    "`</footnotes>` \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_string_translate = \"\"\"{text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_wrap_function_translate(text_block):\n",
    "    return user_message_string_translate.format(text=text_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the the chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = split_xml_text(raw_text_xml, chunk_token_limit=11000)\n",
    "# clean_message_seq = generate_messages(system_message_clean, user_wrap_function_clean, chunks)\n",
    "# cleaned_data = []\n",
    "# translated_chunks = []\n",
    "# for i, msgs in enumerate(clean_message_seq):\n",
    "#     print(f\"cleaning chunk: {i}\")\n",
    "#     completion_clean = run_immediate_chat_process(msgs)\n",
    "#     if completion_clean:\n",
    "#         clean_response = completion_clean.choices[0].message.content\n",
    "#         cleaned_data.append(clean_response)\n",
    "#     else:\n",
    "#         break\n",
    "#         print(\"failed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count(full_cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_text_to_file(\"phat-giao-viet-nam-1956-25-26/full_cleaned_text.xml\", full_cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_tx_text = get_text_from_file(\"phat-giao-viet-nam-1956-25-26/full_cleaned_text.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_message_seq = generate_messages(system_message_translate, user_wrap_function_translate, cleaned_chunks)\n",
    "for i, tx_msgs in enumerate(translation_message_seq):\n",
    "    print(f\"translating chunk: {i}\")\n",
    "    \n",
    "    if completion_tx:\n",
    "        tx_response = completion_tx.choices[0].message.content\n",
    "        translation_data.append(tx_response)\n",
    "    else:\n",
    "        print(\"failed.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_message_seq = generate_messages(system_message_translate, user_wrap_function_translate, cleaned_chunks)\n",
    "for i, tx_msgs in enumerate(translation_message_seq):\n",
    "    print(f\"translating chunk: {i}\")\n",
    "    \n",
    "    if completion_tx:\n",
    "        tx_response = completion_tx.choices[0].message.content\n",
    "        translation_data.append(tx_response)\n",
    "    else:\n",
    "        print(\"failed.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation_data = []\n",
    "# input_text_chunks = [to_tx_text]\n",
    "# translation_message_seq = generate_messages(system_message_translate, user_wrap_function_translate, input_text_chunks)\n",
    "# for i, tx_msgs in enumerate(translation_message_seq):\n",
    "#     print(f\"translating chunk: {i}\")\n",
    "#     completion_tx = run_immediate_chat_process(tx_msgs)\n",
    "#     if completion_tx:\n",
    "#         tx_response = completion_tx.choices[0].message.content\n",
    "#         translation_data.append(tx_response)\n",
    "#     else:\n",
    "#         print(\"failed.\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translation_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_translated_text = \"\\n\".join(translation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text_to_file(\"journal_1956_25_26_translation_full.xml\", full_translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_schema = {\n",
    "#   \"name\": \"text_cleaning\",\n",
    "#   \"strict\": True,\n",
    "#   \"schema\": {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#       \"cleaned_text\": {\n",
    "#         \"type\": \"string\",\n",
    "#         \"description\": \"The resulting cleaned text after applying all cleaning steps.\"\n",
    "#       },\n",
    "#       \"cleaning_steps\": {\n",
    "#         \"type\": \"array\",\n",
    "#         \"description\": \"A series of notes that explain each step taken to clean the input text.\",\n",
    "#         \"items\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"step_description\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"A detailed description of the cleaning step.\"\n",
    "#             },\n",
    "#             \"text_example\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"an example fragment of text that was corrected.\"\n",
    "#             },\n",
    "#             \"confidence_level\": {\n",
    "#               \"type\": \"integer\",\n",
    "#               \"description\": \"3 for high confidence, 2 for medium, 1 for low confidence corrections.\"\n",
    "#             }\n",
    "#           },\n",
    "#           \"required\": [\n",
    "#             \"step_description\",\n",
    "#             \"text_example\",\n",
    "#             \"confidence_level\"\n",
    "#           ],\n",
    "#           \"additionalProperties\": False\n",
    "#         }\n",
    "#       }\n",
    "#     },\n",
    "#     \"required\": [\n",
    "#       \"cleaned_text\",\n",
    "#       \"cleaning_steps\"\n",
    "#     ],\n",
    "#     \"additionalProperties\": False\n",
    "#   }\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
