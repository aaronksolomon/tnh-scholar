{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook to solidfy journal cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport json\n",
    "from pathlib import Path\n",
    "%aimport re\n",
    "from typing import List, Dict\n",
    "%aimport os\n",
    "from xml.sax.saxutils import escape\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from math import floor\n",
    "from datetime import datetime\n",
    "%aimport re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.text_processing import get_text_from_file, set_working_directory, get_working_directory\n",
    "from data_processing.text_processing import normalize_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.text_processing import write_text_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.gpt_processing import (\n",
    "    token_count, get_active_batches,\n",
    "    generate_messages, create_jsonl_file_for_batch, start_batch_with_retries,\n",
    "    get_completed_batches, get_batch_response, set_model_settings, delete_old_files, run_immediate_chat_process,\n",
    "    get_completion_content\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.xml_processing import split_xml_pages, split_xml_on_pagebreaks, save_pages_to_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(\"/Users/phapman/Desktop/tnh-scholar/\")\n",
    "data_dir = project_dir / \"data_processing\"\n",
    "journal_dir = data_dir / \"processed_journal_data\"\n",
    "journal_name = \"phat-giao-viet-nam-1956-02\"\n",
    "working_dir = journal_dir / journal_name\n",
    "cleaned_xml_path = working_dir / f\"full_cleaned_{journal_name}.xml\"\n",
    "batch_job_dir = working_dir / \"processing_batch_files\"\n",
    "batch_file_path = batch_job_dir / f\"clean_batch_{journal_name}.jsonl\"\n",
    "clean_batch_jsonl = working_dir / \"clean_batch.jsonl\"\n",
    "ocr_file_to_process = journal_dir / journal_name / f\"full_OCR_{journal_name}.xml\"\n",
    "logfile = data_dir / \"gpt_processing\" / \"processing_info.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the logger\n",
    "def setup_logger(log_file_path):\n",
    "    \"\"\"\n",
    "    Configures the logger to write to a log file and the console.\n",
    "    \"\"\"\n",
    "    # Remove existing handlers\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",  # Include logger name\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file_path, encoding=\"utf-8\"),\n",
    "            logging.StreamHandler()  # Optional: to log to the console as well\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Suppress DEBUG/INFO logs for specific noisy modules\n",
    "    modules_to_suppress = [\"httpx\", \"httpcore\", \"urllib3\", \"openai\"]\n",
    "    for module in modules_to_suppress:\n",
    "        logger = logging.getLogger(module)\n",
    "        logger.setLevel(logging.WARNING)  # Suppress DEBUG and INFO logs\n",
    "\n",
    "    \n",
    "    return logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_logger(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_string_clean = \"\"\"{text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_wrap_function_clean(text_block):\n",
    "    return user_message_string_clean.format(text=text_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_clean = \"\"\"You are a meticulous and consistent world expert at cleaning OCR-generated Vietnamese text. \n",
    "You are cleaning pages from a 1950's Buddhist Journal. \n",
    "Each line of scanned data will be enclosed in <> brackets. Leave <> brackets in place.\n",
    "Your goal is to minimally modify the text to generate a cleaned version.\n",
    "Do not remove any content from the main body of the text. \n",
    "Do not change the line formatting. \n",
    "\n",
    "You can use the semantic meaning of the text to infer correctionsâ€”but make no semantic changes. \n",
    "You can also add diacritical marks if they are missing or clearly inaccurate. \n",
    "Do not change any proper names, except to add missing diacritical marks or to fix orthographic errors if the context is clear.  \n",
    "\n",
    "This particular text has a title marker in the footer, \"Phat Giao Viet Nam,\" and also a publishing mark diagonally across the text.  \n",
    "The publishing mark is \"TU VIEN HUE QUANG\"  and is faint so only parts of it may appear in some locations in the text. \n",
    "Text corresponding to these marks (or part thereof) and page numbers can be omitted.\n",
    "\n",
    "IMPORTANT: If the page is blank return: blank page \n",
    "IMPORTANT: Output the corrected text only with no comments (including ``` xml)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_tokens_for_clean(data: str, factor: float=1, buffer: int=100):\n",
    "    return floor(token_count(data) * factor) + buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text_from_file(ocr_file_to_process)\n",
    "pages = split_xml_on_pagebreaks(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_lines(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Encloses each line of the input text with angle brackets.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string containing lines separated by '\\n'.\n",
    "\n",
    "    Returns:\n",
    "        str: A string where each line is enclosed in angle brackets.\n",
    "    \n",
    "    Example:\n",
    "        >>> enclose_lines(\"This is a string with   \\n   two lines.\")\n",
    "        '<This is a string with  >\\n<    two lines.>'\n",
    "    \"\"\"\n",
    "    return '\\n'.join(f\"<{line}>\" for line in text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_all_lines(pages):\n",
    "    return [wrap_lines(page) for page in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_lines(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes angle brackets (< >) from encapsulated lines and merges them into \n",
    "    a newline-separated string.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input string with encapsulated lines.\n",
    "\n",
    "    Returns:\n",
    "        str: A newline-separated string with the encapsulation removed.\n",
    "    \n",
    "    Example:\n",
    "        >>> merge_encapsulated_lines(\"<Line 1> <Line 2> <Line 3>\")\n",
    "        'Line 1\\nLine 2\\nLine 3'\n",
    "        >>> merge_encapsulated_lines(\"<Line 1>\\n<Line 2>\\n<Line 3>\")\n",
    "        'Line 1\\nLine 2\\nLine 3'\n",
    "    \"\"\"\n",
    "    # Find all content between < and > using regex\n",
    "    matches = re.findall(r\"<(.*?)>\", text)\n",
    "    # Join the extracted content with newlines\n",
    "    return '\\n'.join(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_all_lines(pages):\n",
    "    result = []\n",
    "    for page in pages:\n",
    "        if page == \"blank page\":\n",
    "            result.append(page)\n",
    "        else:\n",
    "            result.append(unwrap_lines(page))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_batch(\n",
    "    input_xml_file: str,\n",
    "    output_file: str,\n",
    "    system_message: str,\n",
    "    user_wrap_function,\n",
    "    immediate: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a batch file for the OpenAI (OA) API using a single input XML file.\n",
    "\n",
    "    Parameters:\n",
    "        batch_file (str): Full path to the input XML file to process.\n",
    "        output_file (str): Full path to the output batch JSONL file.\n",
    "        system_message (str): System message template for batch processing.\n",
    "        user_wrap_function (callable): Function to wrap user input for processing pages.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the created batch file.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during file processing.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Read the OCR text from the batch file\n",
    "        text = get_text_from_file(input_xml_file)\n",
    "        logger.info(f\"Processing file: {input_xml_file}\")\n",
    "\n",
    "        # Split the text into pages for processing\n",
    "        pages = split_xml_on_pagebreaks(text)\n",
    "        pages =  wrap_all_lines(pages) # wrap lines with brackets.\n",
    "        if not pages:\n",
    "            raise ValueError(f\"No pages found in XML file: {input_xml_file}\")\n",
    "        logger.info(f\"Found {len(pages)} pages in {input_xml_file}.\")\n",
    "\n",
    "        max_tokens = [get_max_tokens_for_clean(page) for page in pages]\n",
    "\n",
    "        # Generate messages for the pages\n",
    "        batch_message_seq = generate_messages(system_message, user_wrap_function, pages)\n",
    "\n",
    "        if immediate:\n",
    "            logger.info(\"Running immediate chat process for cleaning:\")\n",
    "            for message in batch_message_seq:\n",
    "                logger.info(\"Starting page {i+1}...\")\n",
    "                result = run_immediate_chat_process(batch_message_seq, max_token_list=max_tokens)\n",
    "                \n",
    "            \n",
    "        # Save the batch file\n",
    "        create_jsonl_file_for_batch(batch_message_seq, output_file, max_token_list=max_tokens)\n",
    "        logger.info(f\"Batch file created successfully: {output_file}\")\n",
    "\n",
    "        return output_file\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found.\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Value error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error while processing {input_xml_file}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_immediate_clean(\n",
    "    input_xml_file: str,\n",
    "    system_message: str,\n",
    "    user_wrap_function,\n",
    "):\n",
    "    try:\n",
    "        result_list = []\n",
    "        # Read the OCR text from the batch file\n",
    "        text = get_text_from_file(input_xml_file)\n",
    "        logger.info(f\"Processing {input_xml_file} for immediate cleaning:\")\n",
    "\n",
    "        # Split the text into pages for processing\n",
    "        pages = split_xml_on_pagebreaks(text)\n",
    "        pages =  wrap_all_lines(pages) # wrap lines with brackets.\n",
    "        if not pages:\n",
    "            raise ValueError(f\"No pages found in XML file: {input_xml_file}\")\n",
    "        logger.info(f\"Found {len(pages)} pages in {input_xml_file}.\")\n",
    "\n",
    "        max_tokens = [get_max_tokens_for_clean(page) for page in pages]\n",
    "\n",
    "        # Generate messages for the pages\n",
    "        batch_message_seq = generate_messages(system_message, user_wrap_function, pages)\n",
    "\n",
    "        for i, message in enumerate(batch_message_seq):\n",
    "            logger.info(f\"Starting page {i+1}...\")\n",
    "            completion = run_immediate_chat_process(message, max_tokens=max_tokens[i])\n",
    "            if completion:\n",
    "                result_list.append(get_completion_content(completion))\n",
    "            else:\n",
    "                logger.error(\"Chat completion failed.\")\n",
    "                raise RuntimeError(\"Chat could not complete.\")\n",
    "            \n",
    "        logger.info(f\"Cleaning completed successfully.\")\n",
    "\n",
    "        return result_list\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found.\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Value error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error while processing {input_xml_file}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings_clean = {\n",
    "    \"gpt-4o\": {\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "}\n",
    "set_model_settings(model_settings_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_file_to_process, ocr_file_to_process.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_result = run_immediate_clean(ocr_file_to_process, system_message_clean, user_wrap_function_clean)\n",
    "# cleaned_data = clean_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_clean_batch(ocr_file_to_process, batch_file_path, system_message_clean, user_wrap_function_clean)\n",
    "job_description = f\"cleaning test for {journal_name} on file: {ocr_file_to_process}\"\n",
    "cleaned_data = start_batch_with_retries(batch_file_path, job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup files:\n",
    "delete_old_files(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = unwrap_all_lines(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pages_to_xml(cleaned_xml_path, cleaned_data, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cleaned_text = join_pages(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cleaned_path = journal_dir / basename / f\"full_cleaned_{basename}.xml\"\n",
    "full_cleaned_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_text_to_file(full_cleaned_path, full_cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cleaned_current = get_text_from_file(full_cleaned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_cleaned_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count(full_cleaned_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pages = split_xml_pages(full_cleaned_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_page_tags(text):\n",
    "    \"\"\"\n",
    "    Removes <page ...> and </page> tags from a text string.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text containing <page> tags.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text with <page> tags removed.\n",
    "    \"\"\"\n",
    "    # Remove opening <page ...> tags\n",
    "    text = re.sub(r\"<page[^>]*>\", \"\", text)\n",
    "    # Remove closing </page> tags\n",
    "    text = re.sub(r\"</page>\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pages = [remove_page_tags(page) for page in cleaned_pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pages[8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
