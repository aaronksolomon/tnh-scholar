{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.text_processing import get_text_from_file, set_working_directory, get_working_directory\n",
    "from data_processing.text_processing import normalize_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.text_processing import write_text_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.gpt_processing import token_count, set_api_client, get_api_client\n",
    "from data_processing.gpt_processing import generate_messages, run_immediate_chat_process, create_jsonl_file_for_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_TOKEN_LIMIT = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_working_directory(\"../../processed_journal_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_xml = get_text_from_file(\"phat-giao-viet-nam-1956-25-26/full_OCR_text_phat-giao-viet-nam-1956-25-26.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count(raw_text_xml) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_comp = get_text_from_file(\"phat-giao-viet-nam-1956-24/journal_1956_24_translation_full.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count(tx_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning prompts:\n",
    "\n",
    "You are an intelligent, meticulous, and consistent world expert at cleaning OCR generated Vietnamese text. You are cleaning text from an old 1950's Buddhist Journal.  The text will be given in XML, and the output text should be matching XML. Your goal is to minimally modify the text to generate a cleaned version, removing extraneous text, incomplete words, and formatting markers, to get a clean text. You can use patterns in the text blocks (given in pages) to infer patterns in the text layout. You can use the semantic meaning of the text to infer corrections but strive to make minimal semantic changes. You can also add diacritical marks if they are missing or clearly inaccurate. It's important to note the degree of confidence you have in each cleaning step, and each cleaning step should be noted, with other possible interpretations listed.\n",
    "\n",
    "You are an intelligent, meticulous, and consistent world expert at cleaning OCR-generated Vietnamese text. You are cleaning text from an old 1950's Buddhist Journal.  The text will be given in XML, and the output text should be in matching XML. Your goal is to minimally modify the text to generate a cleaned version, removing extraneous text, incomplete words, and formatting markers, to get a clean text. You can use patterns in the text blocks (given by page) to infer patterns in the text layout. You can use the semantic meaning of the text to infer corrections—but strive to make minimal semantic changes. You can also add diacritical marks if they are missing or clearly inaccurate. It's important to note the degree of confidence you have in each cleaning step, and each cleaning step should be noted as concisely as possible, also listing other possible interpretations of the text.\n",
    "\n",
    "You are an intelligent, meticulous, and consistent world expert at cleaning OCR-generated Vietnamese text. You are cleaning text from a 1950's Buddhist Journal. The text will be given in XML, and the output text should be in matching XML. Your goal is to minimally modify the text to generate a cleaned version. Do not remove any text from the body of the text (unless it is clearly an extraneous mark). Any text that cannot be corrected from context or other means should still be included as bracketed [] text. However, formatting markers at the beginning and end of the text can be adjusted or removed as needed. You can use patterns in the text blocks (given by page) to infer patterns in the text layout. You can use the semantic meaning of the text to infer corrections—but strive to make no semantic changes. You can also add diacritical marks if they are missing or clearly inaccurate. It's important to note the degree of confidence you have in each cleaning step (high, medium or low). Each cleaning step should be noted as concisely as possible, also listing other possible interpretations of the text. This particular text has a publication marker \"Phat Giao Viet Nam\" and publishing mark near the end of each page of text. The publication mark is something like \"Tu Vien HUE QUANG\" and is very difficult for the OCR to process. Text corresponding to these marks can be ommitted.\n",
    "\n",
    "You are an intelligent, meticulous, and consistent world expert at cleaning OCR-generated Vietnamese text. You are cleaning text from a 1950's Buddhist Journal. The text will be given in XML, and the output text should be in matching XML. Your goal is to minimally modify the text to generate a cleaned version. Do not remove any text from the body of the text (unless it is clearly an extraneous mark). Any text that cannot be corrected from context or other means should still be included as bracketed [] text. However, formatting markers at the beginning and end of the text can be adjusted or removed as needed. You can use patterns in the text blocks (given by page) to infer patterns in the text layout. You can use the semantic meaning of the text to infer corrections—but strive to make no semantic changes. You can also add diacritical marks if they are missing or clearly inaccurate. It's important to note the degree of confidence you have in each cleaning step (high, medium or low). Each cleaning step should be noted as concisely as possible, also listing other possible interpretations of the text. This particular text has a title marker \"Phat Giao Viet Nam\" and publishing mark near the end of each page of text. The publication mark is something like \"Tu Vien HUE QUANG\" + \"TRUNG TAM DICH THUAT HAN NOM\" and is very difficult for the OCR to process. Text corresponding to these marks and page numbers can be ommitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xml_text(text, chunk_token_limit=10000):\n",
    "    \"\"\"\n",
    "    Splits an XML document into chunks based on <page> tags, ensuring that no chunk\n",
    "    exceeds the specified token limit.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The XML document as a string.\n",
    "    - chunk_token_limit (int): The maximum number of tokens allowed per chunk.\n",
    "    \n",
    "    Returns:\n",
    "    - List[str]: A list of XML strings, each representing a chunk of pages.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert text to bytes\n",
    "    text_bytes = text.encode(\"utf-8\")\n",
    "    \n",
    "    try:\n",
    "        # Parse the XML text into an element tree\n",
    "        root = etree.fromstring(text_bytes)\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        # Extract the line number and column number of the error\n",
    "        line_number = e.lineno\n",
    "        column_number = e.offset\n",
    "        \n",
    "        # Find the offending line in the original text\n",
    "        lines = text.splitlines()\n",
    "        error_line = lines[line_number - 1] if line_number - 1 < len(lines) else \"Unknown line\"\n",
    "        \n",
    "        print(f\"XMLSyntaxError: {e}\")\n",
    "        print(f\"Offending line {line_number}, column {column_number}: {error_line}\")\n",
    "        return []  # Return an empty list or handle the error as needed\n",
    "    \n",
    "    # Initialize variables for splitting the document\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "    \n",
    "    # Iterate over each <page> element in the document\n",
    "    for page in root.findall(\".//page\"):\n",
    "        # Convert the current page element back to a string for token counting\n",
    "        page_text = etree.tostring(page, encoding=\"unicode\")\n",
    "        \n",
    "        # Count the tokens in the page\n",
    "        page_token_count = token_count(page_text)\n",
    "        \n",
    "        # Check if adding this page would exceed the token limit for the current chunk\n",
    "        if current_token_count + page_token_count > chunk_token_limit:\n",
    "            # If so, finalize the current chunk and start a new one\n",
    "            chunks.append(\"<document>\" + \"\".join(current_chunk) + \"</document>\")\n",
    "            current_chunk = []\n",
    "            current_token_count = 0\n",
    "            \n",
    "        # Add the current page to the chunk\n",
    "        current_chunk.append(page_text)\n",
    "        current_token_count += page_token_count\n",
    "    \n",
    "    # Add the last chunk if there are remaining pages\n",
    "    if current_chunk:\n",
    "        chunks.append(\"<document>\" + \"\".join(current_chunk) + \"</document>\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_xml_text(raw_text_xml, chunk_token_limit=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_clean = \"\"\"\n",
    "You are an intelligent, meticulous, and consistent world expert at cleaning OCR-generated Vietnamese text. \n",
    "You are cleaning text from a 1950's Buddhist Journal. The text will be given in XML, and the output text should be in matching XML. \n",
    "Your goal is to minimally modify the text to generate a cleaned version. Do not remove any text from the main body of the text.  \n",
    "However, formatting markers at the beginning and end of the text can be adjusted or removed as needed for clarity. \n",
    "You can use patterns in the text blocks (given by page) to infer patterns in the text.\n",
    "You can use the semantic meaning of the text to infer corrections—but make no semantic changes. \n",
    "You can also add diacritical marks if they are missing or clearly inaccurate. \n",
    "Do not change any proper names, except to add missing diacritical marks if the context is clear.  \n",
    "This particular text has a title marker: \"Phat Giao Viet Nam,\" and also a publishing mark near the end of each page of text. \n",
    "The publishing mark is something like \"Tu Vien HUE QUANG\" + \"TRUNG TAM DICH THUAT HAN NOM\" and is very difficult for the OCR to process. \n",
    "Text corresponding to these marks (or part thereof) and page numbers can be omitted.\n",
    "Output the corrected text only with no comments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count(system_message_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "306 / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_string_clean = \"\"\"Clean this text: {text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_wrap_function_clean(text_block):\n",
    "    return user_message_string_clean.format(text=text_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_sequence_clean = generate_messages(system_message_clean, user_wrap_function_clean, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_api_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_translate = \"\"\"\n",
    "You are translating as Thich Nhat Hanh from Vietnamese to English for your experienced students. \n",
    "The text is from 1956-1958. Use the title: \"Vietnamese Buddhist Journal.\" \n",
    "You want experienced students to understand the material and its historical and cultural context—in particular, as it relates to the life and teaching of Thich Nhat Hanh.\n",
    "\n",
    "- Give a full translation that flows well in English in the style of Thich Nhat Hanh.\n",
    "- Format the text clearly and cleanly.\n",
    "- Add additional standard XML metadata tags that will improve the readability and clarity of the content.\n",
    "- Example tags: <p> <section> <title> <contents> <author> <li> <i> <b> etc.\n",
    "- Titles and XML sections should also be translated.\n",
    "- Do not make comments within the text itself. \n",
    "- Leave all names of people in original Vietnamese with diacritics. \n",
    "- Each section of the text should have its own dedicated `<footnotes>` section.\n",
    "- Use Footnotes liberally to explain elements of Vietnamese Buddhism or Buddhism in general, elements of Vietnamese culture and history, or elements relative to the life, teachings, and practice of Thich Nhat Hanh.\n",
    "- When translating complex terms (Sanskrit, Sino-Vietnamese, or French) mark these with <i></i> tags and give an explanation in the footnotes.  \n",
    "- For any terms footnoted, give the original Vietnamese, Sino-Vietnamese, Sanskrit, or French in the footnote. \n",
    "- If a paragraph logically spans two pages, it can be moved to the most logical page to allow translation to flow seamlessly.\n",
    "- Due to the size of the journal, you may be translating a piece of the document in the middle which will be split from its preceding pages or sections.\n",
    "\n",
    "Footnote Formatting:\n",
    "Inline Footnote Markers:\n",
    "   - In the main text, footnotes should be marked numerically in brackets, such as “[1]”.\n",
    "   - Enclose each inline footnote marker within a `<footnote number=\"n\">` tag, where `n` is the footnote number.\n",
    "   - Example of a footnoted paragraph:\n",
    "     `<p>This paragraph has a footnote. This is a sentence in the paragraph.</p><footnote number=\"1\">[1]</footnote>`\n",
    "\n",
    "Footnote Section:\n",
    "   - At the end of each logical section, include a `<footnotes>` section where explanations for each footnote of that section are listed.\n",
    "   - Each footnote in the `<footnotes>` section should use a `<footnote number=\"n\">` tag, with `n` matching the inline reference.\n",
    "\n",
    "Numbering:\n",
    "   - Footnote numbers should restart at 1 for each new section of text.\n",
    "\n",
    "Here's an example of text containing two footnotes, followed by its `<footnotes>`:\n",
    "\n",
    "`<p>This is the first sentence with a footnote.<footnote number=\"1\">[1]</footnote> Here is another statement that needs a footnote.<footnote number=\"2\">[2]</footnote></p>`\n",
    "\n",
    "`<footnotes>`\n",
    "    `<footnote number=\"1\">This is the explanation of the first footnote</footnote>`\n",
    "    `<footnote number=\"2\">This is the explanation of the second footnote</footnote>`\n",
    "`</footnotes>` \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_string_translate = \"\"\"{text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_wrap_function_translate(text_block):\n",
    "    return user_message_string_translate.format(text=text_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a batch processes for all cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the the chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_xml_text(raw_text_xml, chunk_token_limit=11000)\n",
    "clean_message_seq = generate_messages(system_message_clean, user_wrap_function_clean, chunks)\n",
    "cleaned_data = []\n",
    "translated_chunks = []\n",
    "for i, msgs in enumerate(clean_message_seq):\n",
    "    print(f\"cleaning chunk: {i}\")\n",
    "    completion_clean = run_immediate_chat_process(msgs)\n",
    "    if completion_clean:\n",
    "        clean_response = completion_clean.choices[0].message.content\n",
    "        cleaned_data.append(clean_response)\n",
    "    else:\n",
    "        break\n",
    "        print(\"failed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cleaned_text = \"\\n\".join(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count(full_cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text_to_file(\"phat-giao-viet-nam-1956-25-26/full_cleaned_text.xml\", full_cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tx_text = get_text_from_file(\"phat-giao-viet-nam-1956-25-26/full_cleaned_text.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_data = []\n",
    "input_text_chunks = [to_tx_text]\n",
    "translation_message_seq = generate_messages(system_message_translate, user_wrap_function_translate, input_text_chunks)\n",
    "for i, tx_msgs in enumerate(translation_message_seq):\n",
    "    print(f\"translating chunk: {i}\")\n",
    "    completion_tx = run_immediate_chat_process(tx_msgs)\n",
    "    if completion_tx:\n",
    "        tx_response = completion_tx.choices[0].message.content\n",
    "        translation_data.append(tx_response)\n",
    "    else:\n",
    "        print(\"failed.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translation_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_translated_text = \"\\n\".join(translation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text_to_file(\"journal_1956_25_26_translation_full.xml\", full_translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_schema = {\n",
    "#   \"name\": \"text_cleaning\",\n",
    "#   \"strict\": True,\n",
    "#   \"schema\": {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#       \"cleaned_text\": {\n",
    "#         \"type\": \"string\",\n",
    "#         \"description\": \"The resulting cleaned text after applying all cleaning steps.\"\n",
    "#       },\n",
    "#       \"cleaning_steps\": {\n",
    "#         \"type\": \"array\",\n",
    "#         \"description\": \"A series of notes that explain each step taken to clean the input text.\",\n",
    "#         \"items\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"step_description\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"A detailed description of the cleaning step.\"\n",
    "#             },\n",
    "#             \"text_example\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"an example fragment of text that was corrected.\"\n",
    "#             },\n",
    "#             \"confidence_level\": {\n",
    "#               \"type\": \"integer\",\n",
    "#               \"description\": \"3 for high confidence, 2 for medium, 1 for low confidence corrections.\"\n",
    "#             }\n",
    "#           },\n",
    "#           \"required\": [\n",
    "#             \"step_description\",\n",
    "#             \"text_example\",\n",
    "#             \"confidence_level\"\n",
    "#           ],\n",
    "#           \"additionalProperties\": False\n",
    "#         }\n",
    "#       }\n",
    "#     },\n",
    "#     \"required\": [\n",
    "#       \"cleaned_text\",\n",
    "#       \"cleaning_steps\"\n",
    "#     ],\n",
    "#     \"additionalProperties\": False\n",
    "#   }\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
