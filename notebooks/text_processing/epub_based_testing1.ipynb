{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from ebooklib import epub\n",
    "import spacy\n",
    "\n",
    "#from initial_cleaning_scripts import extract_text_and_metadata, clean_and_tag_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model for POS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_title_case(text):\n",
    "    \"\"\"\n",
    "    Check if a string is in title case using POS tagging.\n",
    "    Content words (nouns, verbs, adjectives, pronouns) should be capitalized,\n",
    "    and functional words (conjunctions, prepositions, determiners) are allowed to be lowercase.\n",
    "    The first word of the sentence should always be capitalized.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The string to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the string follows title case rules, False otherwise.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    Example use cases:\n",
    "        print(is_title_case(\"My Love Is Kind\"))  # True\n",
    "        print(is_title_case(\"my Love Is Kind\"))  # False (first word not capitalized)\n",
    "        print(is_title_case(\"The Quick Brown Fox Jumps Over the Lazy Dog\"))  # True\n",
    "        print(is_title_case(\"The quick Brown Fox Jumps Over the Lazy Dog\"))  # False\n",
    "        print(is_title_case(\"A Tale of Two Cities\"))  # True\n",
    "    \"\"\"\n",
    "    # Process the text with spaCy\n",
    "\n",
    "    if not text: \n",
    "        return False\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Define parts of speech that should be capitalized (content words + pronouns)\n",
    "    capitalizable_pos = {\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\", \"ADV\", \"PRON\"}  \n",
    "    \n",
    "    # Ensure the first word is always capitalized\n",
    "    if not doc[0].text.istitle():\n",
    "        return False\n",
    "    \n",
    "    # Iterate through each token in the document\n",
    "    for token in doc:\n",
    "        # If the token is a content word or pronoun, it should be capitalized\n",
    "        if token.pos_ in capitalizable_pos and not token.text.istitle():\n",
    "            return False\n",
    "        \n",
    "        # If the token is a functional word, it should be lowercase, but we allow for caps, as this may be a typo.\n",
    "        #elif token.pos_ not in capitalizable_pos and token.text.islower() is False:\n",
    "        #    print(f\"capitalized and needs: {token}\")\n",
    "        # return False\n",
    "    \n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_title_case_fast(text):\n",
    "    \"\"\"\n",
    "    Check if the given text follows proper title case rules:\n",
    "    - First and last word are capitalized.\n",
    "    - All other words, except members of a predefined list, are capitalized.\n",
    "    - If a word has two or more letters, the second letter must be lowercase.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The input string (title) to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the text follows title case rules, False otherwise.\n",
    "\n",
    "    # Example usage:\n",
    "    print(is_title_case_fast(\"The Fault in Our Stars\"))  # True\n",
    "    print(is_title_case_fast(\"Gone with the Wind\"))      # True\n",
    "    print(is_title_case_fast(\"IV.\"))                     # False (second letter is capitalized, not valid)\n",
    "    print(is_title_case_fast(\"To Kill a Mockingbird\"))   # True\n",
    "    \"\"\"\n",
    "    possibly_not_capitalized = [\n",
    "        \"a\", \"an\", \"the\", \"and\", \"but\", \"or\", \"nor\", \"for\", \"so\", \"yet\", \"as\", \"if\", \"because\", \"than\", \n",
    "        \"that\", \"though\", \"when\", \"where\", \"while\", \"although\", \"at\", \"by\", \"for\", \"from\", \"in\", \"of\", \n",
    "        \"on\", \"to\", \"up\", \"with\", \"off\", \"out\", \"over\", \"into\", \"near\", \"upon\", \"onto\", \"down\", \"about\", \n",
    "        \"across\", \"after\", \"along\", \"around\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \n",
    "        \"beyond\", \"during\", \"except\", \"inside\", \"outside\", \"through\", \"under\", \"until\", \"within\", \"without\", \n",
    "        \"once\", \"like\", \"now\", \"since\", \"till\"\n",
    "    ]\n",
    "    \n",
    "    # Split the title into words\n",
    "    words = text.split()\n",
    "    \n",
    "    if not words:\n",
    "        return False  # Empty string case\n",
    "\n",
    "    # Check if the first and last word are capitalized\n",
    "    if not (words[0][0].isupper() and words[-1][0].isupper()):\n",
    "        return False\n",
    "\n",
    "    # Check the capitalization of all words\n",
    "    for word in words:\n",
    "        # If the word is not in the list of possibly uncapitalized words\n",
    "        if word.lower() not in possibly_not_capitalized:\n",
    "            # Check if the first letter is uppercase\n",
    "            if not word[0].isupper():\n",
    "                return False\n",
    "            # Check if the second letter (if it exists) is lowercase\n",
    "            if len(word) > 1 and not word[1].islower():\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_rules_TH = {\n",
    "    \"part\": {\n",
    "        \"tags\": ['p'],  # Handle more flexible tags, e.g., spans\n",
    "        \"patterns\": [\"Part \\\\d+\", \"Book \\\\d+\"],\n",
    "        \"tests\": []\n",
    "    },\n",
    "    \"chapter\": {\n",
    "        \"tags\": ['p'],\n",
    "        \"patterns\": [\"Chapter \\\\d+\", \"Ch\\\\. \\\\d+\"],\n",
    "        \"tests\": []\n",
    "    },\n",
    "    \"section\": {\n",
    "        \"tags\": ['p'],\n",
    "        \"patterns\": [\"Section \\\\d+\", \"Sec\\\\. \\\\d+\", \"Table of Contents\", \"Introduction\", \"Forward\"],\n",
    "        \"tests\": [is_title_case_fast]\n",
    "    },\n",
    "    \"sub_section\": {\n",
    "        \"tags\": ['p', 'span'],\n",
    "        \"patterns\": [\"Summary\", \"Notes\", \"Sutra\", \"Exercise\"],\n",
    "        \"tests\": [lambda text: text.isupper()]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_rules_LiA = {\n",
    "    \"part\": {\n",
    "        \"tags\": ['p', 'blockquote'],  # Handle more flexible tags, e.g., spans\n",
    "        \"patterns\": [\"Part \\\\d+\", \"Book \\\\d+\"],\n",
    "        \"tests\": []\n",
    "    },\n",
    "    \"chapter\": {\n",
    "        \"tags\": ['p', 'blockquote'],\n",
    "        \"patterns\": [\n",
    "            r\"Chapter \\d+\", \n",
    "            r\"Ch\\. \\d+\"\n",
    "            r\"-\\s*Chapter\\s+\\d+\\s*-\",       # Matches \"-Chapter 1-\", \"- Chapter 1 -\" with spaces around \"-\"\n",
    "            r\"-\\s*Chapter\\s+[A-Za-z]+\\s*-\",  # Matches \"-Chapter One-\", \"- Chapter One -\" with spaces around \"-\"\n",
    "            r\"-\\s*Ch\\.\\s*\\d+\\s*-\",           # Matches \"-Ch. 1-\", \"- Ch. 1 -\" with spaces around \"-\"\n",
    "            r\"-\\s*Ch\\.\\s*[A-Za-z]+\\s*-\",     # Matches \"-Ch. One-\", \"- Ch. One -\" with spaces around \"-\"\n",
    "        ],\n",
    "        \"tests\": []\n",
    "    },\n",
    "    \"section\": {\n",
    "        \"tags\": ['p', 'blockquote'],\n",
    "        \"patterns\": [r\"Section \\d+\", r\"Sec\\. \\d+\", \"Table of Contents\", r\"-\\s*Introduction\\s*-\", r\"-\\s*Forward\\s*-\"],\n",
    "        \"tests\": [is_title_case_fast]\n",
    "    },\n",
    "    \"sub_section\": {\n",
    "        \"tags\": ['p', 'span', 'blockquote'],\n",
    "        \"patterns\": [\"Summary\", \"Notes\", \"Sutra\", \"Exercise\"],\n",
    "        \"tests\": [lambda text: text.isupper()]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiA_paragraph_rules = {\n",
    "#     \"paragraph\": {\n",
    "#         \"tags\": ['blockquote'],  # list of tags\n",
    "#         \"attributes\": [] # list of attributes\n",
    "#     \"quote\": [],\n",
    "#     \"emphasized\" ['bold']\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_test = lambda text: text.isupper()\n",
    "upper_test(\"THIS IS AN ALL CAPS SENTENCE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans a given text by replacing specific unwanted characters such as \n",
    "    newline, tab, and non-breaking spaces with regular spaces.\n",
    "\n",
    "    This function takes a string as input and applies replacements \n",
    "    based on a predefined mapping of characters to replace.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text with unwanted characters replaced by spaces.\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"This is\\\\n an example\\\\ttext with\\\\xa0extra spaces.\"\n",
    "        >>> clean_text(text)\n",
    "        'This is an example text with extra spaces.'\n",
    "\n",
    "    \"\"\"\n",
    "    # Define a mapping of characters to replace\n",
    "    replace_map = {\n",
    "        '\\n': ' ',       # Replace newlines with space\n",
    "        '\\t': ' ',       # Replace tabs with space\n",
    "        '\\xa0': ' ',     # Replace non-breaking space with regular space\n",
    "        # Add more replacements as needed\n",
    "    }\n",
    "\n",
    "    # Loop through the replace map and replace each character\n",
    "    for old_char, new_char in replace_map.items():\n",
    "        text = text.replace(old_char, new_char)\n",
    "    \n",
    "    return text.strip()  # Ensure any leading/trailing spaces are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is\\nan example\\ttext with\\xa0extra spaces.\"\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sent_with_punctuation(text):\n",
    "    \"\"\"\n",
    "    Check if a sentence ends with valid English sentence-ending punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The sentence to check.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the sentence ends with a valid punctuation mark, False otherwise.\n",
    "    \"\"\"\n",
    "    # Regular expression to match a valid sentence end\n",
    "    pattern = r'[.?!][\"\\']?$'\n",
    "    \n",
    "    # Check if the sentence matches the pattern\n",
    "    return bool(re.search(pattern, text.strip()))\n",
    "\n",
    "# Example use cases:\n",
    "print(sent_with_punctuation('This is a valid sentence.'))  # True\n",
    "print(sent_with_punctuation('Is this a question?'))  # True\n",
    "print(sent_with_punctuation('He exclaimed, \"Amazing!\"'))  # True\n",
    "print(sent_with_punctuation('It\\'s a test.'))  # True\n",
    "print(sent_with_punctuation('This sentence has no punctuation'))  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_valid_heading(text, tag):\n",
    "    \"\"\"\n",
    "    Determines if the text is likely a heading or chapter title based on its content\n",
    "    and HTML tag properties like class or style (e.g., bold, italic, underline).\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check.\n",
    "        tag (Tag): The BeautifulSoup tag that contains the text.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the text is likely a heading, False otherwise.\n",
    "    \"\"\"\n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Heuristic: Check if the text starts with large Roman numeral\n",
    "    if re.match(r'^[IVXLCDM]+\\.', text):  # Roman numerals for chapter markers\n",
    "        return True\n",
    "\n",
    "    # Ignore very short or empty strings\n",
    "    if len(text) < 3:\n",
    "         return False\n",
    "\n",
    "    if text.isupper():\n",
    "        return True\n",
    "    \n",
    "    # Ignore long strings:\n",
    "    if len(text.split()) > 10:\n",
    "        return False\n",
    "    \n",
    "    # Heuristic: Title formatting\n",
    "    if is_title_case(text):\n",
    "        return True\n",
    "    \n",
    "    # Heuristic: Check for common heading words like \"Chapter\", \"Exercise\", \"Sutra\"\n",
    "    common_heading_words = [\"Chapter\", \"Exercise\", \"Sutra\", \"Introduction\", \"Appendix\"]\n",
    "    if any(word in text for word in common_heading_words):\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_hierarchy(soup, segment_rules, paragraph_rules):\n",
    "    \"\"\"\n",
    "    Extracts hierarchical content (book title, parts, chapters, sections, sub-sections)\n",
    "    from a BeautifulSoup object according to the specified rules. Additionally,\n",
    "    collects all paragraphs and blockquotes.\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): The parsed HTML content from the EPUB.\n",
    "        segment_rules (dict): Dictionary defining tags, attributes, and patterns for each segment type.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing extracted content in a hierarchical format.\n",
    "    \"\"\"\n",
    "    hierarchy = {\n",
    "        \"parts\": [],\n",
    "        \"chapters\": [],\n",
    "        \"sections\": [],\n",
    "        \"sutras\": [],\n",
    "        \"sub_sections\": [],\n",
    "        \"poems\": [],\n",
    "        \"excersizes\": [],\n",
    "        \"paragraphs\": [], # Collect paragraphs and text content\n",
    "        \"quotes\": [],\n",
    "        \"emphasized\": [],\n",
    "        \"other\": [], # unrecognized headings\n",
    "        \"segment_index\": 0    \n",
    "    } \n",
    "\n",
    "    # Iterate over segment rules\n",
    "    seen_text = set()\n",
    "    for segment, rules in segment_rules.items():\n",
    "        # Check for matching tags and their attributes\n",
    "        for tag in soup.find_all(rules['tags']):\n",
    "            text = tag.get_text().strip()\n",
    "            \n",
    "            if not text or text in seen_text:\n",
    "                continue\n",
    "            \n",
    "            tests = rules['tests']\n",
    "\n",
    "            pattern_check = any(re.search(pattern, text) for pattern in rules.get('patterns', []))\n",
    "            if pattern_check:\n",
    "                test_check = None # don't need to check\n",
    "            else:\n",
    "                test_check = any(test(text) for test in tests)  # only check if necessary\n",
    "\n",
    "            # Check for matching patterns / tests and using valid headings\n",
    "            if (test_check or pattern_check) and is_valid_heading(text, tag):\n",
    "                if segment == \"part\":\n",
    "                    hierarchy['parts'].append(text)\n",
    "                elif segment == \"chapter\":\n",
    "                    hierarchy['chapters'].append(text)\n",
    "                elif segment == \"section\":\n",
    "                    hierarchy['sections'].append(text)\n",
    "                elif segment == \"sub_section\":\n",
    "                    hierarchy['sub_sections'].append(text)\n",
    "                elif segment == \"poem\":\n",
    "                    hierarchy['poem'].append(text)\n",
    "                elif segment == \"exercises\":\n",
    "                    hierarchy['exercise'].append(text)\n",
    "                else:\n",
    "                    hierarchy['other'].append(text)\n",
    "                seen_text.add(text)\n",
    "                \n",
    "    # Collect paragraphs and inline content like blockquotes, spans, divs, etc.\n",
    "    # Process paragraphs\n",
    "    for para in soup.find_all('p'):\n",
    "        text = clean_text(para.get_text())\n",
    "        if text and text not in seen_text:  # Avoid adding empty or duplicate text\n",
    "            hierarchy['paragraphs'].append(text)\n",
    "            seen_text.add(text)\n",
    "\n",
    "    # Process blockquotes\n",
    "    for quote in soup.find_all('blockquote'):\n",
    "        text = clean_text(quote.get_text())\n",
    "        if text and text not in seen_text:\n",
    "            hierarchy['quotes'].append(text)\n",
    "            seen_text.add(text)\n",
    "\n",
    "    # Process spans with class 'italic' (emphasized text)\n",
    "    for italic_span in soup.find_all('span', {'class': 'italic'}):\n",
    "        text = clean_text(italic_span.get_text())\n",
    "        if text and text not in seen_text:\n",
    "            hierarchy['emphasized'].append(text)\n",
    "            seen_text.add(text)\n",
    "\n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_metadata(file_path, segment_rules, paragraph_rules=None):\n",
    "    \"\"\"\n",
    "    Extracts text, chapter titles, and metadata (like book title and author) from an EPUB file\n",
    "    using a hierarchical content extraction process.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the EPUB file to be processed.\n",
    "        segment_rules (dict): Dictionary defining tags, attributes, and patterns for each segment type.\n",
    "        paragraph_rule (dict): Dictionary defining tags for extracting main body text, paragraphs, quotes, and emphasized text.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with metadata (title, author) and a hierarchical structure of the book content.\n",
    "    \"\"\"\n",
    "    # Read the EPUB file\n",
    "    book = epub.read_epub(file_path)\n",
    "    content_with_metadata = {\n",
    "        \"metadata\": {},\n",
    "        \"content_hierarchy\": []\n",
    "    }\n",
    "\n",
    "    # Try extracting title and author with the standard 'DC' namespace\n",
    "    try:\n",
    "        metadata = book.get_metadata('DC', 'title')\n",
    "        if metadata:\n",
    "            content_with_metadata['metadata']['title'] = metadata[0][0]\n",
    "    except KeyError:\n",
    "        print(\"Title not found in 'DC' namespace.\")\n",
    "\n",
    "    try:\n",
    "        author = book.get_metadata('DC', 'creator')\n",
    "        if author:\n",
    "            content_with_metadata['metadata']['author'] = author[0][0]\n",
    "    except KeyError:\n",
    "        print(\"Author not found in 'DC' namespace.\")\n",
    "    \n",
    "    # Additional metadata (language, publisher, date, etc.)\n",
    "    metadata_fields = {\n",
    "        'language': 'language',\n",
    "        'publisher': 'publisher',\n",
    "        'date': 'date'\n",
    "    }\n",
    "    \n",
    "    for key, field in metadata_fields.items():\n",
    "        try:\n",
    "            metadata = book.get_metadata('DC', field)\n",
    "            if metadata:\n",
    "                content_with_metadata['metadata'][key] = metadata[0][0]\n",
    "        except KeyError:\n",
    "            print(f\"{key.capitalize()} not found in 'DC' namespace.\")\n",
    "            content_with_metadata['metadata'][key] = None  # Handle missing metadata gracefully\n",
    "\n",
    "    # Get items of the desired media type (HTML content)\n",
    "    segment_number = 0\n",
    "\n",
    "    for item in book.get_items_of_media_type('application/xhtml+xml'):\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(item.get_body_content(), 'html.parser')\n",
    "\n",
    "        # Extract the hierarchical content using the defined segment rules\n",
    "        hierarchy = extract_content_hierarchy(soup, segment_rules, paragraph_rules)\n",
    "        \n",
    "        # Add the hierarchy to the content, only if it has meaningful structure\n",
    "        if hierarchy.get(\"chapters\") or hierarchy.get(\"sections\") or hierarchy.get(\"sub_sections\"):\n",
    "            hierarchy['segment_index'] = segment_number\n",
    "            content_with_metadata[\"content_hierarchy\"].append(hierarchy)\n",
    "            \n",
    "        segment_number += 1\n",
    "    \n",
    "    return content_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH_book_path = \"../books/private_books/Transformation_and_Healing-Thich_Nhat_Hanh.epub\"\n",
    "LiA_book_path = \"../books/private_books/Love_in_Action-Thich_Nhat_Hanh.epub\"\n",
    "TH_epub_book = epub.read_epub(TH_book_path)\n",
    "LiA_epub_book = epub.read_epub(LiA_book_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata = extract_text_and_metadata(TH_book_path, segment_rules_TH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_and_metadata(LiA_book_path, segment_rules_LiA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Satipatthana Sutta (Theravada) from Majjhima Nikaya, 10.\"\n",
    "is_title_case(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_parse_tag import reduced_tags_and_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = [BeautifulSoup(item.get_body_content(), 'html.parser') for item in TH_epub_book.get_items_of_media_type('application/xhtml+xml')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soups = [BeautifulSoup(item.get_body_content(), 'html.parser') for item in LiA_epub_book.get_items_of_media_type('application/xhtml+xml')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for soup in soups:\n",
    "    for tag in soup.find_all(True, class_=re.compile(r'calibre.*')):\n",
    "        del tag['class']  # Removes the class attribute if it matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = reduced_tags_and_text(soups[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sp.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_parse_tag import extract_tags_by_attributes, get_all_tag_names, get_all_attributes_for_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, soup in enumerate(soups):\n",
    "    tagset = set()\n",
    "    tags = get_all_tag_names(soup)\n",
    "    for tag in tags:\n",
    "        tagset.add(tag)\n",
    "\n",
    "print(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in soups[4].find_all('span'):\n",
    "    print(element.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for soup in soups:\n",
    "    for tag in tagset:\n",
    "        print(get_all_attributes_for_tag(soup, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagset = {\n",
    "    'span': {'class': 'italic'}  \n",
    "}\n",
    "for soup in soups:\n",
    "    dict = extract_tags_by_attributes(soup, tagset)\n",
    "    print(dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagset = {\n",
    "    'span': {'class': ''}  \n",
    "}\n",
    "for soup in soups:\n",
    "    dict = extract_tags_by_attributes(soup, tagset)\n",
    "    print(dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tag_content(soups, tag_name):\n",
    "    \"\"\"\n",
    "    This function takes a list of BeautifulSoup objects and a tag name, \n",
    "    finds the content inside the specified tag for each soup, and returns \n",
    "    a list of BeautifulSoup objects containing the content inside the specified tag.\n",
    "    \n",
    "    :param soups: List of BeautifulSoup objects representing parsed HTML documents\n",
    "    :param tag_name: The name of the tag to search for (e.g., 'blockquote', 'div', etc.)\n",
    "    :return: A list of BeautifulSoup objects containing the content inside the specified tag\n",
    "    \"\"\"\n",
    "    tag_soups = []\n",
    "\n",
    "    for soup in soups:\n",
    "        tags = soup.find_all(tag_name)  # Find all instances of the specified tag\n",
    "        for tag in tags:\n",
    "            # Append the content inside the tag as a new BeautifulSoup object\n",
    "            tag_soups.append(tag)\n",
    "\n",
    "    return tag_soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tag_content_with_attr(soups, tag_name, attrs=None):\n",
    "    \"\"\"\n",
    "    This function takes a list of BeautifulSoup objects, a tag name, and an optional dictionary \n",
    "    of attributes. It finds the content inside the specified tag with matching attributes and \n",
    "    returns a list of BeautifulSoup objects containing the content inside the matching tags.\n",
    "    \n",
    "    :param soups: List of BeautifulSoup objects representing parsed HTML documents\n",
    "    :param tag_name: The name of the tag to search for (e.g., 'blockquote', 'div', etc.)\n",
    "    :param attrs: Dictionary of tag attributes to match (e.g., {'class': 'italic'})\n",
    "    :return: A list of BeautifulSoup objects containing the content inside the matching tags\n",
    "    \"\"\"\n",
    "    tag_soups = []\n",
    "\n",
    "    for soup in soups:\n",
    "        tags = soup.find_all(tag_name, attrs=attrs)  # Find all instances of the tag with the given attributes\n",
    "        for tag in tags:\n",
    "            # Append the content inside the tag as a new BeautifulSoup object\n",
    "            tag_soups.append(tag)\n",
    "\n",
    "    return tag_soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_tag_content(soups, \"blockquote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_tag_content_with_attr(soups, 'span', {'class': 'italic'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for soup in soups:\n",
    "    for tag in soup.find_all(True):\n",
    "        del tag['class']\n",
    "    for a_tag in soup.find_all(\"a\"):\n",
    "        a_tag.unwrap()  # This removes the <a> tag but keeps the text inside\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_healing_book = epub.read_epub(\"../books/Transformation_and_Healing-Thich_Nhat_Hanh.epub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_in_action_book = epub.read_epub(\"../books/Love_in_Action-Thich_Nhat_Hanh.epub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1 = love_in_action_book\n",
    "book2 = trans_healing_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = [x for x in book1.get_items_of_media_type('application/xhtml+xml')]\n",
    "L2 = [x for x in book2.get_items_of_media_type('application/xhtml+xml')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(L1), len(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = L1[20].get_body_content()\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = [BeautifulSoup(element.get_body_content(), 'html.parser') for element in L1]\n",
    "S2 = [BeautifulSoup(element.get_body_content(), 'html.parser') for element in L2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2[8:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(soup):\n",
    "    \"\"\"\n",
    "    Extracts content from a BeautifulSoup object considering blockquotes, spans, and paragraphs.\n",
    "    \"\"\"\n",
    "    chapter_title = None\n",
    "\n",
    "    # Look for blockquote or span elements that might contain the chapter title\n",
    "    possible_titles = soup.find_all(['blockquote', 'span'])\n",
    "    for title in possible_titles:\n",
    "        text = title.get_text().strip()\n",
    "        if \"Chapter\" in text:  # Basic check for chapter titles\n",
    "            chapter_title = text\n",
    "            break  # Stop after finding the first relevant title\n",
    "    \n",
    "    # Collect paragraphs and blockquote content\n",
    "    paragraphs = []\n",
    "    for para in soup.find_all(['p', 'blockquote']):\n",
    "        text = para.get_text().strip()\n",
    "        if text:  # Only add non-empty text\n",
    "            paragraphs.append(text)\n",
    "\n",
    "    return chapter_title, paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_content(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnh-scholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
